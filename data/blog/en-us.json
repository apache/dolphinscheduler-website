[
  {
    "name": "2_The_most_comprehensive_introductory_tutorial_written_in_a_month",
    "content": "Community Star Series | 2 Apache DolphinScheduler MasterServer start-up tutorial\n\n\n\nAuthor | Ouyang Tao, Big Data Development Engineer\n\n\n\n##02 Master start-upÂ process\n###2.1 Starting up the MasterServer\nBefore we start, I want to give you some confidence. We know that starting the Master is starting the MasterServer, which is essentially similar to other SpringBoot projects, i.e. starting the main function inside. But before you start to use it, the beginners will find that there are more than a dozen autowired injected by beasns.\nNot a few people are confused by the injection of multiple beans. But I would like to point out that these paper tigers, do not be afraid, I will lead you to dissect and categorize these beans. Let's begin!\nThe first category: MasterConfig, MasterRegistryClient, MasterSchedulerService, and Scheduler. Literally, MasterConfig is related to the Master configuration, MasterRegistryClient is responsible for registration-related, and MasterSchedulerService is related to Master scheduling, which is, frankly speaking, something internal to the Master.\nThe second category is the beans with the suffix name of Processors, such as taskExecuteRunningProcessor. The beans with the same suffix handle the same task, which must be loaded together by something later on.\nThe third category: EventExecuteService and FailoverExecuteThread, these can be guessed based on their names to be something related to event execution and disaster recovery conversion, these also are something internal to the Master and in theory, should be classified as the first category.\nThe fourth category: the LoggerRequestProcessor is related to printing logs, and the specific details of this category will be explained later.\nAfter the main method is executed, the run method is executed based on the spring feature. In the run method, the nettyRemotingServer object is created (this object is not managed by spring, but newly created directly). Then a bunch of Processors of the second class is put inside the netty's Processor. We can infer by this point that the communication between the Master and the Worker must be connected by netty.\nLook at the code below, which executes the initial and start methods of the first class beans.\nIn conclusion, the Master is like a commander-in-chief who calls the start method of the beans, and these beans start to perform their functions, as to what kind of functions these beans perform, the MasterServer is too lazy to care, and there is no need to care.\nSummary of this section.\nMasterServer execution process is over here. In the next section, we will talk about the use and functionality of each bean one by one.\n\n\n\n2.2 Information about MasterConfig and registration of MasterRegistry Client\nMasterConfig gets the configuration information from application.yml and loads it into the MasterConfig class. The specific configuration information obtained is as follows.\n\n\n\nIn the MasterServer, MasterRegisterConfig executes the init() and start() methods.\nThe init() method creates a new heartbeat thread pool. Note that at this point, only a pool of threads is created without heartbeat tasks in it yet.\nThe start() method gets a lock from zk (getLock), registration information (registry), and a message to listen to the registration (subscribe).\nBy information registration, two things have been done.\nFirst: constructs the heartbeat message and drops it into the thread pool to run the heartbeat task.\nSecond: register the Master message temporarily with zk and remove the useless Master messages.\nThe heartbeat task checks for dead nodes and registers the latest machine information, including machine CPU, memory, PID, etc., to zk every 10s (heartbeatInterval).\nListens for subscriptions and immediately senses any changes to the registered information and prints a log if a machine has been added. If a machine is reduced, it is removed and the log is printed at the same time. This section is shown in the image below:\n\n2.3 Running of ServerNodeManger\nThe first two sections are about the start-up process of the MasterServer and the registration process of the MasterRegisterConfig. After all these procedures, how do Master and Worker be managed and synchronically saved to the database?\nThe ServerNodeManager is set for this task.\nServerNodeManager implements the InitializingBean interface, and executes the AfterPropertiesSet() method after building this object based on the characteristics of Spring, Three goals are achieved by now.\n\nload(). Loading node information from zk to MasterPriorityQueue via UpdateMasterNodes().\nA new thread synchronizes data from zk's node information to the database every ten seconds.\nListening to zk nodes and updating the latest data to the MasterPriorityQueue in real-time via the UpdateMasterNodes() method.\n\nAlmost all update operations are implemented via reentrant locks, which ensures that the system is safe under multiple threads. There is also a detail that should be noted that a warning message is sent if a node is removed.\nThe MasterProrityQueue has a HashMap inside, with each machine corresponding to an index, and slots are constructed in this way. The index is used to find the Master information later on.\nAs for the content of the MasterBlockingQueue, how to synchronize to the database, how to put data into the queue and remove data from the queue, etc., these are all pure crud content, you can read on your own.\n2.4 Starting the MasterSchedulerService\n2.1 to 2.3 are all about the node information managed by zk. Why do we talk about node information after the Master is started?\nThe reason is simple: both Masters and Workers are ultimately machines. If DS doesn't know about the machine's crash or addition, it will cause waste. Only when the machines are running properly, configured properly and all managed well, can the DS operation run smoothly. Other big data components are similar to the situation.\nAfter the MasterRegisterClient in the MasterServer executes the init() and start() methods, the MasterSchedulerService executes the init() and start() methods immediately afterwards. This is where the Master gets to work.\nThe init() method creates a Master-Pre-Exec-Thread thread pool and a netty client.\nThe Pre-Exec-Thread thread pool contains a fixed number of 10 threads (in 2.1 this corresponds to the pre-exec-threads in the MasterConfig configuration). These threads handle the process of building the ProcessInstance process from the command.\nThe start method is the thread that starts the StateWheelExecutorThread, which is dedicated to checking task, process, workflow timeouts, and task status and removing all that meet the criteria.\nThe MasterSchedulerService itself inherits from the thread class and executes the run method immediately after the start method. After ensuring that the machine has enough CPU and memory in the run method, the ScheduleProcess method is executed. What ScheduleProcess does will be explained in 2.5.\n2.5 Execution of the MasterSchedulerService\n\nThe ScheduleProcess method\n\n\n\nScheduleProcess is inside a while dead loop in the MasterSchedulerService, so it will loop through the following 4 methods to execute them.\n\n\nThe FindCommands method. This method retrieves 10 data from the t_ds_command table at a time, which are found by the slot, and will be configured in MasterConfig.FetchCommandNum once the search is complete.\n\n\nCommandProcessInstance converts these command tables into a ProcessInstance, where a CountdownLatch is used so that the subsequent methods are not executed until all conversions are complete.\n\n\nThe converted processInstance is built into workFlowExecuteThread objects, executed through threads in the workFlowExecuteThreadPool thread pool one by one, and the task instances and workflows executed in the processInstanceExecCacheManager are cached.\n\n\nAfter running the StartWorkFlow method in this thread pool, the StartProcess method of the WorkFlowExecuteThread is executed, and what the StartProcess does will be described in 2.6.\n\n\nThis background thread pool is managed by Spring, of which the maximum number and the core number of threads in the pool are 100 (MasterConfig.getExecThreads). The details are shown below:\n\n\n\nThere are two details to clarify here.\nFirst: WorkflowExecuteThread does not inherit from the Thread class but a normal class with a Thread after the class name, so don't look for start or run methods in this class when reading.\nSecond: If the ProcessInstance found in the SchedulerProcess method is a timeout, it will be handed over to the state polling thread(stateWheelExecuteThread) to be executed as described in 2.4, and the ProcessInstance will be removed.\n2.6 The StartProcess method is executed in the WorkflowExecutorThread\nThe StartProcess method is shown in the picture below.\n\n\n\nStartProcess does three things, buildFlowDag() builds the DAG, initTaskQueue() initializes the task queue and submitPostNode() submits the node.\nHow the DAG is built? What is done in the initialization queue? And what is done after the node is submitted? All the questions will be answered in sections 2.7 to 2.9.\n2.7 Executing the buildFlowDag method in the WorkflowExecutorThread\nBased on the code inside buildFlowDag, I have sorted out the execution process, which consists of the following 9 steps:\n\nFindProcessDefinition to get the process definition, which is the DAG of the process to be built.\nGetStartTaskInstanceList to get what task instances are under the process, in general, a process must have more than one task.\nFindRelationByCode gets the data in the task relationship table (ProcessTaskRelation).\nGetTaskDefineLogListByRelation determines the task definition log from the task relationship data obtained in step 3.\nTransformTask transforms the Relation and Log gaining in steps 3 and 4 into a TaskNode.\nGetRecoveryNodeCodeList gets the nodeCode of the task.\nParseStartNodeName gets the parameters of the command.\nThe DAG (ProcessDag) of the process is constructed based on the data obtained in 5, 6, and 7.\nThe constructed ProcessDag data is converted into DAG data.\n\nThe basic logic is shown above. Of course, there is some more logic at each step, but these are essentially data structures that change from one to another. You will get my point if only you have coding about service, so I won't go deeper into this topic here.\nFor those who may be interested in what a DAG is, here is a link to a brief introduction to DAGs, which should not be too difficult to read and understand.\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DAG.md\nThis is a theoretical introduction to DAG, if you want to have a deeper understanding of DAG in practice, search for the DagHelperTest class in the test folder of the dao module, there are 5 test operations in it, and you can run them all (in Debug form) to get a deeper understanding of DAG.\nThere are also two links about the modification of task relationships in the DAG. Before version 1.3, the relationships between tasks were only stored as fields, but after it was found to be unfeasible with the amount of data, the fields were split into multiple tables. You can refer to the articles below about it.\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Json_Split.md\nThe purpose of the DAG (directed acyclic graph) is to drag and drop tasks on the front end to tell the Master the order of execution of the tasks, i.e. to tell the Master which tasks are executed first and which ones are executed later.\n2.8 Executing InitTaskQueue method in WorkflowExexutorThread\nThree key aims are reached in The InitTaskQueue.\n\nInitialize four maps, ValidTaskMap, ErrorTaskMap, ActiveTaskProcessorMaps, and CompleteTaskMap, during which the tasks and processes are saved to different maps (these maps take taskCode as the key) by categories of valid (valid), complete (complete), error (failed), and active (running). These maps will be used in the later methods.\nIf the task is retrievable, it is placed in the readyToSubmitTaskQueue queue by addTaskToStandByList.\nIf the complementary state is enabled, then set the specific complementary time and global parameters to update it to the process instance.\n\n(I don't think the name of the InitTaskQueue method is very good, and I prefer InitTask or InitTaskMap instead. That's because Queue can easily be mistaken for a queue, while it only builds 4 maps. And the queue is only for tasks that can be retried, which will play a bigger role, I will explain it in the following sections).\n2.9 Executing the SubmitPostNode method in the WorkFlowExecutorThread\nSubmitPostNode plays six roles.\n1.DagHelper.ParsePostNodes(dag) parses the DAG generated in 2.8 into a TaskNodeList.\n2.Generate a TaskInstance collection from the TaskNodeList.\n3.If only one task is running, pass the TaskInstance parameter configuration to ProcessInstance.\n4.Place the TaskInstance in the ReadyToSubmitTaskQueue queue via the AddTaskToStandByList method.\n5.SubmitStandByTask submits these tasks.\n6.UpdateProcessInstanceState is destined to update the state of the process instance.\nThe last but not least, putting the TaskInstance into the queue and updating the process instance is the key. Updating process instances is purely a data structure change, which is not difficult. What happens to the task that has been placed in the queue and what happens next? What role does SubmitStandByTask play? All these questions will be explained in the next sections, stay tuned to my series work!\n",
    "title": "Donât know how to use Apache DolphinScheduler? A community talent writes the usage tutorial of it in one month!ï¼2ï¼",
    "time": "2022-5-24"
  },
  {
    "name": "Apache-DolphinScheduler-2.0.1",
    "content": "Apache DolphinScheduler 2.0.1 is here, and the highly anticipated one-click upgrade and plug-in finally come!\nGood news! Apache DolphinScheduler 2.0.1 version is officially released today!\nIn this version, DolphinScheduler has undergone a microkernel + plug-in architecture improvement, 70% of the code has\nbeen refactored, and the long-awaited plug-in function has also been emphatically optimized. In addition, there are many\nhighlights in this upgrade, such as a one-click upgrade to the latest version, &quot;de-ZK&quot; in the registration center, and\nnew task parameter transfer functions, etc..\nDownload Apache DolphinScheduler 2.0.1ï¼https://dolphinscheduler.apache.org/en-us/download/2.0.1\nThe workflow execution process activities of Apache DolphinScheduler 2.0.1 are shown in the following figure:\n\n\n\nStart process activity diagram\nVersion 2.0.1 enhanced the system's processing capabilities by optimizing the kernel, thereby greatly improving\nperformance. The new UI interface also greatly improved the user experience. More importantly, there are two major\nchanges in version 2.0.1: plug-in and refactoring.\nhttps://dolphinscheduler.apache.org/en-us/docs/2.0.1/guide/upgrade\n01 Plug-in\nPreviously, some users had feedback that they hoped that Apache DolphinScheduler could be optimized for plug-inization.\nIn respond, Apache DolphinScheduler 2.0.1 has optimized plug-in function, adding alarm plug-ins, registry plug-ins, and\ntask plug-in management functions. With plug-in, users can meet their own functional needs more flexibly, customize\ndevelopment task components based on interfaces more simply, and seamlessly migrate user task components to a higher\nversion of DolphinScheduler. DolphinScheduler is in the process of microkernel + plug-in architecture improvement. All\ncore capabilities such as tasks, alarm components, data sources, resource storage, registry, etc. will be designed as\nextension points. We hope to improve the flexibility and friendliness of Apache DolphinScheduler itself through SPI. The\nrelated code can refer to the dolphinscheduler-spi module, and the extended interfaces of related plug-ins are also\nunder this module. When users need to deploy the plug-in of related functions, it is recommended to read the code of\nthis module first. Of course, it is also recommended that you read the document to save time. We have adopted an\nexcellent front-end module form-create, which supports the generation of front-end UI components based on json. If\nplug-in development involves the front-end, we will use json to generate related front-end UI modules. The plug-in\nparameters are encapsulated in org.apache.dolphinscheduler.spi.params, which converts all relevant parameters into\ncorresponding json. This means that you can completely draw front-end modules (mainly refers to forms) by Java.\n1 Alarm plug-in\nTaking the alert plug-in as an example, Apache DolphinScheduler 2.0.1 enables the loading of related plug-ins when the\nalert-server starts. Alert provides a variety of plug-in configuration methods and currently has built-in alert plug-ins\nsuch as Email, DingTalk, EnterpriseWeChat, and Script. When the plug-in module development work is completed, it can be\nenabled through a simple configuration.\n2 Multi-registry modules\nIn Apache DolphinScheduler 1.X, the Zookeeper module plays a very important role , including monitoring and discovery of\nmaster/worker services, disconnection alarms, fault tolerance notification and so on. In version 2.0.1, we gradually &quot;\nde-ZK&quot; in the registry, weakening the role of Zookeeper, and adding plug-in management functions. In plug-in management,\nusers can increase the support of registry centers such as ETCD, making Apache Dolphinscheduler more flexible and\nadaptable to more complex user needs.\n3 Task module plugin\nThe new version also adds the task plug-in function, which enhances the isolation function of different task components.\nWhen a user develops a custom plug-in, he only needs to implement the plug-in interface. It mainly includes creating\ntasks (task initialization, task running, etc.) and task cancellation.\nIf it is a Yarn task, you need to implement AbstractYarnTask. At present, developers need to use Vue to develop and\ndeploy the front end of the task plug-in. In subsequent versions, we will implement the automatic drawing of front-end\nmodules by Java.\n02 Refactor\nSo far, Apache DolphinScheduler has refactored about 70% of the code and achieved a comprehensive upgrade.\n1 Master core optimization\nIn the upgrade, we refactor the execution process of the Master, changing the previous state polling monitoring to an\nevent notification mechanism, which greatly reduces the polling pressure of the database; removing the global lock,\nadding the fragmentation processing mechanism of the Master, and changing the sequence Read and write commands to\nparallel processing, which enhances the horizontal scalability of the Master; optimizes the workflow processing flow\nreduces the use of the thread pool and greatly increases the number of workflows processed by a single Master; adds the\ncache mechanism, optimizes the database connection method, and simplifies the processing process, reducing unnecessary\ntime-consuming operations, etc.\n2 Workflow and task decoupling\nIn Apache DolphinScheduler 1.x version, tasks and task relationships are saved in the workflow definition table in the\nform of large json. If a workflow is very large, (for example reaches 100 to 1000 tasks), the json will be too big to be\nparsed when in use. This process is more performance-consuming, and tasks cannot be reused; on the other hand, there is\nno good implementation solution in workflow version and task version for big json.\nTherefore, in the new version, we have decoupled the workflow and tasks, added a correlation chart between tasks and\nworkflow, and added a log table to save the historical version of workflow definitions and task definition, which\nImproves the efficiency of workflow operation.\nThe operation flow chart of the workflow and tasks under the API module are shown as below:\n\n\n\n03 Automatic Version Upgrade Function\nAutomatic version upgrade finally comes true in version 2.0.1. The users can automatically upgrade Apache\nDolphinScheduler from version 1. x to version 2.0.1 by one line usage script, and you can use the new version to run the\nprevious workflow without perception:\nsh ./script/create-dolphinscheduler.sh\n\nFor specific upgrade documentation, please refer to:\nhttps://dolphinscheduler.apache.org/en-us/docs/2.0.1/guide/upgrade\nIn addition, future versions of Apache DolphinScheduler can be automatically upgraded, saving the trouble of manual\nupgrades.\n04 List of New Features\nDetails of the new features of Apache DolphinScheduler 2.0.1 are as follows:\n1 New Standalone service\nStandAlone Server is a service created to allow users to quickly experience the product. The registry and database\nH2-DataBase and Zk-TestServer are built-in. After modification, you can start StandAloneServer with one key to\ndebugging.\nIf you want a quick experience, after decompressing the installation package, you only need to configure the JDK\nenvironment to start the Apache DolphinScheduler system with one click, thereby reducing configuration costs and\nimproving R&amp;D efficiency.\nFor detailed usage documentation, please refer to:\nhttps://dolphinscheduler.apache.org/en-us/docs/2.0.1/guide/installation/standalone\nOr use Docker to deploy all services with one\nclick: https://dolphinscheduler.apache.org/en-us/docs/2.0.1/guide/installation/docker\n2 Task parameter transfer function\nCurrently, the transfer between shell tasks and sql tasks is supported. Passing parameters between shell tasks:\nSet an out variable &quot;trans&quot; in the previous &quot;create_parameter&quot; task: echo'${setValue(trans=hello trans)}'\n\n\n\nOnce Keyword: \"${setValue(key=value)}\" is detected in the task log of the current task, the system will automatically parse the variable transfer value, in the post-task, you can directly use the \"trans\" variable:\n\n\n\nThe parameter passing of the SQL task:\nThe name of the custom variable prop of the SQL task needs to be consistent with the field name, and the variable will\nselect the value corresponding to the column with the same variable name in the column name in the SQL query result. The\noutput of user number:\n\n\n\nUse the variable &quot;cnt&quot; in downstream tasks:\n\n\n\n2.0.1 adds switch task and pigeon task components:\n\nswitch task\n\nSetting the judgment condition in the switch task can realize the effect of running different conditional branches\naccording to different conditional judgment results. For example, there are three tasks, the dependency is A -&gt; B\n-&gt; [C, D], where task_a is the shell task and task_b is the switch task.\nIn task A, a global variable named id is defined through a global variable, and the declaration method\nis echo'${setValue(id=1)}' .\nTask B adds conditions and uses the global variables declared upstream to achieve conditional judgment (global variables\nthat exist when the Switch is running are just fine, which means that they can be global variables that are not directly\ngenerated upstream). Next, we set id as 1, run task C, and others run task D.\nConfigure task C to run when the global variable id=1. Then edit ${id} == 1 in the condition of task B, and select C for\nbranch circulation. For other tasks, select D in the branch circulation.\n\n\n\n-pigeon task\nThe pigeon task is a task component that can be docked with third-party systems. It can trigger task execution, cancel\ntask execution, obtain task status, and obtain task logs. The pigeon task needs to configure the API address of the\nabove task operation and the corresponding interface parameters in the configuration file. Enter a target task name in\nthe task component to connect to the third-party system and can operate the task of the third-party system in Apache\nDolphinScheduler.\n3 Adds environmental management function\nThe default environment configuration is dolphinscheduler_env.sh.\nConfigure the worker running environment online. A worker can specify multiple environments, and each environment is\nequivalent to the dolphinscheduler_env.sh file.\n\n\n\nWhen creating a task, select the worker group and the corresponding environment variables. When the task is executed,\nthe worker will execute the task in the corresponding execution environment.\n05 Optimization item\n1 Optimize the RestApi\nWe have updated the new RestApi specification and re-optimized the API part by the specification, making it easier for\nusers to use the API.\n2 Optimize the workflow version management\nWe optimized the workflow version management function and increased the historical version of the workflow and tasks.\n3 Optimize worker group management function\nIn version 2.0, the worker group management function is completed. Users can modify the group information of the worker\nthrough the page configuration, saving the troubÃ¥le to modify the configuration file on the server and restart the\nworker.\nAfter the optimization, each worker node will belong to its worker group, and be grouped to default by default. When the\ntask is executed, the task can be assigned to the designated worker group, and finally run by the worker node in the\ngroup.\nThere are two ways to modify the worker group:\nOpen the &quot;conf/worker.properties&quot; configuration file on the worker node to be grouped, and modify the worker. groups\nparameter. The worker group to which the worker belongs can be modified during operation. If the modification is\nsuccessful, the worker will use this newly created group, ignoring the configuration in worker. properties. Modify step\nby step: Security Center -&gt; Worker Group Management -&gt; Click'New Worker Group' -&gt; Enter'Group Name' -&gt; Select Existing\nWorker -&gt; Click'Submit'.\nOther optimization issues:\nWhen starting the workflow, you can modify the startup parameters; Added workflow state automatically-launching when\nsaving the workflow; Optimized the results returned by the API, and speeded up the page loading speed when creating a\nworkflow; Speeded ââup the loading of workflow instance pages; Optimized the display information of the workflow\nrelationship page; Optimized the import and export function, supporting cross-system import and export workflow;\nOptimized some API operations, such as adding several interface methods, task deletion check, etc.\n06 Changelogs\nIn addition, Apache DolphinScheduler 2.0.1 also fixes some bugs, including:\nFixed the problem that netty client would create multiple pipes; Fixed the problem of importing workflow definition\nerrors; Fixed the problem that the task code would be obtained repeatedly; Fix the problem that the Hive data source\nconnection fails when Kerberos is used; Fix the problem that the Standalone service fails to start; Fix the problem that\nthe alarm group display failure; Fix the problem of abnormal file upload; Fix the problem that the Switch task fails to\nrun; Fix the problem of invalid workflow timeout strategy; Fix the problem that the SQL task cannot send mail.\n07 Acknowledgements\nThanks to the 289 community contributors who participated in the optimization and improvement of version 2.0.1 (in no\nparticular order)!\n\n\n\n\n\n\n",
    "title": "Apache DolphinScheduler 2.0.1 is here, and the highly anticipated one-click upgrade and plug-in finally come!",
    "time": "2021-12-20"
  },
  {
    "name": "Apache_DolphinScheduler_2.0.7",
    "content": "Apache DolphinScheduler 2.0.7 released, fixes complement and fault tolerance\n\nRecently, Apache DolphinScheduler version 2.0.7 was officially released. The new version fixes some complement and fault tolerance bugs and solves problems such as inconsistent batch data.\nMajor bug fixes\nâ1 The problem of continued detection of dependent while the dependent node contains tasks that are prohibited from running\n\nIn version 2.0.6, when a dependent node has a task that is forbidden to run, the dependent node will continue to detect the task instance for unable to find the task instance, since the forbidden task does not generate a task instance, resulting in the post-task of the dependent node not being submitted. This issue has been fixed in version 2.0.7.\nPR: https://github.com/apache/dolphinscheduler/pull/10952\n\nâ2 The problem of +1 day to the complement time by default\n\nIn the previous version, when October 1, 2022 was selected as the complement time, for the time parameter $[yyyy-MM-dd], the output date was 2022â10â02, which is not the expected date. To solve this problem, the new version subtracts one day from the complement date to correct the output date.\nCorresponding PR: https://github.com/apache/dolphinscheduler/pull/12376\n\nâ3 The problem that the task cannot be resubmitted when it is fault-tolerant\n\nIn the previous version, if there are 2 Worker nodes, each Worker is running 10 tasks and 10 internally -queued tasks. When Worker 1 hangs up, the state will turn to a fault-tolerant state because the status of all the running tasks and the tasks waiting to be run needs to be fault-tolerant. However, due to a bug, the fault-tolerant task will not be resubmitted, which will cause the running task background to show that the running is completed, while the task waiting to be run does not run, resulting in inconsistent batch data.\nCorresponding to PR https://github.com/apache/dolphinscheduler/pull/12423\n\nBug fix\n\nFix the problem that the disk monitoring in the monitoring interface is not displayed\nFixed the problem of transferring parameters for tasks that failed to restore\nFix the problem that the dependency node contains forbidden tasks and the dependency continues to detect\nFix HTTP alert including content field in Post\nFix the problem that the complement time is +1 day by default\nFixed an issue where the retry time of failed tasks in the workflow did not work in some cases\nFix the problem that the interface of workflow task relationship creation has no dependency detection\nFix the problem that the task cannot be resubmitted when it is fault-tolerant\nFix the problem that the prompt information is not clear when the tenantâs name is too long\n\nModify records\nhttps://github.com/apache/dolphinscheduler/releases/tag/2.0.7\nDownload\nhttps://dolphinscheduler.apache.org/en-us/download\nAcknowledgement\nThis version, like version 2.0.6, is also tempered on the scheduling practice of 8000+ daily cumulative scheduling jobs in the production environment of Zhengcai Cloud and fixes the problems feed backed by the community. Special thanks to the Zhengcai Cloud Data Platform Department for their support of this release, and to all those who contributed to the release of Apache DolphinScheduler 2.0.7. It is your unremitting efforts to make the community progress!\n\n\n\ndanielfree\nedward-yang\nhstdream\n\n\n\n\nlordk911\nretime123\nzwZjut\n\n\nJekong-hao\nJinyLeeChina\nliqingwang\n\n\n\nHow to contribute:\n\n\nGitHub Code Repository: https://github.com/apache/dolphinscheduler\n\n\nOfficial Website:https://dolphinscheduler.apache.org/\n\n\nMail List:dev@dolphinscheduler@apache.org\n\n\nTwitter:@DolphinSchedule\n\n\n",
    "title": "Apache DolphinScheduler 2.0.7 released, fixes complement and fault tolerance",
    "time": "2022-10-28"
  },
  {
    "name": "Apache_DolphinScheduler_in_XWBank",
    "content": "Three scenarios and five optimizations of Apache DolphinScheduler in XWBank for processing of task instances\n\n\n\nAt XWBank, a large number of task instances are generated every day, with real-time tasks making up the majority. To better handle the task instances, XWBank chose Apache DolphinScheduler to solve this challenge after a comprehensive consideration. Today, several XWBank projects have applied real-time and quasi-real-time batch processing and offline batch processing for metrics management systems in three types of scenarios, i.e. offline data development and task scheduling, quasi-real-time data development and task scheduling, and other non-ETL user-defined data batch processing.\nHow did XWBank adapt the Apache DolphinScheduler to better suit its business needs? At the Apache DolphinScheduler Meetup in April, Chen Wei, Senior Big Data Engineer from the Big Data Center of XWBank, presented their practical Application of Apache DolphinScheduler in the company.\nThe sharing was divided into four sessions.\n\n\n\nBackground of the introduction of Apache DolphinScheduler in XWBank\n\n\n\n\nApplication scenarios of Apache DolphinScheduler\n\n\n\n\nOptimization and transformation of XWBank\n\n\n\n\nThe follow-up plan for XWBank to use Apache DolphinScheduler\n\n\n\n\n\n\nChen Wei\nSenior Big Data Engineer, Big Data Center of XWBank\nHe has 11 years of working experience, earlier engaged in data warehouse construction, then focus on the construction of big data infrastructure platforms, scheduling system, etc. He has experience in the traditional financial industry, internet data warehouse, data mart construction, and many years of experience in scheduling system construction, such as MIGU analysis cloud scheduling system design, and report platform design, now mainly responsible for the construction of DataOps system of XWBank (offline development, indicator system, tagging system).\n01 Background\nWe chose Apache DolphinScheduler based on three main requirements: unification of R&amp;D scenarios, optimization of testing scenarios, and optimization of production deployment scenarios.\n01 R&amp;D scenarios\nIn the past, we did not have a unified development tool in the data development process, so we needed to switch back and forth between multiple tools, resulting in excessive development costs.\nOn the other hand, we were unable to replace parameters during development, could not perform on-the-fly debugging, and had no off-the-shelf tools to support offline tasks in both development and production states.\n02 Test scenarios\nDuring the deployment of test scenarios, when our developers provide scripts to the tests, the documentation returned is rather unfriendly. Especially when multiple scenarios are deployed across multiple versions, the testersâ tasks increase dramatically and the visual deployment is relatively weak, making it impossible to automate tests in a more friendly way.\n03 Deployment\nComplex configuration and poor visualization of the current scheduling system.\nThe development and production environment networks are physically isolated, so the process of deploying code from the development environment to the production environment is long and error-prone. The test environment does not fully reflect the configuration of the production environment, and manual configuration files are prone to errors and omissions.\nInsufficient operation and maintenance monitoring capabilities, poor visualization, inability to view logs online, and complex process of logging into the physical machine to access the monitoring room for troubleshooting.\n02 Usage scenarios\nWe use Apache DolphinScheduler in the following scenarios: offline data development and task scheduling, quasi real-time data development and task scheduling, and other non-ETL user-defined data batch processing.\n01 Offline Data Development and Task Scheduling\nIn offline data development and task scheduling.\nwe mainly use it for our banking data warehouse, data mart, etc. The data includes some offline data, offline processed data by day and month, etc.\n02 Quasi-real-time data development and task scheduling\nThe quasi-real-time data in XWBank is fused and calculated by Flink from the logs of the upstream message queue database, completing the relevant dimensional information and then pushing the data to Clickhouse for processing. However, there are special requirements for batch calculations on a minute-by-minute basis, as opposed to daily batch scheduling.\n03 Other non-ETL user-defined data batch processing\nThis application is functionally deployed through some internal low-code platform where we open up the application to a servicer who can self-analyze the use data without the need for developersâ help. After defining, they can run this part of the data in batches on their own.\n1. Offline data mining and task scheduling\nWe use Apache DolphinScheduler in the offline data mining and task scheduling scenario, which mainly involves five sections: task development modulation, historical task integration, workflow and task separation, project environment variables, and data source finding.\n1.Task development modulation (SQL, SHELL, PYTHON, XSQL, etc.), online development modulation (under view logs, online log . online SQL query return results view). WEBIDE can automatically replace pop-up variables, and dynamically replace variables according to the userâs settings and default processing.\n2.Historical tasks integration\nMost of the warehouses in the banking industry have been established for four or five years and have a lot of historical tasks. Therefore, we do not want our users to need to change the code independently when our new system goes online, which costs relatively high.\n3.Workflow and task separation\nAllows for direct tasks development, debugging, and testing and the workflow directly references the developed tasks, thus separate task development and task scheduling.\n4.Project environment variables\nNew project environment variables are added, and project environment variables are adapted to all jobs within the project by default, saving us from configuring them within each workflow, and each project can refer to them directly.\n5.Data sources\nWe look for data sources by name, and it supports data sources such as phoenix. Later we want it to be able to import and export tasks, but in the process of importing and exporting, the definition of parameters and data sources in our tasks cannot be changed, so that they can be directed from testing to production, which is simpler in terms of production.\n2. Quasi-real-time tasks\n\n\nTask development modulation (SQL), online development modulation (online view of logs, online view of SQL query return results), pop-up windows in WEBIDE to replace script variables.\n\n\nClickhouse data source HA configuration integration support. However, there is a small problem in batch processing offline, i.e. if the current port is not available, an error may be reported directly. This is a problem that needs a fix.\n\n\nFor quasi-real-time workflow single instance running, if there is already an initialized instance, or there is an ongoing workflow instance, the workflow will not be triggered to run even if the next batch is triggered.\n\n\n3. Other non-ETL user-defined data batch processing\n\n\nWe currently have model data calculation tasks pushed from the metrics management platform. The simple user-defined reports will generate SQL dynamically by the platform and subsequently pushed directly to offline scheduling. This process will not involve any developer in the future.\n\n\nIn the tag management system, we mainly adapt it to scenario needs by generating special plug-in tasks.\n\n\n03 Optimisation\n1.The status\nIn XWBank, there are about 9000+ task instances generated every day, with real-time tasks making up the majority. Today, we have used Apache DolphinScheduler to run batches in real-time and quasi-real-time tasks for many projects, offline batches for the metrics management system, including batches for the integrated internal SQL tools that support XSQL.\n\n\n\nIn the picture above on the right, we can see that we have made tasks independent, replacing parameters. Also, in terms of task lineage, especially for SQL-type tasks, we can do automatic parsing and also add them manually. This is mainly used for the automatic orchestration of our workflows, such as the internal task maps of the company.\nTo meet the above business requirements, we have made the following five major optimizations to Apache DolphinScheduler and also listed the corresponding modifications that must be noted during the transformation process.\n\nEnvironment variables are isolated from projects, and environments, but the names of environment variables are kept consistent across environments.\nData sources are isolated through the project, and the environment, but the names of the data sources remain consistent across the different environments.\nNew non-JDBC data sources are added, like ES, Livy, etc. In internal transparent applications, Livy is needed as a data service framework to interface with Spark jobs for data desensitization.\n\n2. Standalone jobs\n\nDevelop standalone task development, debugging, configuration pages, able to support project environment variables\nJDBC, XSQL tasks can refer to data sources by data source name\nImplement interactive WEBIDE debugging and development\nParameter optimization, support for user ${parameter} and referencing of system built-in time functions\nCompletion of independent SQL, XQSL automatic lineage parsing\nComplete automatic SQL parameter parsing\n\n3. Optimization of workflow startup logic\n\nQuasi-real-time workflow single instance run, if there is already a running workflow instance, this run will be ignored.\nAdding environment control policies, where workflows refer to different environment variables, and data source access connections, depending on the environment. For example, if the disaster recovery environment and the production environment are configured in advance, once an error occurs in the production environment, it can be switched to the disaster recovery environment with one click.\nSolve scheduling problems caused by workflow and task separation, mainly including the detection of exceptions\n\n4. Import and export optimization\n\nNew import and export of tasks, task configurations, and their resource files, etc.\nIn the banking and financial industries where the develope&amp;test and production networks are not always the same, there is a need to export a relatively friendly resource script workflow and resource file information when processing data in multiple environments.\nNew workflow import and export logic to deal with data conflicts due to self-incrementing IDs of different database instances\nNavigated import and export, versioning, mainly for emergencies, partial code rollback, etc.\n\n5. Alerting system improvement and optimization\n\nDocking to the internal alert system of XWBank, default alerting of task creators subscribing to the alerting group users\nAdd policy alerts (start-up delay, completion delay) to alert key tasks for start-up and completion delay\n\n\nInterfacing with internal systems\n\n\nModel-type task operation and monitoring\nReport push-tasks operation and monitoring\nInterfacing with internal IAM SSO unified login authentication system\nRestrict specific functions (code editing, workflow running, task running, etc.) by the network\n\nThere is a special phenomenon in the financial industry that the production needs to be done in a specific server room, i.e. we have to restrict certain operations to be done in the server room while reducing the cost of one change.\nWe create reports automatically based primarily on this dimensional model theory. Once configured, we perform a code merge calculation of multiple tables based on the configuration report logic. The aggregation calculation is completed and pushed to the report server. This allows business users to perform data aggregation without the need to write SQL following some of the basic functionality we provide, thus avoiding the business end-user being upset and giving us ad hoc requests.\n04 Future Plans\n\nPromote the offline data development platform to more project teams\nGradually replace the existing scheduling system in the bank to achieve smooth migration of all offline tasks\nScheduling system will be sunk to connect to the bankâs data R&amp;D management system\n\nTechnical objectives\n\nA more intelligent and automated task scheduling system, lowering the threshold for the user\nOperation monitoring and prediction, providing more friendly operation and maintenance monitoring and task completion time prediction functions for the operation and maintenance staff\nGlobal view functionality, providing a global view of offline tasks for development, operations, and maintenance staff, providing data lineage and impact analysis functionality\nFurther integration with in-line custom configuration modularity to reduce development costs for developers\nIntegration with data quality management platforms\nUser-defined template support\n\nThank you all, that's all I have to share today.\n",
    "title": "Three scenarios and five optimizations of Apache DolphinScheduler in XWBank for processing of task instances",
    "time": "2022-5-25"
  },
  {
    "name": "Apache_DolphinScheduler_s_Graduation_From_ASF_Incubator",
    "content": "First Anniversary Celebration of Apache DolphinScheduler's Graduation From ASF Incubator! Here Comes the Project Status Report!\n\n\n\nBefore you know it, Apache DolphinScheduler has graduated from the Apache Software Foundation (ASF) incubator for one year!\nOn April 9, 2021(Beijing Time), the ASF announced that Apache DolphinScheduler had graduated as a top project, bringing the first Chinese-led project in big data workflow scheduling to the world.\nNow, one year on, Apache DolphinScheduler has picked up the pace with the attention and help of ASF and is aiming to play a more important role in the DataOps space.\nAs we mark the first anniversary of Apache DolphinScheduler's graduation from the ASF incubator, we'd like to report on what the project has achieved in this limited time with the help of the ASF and the community.\nKeeping rapid iteration and good health\nAccording to ASF Project Statistics, the Apache DolphinScheduler community has a health score of 9.19, which means the community is in good running.\nCurrently, there are 45 Committers and 19 PMCs in the community, with a Committer-to-PMC ratio of 2:1.\n01 Project activities\nSoftware Development:\nSo far in 2021, we have released 11 versions and refactored 70% of the code, resulting in a 20x performance improvement. We have added Python SDK support, launched WorkflowAsCode functionality, and implemented highly requested community optimizations such as plug-in and one-click upgrades. The latest version is now 2.0.5.\nMeetups and Conference.\n\nThe Apache DolphinScheduler online meetup on 27 November 2021, with around 4,000 people watching.\nThe Apache DolphinScheduler online meetup on 26 February 2022, with approximately 5,000 views.\nJoint online meetup with Apache ShenYu (Incubating) on 26 March 2022, with approximately 6,000 views.\nAfter April 2022, there will be a regular Meetup (including joint Meetups overseas)per month ......\n\n02 Community Health Status\ndev@dolphinscheduler.apache.org Traffic increased by 64% compared to last quarter\n\n\n\n\n297 emails compared to 181 in the previous quarter\n972 commits in the last quarter (123% increase)\n88 code contributors last quarter (up 25%)\n824 new PRs opened on GitHub in the last quarter (up 89%)\n818 PRs closed on GitHub in the last quarter (up 100%)\n593 new issues opened on GitHub in the last quarter (up 90%)\n608 issues closed on GitHub in the last quarter (up 155%)\nContributors added to 300+\n\nMost active GitHub issues/PRs:\n\ndolphinscheduler/issues/8790 [Bug] [Process Definition] Duplicate key TaskDefinition (31 comments)\ndolphinscheduler/issues/9068 [Bug] [API server] could not get flow in exists project after upgrade from 2.0.1 to 2.0.5 (27 comments)\ndolphinscheduler/pull/8340 [Feature-8222] [python] move examples into the scope of source package (17 comments)\ndolphinscheduler/pull/8246 [Feature-8245][Alert] Add Alert Plugin Telegram (14 comments)\ndolphinscheduler/pull/9246 [Fix-9221] [alert-server] Optimize and gracefully close (14 comments)\ndolphinscheduler-website/pull/713 [Feature-8023] [Document] Add example and notice about task type Python (13 comments)\ndolphinscheduler/pull/8747 [Fix-8744][standalone-server] start standalone server failed (13 comments)\ndolphinscheduler-website/pull/667 [Feature-8020][Document] Add example and notice about task type SQL (12 comments)\ndolphinscheduler/issues/7992 [Feature][Alert] Support PagerDuty Plugin &amp;&amp; Alert module judging strategy (11 comments)\ndolphinscheduler/pull/9336 [Improvements-9338][API] show more create datasource exception message (11 comments)\n\nSince its inception, Apache DolphinScheduler has undergone several iterations of feature refinement and performance improvement and has been continuously optimized to fit developers' habits, providing users with a mature workflow scheduling solution that has been tested in production environments.\nAt present, Apache DolphinScheduler has also started to internationalize, for example trying to add Python, AWS, and time zone support to align with international development and usage.\nWitness the speed up of open-source  in China\n2021 is a year of growth for Apache DolphinScheduler, and a year of rapid growth for Chinese open-source projects, too.\n\nOver 20% of CNCF's open-source projects come from China, making the second-largest contribution in the world.\nIn 2021, for the first time,  Chinese(Sheng Wu) was elected to the Board of Directors of the Apache Software Foundation.\nIn 2021, five projects from China (Apache Linkis, Apache Kyuubi, Apache ShenYu, Apache Eventmesh, and Apache SeaTunnel, which are currently incubating) successfully entered the Apache Incubator. To date, there are 14 ASF projects from China.\nThere is also one incubation project that has graduated to the top tier of Apache, which is Apache DolphinScheduler.\n\nAs we can see, there is huge potential in open source, and we are calling on more people who are interested in it to join hands with Apache DolphinScheduler so that we can grow together and push Chinese open source to a higher stage in the world!\n",
    "title": "First Anniversary Celebration of Apache DolphinScheduler's Graduation From ASF Incubator! Here Comes the Project Status Report!",
    "time": "2022-4-14"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.2",
    "content": "Apache DolphinScheduler 2.0.2 Release Announcement:WorkflowAsCode is Launched!\n\n\n\nIn the long-awaited, WorkflowAsCode function is finally launched in version 2.0.2 as promised, bringing good news to users who need to dynamically create and update workflows in batches.\nIn addition, the new version also adds the WeCom alarm group chat message push, simplifies the metadata initialization process, and fixes issues that existed in the former version, such as failure of service restart after forced termination, and the failure to add a Hive data source.\nNew Function\nWorkflowAsCode\nFirst of all, in terms of new functions, version 2.0.2 released PythonGatewayServer, which is a Workflow-as-code server started in the same way as apiServer and other services.\nWhen PythonGatewayServer is enabled, all Python API requests are sent to PythonGatewayServer. Workflow-as-code lets users create workflows through the Python API, which is great news for users who need to create and update workflows dynamically and in batches. Workflows created with Workflow-as-code can be viewed in the web UI just like other workflows.\nThe following is a Workflow-as-code test case:\n# Define workflow properties, including name, scheduling period, start time, tenant, etc.\n\nwith ProcessDefinition(\n    name=&quot;tutorial&quot;,\n    schedule=&quot;0 0 0 * * ? *&quot;,\n    start_time=&quot;2021-01-01&quot;,\n    tenant=&quot;tenant_exists&quot;,\n) as pd:\n    # Define 4 tasks, which are all shell tasks, the required parameters of shell tasks are task name, command information, here are all the shell commands of echo\n\n    task_parent = Shell(name=&quot;task_parent&quot;, command=&quot;echo hello pydolphinscheduler&quot;)\n    task_child_one = Shell(name=&quot;task_child_one&quot;, command=&quot;echo &#x27;child one&#x27;&quot;)\n    task_child_two = Shell(name=&quot;task_child_two&quot;, command=&quot;echo &#x27;child two&#x27;&quot;)\n    task_union = Shell(name=&quot;task_union&quot;, command=&quot;echo union&quot;)\n\n    # Define dependencies between tasks\n    # Here, task_child_one and task_child_two are first declared as a task group through python&#x27;s list\n    task_group = [task_child_one, task_child_two]\n    # Use the set_downstream method to declare the task group task_group as the downstream of task_parent, and declare the upstream through set_upstream\n    task_parent.set_downstream(task_group)\n\n    # Use the bit operator &lt;&lt; to declare the task_union as the downstream of the task_group, and support declaration through the bit operator &gt;&gt;\n    task_union &lt;&lt; task_group\n\n\nWhen the above code runs, you can see workflow in the web UI as follows:\n                / \\\ntask_parent --&gt; --&gt; task_union\n                \\ /\n                  --&gt; task_child_two\n\n2 Wecom alarm mode supports group chat message push\nIn the previous version, the WeChat alarm only supported the message notification; in version 2.0.2, when the user uses the Wecom alarm, it supports pushing the group chat message in the app to the user.\n02 Optimization\n1 Simplified metadata initialization process\nWhen Apache DolphinScheduler is first installed, running create-dolphinscheduler.sh requires a step by step upgrade from the oldest version to the current version. In order to initialize the metadata process more conveniently and quickly, version 2.0.2 allows users to directly install the current version of the database script, which improves the installation speed.\n2 Remove &quot;+1&quot; (days) in complement dates\nRemoved the &quot;+1&quot; day in the complement date to avoid user confusion when the UI date always displays +1 when the complement is added.\n03 Bug Fixes\n[#7661] fix logger memory leak in worker\n[#7750] Compatible with historical version data source connection information\n[#7705] Memory constraints cause errors when upgrading from 1.3.5 to 2.0.2\n[#7786] Service restart fails after a forced termination\n[#7660] Process definition version create time is wrong\n[#7607] Failed to execute PROCEDURE node\n[#7639] Add default configuration of quartz and zookeeper in common configuration items\n[#7654] In the dependency node, an error is reported when there is an option that does not belong to the current project\n[#7658] Workflow replication error\n[#7609] Workflow is always running when worker sendResult succeeds but the master does not receive error report\n[#7554] H2 in Standalone Server will automatically restart after a few minutes, resulting in abnormal data loss\n[#7434] Error reported when executing MySQL table creation statement\n[#7537] Dependent node retry delay does not work\n[#7392] Failed to add a Hive data source\nDownload: https://dolphinscheduler.apache.org/en-us/download\nRelease Note: https://github.com/apache/dolphinscheduler/releases/tag/2.0.2\n04 Thanks\nAs always, we would like to thank all the contributors (in no particular order) who have worked to polish Apache DolphinScheduler 2.0.2 as a better platform. It is your wisdom and efforts to make it more in line with the needs of users.\n\n\n\nThe Way to Join US\nThere are many ways to participate and contribute to the DolphinScheduler community, including:\nDocuments, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.\nWe assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.\nSo the community has compiled the following list of issues suitable for novices: https://github.com/apache/dolphinscheduler/issues/5689\nList of non-newbie issues: https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\nHow to participate in the contribution: https://dolphinscheduler.apache.org/en-us/community\nCommunity Official Websiteï¼\nhttps://dolphinscheduler.apache.org/\nGitHub Code repository:\nhttps://github.com/apache/dolphinscheduler\nYour Star for the project is important, donât hesitate to lighten a Star for Apache DolphinScheduler â¤ï¸\n",
    "title": "Apache DolphinScheduler 2.0.2  Release Announcement:WorkflowAsCode is Launched!",
    "time": "2022-1-13"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.3",
    "content": "Apache DolphinScheduler 2.0.3 Release Announcement: DingTalk alert plugin adds signature verification, and supports data sources to obtain links from multiple sessions\n\n\n\n\nToday, Apache DolphinScheduler announced the official release of version 2.0.3. In this version, DingTalk alert plugin adds signature verification and enables data sources to get links from multiple sessions. In addition, 2.0.3 also optimizes cache management, complement time, data source password display in logs, etc., and fixes several Bug.\n\nFunction Enhancement\nDingTalk alert plugin adds signature verification\n2.0.3 Supports DingTalk robot alarm function through signature verification.\n\n\n\nDingTalk parameter configuration\n\nWebhooks\n\nThe format is as follows: https://oapi.dingtalk.com/robot/send?access_token=XXXXXX\n\nKeyword\n\nCustom keywords for security settings\n\nSecret\n\nSignature of security settings\nWhen a custom bot sends a message, you can specify the &quot;@person list&quot; by your mobile phone number. When a person in the &quot;@people list&quot; receives the message, there will be an @message reminder. Even set to Do Not Disturb mode, there will still be notification reminders for conversations, and the prompt &quot;Someone @ you&quot; will appear on the first screen.\n\n@Mobiles\n\nThe phone number of the person being @\n\n@UserIds\n\nUser userid of @person\n\n@All\n\n@everyone\nFor details, please refer to: https://open.dingtalk.com/document/robots/customize-robot-security-settings\nSupports data source to get connection from multi sessions\nPreviously, we use the JdbcDataSourceProvider.createOneSessionJdbcDataSource() method to create a connection pool in hive/impala set MaximumPoolSize=1, while in scheduling tasks, if hive/impala multitasking runs at the same time, getConnection=null will occur, and the SqlTask.prepareStatementAndBind() method will throw a null pointer exception.\n2.0.3 is optimized to support data sources getting links from multiple sessions.\nImprovements\nIntroduces cache manager, reduce DB query in Master scheduling\nSince numerous database read operations, such as tenant, user, processDefinition, etc., will occur during the scheduling process of the master server, it will bring enormous pressure on the DB on the one hand, and will slow down the entire core scheduling process on the other hand.\nConsidering that this part of business data involves more reading than writing, 2.0.3 introduces a cache module, which mainly acts on the Master node to cache business data such as tenants and workflow definitions, thus reduces database query pressure, and speeds up the core scheduling process. Please check the official website documentation for details: https://dolphinscheduler.apache.org/en-us/docs/3.1.2/user_doc/architecture/cache\nOptimize complement task's date, the complement time interval from '[left, right)' to '[left, right]'\nPreviously, the complement time was &quot;left closed and right open&quot; (startDate &lt;= N &lt; endDate), which is actually not conducive to user understanding. After optimization, the deployment time interval is changed to &quot;closed left and closed right&quot;.\nComplement case: https://dolphinscheduler.apache.org/en-us/docs/3.1.2/guide/project/workflow-definition\nEncrypted data source password in the logs\nThe password in the data source is encrypted to enhance privacy protection.\nBug Fixes\n\nzkRoot in conf/config/install_ config does not take effect\nProblems caused by modifying the administrator user information.\nAdd delete workflow instance when delete process definition\nudf sub resource manage edit modal cant close\nprocess is always running: netty writeAndFlush without retry when failed, leads to worker response to master failed\nAfter deleting the running workflow, the master keeps brushing the error log\nEdit the bug of worker grouping in environment management.\nDependent node ui dislocation\nError in querying historical version information of workflow\nSolve the problem that task log output affects performance under high concurrency\nThe global parameters of the sub_process node are not passed to the associated workflow task\nQuery log can not show contents when task log in master on k8s\nDuplicate processes in the process definition list\nProcess instance is always running: task is failure when process instance FailureStrategy.END\nField âis_directoryâ in t_ds_resources table has error type in PostgreSQL database\nRepair JDBC connection of Oracle\nWhen there is a forbidden node in dag, the execution flow is abnormal\nQuerySimpleList return wrong projectCode\n\nRelease Note: https://github.com/apache/dolphinscheduler/releases/tag/2.0.3\nDownload: https://dolphinscheduler.apache.org/en-us/download\nThanks to contributors\nThanks to the community contributors for their active contributions to this release! This is the list of Contributors, in no particular order:\n\n\n\n",
    "title": "Apache DolphinScheduler 2.0.3 Release Announcement: DingTalk alert plugin adds signature verification, and supports data sources to obtain links from multiple sessions\n",
    "time": "2022-1-27"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.5",
    "content": "\n\n\nToday, Apache DolphinScheduler announced the official release of version 2.0.5. This version has carried out some functional optimizations, such as optimizing the fault tolerance process of Worker, adding the function of re-uploading files in the resource center, and making several bug fixes.\nOptimization\nWorker fault tolerance process\nVersion 2.0.5 optimizes the worker's fault tolerance process so that when the server is interrupted due to excessive pressure, it can normally transfer tasks to other workers to continue execution to avoid task interruption.\nForbid to run task page sign optimization\nOptimized the display of flags on pages where tasks are prohibited from running, which distinguishes from the display of tasks that are normally executed, to prevent users from confusing work status.\n\n\n\nAdded prompts to the task box\nIn version 2.0.5, a prompt is added to the task box to display all the long task names, which is convenient for users.\n\n\n\nAdded the re-uploading files function in the resource center\nThe function of re-uploading files has been added to the resource center. When the user needs to modify the execution, automatical updating of the execution script can be realized without the requirement to reconfigure the task parameters.\nJump to the list page directly when modifying the workflow\nChanged the status that the page remained on the DAG page after modifying the workflow. After optimization, it can jump to the list page, which is convenient for users to follow-up operations.\nMarkdown information type added to DingTalk alert plugin\nAdds the Markdown information type to the alarm content of the DingTalk alarm plugin to enrich the information type support.\nBug Fix\n[#8213] The task run incorrectly when the worker group contains uppercase letters.\n[#8347] Fixed the problem of workflow cannot be stopped when the task fails and retries\n[#8135] JDBC connection parameter cannot input '@'\n[#8367] Fixed complement may not end normally\n[#8170] Fix the problem of failing to enter the sub-workflow from the page\n2.0.5 Download address:\nhttps://dolphinscheduler.apache.org/en-us/download/2.0.5\nRelease Note: https://github.com/apache/dolphinscheduler/releases/tag/2.0.5\nThanks to Contributors\nThanks to the contributors of Apache DolphinScheduler 2.0.5 version, the list of contributor GitHub IDs is as follows (in no particular order):\n\n\n\n",
    "title": "Release News! Apache DolphinScheduler 2_0_5 optimizes The Fault Tolerance Process of Worker",
    "time": "2022-3-7"
  },
  {
    "name": "Apache_dolphinScheduler_3.0.0",
    "content": "Apache DolphinScheduler 3.0.0 Official Version Released!\n\nThe official version 3.0.0 has undergone the most significant changes since its release, adding many new functions and features, aiming to bring users a brand-new experience and more value.\nThe iterative 3.0.0 official version is roughly the same as the primary function and feature updates, optimizations, and bug fixes described in the previous 3.0.0 alpha version update, including the four keywords summary âfaster, stronger, more modern, and easier to maintainâ of this version.\nFor the new functions and optimizations after the version iteration, this article will make supplements.\nKeyword: faster, stronger, more modern, and easier to maintain\n\nFaster and more modern: a reworked UI with a new interface, which is not only tens of times more responsive for users and hundreds of times faster for developers to build but also with a more modern page layout and icon style.\nMore powerful: bringing many exciting new features such as data quality check, custom time zones, support for AWS, and the addition of multiple task plugins and multiple alert plugins.\nEasier to maintain: back-end service separation is more in line with the trend toward containerization and microservices, and also makes maintenance easier by clarifying the responsibilities of each service.\n\nNew features and functionality\nNew UI, more robust and faster front-end code\nThe biggest changes in 3.0.0-alpha are the introduction of a new UI, which eliminates the need to reload pages when switching languages, and the addition of a dark theme. The new UI uses the Vue3, TSX, and Vite-related technology stack. Compared to the earlier UI, the new UI is not only more modern and user-friendly, but the front-end is also more robust, allowing users to check interface parameters if they find problems in the code, resulting in more robust front-end code.\nIn addition, the new architecture and technology stack will not only allow users to operate Apache DolphinScheduler tens of times more responsively, but developers will also be hundreds of times faster at compiling and launching the UI locally, which will significantly reduce the time it takes for developers to debug and package their code.\nExperience the new UI:\n\n\nLocal launch time comparison\nHomepage:\n\nWorkflow instances\n\nShell Tasks page\n\nMySQL Data Sources page\n\nSupport for AWS\nAs the Apache DolphinScheduler user grows, it has attracted many overseas users. However, during the research, users found that there were two bottlenecks that affected their experience with Apache DolphinScheduler, one was the time zone issue and the other was the lack of support for overseas cloud vendors, especially AWS. For this reason, we decided to support the significant components of AWS, and this is one of the most significant changes in this release.\nApache DolphinScheduler now supports AWS for both Amazon EMR and Amazon Redshift task types and has implemented Resource Center support for Amazon S3 storage.\n\nFor Amazon EMR, we have created a new task type and provided its Run Job Flow feature, which allows users to submit multiple steps jobs to Amazon EMR and specify the number of resources to be used. Details can be found at: https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/task/emr\n\nAmazon EMR Task Definition\n\nFor Amazon Redshift, we have now extended support for Amazon Redshift data sources in the SQL task type and users can now run Amazon Redshift tasks by selecting the Redshift data source in the SQL task.\nAmazon Redshift support\n\nFor Amazon S3, we have extended the Apache DolphinSchedulerâs resource center to support not only local resources, HDFS resource storage but also Amazon S3 as a resource centre for storage. Details can be found at: https://dolphinscheduler.apache.org/en-us/docs/3.0.0/guide/resource/intro in resource.storage.type We will be supporting more AWS tasks as users need them, so stay tuned.\nService Separation\nWhile the new UI is the biggest change to the front end of 3.0.0-alpha, the biggest change to the back end is the separation of services. Given the growing buzz around containers and microservices, the Apache DolphinScheduler developers made the decision to class https://dolphinscheduler.apache.org/en-us/docs/3.0.0/guide/resource/intro ify the backend services. By function, we split the service into the following parts.\n\nmaster-server: master service\nworker-server: worker service\napi-server: API service\nalert-server: alert service\nstandalone-server: standalone for a quick experience with Apache DolphinScheduler functionality\nUI: UI resources\nbin: quick-start scripts, mainly scripts to start individual services\ntools: tools-related scripts, mainly database creation and scripts update\nAll services can be started or stopped by executing the following commands.\n\nPlain Text\nbin/dolphinscheduler-daemon.sh &lt;start|stop&gt; &lt;server-name&gt;\nData quality verification\nIn this release, the long-awaited data quality verification comes online, solving data quality issues such as the accuracy of the number of data items synchronized from the source, and alarms for single or multiple tables with weekly or monthly fluctuations exceeding thresholds. Earlier versions of Apache DolphinScheduler solved the problem of running tasks in a specific order and time, while it lacks a commonly used data quality measurement after it had been run, which imposes an additional development cost on the user.\nData quality verification is now natively supported in 3.0.0-alpha, with support for a data quality check process before the workflow is run, enabling strict control of data quality and monitoring of results during the task run by user-defined data quality check rules in the data quality function module.\n\n\nTask groups\nTask groups are used to control the concurrency of task instances and to define the group's priority. When creating a new task definition, the user can configure the task group corresponding to the current task and configure the priority of the task to run within the task group. When a task is configured with a task group, the task can be executed only meeting the condition that all upstream tasks run successfully but also the task currently running in the task group is smaller than the size of the resource pool. When it is greater than or equal to the size of the resource pool, the task will wait until the next check. When multiple tasks in a task group are in the pending queue at the same time, the task with the highest priority will be run first.\n\nCustomizing time zones\nIn versions prior to 3.0.0-alpha, Apache DolphinScheduler defaulted to the UTC+8 time zone, but as the user base expanded, overseas users and those doing business across time zones overseas were often confused by the time zone. 3.0.0-alpha supports time zone switching, which solves the time zone problem for overseas users. For example, if an enterprise business involves a time zone of East 8 and West 5, if you want to use the same DolphinScheduler cluster, you can create multiple users and each user will use their own local time zone, and the time displayed in the corresponding DolphinScheduler object will switch to the local time of the corresponding time zone, which is more in line with local developersâ usage habits. The time displayed in the corresponding DolphinScheduler object will be switched to the local time zone, which is more in line with local developersâ usage habits.\n\nSee link: https://dolphinscheduler.apache.org/en-us/docs/3.0.0/guide/howto/general-setting\nList of task definitions\nWith previous versions of Apache DolphinScheduler 3.0.0-alpha, if a user wanted to manipulate a task, they needed to find the corresponding workflow and locate the task in the workflow before they could edit it. However, when the number of workflows became large or when a single workflow had a large number of tasks, the process of finding the corresponding task became very painful for users, which was not in line with the easy-to-use philosophy of Apache DolphinScheduler. Therefore, we have added a task definition page in 3.0.0-alpha to allow users to quickly locate and edit tasks by task name, allowing for easy bulk task changes.\nSee the link for more details: https://dolphinscheduler.apache.org/en-us/docs/2.0.1/guide/project/task-definition\nNew alert types\nThe 3.0.0-alpha alert type adds support for Telegram and Webexteams alert types.\nNew Python API Features\nIn 3.0.0, the most significant change in the Python API is to integrate the corresponding PythonGatewayServer into the API-Server service and rename it PythonGatewayService. Now the user will start the PythonGatewayService by default when starting the API-server; if you do not want to start the PythonGatewayService, you can set python-gateway.enabled in application.yaml to false.\nAdditionally, the Python API adds CLI and configuration modules. The Configuration module allows users to modify the default configuration of the Python API, such as modifying the default user name of the workflow, worker grouping, etc. The values can be changed through environment variables, direct file modification, and Python dynamic modification.\n\nAt present, the CLI only has two subcommands, version and config, which are used to confirm the current version and add or delete configuration files. In the future, we will introduce more functions to facilitate users to operate DolphinScheduler through the command line.\n\nIt is worth noting that the Python API also supports the function of adding and uploading files in the resource center to facilitate resource management; it also supports writing different names for different workflows of the same project; adding integration tests to make the testing more convenient.\nUnannounced functionality and feature updates from previous releases\nSupport for Flink task types\nIn this release, we have extended the Flink task type to support running Flink SQL tasks, which use sql-client.sh to submit tasks. In the previous version, we only supported submitting tasks through Flink cli. This method needs to combine the resource center, submit the resource file to the resource center, and then refer to the modified resource on the task definition page, which is not friendly for versioning and user transparency. As Flink SQL gradually becomes the mainstream of Flink users and writing SQL directly on the editing page is more user-transparent, we have adopted the Flink SQL function contributed by the community. Users in versions after 3.0.0 can use the Flink task more conveniently.\nFor more details, please refer to: flink sql client\nCorresponding PR: https://github.com/apache/dolphinscheduler/pull/9840\n\nAdd Zepplin task type\nIn this release, we have added the Zeppelin task type for creating and executing Zeppelin-type tasks. When the worker executes this task, it triggers the Zeppelin Notebook section through the Zeppelin Client API.\nCorresponding PR: https://github.com/apache/dolphinscheduler/pull/9810\n\nBash parameter passing function\nThe new version also adds the function of passing parameters through bash. If you want to use bash variables instead of constant value export parameters in downstream tasks, you can achieve this by rough setValue and Bash variables, which is more flexible and allows you to dynamically obtain Existing local or HTTP resources to get set variables.\nA similar syntax can be used\nlines_num=$(wget https://raw.githubusercontent.com/apache/dolphinscheduler/dev/README.md -q -O - | wc -l | xargs)echo &quot;#{setValue(set_val_var=${lines_num})}&quot; \nAllow users to upload files without a suffix\nPreviously, the resource center could only upload files with suffixes. After version 3.0.0, we support users to upload files without suffixes.\nOther functional enhancements\nIn addition to the above new functions, version 3.0.0 has also carried out many detailed function enhancements, such as refactoring task plug-ins and data source plug-in modules to make expansion easier; restoring the support for Spark SQL; making the E2E testing perfectly compatible with the new UI, etc.\nKey optimizations\n\n[#8584] Task back-end plugin optimization, new plugins only need to modify the pluginâs own module\n[#8874] Verify the end time and start time when submitting/creating a cron under workflow\n[#9016] Dependent The global project can be selected when adding dependencies\n[#9221] AlertSender optimization and gracefully close, such as MasterServer\n[#9228] Increase the slot condition to check the database and reduce the returned data records\n[#9230] Slim dist package by migrating python gatewar into Episerver\n[#9372] [python] Migrate pythonGatewayServer into API server\n[#9443] [python] Add missing doc about config and connect remote server\n[#8719] [Master/Worker] Change the task ack to runnning callback\n[#9293] [Master] add task event thread pool\n\nMajor bug fixes\n\n[#7236] Failed to create tenant using S3a Minio\n[#7416] Text file busy\n[#7896] When the project is authorized, it will generate a duplicate authorized project\n[#8089] start server failed because it canât connect to PostgreSQL\n[#8183] message:datasource plugin âsparkâ is not found.\n[#8202] MapReduce generated command built-in parameter location is wrong\n[#8751] Change param user, queue do no work in ProcessDefinition\n[#8756] Process using the dependence component cannot migrate between test and prod environment\n[#8760] Resource file deletion conditions\n[#8791] Rectify the issue affecting the original nodeâs data when editing the form of the copied node.\n[#8951] Worker resources are exhausted and cause downtime\n[#9243] Some types of alarms canât display project name\nProblems with each deployment method in 3.0.0\nWhen the task group is empty, the page reports an error\ntreemap view depth error problem\nThe alarm information is not clear\nParameter verification problem: The parameter verification problem in the data source center, the password is inconsistent when the password is changed, and the alert script is verified before the alarm is sent.\nPython API: The release state cannot be set, the local parameter has a value but the verification fails\nThe token query doesnât follow the timezone\nFix HTTPS and HTTP string recognition issues\nFix alert server health monitoring failure problem\nFix condition task branch failure problem\nFix the issue of docker image does not support multi-platform\nFix the problem that the database cannot be written correctly when the workflow with task group priority is created\nInvalidation of the master task\nFix the issue of serial wait not running\nTime zone error: scheduling time zone error problem, log add time zone support\nRe-run, pause workflow instance failure problem\nResource Center instantiation failure problem\nFix the problem of dividing lines in the email alert template\nFix data initialization problem in Standalone mode\nFixed the page display error when the monitoring center DB does not exist\nFix the issue of invalid creation workflow parameters\nFixed the abnormal problem of zookeeper port during K8S deployment\nFix the problem that the service fails to start in Standalone mode\nFix LDAP login failure problem\nPython API: fix the problem that the task component names of different workflows under the same project do not support the same name\nPython API: fix SQL task component SQL type error\nFix the abnormal problem of resource file renaming form\nFix the problem of getting the executable time of the workflow according to the timing settings\nUpgraded module dependencies such as Logback and Log4j\nFix mission failure issue\nFixe the issue of HDFS NPE\nFix the problem of master deadlock caused by task group exception\nFixed several stability issues\n\nDocument modification\n\nCorrect the deployment documentation\nRepair and update some usage documents: WebexTeams Chinese version documentation, local parameters, global parameter documentation, Kubernetes FAQ documentation, Spark precautions documentation, DataX usage documentation, delete Flink API documentation, fix the open-api errors, fix wrong documentation in data quality; Add stand-alone switch database document; Add document for judging Yarn running status in Shell; Add update system screenshot; Upgrade documents for parameter transfer, global parameters, parameter priority, alarm component wizard, Telegram, Dingding alarm document, alarm FAQ Documentation, Shell Component Documentation, Switch Task Component Documentation, Resource Center Configuration Details Documentation, Workflow Definition Complement Documentation\nCorrected some development documents: clarify the supported operating systems, fix development environment construction documents, and add self-build docker image documents\n\nRelease Note\nGitHub:https://github.com/apache/dolphinscheduler/releases/tag/3.0.0\nDownload: https://dolphinscheduler.apache.org/en-us/download\nThanks to contributors\nIn alphabetical order\nAaron LinãAmy0104ãAssertãBaoLiangãBenedict JinãBenjaminWenqiYuãBrennan FoxãDannilaãDesperado2ãDevosendãDingPengfeiãDuChaoJiaYouãEdwardYangãEric GaoãFrank ChenãGaoTianDuoãHanayoZzãHeChuanãHomminLeeãHua JiangãHwtingãIvan0626ãJeff ZhanãJiajie ZhongãJieguangZhouãJiezhi.GãJinYong LiãJÂ·YãKerwinãKevin.ShinãKingsleyYãKirsãKyoYangãLinKaiãLiuBodongãLongJGunãLuke YanãLyle ShawãManhuaãMartin HuangãMaxwellãMolin WangãMr.AnãOSãPJ FanningãPaul ZhangãQuakeWangãReonYuãSbloodySãSheldonãShiwen ChengãShuiMuNianHuaLPãShuoTiannãSongTao ZhuangãStalaryãSunny LeiãTomãTownãTqãWangJPLeoãWenjun RuanãX&amp;ZãXiaochenNanãYanbin LinãYao WANGãYiming GuoãZonglei DongãaCodingAddictãaaronlinvãaiwenmoãcaishunfengãcalvinãcalvinitãcheneyãchoucãchuxingãczemingãdevosendãexmyãgaojun2048ãguodongãguoshupeiãhjliãhstdreamãhuangxiaohaiãjaneHe13ãjeggerãjiachuan.zhuãjon-qjãjuzimaoãkezhenxu94ãlabbombãleiwingqueenãlgcareerãlhjzmnãlidongdaiãlifengãlilyzhouãlitiliuãliubo1990ãliudi1184ãlongtbãlvshaokangãlyqãmans2singhãmaskãmazhongãmgduoduoãmyangle1120ãnaziDãnobolityãououttãouyangyeweiãpinkhelloãqianli2022ãqinchaofengãrickchengxãrockfangãronyang1985ãseagleãshuai houãsimsiconãsneh-whaãsongjianetãsparklezzzãspringmonsterãsq-qãsyyangs799ãuh001ãwangbowenãwangqiangãwangxj3ãwangyangãwangyizhiãwindãworryãwqxsãxiangzihaoãxiaodi wangãxiaoguaiguaiãxuhhuiãyangyunxiãyc322ãyihongãyimaixinchenãyouzipiãzchongãzekai-liãzhangãzhangxinruuãzhanqianãzhuxt2015ãzixi0825ãzwZjutãå¤©ä»ãå°å¼ ãå¼æ ä¸¶ãå¼ ä¿æ°ãæ­æ­åå­¸ãæ¶åãæºé³ãçå¼ºãç¾å²ãç§å¤©ãç½é­æ¶ãé¿ç¦Chrisãéå®¶åãéç½ãé£ä¾ ç¾å¦ç»\n",
    "title": "Apache DolphinScheduler 3.0.0 Official Version Released!",
    "time": "2022-9-2"
  },
  {
    "name": "Apache_dolphinScheduler_3.0.0_alpha",
    "content": "3.0.0 Alpha Release! Nine New Features and A Brand New UI Unlock New Capabilities For the Scheduling System\n\n\n\nOn April 22, 2022, Apache DolphinScheduler officially announced the release of alpha version 3.0.0! This version upgrade ushers in the biggest changes since the release, with many new features and functions bringing new experiences and value to users.\nThe keywords for 3.0.0-alpha are, in summary, &quot;faster, more modern, more powerful, and easier to maintain&quot;.\n\nFaster and more modern: a reworked UI with a new interface, which is not only tens of times more responsive for users and hundreds of times faster for developers to build but also with a more modern page layout and icon style.\nMore powerful: bringing many exciting new features such as data quality check, custom time zones, support for AWS, and the addition of multiple task plugins and multiple alert plugins.\nEasier to maintain: back-end service seperation is more in line with the trend toward containerization and microservices, and also makes maintenance easier by clarifying the responsibilities of each service.\n\nNew features and functionality\n01 New UI, more robust and faster front-end code\nThe biggest changes in 3.0.0-alpha are the introduction of a new UI, which eliminates the need to reload pages when switching languages, and the addition of a dark theme. The new UI uses the Vue3, TSX, and Vite-related technology stack. Compared to the earlier UI, the new UI is not only more modern and user-friendly, but the front-end is also more robust, allowing users to check interface parameters if they find problems in the code, resulting in more robust front-end code.\nIn addition, the new architecture and technology stack will not only allow users to operate Apache DolphinScheduler tens of times more responsively, but developers will also be hundreds of times faster at compiling and launching the UI locally, which will significantly reduce the time it takes for developers to debug and package their code.\nExperience the new UI:\n\n\n\nLocal launch time comparison\n\n\n\nHomepage\n\n\n\nWorkflow instances\n\n\n\nShell Tasks page\n\n\n\nMySQL Data Sources page\n02 Support for AWS\nAs the Apache DolphinScheduler user grows, it has attracted many overseas users. However, during the research, users found that there were two bottlenecks that affected their experience with Apache DolphinScheduler, one was the time zone issue and the other was the lack of support for overseas cloud vendors, especially AWS. For this reason, we decided to support the significant components of AWS, and this is one of the most significant changes in this release.\nApache DolphinScheduler now supports AWS for both Amazon EMR and Amazon Redshift task types and has implemented Resource Center support for Amazon S3 storage.\n\nFor Amazon EMR, we have created a new task type and provided its Run Job Flow feature, which allows users to submit multiple steps jobs to Amazon EMR and specify the number of resources to be used. Details can be found at: https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/task/emr\n\n\n\n\nAmazon EMR Task Definition\n\nFor Amazon Redshift, we have now extended support for Amazon Redshift data sources in the SQL task type and users can now run Amazon Redshift tasks by selecting the Redshift data source in the SQL task.\n\n\n\n\nAmazon Redshift support\n\nFor Amazon S3, we have extended the Apache DolphinScheduler's resource center to support not only local resources, HDFS resource storage but also Amazon S3 as a resource centre for storage. Details can be found at: https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/resource/configuration in\n\n`resource.storage.type`\n\nWe will be supporting more AWS tasks as users need them, so stay tuned.\n03 Service separation\nWhile the new UI is the biggest change to the front end of 3.0.0-alpha, the biggest change to the back end is the separation of services. Given the growing buzz around containers and microservices, the Apache DolphinScheduler developers made the decision to classify the backend services. By function, we split the service into the following parts.\n\nmaster-server: master service\nworker-server: worker service\napi-server: API service\nalert-server: alert service\nstandalone-server: standalone for a quick experience with Apache DolphinScheduler functionality\nUI: UI resources\nbin: quick-start scripts, mainly scripts to start individual services\ntools: tools-related scripts, mainly database creation and scripts update\nAll services can be started or stopped by executing the following commands.\n\n`bin/dolphinscheduler-daemon.sh &lt;start|stop&gt; &lt;server-name&gt;`\n\n04 Data quality verification\nIn this release, the long-awaited data quality verification comes online, solving data quality issues such as the accuracy of the number of data items synchronized from the source, and alarms for single or multiple tables with weekly or monthly fluctuations exceeding thresholds. Earlier versions of Apache DolphinScheduler solved the problem of running tasks in a specific order and time, while it lacks a commonly used data quality measurement after it had been run, which imposes an additional development cost on the user.\nData quality verification is now natively supported in 3.0.0-alpha, with support for a data quality check process before the workflow is run, enabling strict control of data quality and monitoring of results during the task run by user-defined data quality check rules in the data quality function module.\n\n\n\n\n\n\n05 Task groups\nTask groups are used to control the concurrency of task instances and to define the priority of the group. When creating a new task definition, the user can configure the task group corresponding to the current task and configure the priority of the task to run within the task group. When a task is configured with a task group, the task can be executed only meeting the condition that all upstream tasks run successfully but also the task currently running in the task group is smaller than the size of the resource pool. When it is greater than or equal to the size of the resource pool, the task will wait until the next check. When multiple tasks in a task group are in the pending queue at the same time, the task with the highest priority will be run first.\nSee the link for details: https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/resource/configuration\n\n\n\n06 Customizing time zones\nIn versions prior to 3.0.0-alpha, Apache DolphinScheduler defaulted to the UTC+8 time zone, but as the user base expanded, overseas users and those doing business across time zones overseas were often confused by the time zone. 3.0.0-alpha supports time zone switching, which solves the time zone problem for overseas users. For example, if an enterprise business involves a time zone of East 8 and West 5, if you want to use the same DolphinScheduler cluster, you can create multiple users and each user will use their own local time zone, and the time displayed in the corresponding DolphinScheduler object will switch to the local time of the corresponding time zone, which is more in line with local developers' usage habits. The time displayed in the corresponding DolphinScheduler object will be switched to the local time zone, which is more in line with local developers' usage habits.\n\n\n\nSee link: https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/howto/general-setting\n07 List of task definitions\nWith previous versions of Apache DolphinScheduler 3.0.0-alpha, if a user wanted to manipulate a task, they needed to find the corresponding workflow and locate the task in the workflow before they could edit it. However, when the number of workflows became large or when a single workflow had a large number of tasks, the process of finding the corresponding task became very painful for users, which was not in line with the easy-to-use philosophy of Apache DolphinScheduler. Therefore, we have added a task definition page in 3.0.0-alpha to allow users to quickly locate and edit tasks by task name, allowing for easy bulk task changes.\nSee the link for more details: https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/project/task-definition\n08 New alert types\nThe 3.0.0-alpha alert type adds support for Telegram and Webexteams alert types.\n09 New Python API Features\nThe biggest change to the Python API in 3.0.0-alpha is the integration of the PythonGatewayServer counterpart into the API-Server service, which makes external services support more organized and alleviates the problem of large binary packages caused by services classification. The Python API also adds a CLI and configuration module, allowing users to customize configuration files and modify configurations more easily.\n10 Other new features\nIn addition to the features mentioned above, the 3.0.0-alpha release also buffs many detailed enhancements, such as refactoring the task plugin and data source plugin modules to make scaling easier, restoring support for Spark SQL, E2E testing is now perfectly compatible with the new UI, and more.\nKey optimizations\n[#8584] Task back-end plugin optimization, new plugins only need to modify the plugin's own module\n[#8874] Verify the endtime and starttime when submit/create a cron under workflow\n[#9016] Dependent The global project can be selected when adding dependencies\n[#9221] AlertSender optimization and gracefully close, such as MasterServer\n[#9228] implement use the slot to scan the database\n[#9230] Slim dist package by migrate python gatewar into apiserver\n[#9372] [python] Migrate pythonGatewayServer into api server\n[#9443] [python] Add missing doc about config and connect remote server\n[#8719] [Master/Worker] Change the task ack to runnning callback\n[#9293] [Master] add task event thread pool\nMajor bug fixes\n[#7236] Failed to create tenant using S3a Minio\n[#7416] Text file busy\n[#7896] When the project is authorized, it will generate a duplicate authorized project\n[#8089] start server failed because can't connect to postgresql\n[#8183] message:datasource plugin 'spark' is not found.\n[#8202] MapReduce generated command built-in parameter location is wrong\n[#8751] Change param user, queue do no work in ProcessDefinition\n[#8756] Process using the dependence component cannot migrate between test and prod environment\n[#8760] Resource file deletion conditions\n[#8791] Rectify the issue with affecting the original node's data when editing the form of the copied node.\n[#8951] Worker resources are exhausted and cause downtime\n[#9243] Some types of alarms can't display project name\nRelease Note\nhttps://github.com/apache/dolphinscheduler/releases/tag/3.0.0-alpha\nThanks to contributors\nIn alphabetical order\nAaron Lin, Amy0104, Assert, BaoLiang, Benedict Jin, BenjaminWenqiYu, Brennan Fox, Devosend, DingPengfei, DuChaoJiaYou, EdwardYang, Eric Gao, Frank Chen, GaoTianDuo, HanayoZz, Hua Jiang, Ivan0626, Jeff Zhan, Jiajie Zhong, JieguangZhou, Jiezhi.G, JinYong Li, J-Y, Kerwin, Kevin.Shin, KingsleyY, Kirs, KyoYang, LinKai, LiuBodong, Manhua, Martin Huang, Maxwell, Molin Wang, OS, QuakeWang, ReonYu, SbloodyS, Shiwen Cheng, ShuiMuNianHuaLP, ShuoTiann, Sunny Lei, Tom, Tq, Wenjun Ruan, X&amp;Z, XiaochenNan, Yanbin Lin, Yao WANG, Zonglei Dong, aCodingAddict, aaronlinv, caishunfeng, calvin, calvinit, cheney, chouc, gaojun2048, guoshupei, hjli, huangxiaohai, janeHe13, jegger, jon-qj, kezhenxu94, labbomb, lgcareer, lhjzmn, lidongdai, lifeng, lilyzhou, lvshaokang, lyq, mans2singh, mask, mazhong, mgduoduo, myangle1120, nobolity, ououtt, ouyangyewei, pinkhello, qianli2022, ronyang1985, seagle, shuai hou, simsicon, songjianet, sparklezzz, springmonster, uh001, wangbowen, wangqiang, wangxj3, wangyang, wangyizhi, wind , worry, xiangzihao, xiaodi wang, xiaoguaiguai, xuhhui, yangyunxi, yc322, yihong, yimaixinchen, zchong, zekai-li, zhang, zhangxinruu, zhanqian, zhuangchong, zhuxt2015, zixi0825, zwZjut,Â Tianchou, xiaozhang, shiguang, wangqiang, baisui, hongshu, zhangjunjie, luomingtao\n",
    "title": "3.0.0 Alpha Release! Nine New Features and A Brand New UI Unlock New Capabilities For the Scheduling System",
    "time": "2022-4-16"
  },
  {
    "name": "Apache_dolphinScheduler_3.0.1",
    "content": "Apache DolphinScheduler 3.0.1 released, with the scheduling kernel and UI optimized\n\nThanks to this Release Manager â zhuangchong, who led the release of the 3.0.1 version of Apache DolphinScheduler, guided the community to communicate about the version optimization, tracked issues before the release, dealt with blocking issues, and version quality management, etc. Thanks to his contribution to the community, we also expect more Committers and PMCs to play the role of Release Manager.\nMore than a month after the official version 3.0.0 was released, the community engaged intensively in preparing for the iteration of the new version. We took the advice and feedback from the user interviews(including multiple company users) as the core development point of version 3.0.1, companies, together with hundreds of issue PRs, and nearly 30 contributorsâ participation, finally launched the long-awaited 3.0.1 version. The new version made some adjustments to the UI and scheduling kernel, and at the same time, some bugs in version 3.0.0 have also been fixed.\nRelease note:\nhttps://github.com/apache/dolphinscheduler/releases/tag/3.0.1\n1 Scheduling kernel fixes\n\nWorkflow execution runnable trapped an infinite loop\nThe alert of database fields is problematic when upgrading\nCommon users cannot create dependent tasks\n\nTask, workflow instance priority causes NPE(Null Pointer Exception)\n\nDependent downstream error trigger\nFixed workflow instance possibly cause failing over multiple times\nThe task group gets the task status as running error\nFix the issue of triggering OOM(Out of Memory) when the worker kills the task\n\n2 UI-related optimization\n\nWorkflow list names overlapping\nThe workflow instance does not support running\nData source modification port error report\nWork instance operations are not on the same line\nWorkflow name display abnormal\n\n3 Resource Center\nThe problem of Resource center defaultFs configuration invalidation\n4 Document related\nThe new version of Apache DolphinScheduler complements some reminder languages, such as the full path when uploading documents, the use of the standalone default resource center;\nThe errors in the documents are corrected, such as workflow-related descriptions, creating worker groups, Python API jump links, some spelling mistakes, and community email slack links;\n5 Python API\nThe support of Python 10 and Python 11-dev is implemented in this version, and the switch task type and the SQL task truncate syntax recognition errors are fixed. In addition, the problem of only the latest version of Python API documentation being available is solved.\n6 Bug fix\n\n[Improvement][UI] Unified local parameters UI #11190\n[Fix][UI] Fix the bug where icons are displayed in one line. #11320\n[Improvement-11386][UI] Concise the logic available for task action buttons #11419\n[Fix][UI] Fix the preTask options are unavailable when creating a tasâ¦ #11501\nS3 Resource center supports bucket customization (#12022)\nfix status check issue (#11580) (#12030)\n[LDAP] Config value should return real null instead of ânullâ string (#12031)\n[3.0.1-preapre][cherry-pick]3.0.1 UI (#12020)\n[Fix][UI] Fix bug where crontab special of the month and year #11661\n[UI] Enable highlight and auto-complete for SQL scripts #11659\n[Fix][UI] Fix the problem that the pagination component is not centered in the environment management. #11646\n[Fix][UI] Fix the port in the data source edit. #11624\n[Fix][UI] Fix the table header in the workflow instance. #11487\n[Fix][UI] Fix bug where warn group name not display #11429\n[Feature-10117][UI] Hide node execution when starting from the workflow definition list page #11372\n[Fix-11366][UI] Workflow instance should not support right-click running #11367\n[fix-10961][alert server]Change the content of the alert to an array (#11033)\n[Fix][UI] Fix workflow name overlaps bug (#11862) (#12019)\n[Bug] [Worker] Optimize the getAppId method to avoid worker OOM when killing task (#11994)\nfix heartBeatTaskCount bug (#12024)\nFix kill task failed will cause the taskGroup cannot release and add taskGroup log (#11469) (#12013)\nWorkflow Definition Name Change Validation Optimization. (#11986)\nadd unit test ci timeout (#11655) (#12021)\n[helm] fix worker template persistentVolumeClaim link (#11248) (#12018)\nSet masterâs task running status in runTask to avoid the task group acquire failed, but the task status is in running (#11451) (#12011)\n[python] Refactor structure to avoid cycle import (#11167)\n[python] Add support interpreter version 310 and 311-dev (#11170)\n[bug][python] Fix task switch error when default branch not defined last (#11606)\n[fix][python] Sql pattern add truncate. (#11666)\nAdd dependent task instance log (#11541) (#12014)\nIf the task processor is not found the need to throw an error rather than an exception (#11461) (#12012)\nFix workflow instance may failover many times due to doesnât check the restart time (#11445) (#12010)\nFix find last process instance in dependent task is not correct (#11424) (#12009)\nFix quartz threadPriority config name error (#11596) (#12015)\n[Fix-11413] Cannot set the task status to kill if the task is not running(#11414) (#12007)\nClear thread name in BaseTaskProcessor (#11422) (#12008)\nBug Dependent downstream trigger error when schedule cycle not day. (#11987)\n[CI] Fix cluster test abnormal (#11688) (#11985)\n[Improvement-11380][scp-host.sh] Set StrictHostKeyChecking=no option to ssh (#11382) (#11995)\noptimize the process instance query, change the date time range (#11719) (#11991)\n[Fix-11051][Task]Fix the process exitValue exception when the process times out and thâ¦ (#11099) (#11983)\n[fix-11404]: make the common.properties to configurable on values.yaml (#11441) (#11967)\n][fix-11452][Bug] [Logger Server] Incorrect password regular expression (#11993)\nRemove logger header in the task log file (#11555) (#11968)\n[Bug] [API] The task priority and process instance priority might be null and cause NPE (#11969)\n[Bug] [spark-sql] In spark-sql, select both SPARK1 and SPARK2 versions and execute /bin/spark-sql (#11971)\nUpdate dolphinscheduler_ddl.sql (#11974)\n[fix-#11815] fix ck column names contain special characters (#11973)\n[Bug][Workflow Definition] fix ordinary users can not create the depend task (#11961) (#11976)\n[Fix-11877][UI] Fix the problem that the environment cannot be deleted (#11934)\n[fix-10938]: use dot to replace the source and make the default env work in the shell (#11937)\nfix hdfs defaultFs not working (#11823) (#11936)\n[Quartz] cron did not work as expected (#11932)\n[Bug] [Master] WorkflowExecuteRunnable will face a infinite loop #11838 #11864 (#11949)\n[Bug-#11650][worker] #11650 fix SQL type task, stop task does not take effect (#11938)\n[Fix][db] fix init&amp;upgrade mysql-meta-schema bugs #11887 (#11933)\nProcess definition length too long when copying or importing #10621 (#11893)\n\n7 DOCs\n\n[Doc] Upgrade license. (#9967)\n[Doc] Update README. (#9970)\n[Doc] Fix Doc deadlink in readme (#9972)\n[Doc] Update the Document picture to the new UI (#9953)\n[Doc] Add example and notice about task type Conditions (#9945)\n[Doc] Fix Docker link. (#9998)\n[Doc] Add the description about execute type in SQL task (#9987)\n[Doc] Add example and notice about task type Dependent (#10001)\n[Doc] Correct Kubernetes (#9985)\n[Doc] Correct Doc of development-environment-setup (#9995)\n[Bug][Document] Fix task group management Document link error (#10062)\n[Fix-10083][Doc]Change Docker cmd &amp;&amp; Change WebexTeams Chinese Translation (#10084)\n[improvement-#11630]fix document about common.properties (#11653)\nupdate PyDolphinScheduler documentation link. (#11474)\n[doc] Correct E2E Doc, fix WorkerGroupPage typo (#11629)\nFix the homepage email subscription link (#11622)\n[DOC] should notice that need to set the full path when calling the resource file #11620 (#11621)\n[Doc][Bug] Fix resource center docs for standalone mode (#11600)\nfeat: update slack (#11578)\n[Improvement-11550] [Doc]Document content update (#11577)\n[Doc][Security] Update instructions on worker groups (#11483)\n[Doc][DSIP] Move DSIP docs to the right place (#11442) (#11443)\n[Doc][Resources] Instruct users to use local storage if they have remote storage mounted to local (#11435)\nfeat: Modifying Slack Links (#11416)\n[python] Add multiple versions of a document (#11391)\n[doc] Refine the deployment documentation (#11361)\n[Fix-11217] [Doc] add PostgreSQL config in doc: datasource-setting (#11326)\n[doc] Improve the document introduction of complement (#11275)\n[DOC] improve zk digest doc (#11703) (#11992)\n[Doc] Remove re-upload file function in the 3.0.0âs doc (#11804) (#11984)\n[doc] Add how to obtain IP address from network card in FAQ (#11311) (#11982)\nfix doc about sub-processâs child node describe (#11972)\n[fix][doc] Update the registry related configuration in values.yaml (#11444) (#11980)\n\n8 Acknowledgement\nThanks to all the contributors of version 3.0.1:\n106umaoãAmy0104ãChrisYuanãDarkAssassinatorãEricGao888ãHeZeanãJinyLeeChinaãMonsterChenzhuoãSbloodySãWangJPLeoãabzymeinsjtuãdevosendãfengjian1129ãfuchanghaiãguodongymãhiSandogãhuage1994ãinsist777ãjackfanwanãjieguangzhouãlabbombãlimaiwangãliqingwangãlishiyucnãluoyedeyiãrickchengxãruanwenjunãshangeyaoãsketchmindãsongjianetãstalaryãwendongdiãyutianaiqingtianãzhangshuocnãzhongjiajieãzhuangchongãzhuxt2015\n",
    "title": "Apache DolphinScheduler 3.0.1 released, with the scheduling kernel and UI optimized",
    "time": "2022-9-28"
  },
  {
    "name": "Apache_dolphinScheduler_3.0.3",
    "content": "DolphinScheduler released version 3.0.3, focusing on fixing 6 bugs\n\nRecently, Apache DolphinScheduler released version 3.0.3. This version mainly fixes bugs based on version 3.0.2, with a total of 6 bugs fixed and 3 document modifications. Important bug fixes include: Fix timing scheduling trigger get command parameter null pointer exception #12419 Fix the problem that the scheduled task environment variable does not take effect #12955 Change default unix shell from sh to bash #12180 Malformed LDAP configuration file The full Changelog is as follows:\nBug Fixes\n\n[Bug-12954] [Schedule] Fix the problem that workflow level configuration information does not take effect when timing trigger execution (#12955)\n[Fix][UI] Download resource returns 401 (#12566)\nMake sure all failed tasks will be saved in errorTaskMap (#12424)\nFix timing scheduling triggering main service report to get command parameter null pointer exception (#12419)\nChanged default unix shell executor from sh to bash (#12180)\nFix LDAP error (#12929)\n\nDocument\n\n[FEATURE-8030] [docs] Add Sqoop task documentation (#12855)\n[Documentation fix] Add Slack Alert documentation (#12567)\n[Optimization][Documentation] Update the structure and process startup process pictures (#12912)\n\nRelease Notes\nhttps://github.com/apache/dolphinscheduler/releases/tag/3.0.3\nAcknowledgment\nThanks to the following community contributors for their help in this version releaseðððð\nzhuangchong, ntfs32, DarkAssassinator, simsicon, liqingwang, baihongbin, Tianqi-Dotes, tracehh Participate in contribution\n",
    "title": "DolphinScheduler released version 3.0.3, focusing on fixing 6 bugs",
    "time": "2022-12-01"
  },
  {
    "name": "Apache_dolphinScheduler_3.1.2",
    "content": "Apache DolphinScheduler releases version 3.1.2 with Python API optimizations\n\nRecently, Apache DolphinScheduler released version 3.1.2. This version is mainly based on version 3.1.2, with 6 Python API optimizations, 19 bug fixes, and 4 document updates.\nImportant bug fixes:\n\nWorker kill process does not take effect #12995\nComplement dependency mode generates wrong workflow instance (#13009)\nPython task parameter passing error (#12961)\nFix dependency task null pointer (#12965)\nTask retry error (#12903)\nShell task calls dolphinscheduler_env.sh configuration file exception (#12909)\nCorrected documentation for multiple Hive SQL runs (#12765)\nAdded token authentication for Python API #12893\n\nChange Log\nBug fix\n\n[Improvement] change alert start.sh (#13100)\n[Fix] Add token as authentication for python gateway (#12893)\n[Fix-13010] [Task] The Flink SQL task page selects the pre-job deployment mode, but the task executed by the worker is the Flink local mode\n[Fix-12997][API] Fix that the end time is not reset when the workflow instance reruns. (#12998)\n[Fix-12994] [Worker] Fix kill process does not take effect (#12995)\nFix sql task will send alert if we donât choose the send email #12984\n[Fix-13008] [UI] When using the complement function, turn on the dependent mode to generate multiple unrelated workflow instances (#13009)\n[Fix][doc] python api release link\n[Fix] Python task can not pass the parameters to downstream task. (#12961)\n[Fix] Fix Java path in Kubernetes Helm Chart (#12987)\n[Fix-12963] [Master] Fix dependent task node null pointer exception (#12965)\n[Fix-12954] [Schedule] Fix that workflow-level configuration information does not take effect when timing triggers execution\nFix execute shell task exception no dolphinscheduler_env.sh file execute permission (#12909)\nUpgrade clickhouse jdbc driver #12639\nadd spring-context to alert api (#12892)\n[Upgrade][SQL]Modify the table t_ds_worker_group to add a description field in the postgresql upgrade script #12883\nFix NPE while retry task (#12903)\nFix-12832][API] Fix update worker group exception group name already exists. #12874\nFix and enhance helm db config (#12707)\n\nDocument\n\n[Fix][Doc] Fix sql-hive and hive-cli doc (#12765)\n[Fix][Alert] Ignore alert not write info to db (#12867)\n[Doc] Add skip spotless check during ASF release #12835\n[Doc][Bug] Fix dead link caused by markdown cross-files anchor #12357 (#12877)\n\nPython API\n\n[Fix] python API upload resource center failed\n[Feature] Add CURD to the project/tenant/user section of the python-DS (#11162)\n[Chore][Python] Change name from process definition to workflow (#12918)\n[Feature] Support set execute type to pydolphinscheduler (#12871)\n[Hotfix] Correct python doc link\n[Improvement][Python] Validate version of Python API at launch (#11626)\n\nAcknowledgment\nThanks to all community contributors who participated in the release of Apache DolphinScheduler 3.1.2. Below is the list of the contributors by GitHub ID, in no particular order.\n\n\n\nliqingwang\nliqingwang\nhezean\n\n\n\n\nruanwenjun\nsimsicon\njieguangzhou\n\n\nTianqi-Dotes\nzhuangchong\nzhongjiajie\n\n\n\n",
    "title": "Apache DolphinScheduler releases version 3.1.2 with Python API optimizations",
    "time": "2022-12-24"
  },
  {
    "name": "Application_transformation_of_the_FinTech_data_center_based_on_DolphinScheduler",
    "content": "Application transformation of the FinTech data center based on DolphinScheduler\n\nOn Apache DolphinScheduler Meetup last week, Feng Mingxia, a big data engineer from Chengfang FinTech, brought us the application practice of DolphinScheduler in the field of FinTech. The following is the presentation.\n\nFeng Mingxia, Chengfang Financial Technology Big Data Engineer\nFocusing on real-time and offline data processing and analysis in the field of big data, at present, he is mainly responsible for the research and development of data middle platforms.\nSpeech summary:\nÂ· Use background\nÂ· Secondary transformation based on DolphinScheduler\nÂ· DolphinScheduler plug-in expansion\nÂ· Future and outlook\nUse Background\nData Center Construction\nAt present, big data technology is widely used in the financial field, and the big data platform has become a financial infrastructure. In the construction of a big data platform, the data center is the brightest star, which is the entrance and interface for business systems to use big data, when various business systems are connected to the data center, the data middle office needs to provide unified management and unified access to ensure the security, reliability, efficiency, and reliability of the service.\nAs shown in the figure below, the data middle office is in the middle link between the business systems and the big data platform, each business system accesses the big data platform through the services provided by the data center.\n\nThe core concept of the data middle office is to realize four modernizations, namely, business data, data asset, asset service, and service business. From business to data, and back to the complete closed loop formed by business, support the digital transformation of enterprises.\n\nThe logical architecture of the data center is shown in the figure above, analyzing from bottom to top, First, the bottom layer is the data resource layer, which is the original data generated by various business systems; The next layer is data integration, and the methods of data integration include offline collection and real-time collection, of which the technologies used include Flume, CDC real-time collection, etc.\nThe next layer is the data lake, which puts data in the lake through data integration, stored in Hadoop distributed storage or MPP architecture database.\nThe next layer is the data engine layer, which processes and analyzes the data in the data lake through real-time and offline computing engines like Flink and Spark, form service data is available for the upper layer.\nThe next layer is the data service that the data center needs to provide. At present, the data service includes data development service and data sharing service, providing data development and sharing capabilities for the upper business systems.\nThe data application layer is the specific application of data, including data anomaly detection, data governance, AI decision-making, and BI analysis.\nIn the construction of the whole data middle platform, the scheduling engine is the core position in the data engine layer and is also an important function in the construction of the data middle platform.\nProblems and challenges faced by the data center\nThe data middle office will face some problems and challenges.\nFirst of all, the execution and scheduling of data tasks are the core and key of data development services provided by the data center.\nSecondly, the data center provides unified data service management, service development, service invocation, and service monitoring.\nThird, ensuring the security of financial data is the primary task of FinTech, and the data middle office needs to ensure the security and reliability of data services.\nUnder the above problems and challenges, we investigated some open-source scheduling engines.\n\nAt present, we use a variety of scheduling engines in the production process, such as oozie, XXL job, and DolphinScheduler, which we introduced through research and analysis in 2022, and plays a very important role in the construction of the entire data center.\nFirst of all, DolphinScheduler partially addresses our requirements for unified service management, service development, service invocation, and service management.\nSecondly, it has its own unique design in task fault tolerance, supporting HA, elastic expansion, fault tolerance, and basically ensuring the safe operation of tasks.\nThird, it supports task and node monitoring.\nFourth, it supports multi-tenant and permission control.\nFinally, its community is very active, with rapid version change and problem repair.\nThrough the analysis of DolphinSchedulerâs architecture and source code, we believe that its architecture conforms to the mainstream big data framework design and has similar architecture patterns and designs with excellent foreign products such as Hbase and Kafka.\nRe-development based on DolphinScheduler\nTo make DolphinScheduler more suitable for our application scenarios, we have made a second transformation based on DolphinScheduler, it includes 6 aspects.\n\nAdd asynchronous service call function\nAdd Metabase Oracle adaptation\nAdd multi-environment configuration capability\nAdd log and historical data-cleaning strategy\nAdd access to Yarn logs\nAdd service security strategy\n\nAdd asynchronous service calling function\nFirst, the asynchronous service invocation function is added, the figure above shows the architecture of DolphinScheduler version 2.0.5, and most of them are service components of the native DolphinScheduler. GateWay marked in red is a gateway service added based on DolphinScheduler. It realizes flow control, black and white list, and is also the access for users to access service development. By optimizing the startup interface of the process and returning the unique code of the process, we have added the function of service mapping.\n\nIn the classic DolphinScheduler access mode, the workflow execution instructions submitted by users will enter the command table in the original database, after getting the zk lock, the master component obtains commands from the Metabase, performs DAG parsing, generates actual process instances, delivers the decomposed tasks to the work node for execution through RPC, and then synchronously waits for the execution results.\nIn the native DolphinScheduler request, After the user submits the instruction, The return code for executing the workflow is missing, Therefore, we have added a unique return ID, through which users can query the subsequent process status, download logs, and download data.\nAdd Metabase Oracle adaptation\nOur second transformation is to adapt DolphinScheduler to the Oracle database. At present, the metadatabase of the native DolphinScheduler is MySQL, and we need to convert the original database into an Oracle database according to our production needs. To achieve this, it is necessary to complete the adaptation of the data initialization module and the data operation module.\n\nFirst, for the data initialization module, we modified the install_ config. Conf configuration file to change it to the configuration of Oracle.\nSecondly, the Oracle application needs to be added Yml, we are in dolphinscheduler-2.0*/ the application. yml of Oracle is added to the apache-dolphinscheduler-2.0. * â bin/conf/directory.\nFinally, we convert the data operation module, Modify the mapper file and the file, Because the Dolphinscheduler-dao module is a database operation module, other modules will reference this module to implement database operations. It uses Mybatis for database connection, so you need to change the mapper file, all mapper files are in the resources directory.\nMulti-environment configuration capability\nThe installation of the native DolphinScheduler version cannot be configured according to the environment, Generally, relevant parameters need to be adjusted according to the actual environment. We want to enhance the environment selection and configuration through the installation script, to reduce the cost of manual online modification, Automated installation. It is believed that all partners have encountered similar difficulties. In order to use DolphinScheduler in a development environment, test environment, joint debugging environment, performance environment, quasi-production environment, and production environment, a large number of environment-related configuration parameters need to be modified.\nWe modify the install Sh.file, add the input parameter [dev|test|product], and select the appropriate install_ config_$ {evn}. Conf can be installed to automatically select the environment.\nIn addition, DolphinSchedulerâs workflow is strongly bound to the environment, and workflows in different environments cannot be shared. The following figure shows the JSON file of a workflow exported by the native DolphinScheduler. The grayed part represents the resource resources on which the process depends. The ID is a number, which is generated by the auto-increment of the database. However, if the process instances generated by environment a are placed in environment b, there may be ID primary key conflicts. In other words, workflows generated in different environments cannot be shared.\n\nWe solve this problem by generating the absolute path of the resource as the unique ID of the resource.\nLog and historical data cleaning policy\nThe DolphinScheduler generates a lot of data. The database will generate instance data in the instance table, which will continue to grow with the running of instance tasks. Our strategy is to clean up the data of these tables according to the agreed save cycle by defining the scheduled task of DolphinScheduler.\nSecondly, the data of DolphinScheduler mainly includes log data and task execution directory, including the service log data of the worker, master, API, and the directory executed by the worker. These data will not be automatically deleted at the end of task execution, but also need to be deleted through scheduled tasks. By running the log cleanup script, we can automatically delete logs.\n\n\nIncreased access to Yarn logs\nThe native DolphinScheduler can obtain the log information executed on the worker node, but for tasks on Yarn, you need to log in to the Yarn cluster and obtain it through the command or interface. We obtain the Yarn task ID by analyzing the YARNID tag in the log and obtain the task log through the yarnclient. The process of manually viewing logs is reduced.\n\nService security policy\nAdd Monitor component monitoring\n\nThe above figure shows the interaction between the master and worker, the two core components of DolphinScheduler, and Zookeeper. When the MasterServer service starts, it will register a temporary node with Zookeeper, and conduct fault tolerance processing by listening for changes in Zookeeper temporary nodes. WorkerServer is mainly responsible for task execution. When the WorkerServer service starts, it registers a temporary node with Zookeeper and maintains the heartbeat. At present, Zookeeper plays a very important role, mainly in service registration and heartbeat detection.\nThe relevant parameters can be seen when the master and worker connect to Zookeeper, including connection timeout, session timeout, and a maximum number of retries.\nDue to network jitter and other factors, master and worker nodes may lose connection with zk. After the loss of connection, because the temporary information registered on the zk by the worker and master disappears, it will be determined that the zk is lost from the master and worker, affecting the task execution. Without human intervention, the task will be delayed. We added the monitor component to monitor the service status. Through the scheduled task cron, we run the monitor program every 5 minutes to check whether the worker process and master process are alive. If they are down, they will be restarted.\n\nAdd Kerberos authentication link for service components using zk\n\nThe second security policy is to add the Kerberos authentication link for service components using zk. Kerberos is a network authentication protocol designed to provide powerful authentication services for client/server applications through a key system. Master service components, API service components, and worker service components complete Kerberos authentication at startup, and then use zk for relevant service registration and heartbeat connection to ensure service security.\nDolphinScheduler-based plugin extension\nIn addition, we have extended the plug-in based on DolphinScheduler. We have extended four types of operators, including Richshell, SparkSQL, Dataexport, and GBase operators.\nAdd a new task type Richshell\nFirst of all, Richshell, a new task type, has enhanced the native Shell function. It mainly realizes the dynamic replacement of script parameters through the template engine. Users can replace script parameters through service calls, making users more flexible in using parameters. It is a supplement to global parameters.\n\nAdd a new task type SparkSQL\nThe second operator added is SparkSQL. Users can execute Spark tasks by writing SQL so that tasks can be scheduled on Yarn. DolphinScheduler natively also supports SparkSQL execution in JDBC mode, but there is a situation of resource contention because the number of JDBC connections is limited. The Yarn cluster mode cannot be used for execution through tools such as SparkSQL/Spark beer. By using this task type, SparkSQL programs can be run on the Yarn cluster in cluster mode to maximize the use of cluster resources and reduce the use of client resources.\nAdd a new task type Dataexport\nThe third addition is Dataexport, which is also a data export operator. Users can export data stored in components by selecting different storage components. Components include ES, Hive, Hbase, etc.\n\nThe data in the big data platform may be used for BI display, statistical analysis, machine learning, and other data preparation after being exported. Most of these scenarios require data export, and Sparkâs data processing capability is used to achieve the export function of different data sources.\nAdd a new task type GBase\nThe fourth plug-in added is Gbase. GBase 8a MPP Cluster is a distributed parallel database cluster with column storage and shared nothing architecture. It has the characteristics of high performance, high availability, high expansion, etc. It is suitable for OLAP scenarios (query scenarios), can provide a cost-effective general computing platform for large-scale data management, and is widely used to support various data warehouse systems, BI systems, and decision support systems.\n\nAs an application scenario of data entering the lake, we have added a GBase operator, which supports the import, export, and execution of GBase data.\n",
    "title": "Application transformation of the FinTech data center based on DolphinScheduler",
    "time": "2022-12-6"
  },
  {
    "name": "Awarded_most_popular_project_in_2021",
    "content": "Apache DolphinScheduler Won theã2021 OSC Most Popular Projectsãaward, and Whaleops Open Source Technology Received the honor ofãOutstanding Chinese Open Source Original Startupsã!\n\n\n\nRecently, the &quot;2021 OSC Best China Open Source Projects Pollãinitiated by OSCHINA announced the selection results.\nWith the love and support of the users and the open-source community, the cloud-native distributed big data scheduler Apache DolphinScheduler was awarded the ãOSCHINA Popularity Index Top 50 Open Source Projectsãand ãMost Popular Projectsãin 2021. And Its commercial operating company Whaleops Open Source Technology also was awarded the &quot;Outstanding Chinese Open Source Native Startup&quot; for its outstanding open-source operation performance.\nWon the Honor ofãMost Popular Projectsã\nThis year, the ã2021 OSC Best China Open Source Projects Poll&quot; set up two rounds of voting. The first round of voting selected the ãOSCHINA Popularity Index TOP 50 Open Source Projectsãbased on the number of votes. The second round of voting was conducted based on the TOP 50 projects in the first round, and 30 &quot;Most Popular Projects&quot; were selected.\nIn the first round of voting, OSCHINA selected TOP 5 of 7 major categories ofÂ  projects (basic Software, Cloud-Native, Front-end, DevOps, Development Frameworks and Tools, AI &amp; Big Data, IoT &amp; 5G) based on the number of votes. Apache DolphinScheduler stood out in the &quot;cloud-native&quot; category and was on the list for its excellent cloud-native capabilities.\nAfter the fierce competition in the second round of voting, Apache DolphinScheduler won again and was rewarded the &quot;Most Popular Project&quot;.\nWhaleops Open Source Technology Won the ãOutstanding Chinese Open Source Native Startupsã Award\n&quot;OSC China Open Source Project Poll&quot; organized by Open Source China (OSCHINA, OSC Open Source Community) is the most authoritative and grandest open source project campaign in China. It aims to showcase the status quo of domestic open-source, explore domestic open source trends, and inspire domestic open-source talents, sequentially to promote the improvement of the domestic open source ecosystem.\nChina's open-source software ecosystem is booming. In recent years, numerous outstanding open source software startups have emerged. They have beared original aspirations in mind to contribute to China's open-source software development, deeply cultivated in open-source to gave back to the community, and became an important force that cannot be ignored in the global open-source software ecosystem.\nTo appreciate these open source startups for their outstanding contributions to the open-source community, and to let more developers know and understand these high-quality open-source startup teams that have set a good example for Chinese open-source entrepreneurs, OSCHINA specially set up the &quot;Excellent Chinese Open Source Native Startups&quot; award in this yearâs poll.\n&quot;Open Source Native Startups&quot; refer to companies established based on open source projects and operate around the following business models:\nThe team provides commercial services around self-built open source projects, or\nThe team provides commercial services based on upstream open source projects\nOSCHINA has evaluated open-source native commercial companies in their entrepreneurial stageÂ from the perspective of the company's related open source project communities status, the technical field development prospects of the company's related open source projects, and the company's development status. After professional evaluation, Whaleops Open Source Technology was rewaded the ãExcellent Chinese Open Source Native Startupsãaward.\nAbout Apache DolphinScheduler\nApache DolphinScheduler is a top-level project incubated by the Apache Software Foundation. As a new generation of distributed big data workflow task scheduler, it is committed to &quot;solving the intricate dependencies between big data tasks and making the entire data process intuitive and visible&quot;. DolphinScheduler assembles Tasks in the form of DAG (Directed Acyclic Graph), which can monitor the running status of tasks in real-time. At the same time, it supports operations such as retry, recovery from designated nodes, suspend and Kill tasks, and focuses on continuously improving the 6 capabilities of DAG visualization,Â  rich task types, history, task log/alarm mechanism, and complement.\nSo far, the Apache DolphinScheduler community consists of 280+ experienced code contributors and 110+ non-code contributors, including PMC or Committer from other top Apache projects. Since its creation, the Apache DolphinScheduler open source community has continued to grow, and the WeChat user base has reached 6000+ people. As of January 2022, 600+ companies and institutions have adopted Apache DolphinScheduler in their production environments.\nFinally, we would like to thank OSCHINA for recognizing Apache DolphinScheduler and its commercial operating company Whaleops Open Source Technology, especially to every participant that is inseparable in the community build-up.\nIn the future, we believe the community will scale new heights with this valuable honor, as well as the joint force of every contributor!\nThe Way to Join US\nThere are many ways to participate and contribute to the DolphinScheduler community, including:\nDocuments, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.\nWe assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.\n\n\nSo the community has compiled the following list of issues suitable for novices: https://github.com/apache/dolphinscheduler/issues/5689\n\n\nList of non-newbie issues: https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n\n\nHow to participate in the contribution: https://dolphinscheduler.apache.org/en-us/community\n\n\nCommunity Official Website\nhttps://dolphinscheduler.apache.org/\n\n\nGitHub Code repository: https://github.com/apache/dolphinscheduler\n\n\nYour Star for the project is important, donât hesitate to lighten a Star for Apache DolphinScheduler â¤ï¸\n",
    "title": "Apache DolphinScheduler Won theã2021 OSC Most Popular Projectsãaward, and Whaleops Open Source Technology Received the honor ofãOutstanding Chinese Open Source Original Startupsã!",
    "time": "2022-1-7"
  },
  {
    "name": "Board_of_Directors_Report",
    "content": "Apache DolphinScheduler Board Report: Community Runs Well, Commit Number Grows over 123%\n\n\n\nSince graduating from the Apache Incubator on March 17, 2021, Apache DolphinScheduler has grown with the community for ten months. With the joint participation of the community, Apache DolphinScheduler has grown into a mature scheduling system product that has been tested in the production environment of hundreds of enterprises after several iterations.\nWhat progress has Apache DolphinScheduler made in nearly a year? Today we're going to review the changes that have taken place in the Apache DolphinScheduler and its community with this Apache report.\nBase Data:\nFounded:Â 2021-03-17 (10 months ago)\nChair:Â Lidong Dai\nReporting schedule:Â January, April, July, October\nNext report date: Wed Jan 19 2022\nCommunity Health Score (Chi):Â 7.55 (Healthy)\nProject Composition:\n\nThere are currently 39 committers and 16 PMC members in this project.\nThe Committer-to-PMC ratio is roughly 5:2.\n\nCommunity changes, past quarter:\n\nNo new PMC members. Last addition was Calvin Kirs on 2021-05-07.\nShunFeng Cai was added as committer on 2021-12-18\nZhenxu Ke was added as committer on 2021-12-12\nWang Xingjie was added as committer on 2021-11-24\nYizhi Wang was added as committer on 2021-12-15\nJiajie Zhong was added as committer on 2021-12-12\n\nCommunity Health Metrics:\n\nNotable mailing list trends\nCommit activity\nGitHub PR activity\nGitHub issues\nBusiest GitHub issues/PRs\n\nNotable mailing list trends:\ndev@dolphinscheduler.apache.orgÂ had a 64% increase in traffic in the past quarter (297 emails compared to 181):\n\n\n\nCommit activity:\n\n972 commits in the past quarter (123% increase)\n88 code contributors in the past quarter (25% increase)\n\n\n\n\nGitHub PR activity:\n\n824 PRs opened on GitHub, past quarter (89% increase)\n818 PRs closed on GitHub, past quarter (100% increase)\n\n\n\n\nGitHub issues:\n\n593 issues opened on GitHub, past quarter (90% increase)\n608 issues closed on GitHub, past quarter (155% increase)\n\n\n\n\nBusiest GitHub issues/PRs:\n\ndolphinscheduler/pull/6894[Improvement][logger]Logger server integrate into worker server(15 comments)\ndolphinscheduler/pull/6744[Bug][snowflakeutils] fix snowFlake bug(15 comments)\ndolphinscheduler/pull/6674[Feature][unittest] Recover UT in AlertPluginManagerTest.java [closes: #6619](15 comments)\ndolphinscheduler/issues/7039[Bug] [Task Plugin] hive sql execute failed(14 comments)\ndolphinscheduler/pull/6782[improvement] improve install.sh if then statement(13 comments)\ndolphinscheduler/issues/7485[Bug] [dolphinscheduler-datasource-api] Failed to create hive datasource using ZooKeeper way in 2.0.1(13 comments)\ndolphinscheduler/pull/7214[DS-7016][feat] Auto create workflow while import sql script with specific hint(12 comments)\ndolphinscheduler/pull/6708[FIX-#6505][dao] upgrade the MySQL driver package for building MySQL jdbcUrl(12 comments)\ndolphinscheduler/pull/7515[6696/1880][ui] replace node-sass with dart-sass(12 comments)\ndolphinscheduler/pull/6913Use docker.scarf.sh to track docker user info(12 comments)\n\nThe Way to Join US\nThere are many ways to participate and contribute to the DolphinScheduler community, including:\nDocuments, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.\nWe assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.\nSo the community has compiled the following list of issues suitable for novices: https://github.com/apache/dolphinscheduler/issues/5689\nList of non-newbie issues: https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\nHow to participate in the contribution: https://dolphinscheduler.apache.org/en-us/community\nCommunity Official Website\nhttps://dolphinscheduler.apache.org/\nGitHub Code repository: https://github.com/apache/dolphinscheduler\nYour Star for the project is important, donât hesitate to lighten a Star for Apache DolphinScheduler â¤ï¸\n",
    "title": "Apache DolphinScheduler Board Report: Community Runs Well, Commit Number Grows over 123%",
    "time": "2022-1-13"
  },
  {
    "name": "China_Unicom_revamps_Apache_DolphinScheduler",
    "content": "China Unicom revamps Apache DolphinScheduler Resource Center for cross-cluster calls in billing environments and one-stop access to data scripts\n\n\n\nBy 2022, China Unicom's subscriber base reaches 460 million, accounting for 30% of China's population. With the spread of 5G, operators' IT systems are generally facing the impact of a series of changes such as massive subscribers, massive call orders, diverse services, and network organization models.\nCurrently, China Unicom handles more than 40 billion voice orders per day. Facing the huge data volume, it has become the ultimate goal of China Unicom to improve service levels and provide more targeted services to customers. China Unicom has already emerged as a leader in technologies and applications for massive data aggregation, processing, desensitization, and encryption, which help it grows into an enabler of the development of a digital economy.\nAt the Apache DolphinScheduler April Meetup, we invited Xuesong Bai from China Unicom Software, who shared with us the application of Apache DolphinScheduler in China Unicom's billing environment.\nThe presentation consisted of three parts.\n\nGeneral usage of Apache DolphinScheduler in China Unicom\nSharing on the topic of China Unicom's billing business\nThe next step in planning\n\n\n\n\nXuesong Bai | Big Data Engineer, China Unicom Software, Graduated from China Agricultural University, working on big data platform building and AI platform building, contributing Apache SeaTunnel(Incubating) plug-in for Apache DolphinScheduler and sharing Alluxio plug-in for Apache SeaTunnel(Incubating)\n01 General usage of Apache DolphinScheduler in China Unicom\nLet me start by giving you an overview of China Unicom's overall usage of Apache DolphinScheduler.\n\nRight now we are running our business mainly in 4 clusters at 3 locations\nThe overall number of task flows is around 300\nAverage daily task runs are around 5,000\n\nThe Apache DolphinScheduler components we use include Spark, Flink, SeaTunnel (formerly Waterdrop), as well as Presto, and some shell scripts for stored procedures, covering auditing, revenue sharing, billing, and other operations that need to be automated.\n\n\n\n02 Business introduction\n01 Cross-cluster dual-active service calls\nAs mentioned above, our service runs on 4 clusters at 3 locations, which inevitably leads to mutual data exchange and business calls between clusters. How should we unify the management and scheduling of these cross-cluster data transfer tasks is important. Our data is in the production cluster, which is very sensitive to the cluster network bandwidth and must be managed in an organized manner.\nOn the other hand, we have some operations that need to be invoked across clusters, for example, cluster B has to start statistical tasks after data is available in cluster A. We chose Apache DolphinScheduler as the scheduling and control system to solve both problems.\nWe use HDFS for the underlying data storage. In the cross-cluster HDFS data exchange, we divide the data used into small and large batches, structure tables, configuration tables, etc. according to the size and usage of the data.\nFor small batches, we put them directly onto the same Alluxio for data sharing, so that version issues caused by untimely data synchronization do not occur.\n\nFor things like schedules and other large files, we use a mix of Distcp and Spark.\nFor structured table data, the SeaTunnel on Spark is used.\nSetting speed limits by way of Yarn queues.\nUnstructured data is transferred by Distcp, with speed limits by way of the self-contained parameter Bandwidth.\n\nThese transfer tasks are all run on top of Apache DolphinScheduler, and the overall data flow is mainly data availability detection for cluster A, data integrity verification for cluster A, data transfer between clusters A and B, and data auditing and availability notification for cluster B.\nWe mainly used Apache DolphinScheduler's complementary re-runs to fix failed tasks or incomplete data.\n\n\n\nOnce we have finished synchronizing and accessing data across clusters, we also use Apache DolphinScheduler to make task calls across geographies and clusters.\nWe have two clusters in location A, i.e. test A1 and production A2, and a production B1 cluster in location B. We will take out two machines with intranet IPs as interface machines on each cluster and build a virtual cluster by building Apache DolphinScheduler on the six interface machines so that the content of the three clusters can be manipulated on a unified page.\nQ: How do I go live from test to production?\nA: Develop tasks on A1 test, after passing the test, change the worker nodes directly to A2 production.\nQ: What if I encounter a problem with A2 production, eg. data is not available?\nA: We can switch directly to B1 production to do a manual dual-live disaster recovery switchover.\n\n\n\nFinally, we also have some tasks that are relatively large and need to be calculated simultaneously using two clusters to meet the task timelines, we will split the data into two parts and put them on A2 and B1 respectively, after which we will run the tasks simultaneously and finally pass the results back to the same cluster for merging. These task processes are invoked through Apache DolphinScheduler.\nPlease note that in this process, we use Apache DolphinScheduler to solve several problems.\n\nTask dependency checks for projects across clusters.\nControl of task environment variables at the node level.\n\n02 AI development synchronization of task runs\n1. Unifying data access methods\nWe now have a simple AI development platform that mainly provides some TensorFlow and Spark ML computing environments for users. With the business requirement to bridge the user-trained local file model and the clustered file system, and to be able to provide a unified approach to access and deployment, we use the tools Alluxio-fuse and Apache DolphinScheduler.\n\n\nAlluxio-fuse bridges local and cluster storage\n\n\nApache DolphinScheduler shares local and cluster storage\n\n\nAs the AI platform cluster and the data cluster we built are two data clusters, so we store data on the data cluster, use Spark SQL or Hive to do some data pre-processing, after that we transfer the processed data on Alluxio, and finally mapped to local files through Alluxio fuse across level clusters, so that our development environment based on Conda's development environment allows us to access this data directly, thus to unify the way we access the data of clusters by access the local data.\n\n\n\n2. One-stop access to data scripts\nAfter separating the resources, by pre-processing the big data content through the data cluster, we go through our AI cluster to process the training and prediction models. Here, we use Alluxio-fuse to make changes to Apache DolphinScheduler's resource centre. We connect Apache DolphinScheduler resource centre to Alluxio and mount both the local and cluster files via Alluxio-fuse, so that Apache DolphinSchedule can access both the local training inference scripts and the training inference data stored on hdfs, enabling one-stop access to the data scripts.\n\n\n\n03 Service query logic persistence\nThe third application scenario is that we use Presto and Hue to provide a front-end instant query interface for users that need to run some processing logic and stored procedures regularly after writing SQL through the front-end when testing is completed, so we open up the flow from the front-end SQL to the back-end regular running tasks.\n\n\n\nAnother issue is that Presto does not have resource isolation between tenants. We did a comparison of several solutions before finally choosing the Presto on Spark solution in conjunction with the actual situation.\nOn this multi-tenant platform, the initial solution we offered to users was to use the Hue interface on the front end and run the back end directly on the physical cluster using Presto, which led to a problem of contention for the user resources. When there were certain large queries or large processing logic, it would cause other tenant operations to be on hold for long periods.\nFor this reason, we have had a comprehensive performance comparison between Presto on Yarn and Presto on Spark, and we found that Presto on Spark is more efficient in terms of resource usage, and you can choose the solution that suits your needs.\n\n\n\nOn the other hand, we make the native Presto and Presto on spark coexist, for SQL with small data volume and simple processing logic, we run them directly on native Presto, while for SQL with more complex processing logic and longer running time, we run them on Presto on Spark, so that users can use one set of SQL to switch to different underlying engines.\nIn addition, we have also bridged the Hue to Apache DolphinScheduler timed task scheduling process. After we modulate SQL development on Hue, we connect it to Git for version control by storing it in a local Serve file.\nWe transfer the local file to Alluxio fuse as a synchronous mount for SQL, and finally, we use Hue to create tasks and timed tasks through Apache DolphinScheduler's API to control the process from SQL development to timed runs.\n\n\n\n04 Unified governance of data lakes data\nThe last scenario is the unified governance of the data in data lakes. On our self-developed data integration platform, we use a hierarchical governance approach to unify the management and access of the data in data lakes, in which Apache DolphinScheduler is used as the inbound scheduling and monitoring engine.\nIn the data integration platform, we use Apache DolphinScheduler for batch and real-time tasks such as data integration, data entry, and data distribution.\nThe bottom layer runs on Spark and Flink. For data query and data exploration, which require immediate feedback, we use the embedded Hue to Spark and Presto to explore and query the data; for data asset registration and data audit, we directly query the data source file information and directly synchronize the underlying data information.\n\n\n\nCurrently, we run the quality management of 460 data tables on the integration platform, providing unified management of data accuracy and punctuality.\n03 Next steps and development needs\n01 Resource Centre\nAt the resource centre, to facilitate file sharing between users, we plan to provide resource authorization for all users, as well as assign shared files at the tenant level depending on the tenant it belongs to, making it more friendly for a multi-tenant platform.\n02 User Management\nThe second plan is around user management, we only provide tenant-level administrator accounts, with subsequent user accounts created by tenant administrator accounts.\n03 Task Nodes\nFinally, there are plans for our task nodes, which are now in progress: on the one hand, to complete the optimization of the SQL node so that users can select a resource centre SQL file without having to copy the SQL manually; on the other hand, enable the HTTP node to extract field judgments for custom parsing of the returned JSON, and to provide more friendly handling of complex return values.\n",
    "title": "China Unicom revamps Apache DolphinScheduler Resource Center for cross-cluster calls in billing environments and one-stop access to data scripts",
    "time": "2022-5-07"
  },
  {
    "name": "DAG",
    "content": "Big Data Workflow Task Scheduling - Directed Acyclic Graph (DAG) for Topological Sorting\nReviewing the basicsï¼\nGraph traversalï¼\nA graph traversal is a visit to all the vertices in a graph once and only once, starting from a vertex in the graph and following some search method along the edges of the graph.\nNote : the tree is a special kind of graph, so tree traversal can actually be seen as a special kind of graph traversal.\nThere are two main algorithms for graph traversal\nBreadth First Search (BFS)\nThe basic idea: first visit the starting vertex v, then from v, visit each of v's unvisited adjacent vertices w1, w2 , ... ,wi, then visit all the unvisited adjacent vertices of w1, w2, ... , wi in turn; from these visited vertices, visit all their unvisited adjacent vertices until all vertices in the graph have been visited. and from these visited vertices, visit all their unvisited adjacent vertices, until all vertices in the graph have been visited. If there are still vertices in the graph that have not been visited, choose another vertex in the graph that has not been visited as the starting point and repeat the above process until all vertices in the graph have been visited.\nDepth First Search (DFS)\nThe basic idea: first visit a starting vertex v in the graph, then from v, visit any vertex w~1~ that is adjacent to v and has not been visited, then visit any vertex w~2~ that is adjacent to w~1~ and has not been visited ...... Repeat the above process. When it is no longer possible to go down the graph, go back to the most recently visited vertex, and if it has adjacent vertices that have not been visited, continue the search process from that point until all vertices in the graph have been visited.\nExample\nIn the diagram below, if the breadth first search (BFS) is used, the traversal is as follows: 1 2 5 3 4 6 7. If the depth first search (DFS) is used, the traversal is as follows 1 2 3 4 5 6 7.\n\nTopological Sorting\nThe definition of topological ordering on Wikipedia is :\nFor any Directed Acyclic Graph (DAG), the topological sorting is a linear sorting of all its nodes (there may be multiple such node sorts in the same directed graph). This sorting satisfies the condition that for any two nodes U and V in the graph, if there is a directed edge pointing from U to V, then U must appear ahead of V in the topological sorting.\nIn layman's terms: Topological sorting is a linear sequence of all vertices of a directed acyclic graph (DAG), which must satisfy two conditions :\n\nEach vertex appears and only appears once.\nIf there is a path from vertex A to vertex B, then vertex A appears before vertex B in the sequence.\n\nHow to find out its topological sort? Here is a more commonly used method :\nBefore introducing this method, it is necessary to add the concepts of indegree and outdegree of a directed graph node.\nAssuming that there is no directed edge whose starting point and ending point are the same node in a directed graph, then:\nIn-degree: Assume that there is a node V in the graph, and the in-degree is the number of edges that start from other nodes and end at V. That is, the number of all directed edges pointing to V.\nOut-degree: Assuming that there is a node V in the directed graph, the out-degree is the number of edges that currently have a starting point of V and point to other nodes. That is, the number of edges issued by V.\n\nSelect a vertex with an in-degree of 0 from the DAG graph and output it.\nDelete the vertex and all directed edges starting with it from the graph.\nRepeat 1 and 2 until the current DAG graph is empty or there are no vertices of degree 0 in the current graph. The latter case indicates that a ring must exist in the directed graph.\n\nFor example, the following DAG graph :\n\n\n\n\nNode\nin degree\nout degree\ncount of in degree\ncount of out degree\n\n\n\n\nNode 1\n0\nNode 2,Node 4\n0\n2\n\n\nNode 2\nNode 1\nNode 3,Node 4\n1\n2\n\n\nNode 3\nNode 2,Node 4\nNode 5\n2\n1\n\n\nNode 4\nNode 1,Node 2\nNode 3,Node 5\n2\n2\n\n\nNode 5\nNode 3,Node 4\n0\n2\n0\n\n\n\nIts topological sorting process is:\n\nTherefore, the result of topological sorting is: {1,2,4,3,5}.\nIf there is no node 2 â&gt; the arrow of node 4, then it is as follows:\n\nWe can get its topological sort as: {1,2,4,3,5} or {1,4,2,3,5}, that is, for the same DAG graph, there may be multiple topological sort results .\nTopological sorting is mainly used to solve the dependency problem in directed graphs.\n**When talking about the implementation, it is necessary to insert the following : **\nFrom this we can further derive an improved depth first search or breadth first search algorithm to accomplish topological sorting. Taking the breadth first search as an example, the only difference between this improved algorithm and the ordinary breadth first search is that we should save the degree of entry corresponding to each node and select the node with a degree of entry of 0 at each level of the traversal to start the traversal (whereas the ordinary breadth first search has no such restriction and can start from any node in the tale). The algorithm is described as follows :\n\nInitialize a Map or similar data structure to save the in-degree of each node.\nFor the child nodes of each node in the graph, add 1 to the in-degree of its child nodes.\nSelect any node with an in-degree of 0 to start traversal, and add this node to the output.\nFor each node traversed, update the in-degree of its child node: subtract 1 from the in-degree of the child node.\nRepeat step 3 until all nodes have been traversed.\nIf it is impossible to traverse all the nodes, it means that the current graph is not a directed acyclic graph. There is no topological sort.\n\nThe core Java code for breadth first search topological sorting is as follows :\npublic class TopologicalSort {\n  /**\n   * Determine whether there is a ring and the result of topological sorting\n   *\n   * Only directed acyclic graph (DAG) has topological sorting\n   * The main methods of breadth first search:\n   *    1ãIterate over all the vertices in the graph, and put the vertices whose in-degree is 0 into the queue.\n   *    2ãA vertex is polled from the queue, and the in-degree of the adjacent point of the vertex is updated (minus 1). If the in-degree of the adjacent point is reduced by 1 and then equals to 0, the adjacent point is entered into the queue.\n   *    3ãKeep executing step 2 until the queue is empty.\n   * If it is impossible to traverse all the vertics, it means that the current graph is not a directed acyclic graph. There is no topological sort.\n   *\n   *\n   * @return key returns the state, true if successful (no-loop), value if failed (loop), value is the result of topological sorting (could be one of these)\n   */\n  private Map.Entry&lt;Boolean, List&lt;Vertex&gt;&gt; topologicalSort() {\n // Node queue with an in-degree of 0\n    Queue&lt;Vertex&gt; zeroIndegreeVertexQueue = new LinkedList&lt;&gt;();\n    // Save the results\n    List&lt;Vertex&gt; topoResultList = new ArrayList&lt;&gt;();\n    // Save the nodes whose in-degree is not 0\n    Map&lt;Vertex, Integer&gt; notZeroIndegreeVertexMap = new HashMap&lt;&gt;();\n\n    // Scan all nodes and queue vertices with in-degree 0\n    for (Map.Entry&lt;Vertex, VertexInfo&gt; vertices : verticesMap.entrySet()) {\n      Vertex vertex = vertices.getKey();\n      int inDegree = getIndegree(vertex);\n\n      if (inDegree == 0) {\n        zeroIndegreeVertexQueue.add(vertex);\n        topoResultList.add(vertex);\n      } else {\n        notZeroIndegreeVertexMap.put(vertex, inDegree);\n      }\n    }\n\n // After scanning, there is no node with an in-degree of 0, indicating that there is a loop, and return directly\n    if(zeroIndegreeVertexQueue.isEmpty()){\n      return new AbstractMap.SimpleEntry(false, topoResultList);\n    }\n\n    // Using the topology algorithm, delete the node with an in-degree of 0 and its associated edges\n    while (!zeroIndegreeVertexQueue.isEmpty()) {\n      Vertex v = zeroIndegreeVertexQueue.poll();\n      // Get the adjacent nodes\n      Set&lt;Vertex&gt; subsequentNodes = getSubsequentNodes(v);\n\n      for (Vertex subsequentVertex : subsequentNodes) {\n\n        Integer degree = notZeroIndegreeVertexMap.get(subsequentVertex);\n\n        if(--degree == 0){\n          topoResultList.add(subsequentVertex);\n          zeroIndegreeVertexQueue.add(subsequentVertex);\n          notZeroIndegreeVertexMap.remove(subsequentVertex);\n        }else{\n          notZeroIndegreeVertexMap.put(subsequentVertex, degree);\n        }\n\n      }\n    }\n\n    //notZeroIndegreeVertexMap If it is empty, it means there is no ring\n    AbstractMap.SimpleEntry resultMap = new AbstractMap.SimpleEntry(notZeroIndegreeVertexMap.size() == 0 , topoResultList);\n    return resultMap;\n\n  }\n}\n\nNotice: the output result is one of the topological sorting sequences of the graph.\nEvery time a vertex is taken from a set with an in-degree of 0, there is no special rule for taking out. The order of taking the vertices will result in a different topological sorting sequence (if the graph has multiple sorting sequences).\nSince each vertex is outputted with the edges starting from it removed. If the graph has V vertices and E edges, the time complexity of the algorithm is typically O(V+E). The final key of the algorithm as implemented here returns the state, true if it succeeds (no rings), with rings if it fails, and value if there are no rings as the result of topological sorting (which may be one of these). Note that the output is one of the topologically sorted sequences of the graph.\n",
    "title": " Big Data Workflow Task Scheduling - Directed Acyclic Graph (DAG) for Topological Sorting",
    "time": "2021-05-06"
  },
  {
    "name": "DS-2.0-alpha-release",
    "content": "Refactoring, Plug-in, Performance Improves By 20 times, Apache DolphinScheduler 2.0 alpha Release Highlights Check!\n\nHello community, good news! After nearly 10 months of joint efforts by more than 100 community contributors, we are happy to announce the release of Apache DolphinScheduler 2.0 alpha. This is the first major version of DolphinScheduler since it entered Apache. It has undergone a number of key updates and optimizations, which means a milestone in the development of DolphinScheduler.\nDolphinScheduler 2.0 alpha mainly refactors the implementation of Master, greatly optimizes the metadata structure and processing flow, adds SPI plug-in capabilities, and improves performance by 20 times. At the same time, the new version has designed a brand new UI interface to bring a better user experience. In addition, 2.0 alpha has newly added and optimized some features that are eagerly demanded in the community, such as parameter transfer, version control, import and export functions.\nNote: The current alpha version does not support automatic upgrades, and we will support this feature in the next version.\n2.0 alpha download link: https://dolphinscheduler.apache.org/en-us/download\nOptimize the Kernel and Increase Performance By 20 Times\nCompared with DolphinScheduler 1.3.8, under the same hardware configuration (3 sets of 8-core 16G), 2.0 alpha throughput performance is increased by 20 times, which is mainly due to the reconstruction of the Master, the optimization of master execution process and the workflow processing process, etc. ,including:\n\n\nRefactor the execution process of the Master, change the previous status polling monitoring to an event notification mechanism, which greatly reduces the pressure of the database polling;\nRemove the global lock, increase the fragmentation processing mechanism of the Master, change the sequential read and write commands to parallel processing, and enhance the horizontal expansion ability of the Master;\n\n\nOptimize the workflow processing process, reduce the use of thread pool, and greatly increase the number of workflows processed by a single Master;\nIncrease the caching mechanism to greatly reduce the number of database operations;\n\n\nOptimize the database connection mode, which immensely reduces the time-consuming of database operation;\n\n\nSimplify the processing flow and reduce unnecessary time-consuming operations during the processing.\n\n\nUI Components Optimization Brings Brand New UI Interface\n\n\nUI interface comparison: 1.3.9 (top) VS. 2.0 alpha (bottom)\n\n2.0 UI mainly optimized by:\n\n\nOptimize the display of components: the interface is more concise, and the workflow process display is clearer;\n\n\nHighlight the key content: click the task box can display task details;\n\n\nEnhanced recognizability: The tool bar on the left is marked with names to make the tools easier to identify and easy to operate;\n\n\nAdjust the order of the components: adjust the order of the components to be more in line with user habits.\n\n\nIn addition to changes in performance and UI, DolphinScheduler has also undergone more than 20 new features and bug fixes.\nList of New Features\n\nTask result transfer function\nAdded Switch task and Pigeon task components\nAdded environmental management function\nAdded batch import , export and batch move functions\nNew registration center plug-in function\nNew task plugin function\n\nOptimizations\n\nOptimize the alarm group function\nOptimize RestApi\nOptimize workflow version management\nOptimize import and export\nOptimize worker group management function\nOptimize the install.sh installation script to simplify the configuration process\n\nBug fix\n\n[#6550]The list of environments in the DAG task pop-up window is not updated\n[#6506]Fix install.sh for DS 2.0 and add comment to install_config.conf\n[#6497]Shell task can not use user defined environment correctly\n[#6478]Missing history data in complement data mode\n[#6352]override the old process definition when I use the copy workflow feature\n[#6342]Task instance page date backfill bug\n[#5701]When deleting a user, the accessToken associated with the user should also be deleted\n[#4809]cannot get application status when kerberos authentication is enabled\n[#4450]Hive/Spark data sources do not support multi-tenancy when Kerberos authentication is enabled bug\n\nThanks to Contributors\nThe release of DolphinScheduler 2.0 alpha embodies the wisdom and strength of the community contributors. Their active participation and great enthusiasm open the DolphinScheduler 2.0 era!\nThanks so much for the participation of 100+ contributors (GitHub ID), and we are looking forward to more and more open sourcing enthusiasts joining the DolphinScheduler community co-construction, to contribute yourself to building a more usable big data workflow scheduling platform!\n\n2.0 List of alpha contributors\n",
    "title": "Refactoring, Plug-in, Performance Improves By 20 times, Apache DolphinScheduler 2.0 alpha Release Highlights Check!",
    "time": "2021-10-29"
  },
  {
    "name": "Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS",
    "content": "Deploy the serverless Apache DolphinScheduler task scheduling system on AWS\nBackground\nIn the scenarios of data warehouse ETL, offline and real-time computing, the dependency scheduling relationship of data tasks is getting more and more complex. The AWS platform provides tools with certain scheduling and task orchestration abilities, such as Apache Airflow (MWAA) and Step function, Glue Workflow, etc. But they all lack the support of visually integrated management operations.\nApache DolphinScheduler aims to solve complex big data task dependencies, and provide applications with data and relationships in various OPS orchestrations, as well as solve the problem that data R&amp;D ETL dependencies are too intricate to monitor the health status of tasks.\nTo introduce Apache DolphinScheduler, while considering the robustness of production and ease of maintenance, this article provides a deployment plan using a completely serverless EKS on Fargate container service and Aurora Serverless PostgreSQL database on the AWS platform and gives detailed deployment steps and maintenance guide.\nIntroduction to DolphinScheduler\nApache DolphinScheduler is a distributed and scalable open-source workflow coordination platform with a powerful DAG visualization interface. It assembles tasks in a DAG (Directed Acyclic Graph, DAG) streaming manner, which can monitor the execution status of tasks on time, and supports operations such as retry, specified node recovery failure, suspension, recovery, and termination of tasks.\nThe DolphinScheduler architecture mainly includes MasterServer, WorkerServer, AlertServer, ApiServer, ZooKeeper, and UI. Among them, MasterServer and WorkerServer adopt the distributed and non-central design concept to be responsible for task segmentation, task submission monitoring, task execution, and log service respectively. AlertServer is mainly responsible for processing requests from the front-end UI layer. The service uniformly provides RESTful API to provide external request services. AlertServer provides alerting services. Both MasterServer and WorkerServer nodes in the system use ZooKeeper for cluster management and fault tolerance. The architecture design diagram is as follows:\npicture\nIntroduction to AWS serverless container EKS on Fargate\nAWS Fargate is a technology that provides containers with the right-sized computing capacity on demand. With AWS Fargate, users no longer have to provision, configure, or scale groups of virtual machines themselves to run containers. There is also no need to choose server types, determine when to scale out node groups, and optimize cluster packaging. Users can control which pods are launched on Fargate and how they run using Fargate configuration files. A Fargate profile is defined as part of an Amazon EKS cluster.\nAmazon EKS integrates Kubernetes with AWS Fargate using a controller built by AWS that uses the upstream scalability model provided by Kubernetes. These controllers run as part of the Amazon EKS-managed Kubernetes control plane and are responsible for scheduling native Kubernetes pods onto Fargate. Fargate controllers include a new scheduler that runs alongside the default Kubernetes scheduler, in addition to several transformation and validation admission controllers. When you launch pods that meet the conditions to run on Fargate, the Fargate controller running in the cluster identifies, updates, and schedules the pods onto Fargate.\nIntroduction to AWS Serverless Database Aurora Serverless\nAmazon Aurora Serverless is an on-demand, auto-scaling configuration of Amazon Aurora. Amazon Aurora Serverless automatically starts up, shuts down, and scales capacity up or down based on the needs of your application. Users can run databases on AWS without having to manage database capacity. With Aurora Serverless, users create a database, specify the desired range of database capacity, and connect to applications. You pay only for the database capacity you use per second while the database is active, and you can migrate between standard and serverless configurations in just a few steps in the Amazon Relational Database Service (Amazon RDS) console.\nDeployment instructions\nOverall deployment architecture\npicture\n\nThe EKS cluster is located in two availability zones, deployed in a private subnet, and uses the ECR image warehouse to manage the DolphinScheduler image;\nEKS uses Fargate nodes, persistently stored on EFS, resource storage uses S3 object storage service, and Aurora Serverless pgsql is used to provide metadata database;\nThe DolphinScheduler API, worker, and master nodes are scaled up and down through the springboard machine kubectl command;\nUse aws load balancer controller to deploy internet-facing load balancing, and proxy api ui provides external access.\n\nPreparations\n\nNetwork planning, taking us-east-1 as an example, create a vpc network: 10.9.0.0/16, where the public network segment is in two AZs, 10.9.1.0/24 and 10.9.2.0/24, Pod network Segment 10.9.10.0/24 and 10.9.11.0/24, Node network segment 10.9.20.0/24 and 10.9.21.0/24, the service network segment is generated by the EKS cluster as a virtual network segment, not in the VPC subnet. Create an Internet gateway in the VPC, create a NAT gateway in the public subnet, and create a springboard server for command line management. Add a routing table, the public subnet is associated with the Internet gateway, and other subnets access Internet services through the NAT gateway by default.\nEKS cluster creation\nConveniently, use the AWS console to create an EKS cluster, associate the above VPC and subnet (reference: https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/create-cluster.html), and create a cluster on the springboard Configure communication with the cluster in. We use EKS version 1.24 here.\nDatabase and Storage Services\nAlso use the AWS console to create a serverless Aurora PostgreSQL database cluster in the VPC private subnet (reference: https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.create-cluster.html). We use aurora-postgresql version 14.4 here.\n\npicture\n\nBuild DolphinScheduler custom image\nTo customize based on the official open source image, use AWS ECR for image management, create DolphinScheduler image ECR warehouse respectively, and push the official image to it (reference: https://docs.aws.amazon.com/zh_cn/AmazonECR/latest/ userguide/docker-push-ecr-image.html). We use DolphinScheduler 3.1.2 version here.\n\npicture\nInstall AWS load balancer controller\n\nAssociate the OIDB identity provider with the EKS cluster. Amazon EKS supports the use of OpenID Connect (OIDC) identity providers as a method of authenticating users to your cluster. An EKS cluster has an (OIDC) issuer URL associated with it. To use AWS Identity and Access Management (IAM) roles with service accounts, an IAM OIDC provider must exist for the cluster. Create an OIDC provider for the cluster using eksctl or the AWS Management Console. (Reference: https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/enable-iam-roles-for-service-accounts.html)\nFollow the official documentation steps to create an IAM role, create a Kubernetes service account named aws-load-balancer-controller in the kube-system namespace of AWS Load Balancer Controller, and annotate the Kubernetes service account with the name of the IAM role. Install the AWS Load Balancer Controller using helm. (Reference: https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/aws-load-balancer-controller.html)\n\nUse Helm to deploy Dolphinscheduler\n\nDownload the source package apache-dolphinscheduler--src.tar.gz version 3.1.2 (https://dolphinscheduler.apache.org/zh-cn/download/3.1.2). Unzip it on the jump server.\n\n$ tar -zxvf apache-dolphinscheduler-&lt;version&gt;-src.tar.gz\n$ cd apache-dolphinscheduler-&lt;version&gt;-\nsrc/deploy/kubernetes/dolphinscheduler\n\n\nModify the configuration file values.yaml\n\n##Modify the mirror warehouse address to AWS ecr\nimage:\n   registry: &quot;xxxxxx.dkr.ecr.us-east-1.amazonaws.com&quot; -- ECR mirror address\n   tag: &quot;3.1.2&quot;\n## Use an external data source\npostgresql:\n   enabled: false\nmysql:\n   enabled: false\nexternalDatabase:\n   type: &quot;postgresql&quot;\n   host: &quot;dolphinscheduler.cluster-xxxxx.us-east-1.rds.amazonaws.com&quot;\n   port: &quot;5432&quot;\n   username: &quot;postgres&quot;\n   password: &quot;xxxxxxxx&quot;\n   database: &quot;dolphinscheduler&quot;\n   params: &quot;characterEncoding=utf8&quot;\n  ## Use S3 to store resource files\nconf:\n   common:\n     resource.storage.type: S3\n     resource.aws.access.key.id: xxxxxxx\n     resource.aws.secret.access.key: xxxxxxxxx\n     resource.aws.region: us-east-1\n     resource.aws.s3.bucket.name: dolphinscheduler-resourse\n     resource.aws.s3.endpoint: https://S3.us-east-1.amazonaws.com\n\n\n\nSet resource requirements for alert, api, worker, and\n\nmaster services:\nmaster:\n   resources:\n     limits:\n       memory: &quot;8Gi&quot;\n       cpu: &quot;4&quot;\n     requests:\n       memory: &quot;2Gi&quot;\n       cpu: &quot;500m&quot;\nworker:\n   resources:\n     limits:\n       memory: &quot;8Gi&quot;\n       cpu: &quot;4&quot;\n     requests:\n       memory: &quot;2Gi&quot;\n       cpu: &quot;500m&quot;\napi:\n   ...\nalert:\n   ...\n\n\nCreate namespace dolphinscheduler\n\n$ kubectl create namespace dolphinscheduler\n\n\n\nCreate a fargate configuration file, respectively define the Fargate configuration file associated with the namespace dolphinscheduler and kube-system to specify which pods use Fargate at startup, and then schedule pods on Fargate in the cluster. (Reference: https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/fargate-profile.html)\nPublish dolphinscheduler to the namespace of dolphinscheduler\n\n$ cd apache-dolphinscheduler-&lt;version&gt;-src/deploy/kubernetes/dolphinscheduler\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\n$ helm dependency update .\n$ helm install dolphinscheduler . --set image.tag=3.1.2 -n dolphinscheduler --set region=us-east-1 --set vpcId=vpc-xxx\n\n\nCreate a network load balancer to provide external access uri\n\n$ echo &quot;\napiVersion: v1\nkind: Service\nmetadata:\n   namespace: dolphinscheduler\n   name: service-dolphinscheduler\n   annotations:\n     service.beta.kubernetes.io/aws-load-balancer-type: external\n     service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\n     service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n     service.beta.kubernetes.io/subnets: subnet-xxx, subnet-xxx\nspec:\n   ports:\n     - port: 12345\n       targetPort: 12345\n       protocol: TCP\n   type: LoadBalancer\n   selector:\n     app.kubernetes.io/name: dolphinscheduler-api\n&quot; | kubectl apply -f -\n\nGet load balancing dns service address\n$ kubectl get service service-dolphinscheduler -n dolphinscheduler\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nsample-service LoadBalancer 10.9.240.137 k8s-nlbsampl-nlbsampl-xxxxxxxxxx-xxxxxxxxxxxxxxxxx.elb.region-code.amazonaws.com 12345:32400/TCP 16h\n\n\nVisit dolphinscheduler address: http://k8s-nlbsampl-nlbsampl-xxxxxxxxxx-xxxxxxxxxxxxxxxxx.elb.region-code.amazonaws.com:12345/dolphinscheduler/ui\npicture\npicture\nConnect to Amazon Athena data source test\n\nInstall the Athena JDBC driver to the API server and worker server, create a DockerFile to rebuild the image, and push it to the ECR warehouse.\n\n##Example worker image DokcerFile\nFROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-worker:3.1.2\nRUN apt-get update &amp;&amp; \\\n     apt-get install -y --no-install-recommends python3 &amp;&amp; \\\n     apt-get install -y --no-install-recommends python3-pip &amp;&amp; \\\n     rm -rf /var/lib/apt/lists/*\nRUN cd /opt/dolphinscheduler/libs/ &amp;&amp; \\\n     wget https://s3.cn-north-1.amazonaws.com.cn/athena-downloads-cn/drivers/JDBC/SimbaAthenaJDBC-2.0.31.1000/AthenaJDBC42.jar\n\n\nUpdate dolphinscheduler\n\nhelm upgrade dolphinscheduler\n\n\n\nCreate Athena connection and test\npicture\nExecute the workflow to view the log\npicture\n\nFAQ\nHow to install dependency packages and plug-ins?\nInstalling dependency packages by re-editing the image, usually, you only need to update the worker server image. The instance refers to Section 2.5.\nHow to expand and shrink nodes?\nExecute the kubectl command on the jump server to expand and shrink\n## Scale up and down api to 3 replicas\nkubectl scale --replicas=3 deploy dolphinscheduler-api -n dolphinscheduler\n## Scale master to 2 replicas\nkubectl scale --replicas=2 sts dolphinscheduler-master -n dolphinscheduler\n## Scale worker to 2 replicas\nkubectl scale --replicas=6 sts dolphinscheduler-worker -n dolphinscheduler\n\nHow to make service storage persistent?\n\nInstall the EFS CSI driver (reference: https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/efs-csi.html)\nCreate efs file system and access point (reference: https://docs.aws.amazon.com/zh_cn/efs/latest/ug/creating-using.html)\nCreate a PersistentVolume\n\necho &quot;\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n   name: dolphin-efs-pv\nspec:\n   capacity:\n     storage: 100Gi\n   volumeMode: Filesystem\n   accessModes:\n     - ReadWriteMany\n   persistentVolumeReclaimPolicy: Retain\n   storageClassName: efs-sc\n   csi:\n     driver: efs.csi.aws.com\n     volumeHandle: fs-xxx::fsap-xxx // fsap\n&quot; | kubectl apply -f -\n\n\n\nModify the values.yaml and template/pvc-xx.yaml files, enable service persistent storage and associate PersistentVolume\n\nsharedStoragePersistence: enabled: true mountPath: &quot;/opt/soft&quot; accessModes: - &quot;ReadWriteMany&quot; ## storageClassName must support the access mode: ReadWriteMany storageClassName: &quot;efs-sc&quot; storage: &quot;20Gi&quot;\n\n{{- if .Values.common.sharedStoragePersistence.enabled }}\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n   name: {{ include &quot;dolphinscheduler.fullname&quot; . }}-shared\n   labels:\n     app.kubernetes.io/name: {{ include &quot;dolphinscheduler.fullname&quot; . }}-shared\n     {{- include &quot;dolphinscheduler.common.labels&quot; . | nindent 4 }}\n   annotations:\n     &quot;helm.sh/resource-policy&quot;: keep\nspec:\n   accessModes:\n   {{- range.Values.common.sharedStoragePersistence.accessModes }}\n     - {{ . | quote }}\n   {{- end }}\n   storageClassName: {{ .Values.common.sharedStoragePersistence.storageClassName | quote }}\n   volumeName: dolphin-efs-pv\n   resources:\n     requests:\n       storage: {{ .Values.common.sharedStoragePersistence.storage | quote }}\n{{- end }}\n\n\nUse helm to deploy or update.\n\nWhere can I get support?\n\nFor AWS platform services, seek expert guidance through AWS Support https://aws.amazon.com/cn/premiumsupport/\nCommunicate about DolphinScheduler through the GitHub issue https://github.com/apache/dolphinscheduler\n\nReference link\n\nDolphinschduler architecture design\nEKS subnet tag solution: https://aws.amazon.com/cn/premiumsupport/knowledge-center/eks-load-balancer-controller-subnets/\nRunning stateful workloads with Amazon EKS on AWS Fargate using Amazon EFS: https://aws.amazon.com/blogs/containers/running-stateful-workloads-with-amazon-eks-on-aws-fargate-using-amazon-efs /\nServerless on AWS: https://aws.amazon.com/cn/serverless/\n\n",
    "title": "Deploy the serverless Apache DolphinScheduler task scheduling system on AWS",
    "time": "2022-12-16"
  },
  {
    "name": "DolphinScheduler-Vulnerability-Explanation",
    "content": "[Security Notice] [Low:impact] DolphinScheduler Vulnerability Explanation\nThe Apache DolphinScheduler community mailing list recently reported a vulnerability. Considering that many users have not subscribed to this mailing list, we hereby explain the situation:\nCVE-2021-27644\nImportance: Low\nScope of impact: The exposed service is on the external network and the internal account is leaked. If none of the above, the user can decide whether to upgrade according to the actual demand.\nAffected version: &lt;1.3.6\nVulnerability description:\nThis problem is caused by a vulnerability in mysql connectorj. Logged-in users of DolphinScheduler (users who are not logged in cannot perform this operation. It is recommended that companies conduct account security specifications)Â  can fill in malicious parameters that cause security risks on the data source management page-Mysql data source. (Not affected if Mysql data source is not used)\nRepair suggestion: upgrade to version &gt;=1.3.6\nSpecial thanks to\nSpecial thanks to the reporter of the vulnerability: Jin Chen from the Ant Security FG Lab, who restored the process of the vulnerability and provided the corresponding solution. The whole process showed the skills and expertise of professional security personnel, thanks for their contributions to the security guard of open source projects.\nSuggest\nThanks to users for choosing Apache DolphinScheduler as the big data task scheduling system in enterprises, but it must be reminded that the scheduling system belongs to the core infrastructure of big data construction, please do not expose it to the external network. In addition, security measures should be taken for the account of internal personnel in the enterprise to reduce the risk of account leakage.\nContribute\nSo far, the Apache DolphinScheduler community has nearly 200+ code contributors and 70+ non-code contributors. Among them, there are also PMC or Committer of other top Apache projects. We embrace more partners to participate in the development of the open source community, working together to build a more stable, safe and reliable big data task scheduling system, and also contributing yourself to the rise of China's open source!\nWebSite: https://dolphinscheduler.apache.org/\nMailList: dev@dolphinscheduler@apache.org\nTwitter: @DolphinSchedule\nYouTube: https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\nSlack: https://s.apache.org/dolphinscheduler-slack\nContributor Guide: https://dolphinscheduler.apache.org/en-us/community\nIf you have any questions about the vulnerability, welcome to participate in the discussion and we will wholeheartedly resolve your problems.\n",
    "title": "DolphinScheduler Vulnerability Explanation",
    "time": "2021-10-26"
  },
  {
    "name": "DolphinScheduler_Cisco_Webex_k8s_Integration_Practice_Boosting_Big_Data_Processing_Efficiency",
    "content": "DolphinSchedulerâï¸Cisco Webex: k8s Integration Practice, Boosting Big Data Processing Efficiency!\nSummary: Cisco Webex is a software company that develops and sells online meeting, video conferencing, cloud calling services, and contact center as a service applications. The team has designed and built a big data platform to serve data ingestion and workload data processing for their suite of products. Taking the Webex Meeting product as an example, Webex Meetings generate various metrics. When a meeting is held, both the client and server send numerous metrics and logs to the Kafka cluster, and external and internal customers rely on these metrics to optimize the meeting experience or generate reports. In big data business data processing, Cisco Webex not only achieves efficient scheduling of various jobs through Apache DolphinScheduler, but also accomplishes goals such as data lineage and metadata management, data governance, and daily data integration.\nBusiness Challenges\nSince Cisco Webex is a global collaboration service provider with customers spanning multiple time zones and continents, it has numerous data centers worldwide. These data centers include locally self-managed data centers and clusters managed by cloud providers such as Amazon and Google. In the past, Cisco Webex would use mirroring to aggregate all data from global data centers into a centralized Kafka cluster in the United States, and from there, data processing and integration would begin.\nIn recent years, Cisco Webex has established multiple clusters worldwide for data localization. The data model has shifted from a centralized cluster containing data from all around the world to individual data centers containing locally generated data.\nAnother issue that Cisco Webex's next-generation data platform aims to address is &quot;data silos.&quot; Since different types of services run on different infrastructures, each product has its own data ingestion and data platform implementation, and data sources are diverse with numerous data storage formats. This makes it difficult to provide customers with a unified data source and ensure consistency between different systems.\nCisco Webex's vision is to create a data platform that can serve every internal and external customer, eliminating data silos from a unified architecture, data storage, and data ingestion technology, and integrating all infrastructures. Furthermore, this data platform must be able to adapt to any public cloud and existing private data center within the architecture.\nSolution\n1. DolphinScheduler and k8s Integration\n\nAs shown in the architecture diagram, the left part represents DolphinScheduler's features. Different task types run on these workers. All data processing jobs, such as Flink and Sparks, used to run on multiple separate Yarn clusters. We had a CDH cluster for batch Spark jobs and Flink jobs, and multiple Flink jobs ran on different Flink clusters. In 2021, we decided to build a Kubernetes cluster to replace the Yarn clusters for the following reasons.\n\nUsing Kubernetes makes our daily operations smoother and easier.\nThe second reason for adopting Kubernetes is that it allows us to deploy various containerized services.\n\nAs a result, Cisco Webex built a Kubernetes cluster to replace the Yarn clusters, allowing all data processing jobs to run on the Kubernetes cluster, extending DolphinScheduler's capabilities, and integrating Flink, Spark, and Kubernetes features with DolphinScheduler.\n2. Multi-cluster ETL Job Management\n\nA typical use case for Cisco Webex is deploying the same job on multiple clusters. To minimize deployment work, the approach is to generalize the common processing logic and replace the required configurations for each cluster, enabling one-click development for multiple clusters. Cisco Webex uses a centralized DolphinScheduler as the job scheduling platform for all data processing jobs, running jobs in different data centers. When users submit new jobs to different clusters, DolphinScheduler will distribute the instances and files to the target cluster and run the job based on the user's selection. The resources for running jobs are managed on DolphinScheduler, allowing CPU memory limits to be set for each Namespace on different Kubernetes clusters. In addition, Cisco Webex also adds the commonly used Webex teams alert plugin as a plugin to DolphinScheduler for tracking issues when errors occur.\n3. Kubernetes Multi-cluster Management\n\nCisco Webex has built many Kubernetes compute clusters in private data centers worldwide or in public clouds like AWS. To enable DolphinScheduler to submit and manage jobs for data centers spread across the globe, Cisco Webex has implemented features such as cluster management and namespaces on DolphinScheduler.\n4. Simple ETL Pipeline Drag-and-Drop Generation Framework\n\nFor simple processing jobs without complex competitive logic, Cisco Webex has developed a drag-and-drop pipeline generation framework on DolphinScheduler.\nUsers can generate complex real-time data processing pipelines by dragging and dropping on the canvas. By configuring predefined source filters, mappings, and sync operators, users don't need to write any code. Notably, Cisco Webex has also integrated metadata into the data center for source and map operators to use. As a result, when users choose the topics they want to process, the job list they see comes from the API data in the data center. Users don't need to type in names and Kafka cluster configuration strings on the interface but instead get them automatically from the data center. In map operators, users can define different types of functions for each field.\n5. Flink Jobs on Kubernetes\n\nCisco Webex has also built Flink jobs in DolphinScheduler based on Kubernetes features. Some people might be confused because there is already a Flink task port in the DolphinScheduler workflow. This is because the Flink task in DolphinScheduler only applies to Yarn, but we intend to run all jobs on Kubernetes clusters. We achieved Flink job execution on Kubernetes by adding Kubernetes-related APIs to the current DolphinScheduler architecture.\nUser Benefits\n\nBuilt the next-generation data platform based on DolphinScheduler, enabling running all types of jobs on a single platform;\nBroke down Cisco Webex data silos, connected global data centers, integrated any public cloud and existing private data centers, ensuring consistency across systems;\nRan all data processing jobs on Kubernetes clusters, reducing operational and maintenance costs.\n\nUser Profile\nSan Francisco-based Cisco Webex (WebEx) is a subsidiary of Cisco, a software company that develops and sells online meetings, video conferencing, cloud calling services, and contact center as service applications, creating on-demand software solutions for companies of various sizes.\n",
    "title": "DolphinSchedulerâï¸Cisco Webex: k8s Integration Practice, Boosting Big Data Processing Efficiency!",
    "time": "2022-3-29"
  },
  {
    "name": "DolphinScheduler_Kubernetes_Technology_in_action",
    "content": "Technical Practice of Apache DolphinScheduler in Kubernetes System\n\n\n\nAuthor | Yang Dian, Data and Algorithm Platform Architect | Shenzhen Transportation Center\nEditor | warrior_\n\nEditor's noteï¼\n\n\nKubernetes is a cluster system based on container technology, implements container orchestration, provides microservices and buses, and involves a large number of knowledge systems.\n\n\nStarting from the author's actual work experience, this article shows us the use and technology sharing of DolphinScheduler in practice scenarios, hoping it can inspire those who have the same experience.\n\nWhy do we use DolphinSchedule? What value does it bring to us? And what problems did we encounter?\nApache DolphinScheduler is an excellent distributed and easily scalable visual workflow task scheduling platform.\nIn the field I'm working in, the application of DolphinScheduler can quickly solve the top ten pain points of data development for enterprises:\n\nMulti-source data connection and access, most common data sources in the technical field can be accessed, and adding new data sources does not require too many changes;\nDiversified +specialized + massive data task management, which really revolves problems focusing on big data (Hadoop family, Flink, etc.) task scheduling, and is significantly different from traditional schedulers;\nGraphical task arrangement, super convenient user experience, competitive ability with commercial products, especially to most foreign open source products that cannot directly generate data tasks by dragging and dropping;\nTask details, rich viewing of tasks, log, and time-running axis display, which well meet developers' requirement of refined data tasks management, quickly locating slow SQL and performance bottlenecks;\nSupport for a variety of distributed file systems, enrich users' choices for unstructured data;\nNative multi-tenant management to meet the data task management and isolation requirements of large organizations;\nFully automatic distributed scheduling algorithm to balance all scheduling tasks;\nThe native function of cluster monitoring, which can monitor CPU, memory, number of connections, Zookeeper status is suitable for one-stop operation and maintenance for SME;\nThe native task alarm function, which minimizes the risk of task operation;\nThe strong community-based operation, listening to the real voice of customers, constantly adding new functions, and continuously optimizing the customer experience.\n\nI also encountered many new challenges in the projects launching various types of Apache DolphinScheduler:\n\nHow to deploy Apache DolphinScheduler with less human resources, and can a fully automatic cluster installation and deployment mode be realized?\nHow to standardize technical component implementation specifications?\nCan unmanned supervision and system self-healing be achieved?\nHow to install and update the air-gap mode under the network security control?\nCan it automatically expand without disturbing?\nHow to build and integrate the monitoring system?\n\nTo solve the above challenges, we repeatedly integrated Apache DolphinScheduler into the existing Kubernetes cloud-native system to tackle problems and make Apache DolphinScheduler technology more powerful.\nKubernetes Technical System Bring New Technical Features to Apache DolphinScheduler\nAfter using Kubernetes to manage Apache DolphinScheduler, the overall technical solutions are quickly enriched and efficient technical features added, by which the above practical challenges tackled quickly:\n\nThe development environment and production environment was rapidly established in various independent deployment projects, and all can be implemented by one-key deployment and one-key upgrade;\nOverall supports offline installation, and the installation speed is faster;\nUnify the installation configuration information as much as possible to reduce the abnormal configuration of multiple projects. All configuration items can be managed through the internal git of the enterprise based on different projects;\nCombine with object storage technology to unify unstructured data technology;\nThe convenient monitoring system is integrated with the existing prometheus monitoring system;\nMixed-use of multiple schedulers;\nFully automatic resource scheduling capability;\nFast self-healing ability, automatic abnormal restart, and restart mode based on probe mode.\n\nThe cases in this article are based on Apache DolphinScheduler version 1.3.9.\nAutomated and Efficient Deployment Based on Helm tools\nFirst, let's introduce the installation method based on the Helm provided by the official website. Helm is the best way to find, share and use software to build Kubernetes, which is one of the graduate projects of cloud-native CNCF.\n\n\n\nThere are very detailed configuration files and cases on Apache DolphinScheduler official website and GitHub. Here Iâll highlight some of the FAQs in the community.\nOfficial website document address\nhttps://dolphinscheduler.apache.org/en-us/docs/1.3.9/kubernetes-deployment\nGitHub folder address\nhttps://github.com/apache/dolphinscheduler/tree/1.3.9/docker/kubernetes/dolphinscheduler\nModify the image in the value.yaml file for offline installation (air-gap install);\nimage:\nrepository: &quot;apache/dolphinscheduler&quot;\ntag: &quot;1.3.9&quot;\npullPolicy: &quot;IfNotPresent&quot;\n\nPull, tag, and push for harbors installed in the company or private warehouses of other public clouds. Here we assume that the private warehouse address is harbor.abc.com, the host where the image is built has been docker login harbor.abc.com, and the new apache project under the private warehouse has been established and authorized.\nexecute shell commands\ndocker pull apache/dolphinscheduler:1.3.9\ndock tag apache/dolphinscheduler:1.3.9\nharbor.abc.com/apache/dolphinscheduler:1.3.9\ndocker push apache/dolphinscheduler:1.3.9\n\nThen replace the image information in the value file. Here we recommend using the Always method to pull the image. In the production environment, try to check whether it is the latest image content every time to ensure the correctness of the software product. In addition, many coders are used to writing the tag as latest, and making the image without adding the tag information, which is very dangerous in the production environment. Because the image of the latest will be changed once anyone pushes the image and it is impossible to judge the version of the latest. So, it is recommended to be clear about the tags of each release, and use Always.\nimage:\nrepository: &quot;harbor.abc.com/apache/dolphinscheduler&quot;\ntag: &quot;1.3.9&quot;\npullPolicy: &quot;Always&quot;\n\nCopy the entire directory of https://github.com/apache/dolphinscheduler/tree/1.3.9/docker/kubernetes/dolphinscheduler to a host that can execute the Helm command, and then execute\nkubectl create ns ds139\nhelm install dolphinscheduler . -n ds139 following the official website instruction to install offline.\n\nto install offline.\n\nTo integrate DataX, MySQL, Oracle client components, first download the following components:\nhttps://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.49/mysql-connector-java-5.1.49.jar\nhttps://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/\nhttps://github.com/alibaba/DataX/blob/master/userGuid.md Compile\n\nbuild and compile according to the prompt, and the file package is located in {DataX_source_code_home}/target/datax/datax/\nCreate a new dockerfile based on the above plugin components, and the image that has been pushed to the private warehouse can be applied to the basic image.\nFROM harbor.abc.com/apache/dolphinscheduler:1.3.9\nCOPY *.jar /opt/dolphinscheduler/lib/\nRUN mkdir -p /opt/soft/datax\nCOPY datax /opt/soft/datax\n\nSave the dockerfile and execute the shell command\ndocker build -t harbor.abc.com/apache/dolphinscheduler:1.3.9-mysql-oracle-datax . #Don't forget the last point\ndocker push harbor.abc.com/apache/dolphinscheduler:1.3.9-mysql-oracle-datax\n\nModify the value file\nimage:\nrepository: &quot;harbor.abc.com/apache/dolphinscheduler&quot;\ntag: &quot;1.3.9-mysql-oracle-datax&quot;\npullPolicy: &quot;Always&quot;\n\nExecute helm install dolphinscheduler . -n ds139\nor helm upgrade dolphinscheduler -n ds139, or firstly helm uninstall dolphinscheduler -n ds139, and then execute helm install dolphinscheduler . -n ds139.\n\nGenerally, it is recommended to use an independent external PostgreSQL as the management database in the production environment, and use the independently installed Zookeeper environment (I used the Zookeeper operator https://github.com/pravega/zookeeper-operator in this case, which is scheduled in the same Kubernetes cluster as Apache DolphinScheduler ). We found that after using the external database, completely deleting and redeploying the Apache DolphinScheduler in Kubernetes, the task data, tenant data, user data, etc. are retained, which once again verifies the high availability of the system and data integrity. (If the pvc is deleted, the historical job log will be lost)\n\n## If not exists external database, by default, Dolphinscheduler's database will use it.\npostgresql:\nenabled: false\npostgresqlUsername: &quot;root&quot;\npostgresqlPassword: &quot;root&quot;\npostgresqlDatabase: &quot;dolphinscheduler&quot;\npersistence:\n  enabled: false\n  size: &quot;20Gi&quot;\n  storageClass: &quot;-&quot;\n\n## If exists external database, and set postgresql.enable value to false.\n## external database will be used, otherwise Dolphinscheduler's database will be used.\nexternalDatabase:\ntype: &quot;postgresql&quot;\ndriver: &quot;org.postgresql.Driver&quot;\nhost: &quot;192.168.1.100&quot;\nport: &quot;5432&quot;\nusername: &quot;admin&quot;\npassword: &quot;password&quot;\ndatabase: &quot;dolphinscheduler&quot;\nparams: &quot;characterEncoding=utf8&quot;\n\n## If not exists external zookeeper, by default, Dolphinscheduler's zookeeper will use it.\nzookeeper:\nenabled: false\nfourlwCommandsWhitelist: &quot;srvr,ruok,wchs,cons&quot;\npersistence:\n  enabled: false\n  size: &quot;20Gi&quot;\n  storageClass: &quot;storage-nfs&quot;\nzookeeperRoot: &quot;/dolphinscheduler&quot;\n\n## If exists external zookeeper, and set zookeeper.enable value to false.\n## If zookeeper.enable is false, Dolphinscheduler's zookeeper will use it.\nexternalZookeeper:\nzookeeperQuorum: &quot;zookeeper-0.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.zookeeper.svc.cluster.local:2181&quot;\nzookeeperRoot: &quot;/dolphinscheduler&quot;\n\nHow to deploy GitOps based on Argo CD\nArgo CD is a declarative GitOps continuous delivery tool based on Kubernetes. GitOps is an incubation project of CNCF and a best practice tool for GitOps. For more details of GitOps, please refer to https://about.gitlab.com/topics/gitops/\n\n\n\nGitOps can bring the following advantages to the implementation of Apache DolphinScheduler.\n\nGraphical &amp; one-click installation of clustered software;\nGit records the full release process, one-click rollback;\nConvenient DolphinScheduler tool log viewing.\n\nImplementation installation steps using Argo CD:\n\nDownload the Apache DolphinScheduler source code from GitHub, modify the value file, and refer to the content that needs to be modified in the helm installation in the previous chapter;\nCreate a new git project in the modified source code directory, and push it to the company's internal GitLab. The directory name of the GitHub source code is docker/kubernetes/dolphinscheduler;\nConfigure GitLab information in Argo CD, we use https mode here;\n\n\n\n\n\nArgo CD Create a new deployment project and fill in the relevant information\n\n\n\n\n\n\n\nRefresh and pull the deployment information in git to complete the final deployment work. You can see that pod, configmap, secret, service, ingress and other resources are automatically pulled up, and Argo CD displays the commit information and submitter username used by git push before, which completely records all release event information. At the same time, you can also roll back to the historical version with one click.\n\n\n\n\n\n\n\nRelevant resource information can be seen through the kubectl command;\n\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME READY STATUS RESTARTS AGE\ndolphinscheduler-alert-96c74dc84-72cc9 1/1 Running 0 22m\ndolphinscheduler-api-78db664b7b-gsltq 1/1 Running 0 22m\ndolphinscheduler-master-0 1/1 Running 0 22m\ndolphinscheduler-master-1 1/1 Running 0 22m\ndolphinscheduler-master-2 1/1 Running 0 22m\ndolphinscheduler-worker-0 1/1 Running 0 22m\ndolphinscheduler-worker-1 1/1 Running 0 22m\ndolphinscheduler-worker-2 1/1 Running 0 22m\n\n[root@tpk8s-master01 ~]# kubectl get statefulset -n ds139\nNAME READY AGE\ndolphinscheduler-master 3/3 22m\ndolphinscheduler-worker 3/3 22m\n\n[root@tpk8s-master01 ~]# kubectl get cm -n ds139\nNAME DATA AGE\ndolphinscheduler-alert 15 23m\ndolphinscheduler-api 1 23m\ndolphinscheduler-common 29 23m\ndolphinscheduler-master 10 23m\ndolphinscheduler-worker 7 23m\n\n[root@tpk8s-master01 ~]# kubectl get service -n ds139\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\ndolphinscheduler-api ClusterIP 10.43.238.5 &lt;none&gt; 12345/TCP 23m\ndolphinscheduler-master-headless ClusterIP None &lt;none&gt; 5678/TCP 23m\ndolphinscheduler-worker-headless ClusterIP None &lt;none&gt; 1234/TCP,50051/TCP 23m\n\n[root@tpk8s-master01 ~]# kubectl get ingress -n ds139\nNAME CLASS HOSTS ADDRESS\ndolphinscheduler &lt;none&gt; ds139.abc.com\n\n\nYou can see that all pods are scattered on different hosts in the Kubernetes cluster, for example, workers 1 and 2 are on different nodes.\n\n\n\n\n\n\n\nWe have configured ingress, and the company can easily use the domain name for access by configuring the pan-domain name within the company;\n\n\n\nYou can log in to the domain name for accessï¼\nhttp:ds.139.abc.com/dolphinscheduler/ui/home\n\nThe specific configuration can modify the content in the value file:\n\ningress:\nenabled: true\nhost: &quot;ds139.abc.com&quot;\npath: &quot;/dolphinscheduler&quot;\ntls:\n  enabled: false\n  secretName: &quot;dolphinscheduler-tls&quot;\n\n\n\nIt is convenient to view the internal logs of each component of Apache DolphinScheduler:\n\n\n\n\n\nCheck the deployed system, 3 masters, 3 workers, and Zookeeper are all configured normally;\n\n\n\n\n\n\n\n\nUsing Argo CD, it is very convenient to modify the number of replicas of components such as master, worker, api, alert, etc. Apache DolphinScheduler's helm configuration also reserves the setting information of CPU and memory. Here we modify the copy value in value. After modification, git push it to the company's internal GitLab.\n\nmaster:\n\n ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\npodManagementPolicy: &quot;Parallel&quot;\n ## Replicas is the desired number of replicas of the given Template.\nreplicas: &quot;5&quot;\n\nworker:\n ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\npodManagementPolicy: &quot;Parallel&quot;\n ## Replicas is the desired number of replicas of the given Template.\nreplicas: &quot;5&quot;\n\n\nalert:\n ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\nreplicas: &quot;3&quot;\n\napi:\n ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\nreplicas: &quot;3&quot;\n\n\nJust click sync on Argo CD to synchronize, and the corresponding pods will be added as required\n\n\n\n\n\n\n\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME                  READY STATUS RESTARTS AGE\ndolphinscheduler-alert-96c74dc84-72cc9 1/1  Running 0    43m\ndolphinscheduler-alert-96c74dc84-j6zdh 1/1  Running 0    2m27s\ndolphinscheduler-alert-96c74dc84-rn9wb 1/1  Running 0    2m27s\ndolphinscheduler-api-78db664b7b-6j8rj 1/1  Running 0    2m27s\ndolphinscheduler-api-78db664b7b-bsdgv 1/1  Running 0    2m27s\ndolphinscheduler-api-78db664b7b-gsltq 1/1  Running 0    43m\ndolphinscheduler-master-0       1/1  Running 0    43m\ndolphinscheduler-master-1       1/1  Running 0    43m\ndolphinscheduler-master-2       1/1  Running 0    43m\ndolphinscheduler-master-3       1/1  Running 0    2m27s\ndolphinscheduler-master-4       1/1  Running 0    2m27s\ndolphinscheduler-worker-0       1/1  Running 0    43m\ndolphinscheduler-worker-1       1/1  Running 0    43m\ndolphinscheduler-worker-2       1/1  Running 0    43m\ndolphinscheduler-worker-3       1/1  Running 0    2m27s\ndolphinscheduler-worker-4       1/1  Running 0    2m27s\n\n\nApache DolphinScheduler integrates with S3 Object Storage Technology\nHow to configure the integration of s3 minio is one of the FAQs in the community. Here is the method of helm configuration based on Kubernetes.\n\nModify the s3 part of value, it is recommended to use ip+port to point to the minio server.\n\ncommon:\n ##Configmap\nconfigmap:\n  DOLPHINSCHEDULER_OPTS: &quot;&quot;\n  DATA_BASEDIR_PATH: &quot;/tmp/dolphinscheduler&quot;\n  RESOURCE_STORAGE_TYPE: &quot;S3&quot;\n  RESOURCE_UPLOAD_PATH: &quot;/dolphinscheduler&quot;\n  FS_DEFAULT_FS: &quot;s3a://dfs&quot;\n  FS_S3A_ENDPOINT: &quot;http://192.168.1.100:9000&quot;\n  FS_S3A_ACCESS_KEY: &quot;admin&quot;\n  FS_S3A_SECRET_KEY: &quot;password&quot;\n\n\nThe name of the bucket that stores dolphin files in minio is dolphinscheduler. I create new folders and files for testing here. The directory of the minio is under the tenant of the upload operation.\n\n\n\n\nApache DolphinScheduler Integrates with Kube-Prometheus Technology\n\n\nWe use the Kube-prometheus operator technology in Kubernetes to automatically monitor the resources of each component of Apache DolphinScheduler after deploying.\n\n\nPlease pay attention to that the version of kube-prometheus needs to correspond to the major version of Kubernetes. https://github.com/prometheus-operator/kube-prometheus\n\n\n\n\n\n\n\n\n\n\n\nTechnical Integration of Apache DolphinScheduler and Service Mesh\n\nThrough the service mesh technology, the observability analysis of API external service calls and internal calls of Apache DolphinScheduler can be realized to optimize the Apache DolphinScheduler product services.\n\nWe use linkerd as a service mesh product for integration, which is also one of CNCF's excellent graduate projects.\n\n\n\nJust modify the annotations in the value file of the Apache DolphinScheduler helm and redeploy, you can quickly inject the mesh proxy sidecar, as well as master, worker, API, alert and other components.\nannotations: #{}\n   linkerd.io/inject: enabled\n\nYou can observe the quality of service communication between components, the number of requests per second, etc.\n\n\n\n\n\n\nProspects on Apache DolphinScheduler Based on cloud-native Technology\nAs a new-generation Cloud-Native big data tool, Apache DolphinScheduler is expected to integrate more excellent tools and features in the Kubernetes ecosystem in the future to meet more requirements of diversified user groups and scenarios.\n\nIntegration with Argo-workflow, users can call Argo-workflow single job, dag job, and periodic job in Apache DolphinScheduler through api, cli, etc.;\nUse HPA to automatically expand and shrink workers to achieve unattended horizontal expansion;\nIntegrate the Spark operator and Flink operator tools of Kubernetes to achieve comprehensive cloud-native;\nImplement multi-cloud and multi-cluster distributed job scheduling, and strengthen the architectural attributes of serverless+faas classes;\nUse sidecar to periodically delete worker job logs to realize carefree operation and maintenance;\n\nFinally, I strongly recommend you to use Slack to communicate with the Apache DolphinScheduler community, which is officially recommendedï¼\n\n\n\n",
    "title": "Technical Practice of Apache DolphinScheduler in Kubernetes System",
    "time": "2022-2-24"
  },
  {
    "name": "DolphinScheduler_in_T3Go_One-stop_Platform",
    "content": "ApacheCon Asia 2022 Review | Application of DolphinScheduler in T3Go One-stop Platform\nAt the ApacheCon Asia 2022, big data engineers Li Xinkai &amp; Zhao Yuwei at T3Go shared the companyâs functional iteration and integration enhancement based on Apache DolphinScheduler in their development of a one-stop platform.\nThis sharing mainly focuses on the following four parts:\n\nWhy we choose Apache DolphinScheduler\nData Lake Architecture of T3Go\nDifficulties in using Apache DolphinScheduler\nSolutions to the issues\n\nWhy we choose Apache DolphinScheduler\nWhat is DolphinScheduler\nApache DolphinScheduler is a distributed, decentralized, and easily scalable visual DAG workflow task scheduling platform committed to dealing with the intricate dependencies in the data process so that the scheduling system can be used out of the box.\n02 Why we choose DolphinScheduler\n\nWe choose Apache DolphinScheduler mainly based on the following four reasons:\n\nHigh availability\nApache DolphinScheduler provides decentralized multi-worker and master with higher availability, self-support, and overload handling.\nUser-friendly\nSince all process definitions are visualized, Apache DolphinScheduler reduces learning costs by enabling people with no coding skills (such as data analysts) to create complex workflows.\nRich scenarios\nApache DolphinScheduler provides multiple task types, including Spark, Hive, Python, Flink, MapReduce, and more. Additionally, the multi-tenancy of the platform increases efficiency and provides a high level of scalability.\nHigh scalability\nApache DolphinScheduler can linearly increase its overall scheduling capability with the rising volume of the cluster.\n\n03 Status of use of DolphinScheduler in T3Go\n\nApache DolphinScheduler in T3Go is mainly applied to combine with Kyuubi on Spark tasks to process 30,000+ offline scheduling tasks every day; at the same time, it schedules 300+ Spark Streaming tasks, 100+ Flink tasks, and 500+ Kylin, ClickHouse, and Shell tasks every day.\n04 Optimization based on DolphinScheduler\nA huge volume of scheduling tasks of T3Go heavily relies on Apache DolphinScheduler. This time, weâd like to share the following optimization work we had done based on it:\n\n\nAdded the function of linkage complement. Our task flow handles some upstream and downstream relationships across task flows through depends components. The relationship between these tasks is relatively complex. If the upstream complement is required, it is very complicated to sort out the upstream and downstream relations manually. So we made the linkage complement function.\n\n\n\nAdded task view function. For example, if a workflow is set to schedule on T-1, then a work instance with the running type of scheduling execution will be generated every day. After adding the task view function, you can see the operation of the workflow in the scheduling cycle at a glance, such as whether the scheduling is successful, whether there is a failure complement, recovery failure, etc., to improve development efficiency.\n\nData Lake Architecture of T3 Go\nNext, I will introduce the data lake architecture of T3 travel.\n01 Data Lake Architecture\n\nThe bottom-level data sources in the data lake architecture of T3Go are mainly Kafka data and some database data subscribed by cannel. The data obtained by CDC is mainly stored in OBS object storage in Hudi format, and the unified resource arrangement of the cluster is performed through YARN. Computing engines such as Spark, Hive, Flink, Presto, and ClickHouse are integrated uniformly on the top of YARN and are connected to various businesses through the computing middleware of Kyuubi and Linkis, including Data Map, Hue, machine learning platforms, etc. The ETL operations during the process are mainly scheduled by Apache DolphinScheduler.\nT-1 Scheduling Process\n\nThe scheduling process of T-1 is to allocate different resource policies for different tenants by connecting to Kyuubi. Apache DolphinScheduler connects different tenants to Kyuubi and performs computations on the Spark engine. In addition, we also did incremental data processing for the storage data mainly based on Hudi.\nOther usage scenarios\n\nAt the same time, data development and some OLAP scenarios are mainly performed by connecting to Kyuubi. The reporting system connects to the Presto engine by connecting to Kyuubi to query data lake data. You can also connect to the Spark engine to write Spark SQL for data exploration and data development.\nDifficulties in using Apache DolphinScheduler\nIn the process of using Apache DolphinScheduler, we encountered some difficulties (only for version 1.3), as follows:\n\nNo dedicated development tools\n\nFirstly, there was no dedicated data development module in Apache DolphinScheduler at the time, so we developed it on Hue originally installed on the CDH cluster. Developed statements also need to be manually copied to GitLab for version management.\nLack of big data management CI/CD\n\nSecondly, to publish the developed statement on Apache DolphinScheduler, we needed to manually copy it to Apache DolphinScheduler for node configuration. At the same time, the conversion and configuration of some parameters and variables also require manual operations. The lack of an integrated CI/CD process greatly increases the cost of operation and maintenance.\nNo version management &amp; code sharing mechanism\n\nWe previously used Apache DolphinScheduler version 1.3, which lacked the workflow version management and code sharing mechanism. This results in the inability to submit code to GitLab, version switching, code sharing in the same group and department, and repeated wheel building.\n\nSolutions to the issues\nFor the above issues, we have invested a lot of energy to optimize, mainly covers:\nIntroducing Datasphere Studio\n\nAt the beginning of this year, we introduced the open source Datasphere Studio (DSS) of WeBank as a one-stop application interaction platform and customized development to the demands of the companyâs business.\nCode sharing\n\nWe call the developed files by integrating GitLabâs SDK and submit the code to the corresponding GitLab code branch and project. In this way, users in the same workspace can see and share each otherâs code, which helps to avoid repeated wheels building.\nDSS Workflow\n\nThe DSS workflow supports version management, which is convenient for rollback and version iteration. It supports the pick-up operation for the specified corresponding version. When pulling and arranging the workflow, you can select the associated script (the developed code files placed in those directories).\n\nWe integrated the DSS workflow so that it can be published directly into the Apache DolphinScheduler workflow.\nDSS workflow is similar to Apache DolphinScheduler, which is also in JSON format, stored in MySQL, and read through saved task flow. We parse the JSON fields in the DSS workflow and convert them into JSON format files that conform to the DolphinScheduler task flow.\n\nAfter storage, a task flow that conforms to its style is formed in DolphinScheduler. Now, the node types we supported cover SQL, shell scripts, and depends components, SeaTunnel(watedrop) tasks, Spark tasks and Python, etc.\n\nWe use DSS workflow for version management. In essence, itâs an up-and-down-online process of multiple versions corresponding to the DolphinScheduler task flow. If the online version is selected, the corresponding version workflow on the DS side will go online, and the workflow of the non-selected version will go offline.\nThe compliment, timing schedule, and other operations are performed on DolphinScheduler, which is much more powerful than DSS.\nOf course, the new version of DolphinScheduler already supports the version switching function, and we will integrate it later.\n\nAfter integrating DSS, the changes to the T3Go data lake architecture are shown in the figure.\nFirst, the bottom layer is still CDC in the data lake. Hudi is on top of object storage, resource orchestration is still performed on YARN, and the engine is still acted by Spark, Flink, Presto, and ClickHouse. Connection is still conducted through computing middleware, and we replaced Hue with the DSS scripts data development platform to connect with various business systems. DSSâs single sign-on authentication process facilitates unified login.\nAfter the introduction of DSS in the T3Go big data platform, it complements Apache DolphinScheduler and unifies the integration process of code development and business online scheduling system, manages the big data CI/CD in a closed loop, and lowers the business barriers by helping them focus on business and demands and relieve the pressure of data development. With all these efforts, we narrow the gap to the goal of building a one-stop development platform.\nApache DolphinScheduler has developed rapidly in the past year and enhanced the functions we lacked before, such as data quality and task flow version management. We hope that these capabilities of Apache DolphinScheduler can be applied in our usage scenarios in subsequent iterations, and we also expect Apache DolphinScheduler, as one of the leaders of the scheduling platform, to grow more powerful!\n",
    "title": "ApacheCon Asia 2022 Review | Application of DolphinScheduler in T3Go One-stop Platform",
    "time": "2022-8-30"
  },
  {
    "name": "DolphinScheduler_launches_aws_ami",
    "content": "DolphinScheduler launches on the AWS AMI Application Marketplace\n\nIntroduction\nApache DolphinScheduler has officially launched on the AWS EC2 AMI application marketplace, which means that if you want to use or experience DolphinScheduler in AWS, you can directly use this EC2 AMI to start the DolphinScheduler service. When started, your EC2 instance will start a DolphinScheduler standalone service, which is a complete DolphinScheduler component and can perform various tasks of DolphinScheduler.\nIt should be noted that standalone is not used in the production environment, because all its services are in one process, the metadata is stored in memory by default, and the data will no longer exist when the service is terminated. But it is still very useful because we can quickly start a service for experience verification. If you want to start a DolphinScheduler cluster, you can refer to the Launch Cluster chapter of this article. For more AMI-related information, please check https://aws.amazon.com/marketplace/pp/prodview-cbwnzxolq46yo\nWhy launch in AWS?\nAmazon Web Services (AWS) is the worldâs most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. Millions of customers â including the fastest-growing startups, largest enterprises, and leading government agencies â are using AWS to lower costs, become more agile, and innovate faster.\nWhatâs EC2?\nAmazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up-front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic. Users can choose services that suit their needs on EC2, and can choose task capacity, CPU, and memory models. Before launching EC2 instance, users can choose different operating systems, such as Windows, Linux, and Mac, which are called Amazon Machine Images (AMI) in EC2.\nWhatâs EC2 AMI?\nAs mentioned above, when starting an EC2 instance, you can specify a mirror, which is Amazon Machine Image (AMI). An Amazon Machine Image (AMI) is a supported and maintained image provided by AWS that provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you require multiple instances with the same configuration. You can use different AMIs to launch instances when you require instances with different configurations. An AMI includes the following:\n\nOne or more Amazon Elastic Block Store (Amazon EBS) snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance (for example, an operating system, an application server, and applications).\nLaunch permissions that control which AWS accounts can use the AMI to launch instances.\nA block device mapping specifies the volumes to attach to the instance when itâs launched.\n\nThe AMIs officially provided by AWS are all basic images, which generally only contain operating systems, such as Windows, Linux, and Mac. Another powerful function of AMI is that it allows users to customize AMI based on the base image, which means that users can install any software in the running EC2 instance, and finally take a snapshot of the instance content to define an AMI.\nUsage\nLaunch Single Instance\nThe way to launch EC2 instance through DolphinScheduler AMI is very simple, just click the launch instance on the EC2 page, then click browse more AMIs, and enter DolphinScheduler in the input box. Then select AMI and fill in the instance type, key pair, network, and click launch instance to start the instance.\n\n\n\n\nIt takes some time to start the instance. We can check the network configuration at this time. We need to make sure that ports 12345 and 22 are enabled, which are the port of the DolphinScheduler service and the ssh port.\n\n3â5 minutes later, you can access the DolphinScheduler service through the public DNS or public IPv4 of the EC2 instance plus port 12345. The username and password to log in to the DolphinScheduler service are user and the current EC2 instance id respectively.\nNote: The password of DolphinScheduler is dynamic, and it will be automatic change after EC2 instance launch to keep your service safe. And you can find it in your EC2 console home. See the AMI container product for more detail about AWS requests for AMI providers.\nLaunch Cluster\nhttps://github.com/WhaleOps/packer_tmpl/blob/main/aws/ami/dolphinscheduler/README.md#cluster-server\nThe above tutorial tells us how to start a single instance, so what should we do if we want to start a DolphinScheduler cluster?\nAdd New Key Pairs for Cluster\nIn the next step we will use ssh connect to existing EC2 instances, and currently our cluster.sh script only supports one single key pair. So we need to create a new one and then use it when launching instances. Go to EC2 -&gt; Network &amp; Security -&gt; Key Pairs -&gt; Create Key Pair. Keep it carefully, otherwise, you can not log in to your instance.\nAdd a New Security Group for Cluster\nGo to EC2 -&gt; Network &amp; Security -&gt; Security Groups -&gt; Create Security Group to create a new security group, you should add the following points to Inbound rules for this security group:\n\n22: default ssh point\n2181: Zookeeper connect point\n5432: PostgreSQL connect point\n1234: Dolphin Schedulerâs MasterServer point\n5678: DolphinSchedulerâs WorkerServer point\n12345: DolphinSchedulerâs web UI point\n\n\nLaunch Multiple EC2 Instances\nCurrently, you have to build this AMI by yourself and then launch a new EC2 instance from EC2 -&gt; Images -&gt; AMIs sidebar path, select the AMI you built, and then click Launch instance from AMI bottom, In EC2 -&gt; Instances -&gt; Launch an instance page, you should choose the existing key pair which you created in new key pair for cluster section, it can be found in Key pair -&gt; Select key pair. Also, you should choose the existing security group which you created in the new security group for the cluster section, it can be found in Network settings -&gt; Select existing security group -&gt; Select security group. At last, launch multiple instances based on your cluster number(in this tutorial we use 8 instances to create a cluster), from the Number of instances input box in the Summary plane.\nGet cluster.sh and cluster_env.sh Script\nIf you already clone this project, you should go to the directory packer_tmpl/aws/ami/dolphinscheduler/bin, and you can see two scripts named cluster.sh and cluster_env.sh. And if you do not clone this repository from GitHub, you can get those two scripts by the following command\nwget https://raw.githubusercontent.com/WhaleOps/pa\ncker_tmpl/main/aws/ami/dolphinscheduler/bin/cluster.sh\nwget https://raw.githubusercontent.com/WhaleOps/packer_tmpl/main/aws/ami/dolphinscheduler/bin/cluster_env.sh\n\n\nNOTE: If your network can not connect to GitHub, the above command will fail with an error log like connecting to raw.githubusercontent.com (raw.githubusercontent.com)|0.0.0.0|:443â¦ failed: Connection refused. You should find out a method to make your network can connect to host raw.githubusercontent.com or download these two scripts from the GitHub website.\n\nModify cluster_env.sh Script\nYou have to modify your cluster_env.sh script, which includes your key pair and EC2 instance's IPv4 address or IPv4 DNS. For example, we launch 8 EC2 instances, and we want to deploy two master-server, 3 worker-server, an API -server, an alert server, one database, and a zookeeper server, and each instance IPv4 address as below:\n\n192.168.1.1: master-server\n192.168.1.2: master-server\n192.168.1.3: worker-server\n192.168.1.4: worker-server\n192.168.1.5: worker-server\n192.168.1.6: API-server\n192.168.1.7: alert-server\n192.168.1.8: metadata database (PostgreSQL), zookeeper\n\nWe should tell our deploy plan to cluster_env.sh otherwise it will never know how to deploy it(here we only show some necessary changed content without comment)\nexport ips=&quot;192.168.1.1,192.168.1.2,192.168.1.3,192.168.1.4,192.168.1.5,192.168.1.6,192.168.1.7,192.168.1.8&quot;\n\n\nexport masters=&quot;192.168.1.1,192.168.1.2&quot;\nexport workers=&quot;192.168.1.3:default,192.168.1.4:default,192.168.1.5:default&quot;\nexport alertServer=&quot;192.168.1.6&quot;\nexport apiServers=&quot;192.168.1.7&quot;\nexport DATABASE_SERVER=&quot;192.168.1.8&quot;\nexport REGISTRY_SERVER=&quot;192.168.1.8&quot;\n\nShould also add your key pair location which you create in new key pair for the cluster, an absolute path is encouraged to use(here we only show some necessary changed content without comment)\n# Do not change this if you use this AMI to launch your instance\nexport INSTANCE_USER=${INSTANCE_USER:-&quot;ubuntu&quot;}\n# You have to change to your key pair path\nexport INSTANCE_KEY_PAIR=&quot;/change/to/your/personal/to/key/pair&quot;\n\nExecute cluster.sh Script\nAfter modifying cluster_env.sh you can execute the script by command\n./cluster.sh start\nThe time it takes depends on your network speed, and after it finishes, your EC2 instances will be combined into a DolphinScheduler cluster.\nWhat Should I do After Execute cluster.sh\nAfter that, you can log in DolphinScheduler service with user/EC2_DYNAMIC_INSTANCE_ID as the default username/password via instanceâs [API-SERVER-Public-IPv4-address]:12345/dolphinscheduler/ui or [API-SERVER-Public-IPv4-DNS]:12345 /dolphinscheduler/ui. For about how to use DolphinScheduler you view the detail in the functions of DolphinScheduler.\nContributing\nWe build AMI through the packer and make it completely open source. We welcome anyone interested in the project to view and contribute. DolphinScheduler AMI source code can be viewed at ami-dolphinscheduler. For how to contribute code, you can learn how to contribute.\nRecap\n\nBriefly introduce what AWS, EC2, and EC2 AMI are, and how to create an EC2 instance through AMI;\nIntroduces how to use DolphinScheduler AMI, how to start a single instance and form a cluster;\nEmphasize again that standalone is only for testing and experience\nHow to contribute if you are interested in the project\n\n",
    "title": "DolphinScheduler launches on the AWS AMI Application Marketplace",
    "time": "2022-12-10"
  },
  {
    "name": "DolphinScheduler_python_api_ci_cd",
    "content": "Best Practice | DolphinScheduler Python API CI/CD\n\nWritten by Zhong Jiajie, Apache DolphinScheduler PMC\nDolphinScheduler and Python API Introduction\nApache DolphinScheduler is a distributed and extensible workflow scheduler platform with powerful DAG visual interfaces. It helps users easier to build and maintain workflow on any scale.\n\nIn order to meet the needs of all users that have different preferences in the same team, DolphinScheduler provides a variety of ways to create workflows. The most popular way is through the web UI, which creates workflows with simple drag and drop and allows non-engineers to create. If you are an engineer and prefer to define workflows programmatically, you may consider using its Python API or YAML file definition to create it.\nPyDolphinScheduler is a Python API for Apache DolphinScheduler, which allows you to define your workflow by Python code, aka workflow-as-codes. You can write python code in any editor you like, just like using other python libraries, to create DolphinSchedulerâs users, environment, project, and workflow. For more practice examples, you can refer to [DolphinScheduler can schedule workflows with Python scripts!] for more detail.\nA Simple Python API Example\nAs an out-of-box tool, Python API has an example named tutorial, which includes the basic concept and minimum codes to create and run our very first workflows, you can see more detailed code at https://github.com/apache/dolphinscheduler-sdk-python/blob/main/src/pydolphinscheduler/examples/tutorial.py. The core concept of DolphinScheduler and its Python API is DAG, also calls workflow in Python API. More figuratively, it is the whole picture shown in the following figure. Each DAG contains multiple nodes and connections between nodes, like the node named A, B, C and etc, and the link between. In Python API, the task represents the node of DAG and the dependence for the connections between nodes.\n\nHow to Trigger Python API Workflow\nSingle One\nSo we already know the basic concept of Python API and we already have an example of it, then how can we trigger it and make it run and get our job done? To make it more pythonic, you can run it just like other Python scripts, via the terminal with a simple command.\npython tutorial.py\n\nThe PyDolphinScheduler will help you set all the things and create a new workflow, after that you can see the new one in DolphinScheduler web UI.\nMultiple Workflows\nWhat if I have multiple files with multiple workflows, how can I trigger them? Yeah, you may already think about it, you can trigger them one by one, just like we trigger the single one. we can be done with\npython workflow1.py\npython workflow2.py\npython workflow3.py\n...\npython workflowN.py\n\nWe can add all the above commands into a single bash script, after that, all we need is to execute the bash script\nbash &lt;bash-script-contain-all&gt;\n\nIt is useful, but when some workflows add or delete, we have to change the bash script too, which means we must consider changing our code synchronously. Otherwise, the bash script will fail, or some of our new workflows will not be triggered.\nThe way to fix it is simple, we can dynamic detection the Python script in specific directories, and then pass the existing script to the Python interpreter, and we can change our script like\nfor file in $(find . -name &quot;*.py&quot;); do\n    python &quot;$file&quot;\ndone\n\nAnd that is, that is the final script we trigger all our DolphinScheduler Python API workflows on any scale. But it is a little different in reality, I mean that nearly no one deploys production workflows by manually triggering. So the next step we will talk about how to trigger our workflow in CI\nTrigger in GitHub Action\nIn this section, we will trigger our workflows via CI, we use GitHub Action as an example and believe other CI tool is almost the same.\nWhat is GitHub Action\nGitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want. With the popularity of GitHub and the open-source project movement, GitHub Action is very popular currently. You can see more detail in the GitHub Action document. And here is a hello world of GitHub Action:\nname: GitHub Actions Demo\non:\n  push:\n    branches:\n      - main\njobs:\n  hello-world:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Run my very first GitHub Actions\n        run: echo &quot;ð Hello World.&quot;\n\nYou can save it as a YAML file and put it in your project with the path\n.github/workflows/hello.yaml to tell GitHub what you ask her to do. After that, each time you have a commit and push it to the branch named main, our config file named hello.yaml will be trigged, and it will do nothing but one, execute the bash command and echo âð Hello World.â to the GitHub Actions console.\nCombine GitHub Actions to Trigger Multiple Workflows\nIn the above example, you may have already realized that GitHub Actions can run bash commands. And our workflows batch trigger script is also a bash script. To get triggered through GitHub Actions, we may change the command in our GitHub Actions example.\nname: Execute Workflows\non:    \n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        run: |\n          for file in $(find . -name &quot;*.py&quot;); do\n            python &quot;$file&quot;\n          done\n\nIt can only trigger and deploy workflows to GitHub Actions running hosts. Our DolphinScheduler cluster runs on the self-host server or could service like AWS instead of GitHub Actions, so we have to tell the bash script to submit our code to the DolphinScheduler cluster instead of the GitHub Actions servers. Fortunately, DolphinScheduler Python API provides a user-friendly configuration change in three ways. I recommend you change it via bash to change environment variables during the GitHub Actions which is simply by\n# Modify Java Gateway Address\nexport PYDS_JAVA_GATEWAY_ADDRESS=&quot;&lt;YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER&gt;&quot;\nexport PYDS_JAVA_GATEWAY_PORT=&quot;&lt;PORT-RUN-DOLPHINSCHEDULER-API-SERVER&gt;&quot;\n\nGitHub Actions support env syntax in the YAML file, which you can see more detail in github-actions: environment-variables, and we can now change our GitHub Actions config to\nname: Execute Workflows\non:\n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: &lt;YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER&gt;\n          PYDS_JAVA_GATEWAY_PORT: &lt;PORT-RUN-DOLPHINSCHEDULER-API-SERVER&gt; \n        run: |\n          for file in $(find . -name &quot;*.py&quot;); do\n            python &quot;$file&quot;\n          done\n\nTherefore, each time our main branch has new commits, whether it is generated by merging PR or pushed from locally, it will trigger and deploy all of our workflows defined in DolphinScheduler Python API, to where your DolphinScheduler cluster deployed.\nIn the not released version, we add a new mechanism token for authentication to DolphinScheduler Python API, which means in the next version we have to add a token when we try to connect from Python API to DolphinScheduler, see https://github.com/apache/dolphinscheduler-sdk-python/pull/13 for more detail. Also, we highly recommend our users turn on the token authentication to make our connection safe. Just like other configurations, the token can also change via environment variables by bashã\nBut how to trigger from GitHub Actions when we enabled and turn on the token? In this case, we have to use GitHub Encrypted secrets for help. You can follow the step motion in the link to create your very first safety secret to your repository and remember the name of your secrets. And then use it in the GitHub Actions config\nname: Execute Workflows\non:\n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: &lt;YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER&gt;\n          PYDS_JAVA_GATEWAY_PORT: &lt;PORT-RUN-DOLPHINSCHEDULER-API-SERVER&gt;\n          PYDS_JAVA_GATEWAY_AUTH_TOKEN: ${{ secrets.YOUR-SECRET-NAME }} \n        run: |\n          for file in $(find . -name &quot;*.py&quot;); do\n            python &quot;$file&quot;\n          done\n\nSeeï¼ That is not complex, it is like using ordinary environment variables in GitHub Actions. And that is all we need to do about deploying workflows from GitHub Actions.\nAbout CI\nDolphinScheduler Python API script is a Python script, so itâs CI for Python language which may include black, Pylint, flake8, sort, autoflake and etc. If you choose to use Python API to create and maintain workflow instead of via web UI. I believe you already have personally preferred tools for code format and style check. I decide to talk about CI after then CD because it optional section. If you have your favorite, you can just use it and skip this section, but if you do not have one, I may share what I prefer to use and also Python API lint.\nFirst of all, I prefer using pre-commit, it will run each time when Git is committed, it is useful due to I can detect some easy but often overlooked detail before I push the code to remote. pre-commit needs a config file and I would like to share what Python API uses for itself code-style and lint code, you can see more detail at https://github.com/apache/dolphinscheduler-sdk-python/blob/main/.pre-commit-config.yaml\ndefault_language_version:\n  # force all python hooks to run python3\n  python: python3\nrepos:\n  # Python API Hooks\n  - repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n      - id: isort\n        name: isort (python)\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n  - repo: https://github.com/pycqa/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\n        additional_dependencies: [\n          'flake8-docstrings&gt;=1.6',\n          'flake8-black&gt;=0.2',\n        ]\n        # pre-commit run in the root, so we have to point out the full path of configuration\n        args: [\n          --config,\n          .flake8\n        ]\n  - repo: https://github.com/pycqa/autoflake\n    rev: v1.4\n    hooks:\n      - id: autoflake\n        args: [\n          --remove-all-unused-imports,\n          --ignore-init-module-imports,\n          --in-place\n        ]\n\nIt does not run complex checks, all of them are easy to know, and to keep pre-commit can be done as soon as possible. The detail is:\n\nisort: Sort Python imports automatically\nblack: Formatter Python code automatically\nautoflake: Removes unused imports and unused variables as reported by pyflakes automatically\nflake8: Detect other code and documentation\n\nAnd pre-commit is for the local check, you can also run it in your GitHub Actions by adding a new job before the existing job named execute\non:\n  push:\n    branches:\n      - main\n  pull_request:\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pre-commit\n      - name: lint\n        run: |\n          pre-commit install\n\t\t  pre-commit run --all-files\n  execute:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push'\n    needs: lint\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: &lt;YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER&gt;\n          PYDS_JAVA_GATEWAY_PORT: &lt;PORT-RUN-DOLPHINSCHEDULER-API-SERVER&gt;\n          PYDS_JAVA_GATEWAY_AUTH_TOKEN: ${{ secrets.YOUR-SECRET-NAME }} \n        run: |\n          for file in $(find . -name &quot;*.py&quot;); do\n            python &quot;$file&quot;\n          done\n\nSome of you may notice that besides adding a new job, we also add pull_request node under on and if node under execute job. Because code lint check should test both push and pull requests event, but we only want to execute workflow when there is a new commit to branch main. If we make execute workflow for the pull requests event, each commit to pull requests will be executed and deployed to our production environment, even though the pull requests are not accessed or not prepared to merge. So we must set a condition to execute the workflow.\nRecap\n\nWe show what DolphinScheduler and its Python API, and GitHub Actions are, How to create our very first workflow via DolphinScheduler Python API, and the first workflow in GitHub Actions.\nThen we show how to create DolphinScheduler Python APIâs CI/CD base on GitHub Actions step by step.\nFinally, we create a GitHub Actions to detect code style, and auto lint our DolphinScheduler Python APIâs workflow code.\n\n",
    "title": "DolphinScheduler Python API CI/CD",
    "time": "2022-12-16"
  },
  {
    "name": "Eavy_Info",
    "content": "Eavy Info Builds Data Asset Management Platform Services Based on Apache DolphinScheduler to Construct Government Information Ecology | Use Case\n\n\n\nBased on the Apache DolphinScheduler, the cloud computing and big data provider Eavy Info has been serving the business operations in the company for more than a year.\nCombining with the government affairs informatization ecological construction business, Shandong Eavy Info built the data service module of its self-develop Asset Data Management and Control Platform based on Apache DolphinScheduler. How do they use Apache DolphinScheduler? Sun Hao, the R&amp;D engineer of Evay Information, shared their experiences on their business practice.\nR&amp;D Background\nThe prime operating of Eavy Info is focusing on ToG business, and data collection &amp; sharing take a large proportion of their work. However, Traditional ETL tools, such as kettle, are not simple and easy enough to get started and employed for on-site project operation and maintenance by the front-line implementers. Therefore, creating a set of data acquisition (synchronization)-data processing-data management platform is particularly important.\nOut of this consideration, we have developed a Data Asset Management Platform, of which the core is a data service module based on Apache DolphinSchduler (referred to as DS below).\nApache DolphinScheduler is a distributed, decentralized, easy-to-expand visual DAG scheduling system that supports multiple types of tasks including Shell, Python, Spark, Flink, etc., and has good scalability. Its overall structure is shown in the figure below:\n\n\n\nThis is a typical master-slave architecture with strong horizontal scalability. The scheduling engine Quartz is a Java open source project of Spring Boot, it is easier to integrate and use for those familiar with Spring Boot development.\nAs a scheduling system, DS supports the following functions:\n**Scheduling mode: ** The system supports timing scheduling and manual scheduling based on cron expressions. And it supports command types like workflow starting, execution starting from the current node, the fault-tolerant workflow resume, the paused process resume, execution starting from the failed node, complement, timing, rerun, pause, stop, and resume joinable threads. Among them, restoring the fault-tolerant workflow and restoring the joinable threads are two command types that are controlled internally by the scheduling and cannot be called externally.\nTiming schedule: The system uses quartz distributed scheduler and supports the visual generation of cron expressions.\n**Dependency: ** The system not only supports the dependency between the simple predecessor and successor nodes of the DAG but also provides task-dependent nodes to support custom task dependencies between processes.\nPriority: Support the priority of the process instance and task instance. If the priority of the process instance and task instance is not set, the default is first-in-first-out.\n**Email alert: ** Support SQL task query result email sending, process instance running result email alert, and fault tolerance alert notification.\nFailure strategy: For tasks that run in parallel, if there are tasks that fail, two failure strategy processing methods are provided. Continue refers to regardless of the status of the parallel running tasks until the end of the process failure. End means that once a failed task is found, the running parallel task will be killed at the same time, and the failed process will end.\nComplement: Complement historical data, support interval parallel, and serial complement methods.\nBased on Apache DolphinScheduler, we carry out the following practices.\nBuilding A Data Synchronization Tool Based on DS\nIn our business scenario, there are many types of business needs for data synchronization, but the amount of data is not particularly large and is real-time-undemanding. So at the beginning of the architecture selection, we chose the combination of Datax+Apache DolphinScheduler and implemented the transformation of the corresponding business. Now it is integrated into various projects as a service product to provide offline synchronization services.\n\n\n\nSynchronization tasks are divided into periodic tasks and one-time tasks. After the configuration tasks of the input and output sources, the corn expression needs to be configured for periodic tasks, and then the save interface is called to send the synchronization tasks to the DS scheduling platform.\n\n\n\nSynchronization tasks are divided into periodic tasks and one-time tasks. After the configuration tasks of the input and output sources are configured, the corn expression needs to be configured for periodic tasks, and then the save interface is called to send the synchronization tasks to the DS scheduling platform.\nWe gave up the previous UI front-end of DS after comprehensive consideration and reused the DS back-end interfaces to carry the online procedure, start and stopping, deleting, and log viewing.\nThe design of the entire synchronization module is aimed to reuse the diversity of input and output plugins of the Datax component and integrate with the optimization of DS to achieve an offline synchronization task. This is a component diagram of our current synchronization.\n\n\n\nSelf Development Practices Based on DS\nAnyone familiar with Datax knows that it is essentially an ETL tool, which provides a transformer module that supports Groovy syntax, and at the same time further enrich the tool classes used in the transformer in the Datax source code, such as replacing, regular matching, screening, desensitization, statistics, and other functions. That shows its property of Transform. Since the tasks are implemented with DAG diagrams in Apache DolphinScheduler, we wonder that is it possible to abstract each Datax or SQL into a small data governance module for a table or several tables? Each module is designed based on the DAG diagram, and the data can be transferred between upstream and downstream and is can be implemented by drag-and-drop like DS. Therefore, we self-developed a module based on the previous work on Datax and DS.\n\n\n\nEach component is regarded as a module, and the dependency between the functions of each module is dealt with the dependency of DS. The corresponding component and the component transfer data are stored at the front-end, which means the front-end performs the transfer and logical judgments between most of the components after introducing input (input component) , since each component can be seen as an output/output of Datax. Once all parameters are set, the final output is determined. That is also the reason why we abandoned the UI front end of DS. After that, we assemble this DAG diagram into the defined type of DS and deliver it to the DS task center.\nPS: Because our business scenarios may involve cross-database queries (MySQL combined query of different instances), our SQL component uses Presto to implement a unified SQL layer, so that you can also use Presto to do combined retrieval even when data sources are under various IP instances (business-related).\nOther Attempts\nPeople dabble in the governance process know that a simple governance process can lead to a quality report. We write part of the government records into ES, and then use the aggregation capabilities of ES to obtain a quality report.\n\n\n\nThe above are some practices that we have made based on DS and middlewares like Datax, combining with businesses to meet our own needs.\nFrom EasyScheduler to the current Apache DolphinScheduler 2.0, we are more often a spectator or follower, but today we shared our practical experience to build data service modules of Data Asset Management and Control Platform based on Apache DolphinScheduler. Currently, we have served the on-site operation of multiple project departments of the company based on the Apache DolphinScheduler scheduling platform for more than a year. With the release of Apache DolphinScheduler 2.0, we have also grown up with it in an evolving community environment. We hope Apache DolphinScheduler will be better in the future!\n",
    "title": "Eavy Info Builds Data Asset Management Platform Services Based on Apache DolphinScheduler to Construct Government Information Ecology",
    "time": "2021-12-30"
  },
  {
    "name": "Exploration_and_practice_of_Tujia_Big_Data_Platform_Based",
    "content": "Exploration and practice of Tujia Big Data Platform Based on Apache DolphinScheduler\n\n\n\nTujia introduced Apache DolphinScheduler in 2019. At the recent Apache DolphinScheduler Meetup in February, Tujia Big Data Engineer Xuchao Zan introduced the process of Tujia's access to Apache DolphinScheduler and the functional improvements in detail.\n\n\n\nXuchao Zan, Big Data Engineer, Data Development engineer from Tujia, is mainly responsible for the development, maintenance, and tuning of the big data platform.\nWatch the record here:https://www.bilibili.com/video/BV1Ki4y117WV?spm_id_from=333.999.0.0\nThis speech mainly consists of 4 parts. The first part is the current status of Tujia's platform, introducing the process of Tujia's data flow, how Tujia provides data services, and the role of Apache DolphinScheduler in the platform. The second part will introduce the scheduler selection process of Tujia, which mainly refers to some features of the scheduler and the process of access. The third part is mainly about some improvements and functional expansions of the system, including support for function table dependencies, mail tasks expansion, and data synchronization functions. The fourth part details some new functions we have developed to meet our business requirements, such as reforming the Spark jar packages to support the publishing system, connecting the scheduler to data quality, and displaying the Data Lineage.\nStatus of Tujia Big Data Platform\n01 Big Data Platform schema\nFirst, let's introduce the schema of Tujia Big Data Platform and the role of Apache DolphinScheduler in the platform.\n\n\n\nThe architecture of Tujia Big Data Platform\nThe picture above shows the architecture of our data platform, which mainly includes data source, data collection, data storage, data management, and finally service provision.\nThe main source of the data includes three parts: data synchronization of the business from library MySQL API, involving the Dubbo interface, the http interface, and the embedded point data of the web page.\nData collection adopts real-time and offline synchronization, business data is incremental synchronized based on Canal, logs are collected by Flume, and Kafka is collected in real-time and falls on HDFS.\nThe data storage process mainly involves some data synchronization services. After the data falls into HDFS, it is cleaned, processed, and then pushed online to provide services.\nAt the data management level, the data dictionary records the metadata information of the business, the definition of the model, and the mapping relationship between various levels, which is convenient for users to find the data they care about; the log records the operation log of the task, and the alarm configures the fault information, etc. The dispatching system, as a command center of big data, rationally allocates dispatching resources and can better serve the business. Metrics library records the dimensions and attributes, normative definitions of business process metrics for better management and usage of data. Abtest documents the impact of different metrics and strategies on product functionality, and data quality is the basis for the effectiveness and accuracy of data analysis.\nThe last part is thedata service, which mainly includes ad hoc data query, report preview, data download and upload analysis, online business data support release, etc.\n02 The role of Apache DolphinScheduler in the platform\nThe following focuses on the role of the scheduler in the platform. The data tunnel is synchronized, and incremental data is pulled regularly every morning. After the data is cleaned and processed, it is pushed online to provide services. It also charges the processing of the data model and the interface configuration greatly improves Productivity. The service of the timed report, push email, support the display of attachments, text table, and line chart. The report push function allows the analysts to configure data dashboards, and Â DataX pushes the calculated data to MySQL every day for report display after data processing.\nIntroduces Apache DolphinScheduler\nThe second part is about the work we have done by introducing Apache DolphinScheduler.\nApache DolphinScheduler is advanced in many aspects. As a command center of big data, it's undoubtedly reliable. The decentralized design of Apache DolphinScheduler avoids the single point of failure problem, and once a problem occurs with one node, the task will be automatically restarted on other nodes, which greatly improves the system's reliability.\nIn addition, Apache DolphinScheduler is simple and practical, which reduces learning costs and improves work efficiency. Now many staff in the company are using it, including analysts, product operational staff, and developers.\nThe scalability of scheduling is also very important because as the amount of tasks increases, the cluster can add resources in time to provide services. A wide range of applications is also a key reason for us to choose Apache DolphinScheduler. It supports a variety of task types: Shell, MR, Spark, SQL (MySQL, PostgreSQL, Hive, SparkSQL), Python, Sub_Process, Procedure, etc. and enables workflow timing scheduling and dependency scheduling, manual scheduling, manual pause/stop/resume, as well as retry/alarm on failure, recovery from specified node failure, killing tasks, etc. It has so many advantages, and I cannot list them one by one, you should try it by yourself.\nNext is the upgrade of our time scheduling.\nBefore adopting Apache DolphinScheduler, our scheduling was quite confusing. Some people deployed their own local Crontab, some used Oozie for scheduling, and some used the system for time schedules. The management is chaotic, the timeliness&amp; accuracy cannot be guaranteed, and tasks cannot be found from time to time due to lacking one unified scheduling platform. In addition, the self-built scheduler is not stable enough due to lacking configuration dependency and data output guarantee, and the product function is limited, which supports limited task scheduling.\n\n\n\nIn 2019, we introduced Apache DolphinScheduler, and it has been running stably for nearly 3 Â years.\nBelow is some data of our system transfer.\nWe have built an Apache DolphinScheduler cluster with a total of 4 physical machines. Currently, a single machine supports 100 task scheduling concurrently.\nAlgorithms are also configured with special machines and be isolated.\nMost of our tasks are processed by Oozie, which are mainly Spark and Hive tasks. There are also some scripts on Crontab, some mailing tasks, and scheduled tasks of the reporting system.\nThe Scheduling System Re-built Based on Apache DolphinScheduler\nBefore reconstruction, we have optimized the system, such as supporting table-level dependencies, extending the mail function, etc. After introducing Apache DolphinScheduler, we re-built a scheduling system based on it to provide better services.\n**Firstly, the synchronization of table dependencies was supported.**At that time, considering the task migration may be parallel, the tasks could not be synchronized all at once and needed to be marked as the table tasks run successfully. Therefore, we developed the function to solve the dependency problem in task migration. However, the different naming style of users makes it difficult to locate the task of the table when configuring dependencies, and we cannot identify which tables are included in the task, and where the task is located in the table, which causes a lot of trouble for us.\n\n\n\n**Secondly, Mail tasks support multiple tables.**Scheduling has its self-installed mail push function, but only supports a single table. With more and more business requirements, we need to configure multiple tables and multiple sheets, and the number of text and attachments required to be displayed differently, which needs to be configured. In addition, it is necessary to support the function of line charts to enrich the text pages. Furthermore, users also want to be able to add notes to the text or below each table to explain indicators, etc. We use the Spark jar package to implement the email push function, which supports abnormal warnings, table dependencies missing, etc.\n\n\n\n\n\n\nThirdly, it supports rich data source synchronization. Due to some data transmission issues, we needed to modify configuration codes greatly, compile, package and upload data in the previous migration process, which is complex and error-prone, and data and online data cannot be separated due to data source disunity; in terms of development efficiency, there is a lot of repetition in the code, and the lack of a unified configuration tool and the unreasonable parameter configuration lead to high pressure on MySQL and the risk of downtime; after data transmission, there is no duplicate check, when the amount of data is large, the full amount of updates will cause a lot of pressure on MySQL. MySQL's transmission has a single point of failure problem, and task delay affects online services.\n\n\n\n\n\n\nWe simplified the data development process, made MySQL support the high availability of pxc/mha, and improved the efficiency of data synchronization.\nThe input data sources support relational databases and FTP synchronization. As the computing engine, the output data sources of Spark support various relational databases, as well as message middleware Kafka, MQ, and Redis.\nNext, let's get through the process of our implementation.\nWe have extended the data source of Apache DolphinScheduler to support the expansion of Kafka mq and namespace. Before MySQL synchronization, we first calculate an increment locally and synchronize the incremental data to MySQL. Spark also supports the HA of MySQL pxc/qmha. In addition, there is a qps limit when pushing MQ and Redis, we control the number of partitions and concurrency of Spark by the amount of data.\nImprovements\nThe fourth part is mainly about the added functions to the system, including:\n\nSpark supports publishing system\nData quality open up\nDisplay of Data lineage\n\n01 Spark task supports publishing system\nMore than 80% of our usual schedules are Spark jar package tasks, but the task release process and the code modification are not normalized. This leads to code inconsistencies from time to time, and even causes online problems in severe cases.\nThis requires us to refine the release process for tasks. We mainly use the publishing system, the Jenkens packaging function, compile and package to generate btag, and then publish and generate rtag after the test is completed, and the code is merged into master. This avoids the problem of code inconsistency and reduces the steps for jar package upload. After compiling and generating the jar package, the system will automatically push the jar package to the resource center of Apache DolphinScheduler. Users only need to configure the parameters and select the jar package for test release. When running a Spark task, it is no longer necessary to pull the file to the local, but directly read the jar package on HDFS.\n02 Data quality connection\nData quality is the basis for ensuring the validity and accuracy of analytical conclusions. We need a complete data monitoring output process to make the data more convincing. The quality platform ensures data accuracy, integrity, consistency, and timeliness from four aspects, and supports multiple alarm methods such as telephone, WeCom, and email to inform users.\nNext, we will introduce how to connect the data quality and scheduling system. After the scheduling task is completed, the message record is sent, the data quality platform receives the message, and triggers the rule of the data quality monitoring. Abide by the monitoring rule, the downstream operation is blocked or an alarm message is sent.\n03 Data linage display\nData lineage is key to metadata management, data governance, and data quality. It can track the source, processing, and provenance of data, provide a basis for data value assessment, and describe the flow of source data among processes, tables, reports, and ad hoc queries, the dependencies between tables, tables &amp; offline ETL tasks, as well as scheduling platforms &amp; computing engines. The data warehouse is built on Hive, and the raw data of Hive often comes from the production DB, and the calculation results are also exported to external storage. Therefore, the tables of heterogeneous data sources are related.\n\nData trace: When data is abnormal, it helps to trace the cause of the abnormality; impact analysis, tracing the source of the data and data processing process.\nData evaluation: Provide a basis for evaluating data value in terms of data target, update importance, and update frequency.\nLife cycle:  Intuitively obtain the entire life cycle of data, providing a basis for data governance.\nThe collection process of data linage is mainly about: Spark monitors the SQL and inserted tables by monitoring the Spark API, obtains and parses the Spark execution plan.\n\n",
    "title": "Exploration and practice of Tujia Big Data Platform Based on Apache DolphinScheduler",
    "time": "2022-3-10"
  },
  {
    "name": "FAQ",
    "content": "Q: What is Apache DolphinScheduler?\nA: DolphinScheduler is a distributed and extensible workflow scheduler platform with a powerful DAG visual interface. It is dedicated to solving complex task dependencies in data pipelines, making multi types of tasks available out of the box\nQ: Is DolphinScheduler an ETL tool?\nA: Apache DolphinScheduler is an open-source visual workflow automation tool used for setting up and maintaining data pipelines.\nDolphinScheduler isn't an ETL tool. but it can helps you manage ETL pipelines and it also integrates some ETL tools like sqoop and datax.\nQï¼What is DolphinScheduler used for?\nA: Apache DolphinScheduler is a workflow automation and scheduling platform that can be used to assemble and manage data pipelines. Similar systems such as oozie and Azkaban etc\nQ: Who is using Apache DolphinScheduler?\nA: more than 400+ companys are using DolphinScheduler as their scheduler platform, some user cases like IBMãTencentãmeituanãLenovoãNokiaãSHEINãInspurãpinganãICBC and so on\n",
    "title": "FAQ of Apache DolphinScheduler",
    "time": "2021-03-20"
  },
  {
    "name": "Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial",
    "content": "Fast Task Type Expanding On Apache DolphinScheduler | Tutorial\n\n\n\nBackground\nAt present, the scheduler plays an indispensable role in big data ecology. The Apache DolphinScheduler, a top-tier Apache project, is one of the most stable and easy-to-use scheduling systems. With scheduling, distribution, high availability, and ease of use in place, it is only natural that users will want to quickly, easily, and concisely expand the Apache Dolphinscheduler task types as their business grows or as more components are used for various needs. This article shows you how to expand an Apache DolphinScheduler Task easily and quickly.\nAuthor Bio\n\n\n\nBaiqiang Zhang\nBaiqiang Zhang is a big data development engineer, who is interested in researching real-time computing, metadata governance, and big data basic components.\n1 What is SPI?\nSPI (Service Provider Interface) is a service delivery discovery mechanism built into the JDK. Most people will probably rarely use it, as it is positioned primarily for development vendors, and is described in more detail in the java.util.ServiceLoader files. The abstract concept of SPI refers to the dynamic loading of service implementation.\n2 Why did we introduce SPI?\nDifferent enterprises may have their components that need to be executed by tasks, for example, enterprises use Hive, the most commonly used tool in the big data ecosystem, in different ways. Some enterprises execute tasks through HiveServer2, and some use HiveClient to execute tasks. Considering the out-of-the-box Task provided by Apache DolphinScheduler does not support HiveClientâs Task, so most users will execute through the Shell. However, a Shell doesnât work well compared with a TaskTemplate. So, Apache DolphinScheduler supports TaskSPI to enable users to better customize different Tasks according to their business needs.\nFirst of all, we need to understand the history of the Task iteration of Apache DolphinScheduler. In DS 1.3.x, expanding a Task required recompiling the whole Apache DolphinScheduler, which was heavily coupled, so in Apache DolphinScheduler 2.0.x, we introduced SPI. As we mentioned earlier, the essence of SPI is to dynamically load the implementation of a service, so letâs make it more concrete and consider the Task of Apache DolphinScheduler as an execution service, and we need to execute different services according to the userâs choice. If there is no service, we need to expand it ourselves. Compared to 1.3.x we only need to complete our Task implementation logic, then follow the SPI rules, compile it into a Jar and upload it to the specified directory, and use our own written Task.\n3 Who is using it?\na. Apache DolphinScheduler\ni. task\nii. datasource\nb. Apache Flink\ni. Flink sql connector, after the user has implemented a flink-connector, Flink is also dynamically loaded via SPI\nc. Spring boot\ni. spring boot spi\nd. Jdbc\ni. Before jdbc4.0, developers need to load the driver based on Class by forName(âxxxâ), jdbc4 also based on the spi mechanism to discover the driver provider, you can expose the driver provider by specifying the implementation class in the META-INF/services/java. sql. Driver file\ne. More\n\ndubbo\ncommon-logging\nâ¦â¦\n\n4 Whatâs the Apache DolphinScheduler SPI Process?\n\n\n\nNote: SPI Rules\nWhen compiling the specific implementation of the service into a JAR, we need to create the META-INF/services/ folder in the dir of the resource, and then create a fully qualified class name with the file name of the service, which is the fully qualified class name of the integrated interface. The content inside is the fully qualified class name of the implementing class.\nTo explain the above diagram, I have divided Apache DolphinScheduler into logical tasks and physical tasks, logical tasks refer to DependTask, SwitchTask, and physical tasks refer to ShellTask, SQLTask, which are the Task for executing tasks. In Apache DolphinScheduler, we generally expand the physical tasks, which are handed over to the Worker to execute, so what we need to understand is that when we have more than one Worker, we have to distribute the custom task to each machine with Worker, and when we start the worker service, the worker will start a ClassLoader to load the corresponding task lib that implements the rules. Note that HiveClient and SeatunnelTasks are user-defined, but only HiveTasks are loaded by Apache DolphinScheduler TaskPluginManage. The reason is that SeatunnelTask does not follow SPI rules. The SPI rules are also described on the diagram, or you can refer to the class java.util.ServiceLoader, which has a simple reference below (part of the code is extracted):\npublic final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt; {\n    //scanning dir prefix\n    private static final String PREFIX = &quot;META-INF/services/&quot;;\n    //The class or interface representing the service being loaded\n    private final Class&lt;S&gt; service;\n    //The class loader used to locate, load, and instantiate providers\n    private final ClassLoader loader;\n    //Private inner class implementing fully-lazy provider lookup\n    private class LazyIterator implements Iterator&lt;S&gt; {\n        Class&lt;S&gt; service;\n        ClassLoader loader;\n        Enumeration&lt;URL&gt; configs = null;\n        String nextName = null;\n        //......\n        private boolean hasNextService() {\n            if (configs == null) {\n                try {\n                    //get dir all class\n                    String fullName = PREFIX + service.getName();\n                    if (loader == null)\n                        configs = ClassLoader.getSystemResources(fullName);\n                    else\n                        configs = loader.getResources(fullName);\n                } catch (IOException x) {\n                    //......\n                }\n                //......\n            }\n        }\n    }\n}\n\n5 How to extend a data source Task or DataSource ?\n5.1 Creating a Maven project\nmvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.dolphinscheduler \\\n    -DarchetypeArtifactId=dolphinscheduler-hive-client-task \\\n    -DarchetypeVersion=1.10.0 \\\n    -DgroupId=org.apache.dolphinscheduler \\\n    -DartifactId=dolphinscheduler-hive-client-task \\\n    -Dversion=0.1 \\\n    -Dpackage=org.apache.dolphinscheduler \\\n    -DinteractiveMode=false\n\n5.2 Maven dependencies\n&lt;! --dolphinscheduler spi basic core denpendence --&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-spi&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-task-api&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency\n\n5.3 Creating a TaskChannelFactory\nFirst, we need to create the factory for the task service, which mainly targets to help build the TaskChannel and TaskPlugin parameters, and to give the unique identity of the task. The ChannelFactory connects the Task service group of Apache DolphinScheduler, and helps the front and back end interaction to build the TaskChannel.\npackage org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.spi.params.base.PluginParams;\nimport org.apache.dolphinscheduler.spi.task.TaskChannel;\nimport org.apache.dolphinscheduler.spi.task.TaskChannelFactory;\nimport java.util.List;\npublic class HiveClientTaskChannelFactory implements TaskChannelFactory {\n    /**\n     * Create a task channel and execute tasks based on it\n     * @return Task Channel\n     */\n    @Override\n    public TaskChannel create() {\n        return new HiveClientTaskChannel();\n    }\n    /**\n     * Returns the globally unique identifier of the current task\n     * @return Task type name\n     */\n    @Override\n    public String getName() {\n        return &quot;HIVE CLIENT&quot;;\n    }\n    /**\n     * The front-end pages need to be rendered, mainly into\n\n     * @return\n     */\n    @Override\n    public List&lt;PluginParams&gt; getParams() {\n        List&lt;PluginParams&gt; pluginParams = new ArrayList&lt;&gt;();\n        InputParam nodeName = InputParam.newBuilder(&quot;name&quot;, &quot;$t('Node name')&quot;)\n                .addValidate(Validate.newBuilder()\n                        .setRequired(true)\n                        .build())\n                .build();\n        PluginParams runFlag = RadioParam.newBuilder(&quot;runFlag&quot;, &quot;RUN_FLAG&quot;)\n                .addParamsOptions(new ParamsOptions(&quot;NORMAL&quot;, &quot;NORMAL&quot;, false))\n                .addParamsOptions(new ParamsOptions(&quot;FORBIDDEN&quot;, &quot;FORBIDDEN&quot;, false))\n                .build();\n        PluginParams build = CheckboxParam.newBuilder(&quot;Hive SQL&quot;, &quot;Test HiveSQL&quot;)\n                .setDisplay(true)\n                .setValue(&quot;-- author: \\n --desc:&quot;)\n                .build();\n        pluginParams.add(nodeName);\n        pluginParams.add(runFlag);\n        pluginParams.add(build);\n        return pluginParams;\n    }\n}\n\n5.4 Creating a TaskChannel\nAfter we have a factory, we will create a TaskChannel based on it. The TaskChannel contains two methods, canceling and creating, currently, we only need to focus on creating tasks.\nvoid cancelApplication(boolean status);\n    /**\n     * Build executable tasks\n     */\n    AbstractTask createTask(TaskRequest taskRequest);\npublic class HiveClientTaskChannel implements TaskChannel {\n    @Override\n    public void cancelApplication(boolean b) {\n        //do nothing\n    }\n    @Override\n    public AbstractTask createTask(TaskRequest taskRequest) {\n        return new HiveClientTask(taskRequest);\n    }\n}\n\n5.5 Building a Task Implementation\nWith TaskChannel we get the physical task that can be executed, but we need to add the corresponding implementation to the current task to allow Apache DolphinScheduler to execute your task.\nWe can see from the above figure that the tasks based on Yarn execution will inherit AbstractYarnTask, and those that do not need to be executed by Yarn will directly inherit AbstractTaskExecutor, which mainly contains an AppID, and CanalApplication setMainJar. As you can see above, our HiveClient needs to inherit AbstractYarnTask, and before building the task, we need to build the parameters object that fits the HiveClient to deserialize the JsonParam.\npackage org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.ResourceInfo;\nimport java.util.List;\npublic class HiveClientParameters extends AbstractParameters {\n    /**\n     * The easiest way to execute with HiveClient is to just paste in all the SQL, so we only need one SQL parameter\n     */\n    private String sql;\n    public String getSql() {\n        return sql;\n    }\n    public void setSql(String sql) {\n        this.sql = sql;\n    }\n    @Override\n    public boolean checkParameters() {\n        return sql ! = null;\n    }\n    @Override\n    public List&lt;ResourceInfo&gt; getResourceFilesList() {\n        return null;\n    }\n}\n\nAfter implementing the parameters object, letâs implement the Task. The implementation in the example is relatively simple, which is to write the userâs parameters to a file and execute the task via Hive -f.\npackage org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.request.TaskRequest;\nimport org.apache.dolphinscheduler.spi.utils.JSONUtils;\nimport java.io;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\npublic class HiveClientTask extends AbstractYarnTask {\n    /**\n     * hive client parameters\n     */\n    private HiveClientParameters hiveClientParameters;\n    /**\n     * taskExecutionContext\n     */\n    private final TaskRequest taskExecutionContext;\n    public HiveClientTask(TaskRequest taskRequest) {\n        super(taskRequest);\n        this.taskExecutionContext = taskRequest;\n    }\n    /**\n     * task init method\n     */\n    @Override\n    public void init() {\n        logger.info(&quot;hive client task param is {}&quot;, JSONUtils.toJsonString(taskExecutionContext));\n        this.hiveClientParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), HiveClientParameters.class);\n        if (this.hiveClientParameters ! = null &amp;&amp; !hiveClientParameters.checkParameters()) {\n            throw new RuntimeException(&quot;hive client task params is not valid&quot;);\n        }\n    }\n    /**\n     * build task execution command\n     *\n     * @return task execution command or null\n     */\n    @Override\n    protected String buildCommand() {\n        String filePath = getFilePath();\n        if (writeExecutionContentToFile(filePath)) {\n            return &quot;hive -f &quot; + filePath;\n        }\n        return null;\n    }\n    /**\n     * get hive sql write path\n     *\n     * @return file write path\n     */\n    private String getFilePath() {\n        return String.format(&quot;%s/hive-%s-%s.sql&quot;, this.taskExecutionContext.getExecutePath(), this.taskExecutionContext.getTaskName(), this. taskExecutionContext.getTaskInstanceId());\n    }\n    @Override\n    protected void setMainJarName() {\n        //do nothing\n    }\n    /**\n     * write hive sql to filepath\n     *\n     * @param filePath file path\n     * @return write success?\n     */\n    private boolean writeExecutionContentToFile(String filePath) {\n        Path path = Paths.get(filePath);\n        try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {\n            writer.write(this.hiveClientParameters.getSql());\n            logger.info(&quot;file:&quot; + filePath + &quot;write success.&quot;);\n            return true;\n        } catch (IOException e) {\n            logger.error(&quot;file:&quot; + filePath + &quot;write failed. please path auth.&quot;);\n            e.printStackTrace();\n            return false;\n        }\n    }\n    @Override\n    public AbstractParameters getParameters() {\n        return this.hiveClientParameters;\n    }\n}\n\n5.6 Compliance with SPI Rules\n# 1,Create META-INF/services folder under Resource, create the file with the same full class name of the interface\nzhang@xiaozhang resources % tree . /\n. /\nâââ META-INF\n    âââ services\n        ââ org.apache.dolphinscheduler.spi.task.TaskChannelFactory\n# 2, write the fully qualified class name of the implemented class in the file\nzhang@xiaozhang resources % more META-INF/services/org.apache.dolphinscheduler.spi.task.TaskChannelFactory\norg.apache.dolphinscheduler.plugin.task.hive.HiveClientTaskChannelFactory\n\n5.7 Packaging and Deployment\n## 1,Packing\nmvn clean install\n## 2, Deployment\ncp . /target/dolphinscheduler-task-hiveclient-1.0.jar $DOLPHINSCHEDULER_HOME/lib/\n## 3,restart dolphinscheduler server\n\nAfter the above operation, we check the worker log tail -200f $Apache DolphinScheduler_HOME/log/Apache DolphinScheduler-worker.log.\nThatâs all~ The front-end modifications involved above can be found in Apache DolphinScheduler-ui/src/js/conf/home/pages/dag/_source/formModel/\nJoin the Community\nThere are many ways to participate and contribute to the DolphinScheduler community, including:\nDocuments, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.\nWe assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.\nSo the community has compiled the following list of issues suitable for novices:Â https://github.com/apache/dolphinscheduler/issues/5689\nList of non-newbie issues:Â https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\nHow to participate in the contribution:Â  https://dolphinscheduler.apache.org/en-us/community\nGitHub Code Repository:Â https://github.com/apache/dolphinscheduler\nOfficial Websiteï¼https://dolphinscheduler.apache.org/\nMailListï¼dev@dolphinscheduler@apache.org\nTwitterï¼@DolphinSchedule\nYouTubeï¼https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\nSlackï¼https://s.apache.org/dolphinscheduler-slack\nContributor Guideï¼https://dolphinscheduler.apache.org/en-us/community\nYour Star for the project is important, donât hesitate to lighten a Star for Apache DolphinScheduler â¤ï¸\n",
    "title": "Fast Task Type Expanding On Apache DolphinScheduler | Tutorial",
    "time": "2022-4-14"
  },
  {
    "name": "Hangzhou_cisco",
    "content": "Cisco Hangzhou's Travel Through Apache DolphinScheduler Alert Module Refactor\n\n\n\n\nCisco Hangzhou has introduced Apache DolphinScheduler into the company's self-built big data platform. At present, the team of Qingwang Li, Big Data Engineer from Cisco Hangzhouhas basically completed the Alert Module reform, which aims to build a more complete Alert module to meet the needs of complex alerts in business scenarios.\n\n\n\n\nLi Qingwang\nBig Data Engineer, Cisco Hangzhou, is responsible for big data development, such as Spark and the scheduling systems.\nWe encountered many problems in using the original scheduling platform to process big data tasks. For example, for a task of processing and aggregated analysis of data, multiple pre-Spark tasks are used to process and analyze data from different data sources firstly, and the final Spark task aggregates and analyzes the results processed during this period to get the final data we want. Unfortunately, the scheduling platform could not execute multiple tasks serially, and we had to estimate the task processing duration to set the start execution time for multiple tasks. If one of the tasks fails to execute, subsequent tasks need to be manually stopped. This is neither convenient nor elegant.\nTo our surprise, the core function of Apache DolphinScheduler - workflow definition can connect tasks in series, perfectly fits our needs. So, we introduced Apache DolphinScheduler into our big data platform, and I was mainly responsible for the Alert module reform. At present, other colleagues are promoting the integration of K8s, hoping that future tasks will be executed in K8s.\nToday, I will share the reform journey of the Alert module.\n01 Alert Module Design\n\n\n\nDesign of the DolphinScheduler Alert module\nThe Alert mode of Apache DolphinScheduler version 1.0 uses configuring alert.properties to send alerts by configuring emails, SMS, etc., but this method is no longer suitable for the current scenario. The official has also refactored the alarm module. For details of the design ideas, please refer to the official documents:\nhttps://github.com/apache/dolphinscheduler/issues/3049\nhttps://github.com/apache/dolphinscheduler/tree/dev/docs/docs/en/guide/alert\nThe Apache DolphinScheduler alert module is an independently started service, and one of the cores is the AlertPluginManager class. The alarm module integrates many plug-ins, such as DingTalk, WeChat, Feishu, mail, etc., which are written in the source code in an independent form. When the service is started, the plug-in will be parsed and the configured parameters will be formatted into JSON, by which the front-end page will automatically be rendered. AlertPluginManager caches plugins in memory at startup. The AlertServer class will start the thread pool and scan the DB regularly.\nWhen the workflow is configured with a notification policy, the Worker executes the workflow, and the execution result matches the notification policy successfully. After inserting the alarm data into the DB, the thread pool scans the DB and calls the send method of the AlertSender class to transfer the alarm data. Alarm data is bound to an alarm group, which corresponds to multiple alarm instances. The AlertSender class traverses the alert instance, obtains the plug-in instance through the AlertPluginManager class, calls the instance's sending method, and finally updates the result. This is the entire alerting process of Apache DolphinScheduler.\nIt should be noted that the RPC service is also started when the Alert server is started. This is an alarm method designed for special types of tasks, such as SQL query reports. It allows workers to directly access the Alert server through RPC, and use the Alert module to complete the alarm, while the data is not written to DB. But on the whole, the alarm mode of Apache DolphinScheduler is still based on the way of writing DB and asynchronous interaction.\n\n\n\nAfter defining the workflow, you can set the notification policy and bind the alarm group before starting.\n\n\n\nIn the task dimension, you can configure a timeout alarm to be triggered when the task times out. There is no alarm group configuration here, tasks and workflows share the same alarm group. When the task times out, it will be pushed to the alarm group set by the workflow.\n\n\n\nThe above figure is a flowchart of system alarm configuration. It shows that a workflow can be configured with multiple task instances, tasks can be configured to timeout to trigger alarms, and workflow success or failure can trigger alarms. An alarm group can be bound to multiple alarm instances. But this configuration mode is not reasonable. We hope that the alarm instance can also match the status of the workflow/task instance, that is, the success and failure of the workflow call the same alarm group, but trigger different alarm instances. This is more in line with the real scene.\n\n\n\nCreate an alarm group that can be bound to multiple alarm instances.\n02 Big data task alarm scenario\nThe following are some common big data task alarm scenarios in our daily work.\n\n\n\nFor scheduled tasks, notifications are sent before starting execution, when the task goes online, goes offline, or modifies parameters, whether the task execution succeeds or fails. While for different results of the same task, we want to trigger different notifications, such as SMS, DingTalk, or WeChat group notification for successful tasks, and if the task fails, we need to notify the corresponding R&amp;D personnel as soon as possible to get a faster response, and at this time, @corresponding R&amp;D personnel in the DingTalk or WeChat group or phone notification will be more timely. At present, the company's task scheduling platform is set to call the API in the task for notification. This method of strong coupling with the code is extremely inconvenient. In fact, it can be abstracted into a more general module to achieve.\nAlthough the architecture of Apache DolphinScheduler meets the requirements of the actual scenario, the problem is that the page configuration of the alarm module can only choose to trigger the notification for successful or fail tasks, and it is bound to the same alarm group, that is, the way of alarming is the same regardless of success or failure, which does not satisfy our need for different results to be notified in different ways in a real production environment. Therefore, we made some changes to the Alert module.\n03 Alert module modification\n\n\n\nThe first refactor points to alert instance. Previously, when an alarm instance was added, triggering an alarm would trigger the send method of the instance. We hope that when defining an alarm instance, an alarm policy can be bound. There are three options: send if the task succeeds, send on failure, and send on both success and failure.\nIn the task definition dimension, there is a timeout alarm function, which actually corresponds to the failed strategy.\n\n\n\nThe above picture shows the completed configuration page. On the Create Alarm Instance page, we added an alarm type field, choosing to call the plugin on success, failure, or whether it succeeds or fails.\n\n\n\nThe above picture shows the architecture of the Apache DolphinScheduler alarm module after the refactor. We have made two changes to it.\nFirst, when the workflow or task is executed, if an alarm is triggered, when writing to the DB, the execution result of the workflow or task will be saved, whether it succeeds or fails.\nSecond, adds a logical judgment to the alarm instance calling send method, which matches the alarm instance with the task status, executes the alarm instance sending logic if it matches, and filters if it does not match.\nThe alarm module refactored supports the following scenarios:\n\n\n\nFor detailed design, please refer to the issue: https://github.com/apache/dolphinscheduler/issues/7992\nSee the code for details: https://github.com/apache/dolphinscheduler/pull/8636\nIn addition, we also put forward some proposals to the community for the alarm module of Apache DolphinScheduler. Welcome anyone who is interested in this issue to follow up the work together:\n\nWhen the workflow starts or goes online or offline, or when parameters are modified, a notification can be triggered;\nThe alarming scenario is for worker monitoring. If the worker hangs up or disconnects from ZK and loses its heartbeat, it will consider the worker is down, trigger an alarm, and match the alarm group with ID 1 by default. This setting is explained in the source code, which is easy to be ignored, and you won't likely to set the alarm group with ID 1, thus fails you to get the notification of worker downtime instantly;\nThe alarm module currently supports Feishu, DingTalk, WeChat, Email, and other plug-ins, which are commonly used by domestic users. While users abroad are more used to plug-ins like Webex Teams, or PagerDuty, a commonly used alarm plug-in abroad. We re-developed these and plug-ins and contributed them to the community. For now, there are some more commonly used plug-ins abroad, such as Microsoft Teams, etc., anyone who is interested in it is recommended to submit a PR to the community.\nThe last but not least, big data practitioners probably are not skilled with the front-end stuff and may quit by the front-end page development when developing and alarm plug-ins. But I'd like to point out that you do not need to write front-end code at all when developing the Apache DolphinScheduler alarm plug-in. You only need to configure the parameters to be entered on the page or the buttons to be selected in the Java code when creating a new alarm instance plug-in (see org.apache.dolphinscheduler.spi.params for the source code), the system will automatically format it into JSON, and the front-end can automatically render a page through JSON by form-create. Therefore, you don't have to worry about writing the front end at all.\n\n",
    "title": "Cisco Hangzhou's Travel Through Apache DolphinScheduler Alert Module Refactor",
    "time": "2022-3-16"
  },
  {
    "name": "How_Does_360_DIGITECH_process_10_000+_workflow_instances_per_day",
    "content": "How Does 360 DIGITECH process 10,000+ workflow instances per day by Apache DolphinScheduler\n\n\n\nSince 2020, 360 DIGITECH has fully migrated its scheduling system from Azkaban to Apache DolphinScheduler. As a senior user of DolphinScheduler, 360 DIGITECH now uses DolphinScheduler to process 10,000+ workflow instances per day.\nTo meet the practical needs of big data platform and algorithm model business, 360 DIGITECH has made many modifications on DolphinScheduler such as alarm monitoring expansion,  worker maintenance mode adding, multi-server room renovation, etc. to make it more convenient for operation and maintenance.\nHow did they carry out the re-development? Jianmin Liu, a big data engineer at 360 DIGITECH, shared this topic in detail at the Apache DolphinScheduler February Meetup.\n\n\n\nJianmin Liu, a big data engineer from 360 DIGITECH, mainly engaged in the research of ETL task and scheduling framework, and the development of big data platforms and real-time computing platforms.\nMigrate from Azkaban to Apache DolphinScheduler\nBy 2019, 360 DIGITECH used Azkaban for big data processing.\nAzkaban is a batch workflow task scheduler open-sourced by Linkedin. It is easy to install, users can create tasks and upload zip packages for workflow scheduling just by installing the web and executor server.\nAzkaban's web-executor architecture is shown as below:\n\n\n\nDisadvantages of Azkaban\nAzkaban is suitable for simple scheduling scenarios, and after three years of use, we found three fatal flaws in it.\n\nPoor experience\n\nIt has no visual task creation feature, and you need to upload a zip package to create and modify tasks, which is not convenient; in addition, Azkaban has no function to manage resource files.\n\nNot powerful enough\n\nAzkaban lacks some indispensable features for production environments, such as complement and cross-task dependencies; user and permission control are too weak, and scheduling configuration does not support per month, so we need to make up for it in many other ways in production.\n\nNot good enough stability\n\nThe most fatal flaw of Azkaban is that it is not stable enough, and tasks are often backlogged when the executor is overloaded; tasks at the hourly or minute level are prone to miss scheduling; there are no timeout alerts, and although we have developed our own limited SMS alerts, we are still prone to production accidents.\nWe had a retrofit in 2018 to address these shortcomings, but the Azkaban source code was complex and the retrofit was painful, so we decided to re-choose the scheduler. At that time, we tested Airflow, DolphinScheduler, and XXL-job, but the Airflow Python technology stack didn't match ours, and the XXL-job functionality was too simple, so it was clear that DolphinScheduler was the better choice.\nIn 2019, we Folked the code of EasyScheduler 1.0, migrated parts of it with scheduling tasks in 2020, and went it live until now.\nSelection Research\nWe chose DolphinScheduler because of its four advantages:\n\n\nDecentralized structure with multiple masters and multiple workers.\n\n\nScheduling framework is powerful and supports multiple task types with cross-project dependencies and complement capabilities.\n\n\nGood user experience, visual DAG workflows editing, and ease of operation.\n\n\n4.Java stack with good scalability.\nThe conversion process was very smooth and we migrated the scheduling system to DolphinScheduler without any problems.\nUse of DolphinScheduler\nAt 360 DIGITECH, DolphinScheduler is not only used by the Big Data department, but also by the Algorithms department for some of its features. To make it easier for the Algorithms department to use DolphinScheduler's features, we integrated it into our own Yushu Big Data Platform.\nYushu Big Data Platform\n\n\n\nYushu is a big data platform composed of basic components, Yushu platform, monitoring&amp;operation, and business service layer, which can do a query, data real-time calculation, message queue, real-time data warehouse, data synchronization, data governance, and others. Among them, offline scheduling uses DolphinScheduler to schedule the ETL task of scheduling data sources to Hive data warehouse, as well as supports TiDB real-time monitoring, real-time data reports, and other functions.\nDolphinScheduler Nested to Yushu\nTo support the company's algorithm modeling needs, we extracted some common nodes and nested a layer of UI with API calls.\nThe algorithm department mostly uses Python scripts and SQL nodes, timed with box-table logic, then configured with machine learning algorithms for training, and then called the model in Python to generate model scores after assembling data. We wrapped some Kafka nodes to read Hive data and push it to Kafka via Spark.\nTask Type\n\n\n\nDolphinScheduler supports Shell, SQL, Python, and Jar task types. Shell supports Sqoop DataX mr synchronization task and Hive-SQL, Spark-SQL; SQL node mainly supports TiDB SQL (handling upstream sub-base and sub-table monitoring) and Hive SQL. Python task types support offline calls to model scripts, etc.; and Jar packages mainly support Spark Jar offline management.\nTask Scenario\n\n\n\nThe task scenario of DolphinScheduler is mainly about synchronizing various data sources such as MySQL, Hbase, etc. to Hive, and then generating DW directly through ETL workflow, assembling or calling through Python scripts, generating models and rule results, and then pushing them to Kafka, which will offer risk control system quota, approval, and analysis, and feed the results to the business system. This shows a complete workflow example of the DolphinScheduler scheduling process.\nOperations and Maintenance of DolphinScheduler\nCurrently, DolphinScheduler is processing 10000+ workflows per day.  Considering the fact that many offline reports depend on DolphinScheduler, operation and maintenance is very important.\nAt 360 DIGITECH, the operation and maintenance of DolphinScheduler are divided into three parts:\n\nDS Dependent Components Operations and Maintenance\n\nThe DolphinScheduler dependent component of DolphinScheduler is mainly for MySQL monitoring and Zookeeper monitoring.\nBecause workflow definition meta information, workflow instances&amp;task instances, Quartz scheduling, and Commands all rely on MySQL, MySQL monitoring is critical. There was a time when our network in the server room went down causing many workflow instances to miss schedule, and the problems are troubleshoot by the follow-up MySQL monitoring.\nThe importance of Zookeeper monitoring also goes without saying. The master-worker state and task queue both rely on Zookeeper, fortunately, Zookeeper has been stable and no problems have occurred yet.\n\nMaster and Worker Status Monitoring\nAs we all know, the Master is responsible for task slicing, which actually does not have a great impact on the business, so we just use emails to monitor it; however, a hung worker will cause task delays and increase the pressure on the cluster. In addition, if Yarn tasks are killed unsuccessfully, task tolerance may lead to repeated Yarn tasks, so we use phone alerts to monitor worker status.\n\nGrafana Big Board Monitoring Workflow Instance Status\nFor this, we have created a Grafana monitoring dashboard for Ops, which can monitor the status of workflow instances in real-time, including the number of workflow instances, the running status of workflow instances by project, and timeout warning settings.\nDolphinScheduler Retrofit\nExperience optimization\n\n\nAuthorize individuals to resource files and projects, and resource files distinguish between edit and readable to facilitate authorization.\n\n\nExtend global variables (process definition id, task id, etc. included into global variables), allow tasks submitted to yarn to be tracked to the scheduled tasks, facilitate cluster management, lock resources by counting workflow instances to facilitate maintenance.\n\n\nCarry workflow replication, task instance query interface optimization to speed up query speed, and optimize UI.\n\n\nAdd SMS alert\nTo make up for the weakness of the original email alert, we added an SMS alert to the UI to save the workflow definition to ensure the key tasks be monitored properly. In addition, we also changed the alert receivers to user names and extended the AlertType to SMS, email, and other alert methods to associate with user information, such as cell phone numbers, to ensure that we can receive alerts in time when important tasks fail.\nAdd maintenance mode to Worker\nWhen a worker machine needs maintenance, it's necessary to ensure no new tasks are submitted to the worker. For this, we have added a maintenance mode to the worker, which consists of four main points.\n\nThe UI sets the worker into maintenance mode, calling the API to write to the zk path specified by the worker.\n\n2.WorkerServer performs scheduled polling for maintenance mode or not.\n3.WorkerServer in maintenance mode, FetchTaskThread not fetching new tasks.\n\nThe worker task finishes running and can be restarted.\n\nAfter the above reform, the pressure on our O&amp;M is greatly reduced.\nMulti-server room renovation\nFinally, we also performed a multi-server room renovation. Since we had 2 clusters in different server rooms, our goal was to set up multiple server rooms in scheduling, so that when one server room fails, we can switch to others with one click, and use the same scheduling framework in multiple server rooms.\nThe pointcut for the renovation is to set up multi-server rooms in the scheduling to ensure that a certain task can be started in multiple server rooms. The renovation process went like this:\n\n\nZK is deployed in each server room, and the Master and Worker are registered to the corresponding server room, the Master is responsible for task slicing, and the Worker is responsible for task processing.\n\n\nAttach schedule and command with datacenter information.\n\n\nTo ensure dual-server room task switching, resource files are uploaded for upper room tasks, while changing task interfaces, task dependencies, and Master fault tolerance all need to be modified to match the corresponding room.\n\n\n",
    "title": "How Does 360 DIGITECH process 10,000+ workflow instances per day by Apache DolphinScheduler",
    "time": "2022-3-15"
  },
  {
    "name": "How_Does_Live-broadcasting_Platform_Adapt_to_Apache_DolphinScheduler",
    "content": "How Does Live-broadcasting Platform Adapt to Apache DolphinScheduler?\n\n\n\n\nAt the Apache DolphinScheduler Meetupï¼3.26ï¼, Yuan Bingze, a software engineer at YY Live, shared the topic ofÂ YY Liveâs Adaptation and Exploration based on Apache DolphinSchedulerÂ with us. &gt;This presentation consists of four main sections: &gt;1.**Background on the introduction of Apache DolphinScheduler to YY Live &gt;2.**The introduction process of Apache DolphinScheduler &gt;3.**Application and adaptation of Apache DolphinScheduler &gt;4.**YY Liveâs future plans\n\nProfile.\n\n\n\nYuan Bingze, YY Live Software Engineer, has more than 10 years of working experience in big data risk control platform development. Deeply interested in common big data components.\nBackground\nYY Live is the leading voice social platform company in China. Currently, our team is mainly responsible for securing the companyâs business.\n01 Technical Status\nWe currently use a layered technical architecture, with the bottom layer being the data source layer, followed by the collection layer, storage layer, management layer, computation, and application layers in descending order.\nIn the data source layer, we currently pull relational database data from various business parties, as well as data that is transmitted to us through APIs and streams like Kafka.\nThe collection layer uses a data collection system developed by ourselves.\nIn the storage layer, we currently put the data mainly in relational databases, such as Clickhouse, and a small portion will be in some non-relational databases, such as Redis and graph libraries. Of course, most of the data is stored in big data systems.\nThe management team mainly consisted of a big data management system, combined with a computational scheduling and task management system and service governance platform developed by ourselves.\n02 Problems we encountered before adopting Apache DolphinScheduler\n1ãScheduling platform is too complex: In addition to the teamâs Xxl-job based task scheduling, some of the older projects have used Crontab, Springboot, Scheduler, Quartz, etc. to manage the start of tasks.\n2ãStrong demand for task dependencies: The scheduling we currently use can only set up the execution of individual tasks, and cannot form workflows through task dependencies, which rely heavily on personal experience to set up timing. Many tasks need to have dependencies.\n3ãThe tasks are complex and diverse: the current tasks are Spark and Flink tasks based on big data systems, various Java services, Shell, Java applications, Python, etc. tasks in the service governance platform.\nIntroducing Apache DolphinScheduler\nDuring the research, we found a demanding scheduling platform should meet the following conditions.\n1ãUnified management of tasks and dependencies\nWith the increasing demand for business computing, especially a variety of graph computing and tasks, which are scattered among various systems, and very difficult to manage. Besides, some of the tasks have certain dependencies on each other, but the configuration of their time relies on personal experience. There is an urgent need for a product that can unify configuration management dependencies.\n2ãCompatible with the companyâs internal platform system\nThe scheduling task platform is aimed to manage our tasks and to come into service quickly, the scheduling platform needed to be compatible with our companyâs inner platform systems, such as the internal Datax and Crontab services.\n3ãHigh availability, high performance, high concurrency, and easy to use\nFinally, to ensure business stability, we also need this scheduling platform to be highly available, high performance, concurrent, and easy to use.\nThrough our research, we found that Apache DolphinScheduler was perfectly designed for us, and the adaptation process met our needs without much modification.\nApplication and Adaptation\nApache DolphinScheduler is a distributed, decentralized, easily scalable visual DAG workflow task scheduling system dedicated to solving the intricate dependencies in the data processing process and making it work out-of-the-box in the data processing, which fits our needs perfectly.\nFirst, letâs look at the architecture of Apache DolphinScheduler to facilitate understanding the next adaptation cases.\n\n\n\nApache DolphinScheduler has 5 main modules: API, master, worker, log, and alert.\nThe API interface layer is mainly responsible for handling requests from the front-end UI layer. The service provides RESTful API to provide request services to the outside in a unified manner. The interfaces include workflow creation, definition, query, modification, release, downlink, manual start, stop, pause, resume, start execution from that node, etc.\nThe MasterServer adopts the concept of distributed centerless design, and is mainly responsible for DAG task slicing, task submission monitoring, and listening to the health status of other MasterServer and WorkerServer at the same time. The MasterServer service registers temporary nodes with Zookeeper when it starts and performs fault tolerance by listening to the changes of Zookeeper temporary nodes.\nWorkerServer also adopts the distributed centerless design concept, which is mainly responsible for task execution and providing logging services.\nWorkerServer service registers temporary nodes with Zookeeper when it starts and maintains heartbeat, as well as provides logger service.\nAlert provides alarm-related interfaces, which mainly include two types of alarm data storage, query and notification functions. The notification function includes email notification andÂ SNMP (not yet implemented).\nCurrently, we are deploying version 2.0 on 4 physical machines, which have born 2 master instances, 2 API instances, 3 worker and logger instances, and one alert instance.\nNext, we share 3 specific cases of adaptation based on Apache DolphinScheduler.\nFirst is the adaptation to our service governance platform, which aims to do task monitoring; although Apache DolphinScheduler itself provides a task monitoring module, our colleagues have long been accustomed to using the service governance platform to unify management monitoring. So we need to report the status of Apache DolphinScheduler tasks to the service governance platform in time.\n01 Service Governance platform Adaptation â MasterServer Service Description\nBefore the adaption description, we take a detailed look at the MasterServer service first, which provides:\n1ãDistributed Quartz, a distributed scheduling component, is mainly responsible for timing task start and stop operations, when Quartz picks up the task, there will be thread pools inside the Master specifically handling the subsequent operations of the task.\n2ãMasterSchedulerThread is a scanning thread that scans the command table in the database at regular intervals and performs different business operations according to different command types.\n3ãMasterExecThread (WorkflowExecutThread.java) takes charge of DAG task slicing, task submission monitoring, and logical processing of various command types.\n4ãMasterTaskExecThread is mainly responsible for task persistence.\n02 Service Governance Adaptation-code\nWe require to monitor tasks. Through code analysis, we found that task submission and listening are mainly implemented in the methods of the WorkflowExecuteThread class, which starts multiple instance threads responsible for task execution and listening respectively. The flowchart is as follows:\n\n\n\nTask submission and monitoring flow chart\nWe aim to monitor tasks, and after analyzing the code, we found that WorkflowExecuteThread implements task execution and listening by startprocess and handle events respectively. We inject our service governance platform data collection code in the handleEvents method so that the task monitoring situation can be reported to our service governance platform in time.\nThe modified part is as follows:\n\n\n\nThe specific effect of the service governance platform is shown below:\nTask submission\n\n\n\nSuccess rate\n\n\n\nIn addition to monitoring the status of our specific tasks, we will also do some monitoring by project, and finally, we will monitor operation through the service governance platform, for example, if some tasks are important, we will configure some telephone alarms, that is, if the task fails or is not executed on time, we will make telephone notifications.\n03 Datax Service Adaptation Process\nThe second case is about the adaptation of the Datax service. When we were working on Apache DolphinScheduler, we found that it has integrated Datax type tasks, which is very friendly for us. Because we have a significant number of tasks that are implemented through Datax, we have developed some Datax plugins to adapt the data read and write to various internal systems and stores.\nDatax adaptation is divided into two parts, one method is achieved by a custom template, which copies some previous Datax services, and takes slight modification, mainly involving some data interaction between the NoSQL databases.\nFor the interaction between SQL databases, we still need to achieve it through the configuration.\nUnfortunately, we encountered a small bug when configuring Clickhouse read and write tasks at the beginning.\n04 Datax Service Adaptation â Clickhouse Compatible #8092\nWhen we used Datax to read data from Clickhouse data source, we found that in the SQL, the submission would fail once we refer to parameters, no matter time parameters or other parameters. We suspected that there might be some bugs, and when we read the error log, we also found that when Apache DolphinScheduler submitted the SQL, the parameters are not replaced, and directly submitted to Clickhouse for execution. Because Clickhouse did not recognize Apache DolphinScheduler parameters, it directly threw an exception. We combed through the process of Apache DolphinScheduler reading Clickhouse when executing a Datax task. One of the processes in converting our Apache DolphinScheduler configuration to a Datax configuration is as follows:\n\n\n\n\n\n\nThe first thing the system has to do is to parse all the syntax of SQL and then get some column information through the syntax, at which point it has to call the SQL parser. In this process, if the Apache DolphinScheduler does not replace our parameters, errors will occur during the execution of the circle, which will cause the whole task to fail.\nTherefore, in the process of solving, since the parser of Clickhouse may not be obtained, the best way is to directly add a parser. First, we build a JSON file, then format all the chains parsed out, and finally go through a parsing of the syntax, calling it layer by layer, and finally being able to call the target parser.\n05 Time parameter adaptation\nThe last case is about time parameter adaptation.\nWhile Apache DolphinScheduler does provide time parameters, most of our data require unixtime that is accurate to the millisecond level. Reading through the Apache DolphinScheduler documentation, we, unfortunately, found that it does not provide an implementation of this type of time parameter. While going through the source code later, we found that Apache DolphinScheduler provides a timestamp function that can provide unixtime values.\nWhen using timestamp, we found two small problems, firstly, there will be ambiguity when timestamp directly expresses unixtime, and secondly, timestamp is only accurate to the second level, while most of our data needs millisecond level. To make it easier to use, we made some changes.\n\n\n\nAdaptation process\nThe first thing we did was to remove the ambiguity. In Apache DolphinScheduler, Timestamp is the way to express time, which is usually expressed by date plus time, but Unix time uses GMT, from 00:00:00:00 on January 1, 1970, to the present, and does not take into account the microsecond time expression, which uses integers.\nOnce requirements are clear, the next step for us is to figure out how to implement them. We found by code analysis that the implementation of the time parameter function is through the API calling layer by layer, and the final main functions are achieved through the TimePlaceHolderUtils class calculateTime method. During the implementation of this method, the constants in the TaskConstants class that express the name of the time function are also called. So we modified some of the constants of the TaskConstants class. And because we need millisecond-level functions, we added a milli_unixtime function, and finally, to meet the needs of device users, we added some functions with higher precision, such as microsecond and nanosecond functions.\n\n\n\ntime parameter adaptation-calling procedure\n\n\n\n\n\n\n\n\n\nAfter using Apache DolphinScheduler, we only need to check the complementary function when manually executing tasks and fill in the date we want to schedule, then we can directly make the complementary, and we can also fill in the parallelism. This feature is very useful for us, and after Apache DolphinScheduler version 2.0, the problem of time configuration and execution with daily performance difference is also solved, which brings great convenience in use.\nFuture Planning\nIn application, we found that the tasks configured through Apache DolphinScheduler do not currently support a highly available solution in terms of using data sources, which is strongly needed in our case, so we are currently doing the adaptation for high availability as well.\nSecondly, we are currently using Apache DolphinScheduler version 2.0, the community is active and the version upgrade is fast, even a small version upgrade will bring some great features or design changes. For example, in the new version, the alert function has been plugged in, and some complementary date conversion problems have been solved. This also drove our team to upgrade to the new version to experience the new features. Although Apache DolphinScheduler is currently only used internally in our team, we are thinking about the feasibility of making it available to the entire company.\nAlthough Apache DolphinScheduler is very perfect to solve most of our problems and improve our work efficiency drastically, we still encounter some small bugs in various complex situations, and of course, we have developed some features in use, all of which we will submit to the official after fixing in the future.\nJoin the Community\nThere are many ways to participate and contribute to the DolphinScheduler community, including:\nDocuments, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.\nWe assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.\nSo the community has compiled the following list of issues suitable for novices:Â https://github.com/apache/dolphinscheduler/issues/5689\nList of non-newbie issues:Â https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\nHow to participate in the contribution:Â https://dolphinscheduler.apache.org/en-us/community\nGitHub Code Repository:Â https://github.com/apache/dolphinscheduler\nOfficial Websiteï¼https://dolphinscheduler.apache.org/\nMailListï¼dev@dolphinscheduler@apache.org\nTwitterï¼@DolphinSchedule\nYouTubeï¼https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\nSlackï¼https://s.apache.org/dolphinscheduler-slack\nContributor Guideï¼https://dolphinscheduler.apache.org/en-us/community\nYour Star for the project is important, donât hesitate to lighten a Star for Apache DolphinScheduler â¤ï¸\n",
    "title": "How Does Live-broadcasting Platform Adapt to Apache DolphinScheduler?",
    "time": "2022-4-16"
  },
  {
    "name": "How_Does_Ziru_Build_A_Job_Scheduling_System_Popular_Among_Data_Analysts",
    "content": "How Does Ziru Build A Job Scheduling System Popular Among Data Analysts?\n\n\n\n\nAs one of the creators of enterprise data assets, data analysts need to command certain professional skills such as dimension&amp;metrics management, pedigree analysis, and ETL scheduling platform operations. However, for data analysts with varying levels of programming skills, a scheduling platform that is easy to operate and use will give them wings rather than adding additional learning costs. &gt;Compared with most companies, Ziru differs from others in that a large amount of data warehouse processing on its big data platform is not done by professional warehouse engineers, but by data analysts. The reason why the data analysts of Ziru can do the complex data processing and analysis work that can only be done by professional teams is that their scheduling system is migrated to Apache DolphinScheduler. &gt;At the Apache DolphinScheduler &amp; Apache ShenYu(Incubating) Meetup, Liu Tao, the R&amp;D Manager of Ziru Big Data Platform, shared with us what a popular scheduling system for data analysts looks like.\n\nAuthor Bio\n\n\n\nLiu Tao, the R&amp;D manager of Ziru Big Data Platform, is responsible for building the basic platform of Ziru Big Data and constructing a one-stop Big Data development platform.\n1 Status of Ziru Big Data Platform\nZiru Big Data Platform consists of Data Sources, Data Access, Data Process, Data Sink , and Data Application layers. The data sources include MySQL, Oracle, and other business library data, as well as various log data, which are collected through Hive offline T plus 1method, In addition to the use of Hive acid plus Flink to achieve a 10-minute level business library data updates.\nData processing is the part that analysts care about, in which you can configure scheduling, dependencies, and SQL development. As for the data sink, we use ClickHouseâs OLAP engine, and the data application layer uses NetEase Youshu to provide the reporting platform.\nZiruâs big data platform is almost the same as most platforms in the industry, But what is unique is that in addition to professional data warehouse development engineers, a large number of data analysts are also involved in the processing of data warehouses. This requires the big data platform to be simplified enough.\n2 Analystsâ expectations for a job scheduling system\nDue to the varying levels of coding among data analysts, some analysts can write SQL, while others cannot write SQL at all. Even analysts who can write SQL find it difficult to understandÂ task-dependent concepts.\nAs a result, the analystsâ expectations for a scheduler are to be simple and have a low cost of use.\n3 How Airflow is implemented\n\n\n\nHivepartitionsensor in Airflow\n\n\n\nIn the beginning, Ziyu used the Airflow solution, which offers a visualization plug-in Airflow DAG createmanager plug-in for analysts to use. We use hivepartitionsensor at the underlying level, with data dependency configuration scheduling. This set of solutions is a fair experience for analysts but faces several major problems.\n\n\nThe underlying implementation of data dependencies leads to very complex task re-runs;\n\n\nPoor scheduling performance for multi-tasks, with some tasks having a higher calling delay;\n\n\nThe cost for integration with the one-stop big data platform re-development is relatively high;\n\n\nMulti-tenancy is not natively supported.\n\n\n4 Apache DolphinScheduler Transformation and Airflow Task Migration\nThese challenges above prompted us to make a new scheduling selection. After a comparative analysis, we chose Apache DolphinScheduler.\nData dependency is a well-understood concept for analysts, but task dependency is more puzzling.\nA more ideal solution would be to show data dependencies to the analyst, with the underlying implementation being task dependencies, and this data dependency can be produced automatically, without a manual dependency table entered by the analysts.\nTo achieve this goal, we need to solve a problem first,Â how to determine the input and output table of this SQL based on a piece of SQL?\n\n\n\nSince it is in the Hive environment, we need to look at the parsing process of Hive SQL.\nAs shown above, hive uses antlr for syntactic and semantic parsing to generate an abstract syntax tree. For example, a SQL statement is as follows:\n\n\n\nParsed it into a syntax tree:\n\n\n\nTraversing this abstract syntax tree gives us the exact input and output, and we found that we donât need to do it from scratch, which can be simply implemented in Hive 147.\nhttps://issues.apache.org/jira/browse/HIVE-147\n\n\n\nOnce we have parsed the input and output, we can associate the input and output tables with the corresponding Apache DolphinScheduler scheduling tasks, thus achieving the goal of showing the analysts a data dependency while the underlying implementation is a task dependency. Though this implementation will produce small tasks, most of which end up with only one table output, leading to a large number of scheduling tasks. But so far, it does not pose performance problems, and we can see it as a compromise solution.\n\n\n\nThis is followed by the challenge of smoothly migrating tasks from Airflow to Apache DolphinScheduler. The tasks in airflow are all Python files, and the scheduler of airflow keeps scanning the file directory where the Python files are located to generate the scheduling tasks. The core implementation class is DagFileProcessorManager above, we can refer to the implementation of this class to parse the Python tasks and generate the JSON strings needed for the Apache DolphinScheduler task definition, to complete the migration of scheduling tasks.\nThatâs all I have to share, thank you!\nJoin the Community\nThere are many ways to participate and contribute to the DolphinScheduler community, including:\nDocuments, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.\nWe assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.\nSo the community has compiled the following list of issues suitable for novices:Â https://github.com/apache/dolphinscheduler/issues/5689\nList of non-newbie issues:Â https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\nHow to participate in the contribution:Â https://dolphinscheduler.apache.org/en-us/community\nGitHub Code Repository:Â https://github.com/apache/dolphinscheduler\nOfficial Websiteï¼https://dolphinscheduler.apache.org/\nMailListï¼dev@dolphinscheduler@apache.org\nTwitterï¼@DolphinSchedule\nYouTubeï¼https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\nSlackï¼https://s.apache.org/dolphinscheduler-slack\nContributor Guideï¼https://dolphinscheduler.apache.org/en-us/community\nYour Star for the project is important, donât hesitate to lighten a Star for Apache DolphinScheduler â¤ï¸\n",
    "title": "How Does Ziru Build A Job Scheduling System Popular Among Data Analysts?",
    "time": "2022-4-16"
  },
  {
    "name": "How_can_more_people_benefit_from_big_data",
    "content": "How can more people benefit from big data?\n\nDuring the ApacheCon Asia 2022, Chen Wei, who has more than 10 years of experience in Big Data development and is head of an offline data factory development tool from a bank, delivered a keynote speech on how to enable more people to benefit from Big Data.\nMany thanks to volunteer Guan Bo for your text editing work, your contribution to Apache DolphinScheduler is what keeps the community moving forward!\nThis presentation focused on the following four key elements.\nBackground Information\nBusiness managers would like access to reliable data to help them make better business decisions.\nEnd-users want to get their data quickly within a few minutes or hours. They donât want to go through the traditional process where they need to submit requirements to colleagues to the data department and wait weeks to get the data about requirement changes, development, and online implementation.\nData development teams are getting bigger and more sophisticated to manage than ever before. Everyone who works with data wants immediate access to business data to help them make decisions.\nData sources are becoming more complex, such as databases, web pages, logs, files, ERP, external data, etc.\nUser Needs\nAdvanced users are looking for limited self-service, with easy, analytical access to data through self-service.\nApplication developers only care about the business logic, not the specific underlying technology, they also donât want to go deeper into the configuration associated with platform operations and maintenance.\nHiding as much technical detail as possible, allows business developers as well as business users to focus more on the business logic and achieving business goals.\nMore user-friendly for system administrators, allowing them to quickly and easily manage the data platform effectively.\nBased on the above user requirements, we believe that DataOps is the way to go, and DataOps built on top of DevOps is the best way to achieve data agility.\nWhat is DataOps?\nDataOps is a collaborative data management practice focused on improving the communication, integration, and automation of data flows between data administrators, data consumers, and data developers within an organization.\nThe goal of DataOps is to deliver value quicker by establishing predictable delivery as well as managing change to data, data models, and related artifacts.\nDataOps uses technology to automate the design, deployment, and management of data delivery through suitable levels of governance and uses metadata to improve the availability and value of data in a dynamic environment.\nDataOps helps us achieve rapid data innovation, deliver insights to the business customer at a faster rate, and provide data quality to support data personnel.\nIn 2018, DataOps was included in the Technology Maturity Curve for Data Management, marking the official adoption and rollout of DataOps by the wider industry. Although DataOps can lower the barriers to data analytics, it does not make data analytics a simple task.\nFocus of DataOps\n\nRapid innovation and experimentation to deliver better value to customers at an ever-increasing rate\nDelivering extremely high-quality data with very low error rates.\nDelivers interactive value through collaboration between complex arrays of people as well as technologies and environments.\nClarity of measurement and monitoring and transparency of results.\n\nDataOps Core Component\nJob Scheduler\nProvides scheduling of workflows, and offline data development and enables data developers to focus on the implementation of business logic and improve development efficiency.\nDevTools\nMainly provides development tools such as traditional scripting languages SQL, Python, etc. for quick and easy integration with the scheduling platform.\nMigration and Deployment tools\nData migration and deployment management tools. Aimed towards industries where the development and testing environment is network isolated, it requires effective import and export functions for proper deployment to the production environment.\nManagement and monitoring tools\nAdministrator-friendly management, monitoring, and alert tools for offline tasks related to operations and maintenance.\nJob Scheduler\nNext, we will compare different job schedulers together with you.\nAdvantages and disadvantages of mainstream job scheduler\n\n\nOozie\nOozie is an open-source workflow engine-based framework that provides job scheduling and coordination for Hadoop MapReduce, and Pig Jobs. Oozie needs to be deployed to the Java Servlet container to function. It is mainly used for scheduling tasks at regular intervals and multiple tasks can be scheduled in sequential logical order of execution.\n\n\nAirflow\nAirflow is an open-source project for Airbnbâs Workflow, Python-based task management, job scheduling, and monitoring workflow platform. Airflow is a DAG (directed acyclic graph) based job scheduling system, which can be interpreted as an advanced version of crontab, but it solves the task dependency problem that Crontab cannot handle. Compared to Crontab, Airflow makes it easy to monitor the status of tasks (if they were executed successfully or not, time of execution, execution dependencies, etc.), track the history of tasks, receive email notifications when tasks fail, and view error logs.\n\n\nApache Dolphinscheduler\nDolphinScheduler is a decentralized, scalable and easily visualized DAG workflow job scheduling platform. Dedicated to solving the complex dependencies in the data processing process and enabling scheduling systems to be used right out of the box in the data processing process.\n\n\nControl-M\nControl-M is a commercial version of a cross-platform job scheduling management software with powerful features but a lesser degree of programmability.\n\n\nAzkaban\nAzkaban is a bulk workflow job scheduler released by Linkedin to execute a set of jobs and processes within a workflow in a specific order. Azkaban uses job profiles to establish dependencies between jobs and provides an easy-to-use web user interface to maintain and track your workflows. Azkaban requires all nodes to be deployed on a peer-to-peer basis, but under certain circumstances does not require full node peering as long as it supports high availability.\n\n\nScheduler Services\n\nTimer Service\n\nProvides a scheduled service of Crontab expressions to execute workflows on a scheduled cycle.\n\n\nDAG Computation\nDAG computing often refers to the internal division of a computational job into several smaller jobs, which are logically related or sequentially constructed into a DAG (directed acyclic graph) structure.\n\n\nTask Execution\nTask Execution is run by the scheduling systemâs execution engine according to the type of task, parameters, environment, referenced data source, etc.\n\n\nEnvironment Manage\nTargeting distributed scheduling engines, need to provide node management capabilities to facilitate task execution at different nodes. This enables tasks to run at a scale well beyond the limits of a single machine.\n\n\nAlert Service\nNotify users and system administrators when tasks fail, time out, or are not completed within a specified time.\n\n\nWhatâs more?\n\n\nIn addition to the above, we hope that the scheduler can provide environment and data source management functions. After the workflow has been configured with an environment parameter, all tasks can then directly refer to it. Data sources can be made available for other tasks in an injection method, depending on the type of task.\nJob Meta\n\n\nTask Execution Cycle\n\n(minutes, hourly, daily, weekly &amp; monthly)\n\nDependency Meta\n\nUpstream metadata information (files, tables, etc.) depending on the task.\n\nOutput Meta\n\nDownstream metadata information (files, tables, etc.) depends on the task.\n\nDependency Meta Classification\n\nClassify and differentiate the sources of dependent metadata.\n\nDependency Meta Source\n\nReverse processing of task-dependent metadata to obtain upstream and downstream tasks, providing a logical basis for automatic workflow scheduling and initiation.\n\nJob Parallel Execution Info(parallel, serial)\n\nControls the parallel execution status of jobs (parallel, serial).\n\nJob Type\n\nType of job (SQL, SHELL, PYTHON, PROCEDURE, etc.)\nJob Development\nIntegration IDE(script language,shell,SQL,python etc)\nIntegrated IDE development environment (Script language, Shell, SQL, Python tasks, etc.)\nconfigurable resource file(jarï¼spark, etc)\nConfigurable resource filesï¼jar, spark, tasks that cannot be edited using text, etc.ï¼\ncustom components\nCustomizable components, such as off-line data platforms where data synchronization is a specific component, require only a simple configuration of data synchronization to implement the synchronization process.\nThird-Party Job\nThird-party job, where a specific application generates a custom script to accomplish the job goal.\nThird-Party Job Integration\nThere are two methods of integration with third-party jobs, with the second method being the primary method used.\nFirst method: pull\n\nThe job scheduler provides an interface that can be accessed by the third-party system.\nDevelopment within the system and configuring the scheduler in the scheduler system. Configure the job in the third-party system, configure the scheduler in the scheduling system and execute the job.\nSingle-point of failure and bottleneck for job execution.\nDisadvantages: Requires multiple developments in third-party systems and configuration of the scheduler in the scheduling system.\n\nSecond method: push\n\nThe scheduling system provides a programmable API.\nThird-party systems create jobs and push them to the scheduling system, where they are automatically configured and scheduled.\n\nJob Execution And Notification\n\nJob execution: automatically triggered, based on conditions, upstream task dependencies\nJob notifications: progress and status of job execution sent to third-party systems as a push message\nSample Configuration For Data Integration\n\nSample configurations for third-party system integration are listed above.\nThe job scheduling system provides a programmable interface to enable the injection of environmental information. You only need to specify the different types of data sources in the third-party system, you do not need to configure the details of each data source e.g. (IP, port, username, password, etc.), this part is configured in the scheduling system and the data source and environment information are injected to the job. The job then parses the data source and environment from the parameters and accesses the corresponding data source and environment. This simplifies the management of data sources for ETL jobs.\nJob Assignments\nTraditional job scheduling was implemented by users dragging and connecting links, but we have improved the process by:\n\nIntegrating job scheduling into the workflow.\nThe configuration of upstream and downstream jobs is achieved through job-dependent metadata information.\nSome of the tables (some dimension tables) in ETL jobs are immutable and can be added to a whitelist. The workflow eliminates the need to look for business logic upstream of the corresponding table in the whitelist when making automatic changes and resolving dependencies.\nViews are an important factor affecting workflow automation. Views do not have ETL jobs, so we need to consider the view as a virtual job and import the view data into the scheduling platform so that the scheduling system is aware that the jobs that depend on the view are fundamentally dependent on the tasks corresponding to the underlying tables in the view.\n\nJob Impact Analysis\nThanks to the existence of metadata information, analysis of the impact of jobs is very straightforward.\n\nThe dependencies of the upstream and downstream metadata are used to resolve the upstream jobs which are job-dependent, avoiding the use of the manual configuration to reduce the probability of errors.\nThe calculation of some of the latest impacts, e.g. job suspension, failure, and impact of downstream jobs.\n\nLow Code Platform\n\nFor general situations, it is a good choice to build template jobs that can be reused to calculate processes for a large number of job configurations, requiring only a single configuration of the core process to set up the parameters.\nSupport of specific systems, such as metrics management systems, tagging systems, data desensitization systems, etc.\nCoding friendly, able to perform coding when required.\nUser friendly, under certain circumstances, the system coding is used rather than through user coding, the user can obtain the required data to assemble a specific data model without having to re-code it, e.g. the user can assemble a derivative tag based on the base tag by simple click configuration.\n\nHow to contribute:\n\n\nGitHub Code Repository: https://github.com/apache/dolphinscheduler\n\n\nOfficial Website:https://dolphinscheduler.apache.org/\n\n\nMail List:dev@dolphinscheduler@apache.org\n\n\nTwitter:@DolphinSchedule\n\n\n",
    "title": "How can more people benefit from big data?",
    "time": "2022-10-29"
  },
  {
    "name": "How_did_Yili_explore_a_path_for_digital_transformation_based_on_DolphinScheduler",
    "content": "How did Yili explore a âpathâ for digital transformation based on DolphinScheduler?\n\nBackground and introduction of application\nIntroduction of Yili\nPeople in general know about Yili. Every year, 1.3 billion national consumers consume an average of 100 million packs of Yili products every day. The starting point of Yiliâs business can be traced back to the cultivation of grass, the breeding of a cow, the production of a cup of milk, and finally delivered to consumers through a complex supply chain system, which involves the primary, secondary and tertiary industries.\nBackground of the application\nThere is a complex application matrix behind the huge business volume, which poses a great challenge to our technical architecture.\nÂ· The current multi-cloud distribution of applications makes cross-cloud data migration and multi-cloud unified scheduling a rigid requirement\nCurrently, in the process of cooperation with cloud service providers, Yili does not only consider the IaaS and PaaS abilities supported by cloud service providers, but also the ecology behind them, including private domain ecology, e-commerce, logistics, etc. Therefore, our applications are more distributed on multiple clouds, and the data generated by these applications are also found on multiple clouds. The current centralized data structure requires a large amount of physical data relocation, and we need to migrate 8,000 tables per day, involving more than 80 systems. Stable and scalable multi-cloud data integration is thus crucial.\nÂ· Unified technical architecture to combat entropy increase, reduce cost and increase efficiency\nThe first challenge we faced was the ease of use, stability, and scalability of data integration and scheduling abilities due to massive relocation requirements. The second one came from the effect of entropy increase brought by the landing of a large number of applications. In the case of limited technical resources, it was difficult for us to effectively control our costs and improve our efficiency, especially in the face of data integration. We were facing the following core issues:\nâ¢ Duplicate construction of similar tools and products: Due to a lack of unified planning, duplicate construction of similar functional products exists.\nâ¢ Miscellaneous technology selection: AirFlow, Azkaban, Oozie, self-developed scheduling, etc.\nâ¢ High construction and derivative costs: costs such as personnel resource reserves, operation and maintenance, and operation training for multiple technology stacks.\nâ¢ Extensibility issues: extensive support for localized individual requirements.\nBefore the introduction of Apache DolphinScheduler, our overall technical architecture was not unified in scheduling and data integration. To meet our internal rigid requirements, and with the consideration of scalability and stability, we planned to build a unified scheduling &amp; data integration service system. After researching a large number of scheduling products, we decided to use Apache DolphinScheduler as our core engine and make localized transformations on top of it.\nThe positioning and application of the status of the Yili Big Data Scheduling System\nWith limited technical resources, it is challenging to initiate the development of a tool. We must be clear about who our core users are, what the core positioning of this product is, and in what circumstances is this product used.\nAfter sufficient research, we have clarified the core positioning of this tool. It is not a scheduling platform for simple workflow, but a development platform of integrated data that suits the multi-cloud business of Yili. Application developers support the integration of internal and external data, scheduling requirements of the application, visual orchestration, and scheduling of data tasks.\nJudging from the current application of Yili, the number of daily scheduling tasks has reached 13,000, there are 15 nodes of clusters and more than 8000 tables are relocated every day.\nWe have built a unified data platform for data integration, development, scheduling, operation, and maintenance on the multi-cloud infrastructure for big data. Through this platform, we can hide the differences between multi-cloud big data platforms and provide users with a unified development experience.\n**Â· System Overview of the Data Scheduling Service Platform of Yili\n**\nWe have re-abstracted the 2.0.2 version of the entire model of DolphinScheduler, with the project as the top model, abstracting the three concepts of resources, roles, and tools, and bound them to the project. Among them, the toolset is the most important. All the tools used in the data development process would be expanded here. Users can complete data integration, development, operation and maintenance, asset management, and other tasks by authorizing projects in the platform. We have also integrated the tenants of Apache DolphinScheduler with the tenants of the big data platform and completed the fine management of resource quotas and permissions through the tenant management system of Ranger+Ldap+Kerberos on the big data platform.\nPractice &amp; Exploration\nWe have upgraded three times based on Apache DolphinScheduler 2.0.2 version with many function optimization, but there is a logic that runs through our product design and development process, that is, to meet the core needs of users to build new functions, improve user experience and to optimize the old functions. We aim to make the product easy to use, easy to get used to, user-friendly, and with reasonable moving lines.\nScenario-oriented data integration\nWhat is scenario-oriented data integration? Our products are completely open to users. When developers do data integration, they have a very clear context, such as synchronizing Mysql data to Hive, ES data to StarRocks, etc. Therefore, we provide a large number of integration components, which can complete the integration based on the configuration method. In the whole process, no code is needed. This improves the efficiency of data integration development.\nBefore planning the core functions of data integration, we had the following main problems:\n\n\nMultiple data integration components or technologies make it difficult for users to choose\n\n\nAn inconsistent technical structure that leads to increased maintenance costs\n\n\nThe configuration of data integration is troublesome\n\n\nPoor scalability for secondary development\n\n\nWe implemented the following strategies for these four problems :\n\n\nWe only gave users the best choice â there are often no choices to choose from, so we will just provide the best one.\n\n\nBased on the structure of DataX unified data integration, we solved problems related to resources and reduced operation and maintenance costs\n\n\nFor business applications, we used the template generator to generate task templates to simplify operations\n\n\nWe implemented a reasonable code design\n\n\nA lightened asset management\nThe core requirement corresponding to this function is as follows: when applying the native big data capability to build a lightened data warehouse, it is necessary to gain a clear insight into the technical metadata information, including library, table, field, and basic attributes. We listed some business tag information, including subject domains, topics, etc., to facilitate the use of data by analysts. Based on such demands, we extended this lightened asset management function on the platform.\nForward and reverse table building are the core functions. Forward table building refers to supporting users to create table models on the platform and materialize the table models on the big data platform. Reverse table building is used more often. In many cases, users still use traditional development methods to write scripts on the ETL server and submit them to the big data platform for calculation. At this time, the table already exists in advance, rather than being created visually. To reverse building a table is to automatically scan the metadata and register it on the asset platform.\nAsset retrieval provides asset retrieval functions for development and business analysis. It can perform lightweight retrieval based on subject domains, tags, field names, etc., and display asset cards, and upstream and downstream bloodline information.\nScheduling of multi-cloud tasks\nScheduling of multi-cloud tasks is to solve the integrated scheduling problem of multi-cloud platforms. As mentioned above, Yiliâs applications are distributed in multiple clouds, and this function can complete cross-cloud task scheduling based on a canvas. We made some extensions and transformations based on Apache DolphinScheduler. The binding relationship between the environment and the worker group is flexibly used, and the big data infrastructure is used to solve the problem of Kerberos cross-domain and mutual trust of multi-cluster Keytab.\nWe can separately trigger data integration tasks, including data processing tasks on different clouds. After the processing is completed, data calculation and business application-oriented planning are triggered on another cloud, and data services are provided finally. In this way, multi-cloud scheduling is done.\nOptimization of monitoring dashboard\nBased on the original Apache DolphinScheduler dashboard, we did some optimization and upgrade. By focusing on operation and maintenance, we built the core indicators that the operation and maintenance work focuses on and displayed the indicators visually.\nThe entire monitoring board is divided into task monitoring and cluster service monitoring.\nFor task monitoring, we design key indicators like the number of projects, workflow (online/offline), the number of tasks to be scheduled, the number of tasks that have been successfully scheduled, an overview of the hourly operation, the TOP5 statistics on running failures, and the TOP5 statistics on running time.\nFor service monitoring, key indicators include CPU, memory, average load, worker load in each cloud environment, and hourly worker operation overview.\nUpgrade and optimization of usersâ experience\nWe also upgraded and optimized the user experience, such as the optimization of usersâ timed configuration and the overall layout, which aims to optimize the user experience and make it easier to use.\nPlans\nLastly, these are the plans for DolphinScheduler.\nLong-term planning\nIn the long run, both the community and our company need to have a clear positioning for DolphinScheduler. It is not a scheduling platform for simple workflow, but a one-stop intelligent data development platform. There are at least three important characteristics of this platform:\n\n\nMulti-cloud unification: It can perfectly fit with the new decentralized data structure, which is a very important direction;\n\n\nLow-code: From the userâs point of view, it can support one-stop visual modeling, development, and analysis experience for the whole link;\n\n\nIntelligent management: The current data structure is based on reducing the complexity of data management, metadata-driven governance, and realizing intelligent data quality management will also be an important proposition.\n\n\nShort and mid-range plans:\nThree important points will be focused on in the short term :\n\n\nThe combination of cloud-native: The DolphinScheduler community has provided deployment services based on k8S, so our master, worker, and other services will be transformed based on containerization;\n\n\nIntroduce the testing and online process: add roles such as developers and reviewers in the project, and control the online actions, and the metadata will be automatically synchronized after getting online;\n\n\nQuality module: DolphinScheduler 3.0 has added the data quality module. We will integrate this part and carry out the localized expansion, including pre-defining data quality, auditing, introducing data blood relationships, etc.\n\n\nReflects on open-source\nDuring the process of introducing DolphinScheduler, Yili was driven and inspired by some thoughts on collaboration. That is itâs very important to build a good collaboration between the company and open-source.\nInternally, we will assign specific personnel to follow up on the dynamics of open-source communities. The R&amp;D team will proactively find bugs and submit them to the community, and submit PRs after they are fixed.\nAt the same time, we will also actively interact with the community and check the trends through weekly and monthly meetings. Companies with limited technical research and development resources should find an open-source product that fits the actual use, and truly understand and implement the meaning of open-source. It is a powerful tool, as it can solve many problems internally, and notice issues that were never noticed before. Therefore, companies need to implement a suitable process for open-source and grow with the community.\nThatâs all of my talks. Thank you!\nHow to contribute:\n\n\nGitHub Code Repository: https://github.com/apache/dolphinscheduler\n\n\nOfficial Website:https://dolphinscheduler.apache.org/\n\n\nMail List:dev@dolphinscheduler@apache.org\n\n\nTwitter:@DolphinSchedule\n\n\n",
    "title": "How did Yili explore a âpathâ for digital transformation based on DolphinScheduler?",
    "time": "2022-10-31"
  },
  {
    "name": "How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications",
    "content": "In the 2.0.1 version of Apache DolphinScheduler, a plugin architecture improvement was introduced to enhance the flexibility and user-friendliness of the system. Components such as tasks, alert plugins, data sources, resource storage, and registry centers are now designed as extension points. This allows for targeted development of alert plugins to address specific alert requirements in enterprise applications.\nCurrent Version: 3.1.2\nAlert Plugin Development\nLet's take a look at the directory structure of the alert module:\n\n\ndolphinscheduler-alert-api\nThis module serves as the core module for ALERT SPI. It defines the interfaces for extending alert plugins and provides some basic code. The AlertChannel and AlertChannelFactory interfaces are the key interfaces that need to be implemented for developing alert plugins.\ndolphinscheduler-alert-plugins\nThis module includes the official alert plugins provided by DolphinScheduler. Currently, there are dozens of supported plugins, such as Email, DingTalk, and Script.\ndolphinscheduler-alert-server\nThis module represents the alert service, which is responsible for registering alert plugins and sending alert messages via Netty.\n\nIn this article, we will use the official HTTP alert plugin as an example to demonstrate how to develop a plugin.\n\nFirst, let's clarify the requirements. The HTTP alert plugin needs to send requests via HTTP, and to do that, we need to determine the necessary parameters. In the HttpAlertConstants, you can find the definitions of some related parameters.\n\npackage org.apache.dolphinscheduler.plugin.alert.http;\npublic final class HttpAlertConstants {\n    public static final String URL = &quot;$t('url')&quot;;\n\n    public static final String NAME_URL = &quot;url&quot;;\n\n    public static final String HEADER_PARAMS = &quot;$t('headerParams')&quot;;\n\n    public static final String NAME_HEADER_PARAMS = &quot;headerParams&quot;;\n\n...........................Omitting redundant code \n\n    private HttpAlertConstants() {\n        throw new UnsupportedOperationException(&quot;This is a utility class and cannot be instantiated&quot;);\n    }\n}\n\n\nThe corresponding parameters that need to be filled in for the alert instance are shown in the image below.\n\n\nHere, parameters in the style of $t('url') can be added by editing the\n\ndolphinscheduler-ui/src/locales/zh_CN/security.ts\n\nOnce added, the frontend will automatically replace them. Similarly, the English dictionary also needs to be updated to avoid errors when switching to English.\n\nIn the HttpAlertChannelFactory, you need to implement the AlertChannelFactoryinterface and its methods: name,paramsandcreate,The first parameter of InputParam.newBuilder represents the displayed value, and the second parameter represents the parameter name. You can use the constants defined in MailParamsConstants that we mentioned earlier. After defining all the parameters, add them to the paramsList and return.\n\n@AutoService(AlertChannelFactory.class)\npublic final class HttpAlertChannelFactory implements AlertChannelFactory {\n    @Override\n    public String name() {\n        return &quot;Http&quot;;\n    }\n    @Override\n    public List&lt;PluginParams&gt; params() {\n        InputParam url = InputParam.newBuilder(HttpAlertConstants.NAME_URL, HttpAlertConstants.URL)\n                                   .setPlaceholder(&quot;input request URL&quot;)\n                                   .addValidate(Validate.newBuilder()\n                                                        .setRequired(true)\n                                                        .build())\n                                   .build();\n        InputParam headerParams = InputParam.newBuilder(HttpAlertConstants.NAME_HEADER_PARAMS, HttpAlertConstants.HEADER_PARAMS)\n                                            .setPlaceholder(&quot;input request headers as JSON format &quot;)\n                                            .addValidate(Validate.newBuilder()\n                                                                 .setRequired(true)\n                                                                 .build())\n                                            .build();\n        InputParam bodyParams = InputParam.newBuilder(HttpAlertConstants.NAME_BODY_PARAMS, HttpAlertConstants.BODY_PARAMS)\n                                          .setPlaceholder(&quot;input request body as JSON format &quot;)\n                                          .addValidate(Validate.newBuilder()\n                                                               .setRequired(false)\n                                                               .build())\n                                          .build();\n...........................Omitting redundant code \n        return Arrays.asList(url, requestType, headerParams, bodyParams, contentField);\n    }\n    @Override\n    public AlertChannel create() {\n        return new HttpAlertChannel();\n    }\n}\n\n\nIn the HttpAlertChannel, you need to implement the AlertChannel interface and its process method. The alertInfo.getAlertData().getAlertParams() method can be used to retrieve the parameters entered when creating the alert instance. Write the relevant code here to send the request, and return an AlertResult object to indicate whether the request was successfully sent or not.\n\npublic final class HttpAlertChannel implements AlertChannel {\n    @Override\n    public AlertResult process(AlertInfo alertInfo) {\n        AlertData alertData = alertInfo.getAlertData();\n        Map&lt;String, String&gt; paramsMap = alertInfo.getAlertParams();\n        if (null == paramsMap) {\n            return new AlertResult(&quot;false&quot;, &quot;http params is null&quot;);\n        }\n        return new HttpSender(paramsMap).send(alertData.getContent());\n    }\n}\n\nWith that, the plugin development is complete. It's simple, isn't it? This is how elegant and efficient decoupled code should be when designed with a well-structured architecture.\nAfter completing the above development, start the alert service, and you will be able to select the corresponding plugin when adding an alert instance.\n\nSource Code Analysis\nWhen starting the alert service, you can see the information about registering the alert plugins in the logs.\n\nUse this as a starting point to explore the relevant code for plugin implementation.\n\nIn the AlertPluginManager of dolphinscheduler-alert-server, you can find the content for registering the alert plugins. First, get all the classes that implement AlertChannelFactory.class, and then iterate over them to obtain instances of AlertChannel. Add these instances to the database and the channelKeyedById map.\n\n    private final Map&lt;Integer, AlertChannel&gt; channelKeyedById = new HashMap&lt;&gt;();\n    \n    @EventListener\n    public void installPlugin(ApplicationReadyEvent readyEvent) {\n        PrioritySPIFactory&lt;AlertChannelFactory&gt; prioritySPIFactory = new PrioritySPIFactory&lt;&gt;(AlertChannelFactory.class);\n        for (Map.Entry&lt;String, AlertChannelFactory&gt; entry : prioritySPIFactory.getSPIMap().entrySet()) {\n            String name = entry.getKey();\n            AlertChannelFactory factory = entry.getValue();\n            logger.info(&quot;Registering alert plugin: {} - {}&quot;, name, factory.getClass());\n            final AlertChannel alertChannel = factory.create();\n            logger.info(&quot;Registered alert plugin: {} - {}&quot;, name, factory.getClass());\n            final List&lt;PluginParams&gt; params = new ArrayList&lt;&gt;(factory.params());\n            params.add(0, warningTypeParams);\n            final String paramsJson = PluginParamsTransfer.transferParamsToJson(params);\n            final PluginDefine pluginDefine = new PluginDefine(name, PluginType.ALERT.getDesc(), paramsJson);\n            final int id = pluginDao.addOrUpdatePluginDefine(pluginDefine);\n            channelKeyedById.put(id, alertChannel);\n        }\n    }\n\n\nAfter developing and registering the plugins, a polling thread is needed to iterate and perform actions for querying and sending messages. The run method of AlertSenderService handles this.\n\n@Override\npublic void run() {\n    logger.info(&quot;alert sender started&quot;);\n    while (!ServerLifeCycleManager.isStopped()) {\n        try {\n            List&lt;Alert&gt; alerts = alertDao.listPendingAlerts();\n            AlertServerMetrics.registerPendingAlertGauge(alerts::size);\n            this.send(alerts);\n            ThreadUtils.sleep(Constants.SLEEP_TIME_MILLIS * 5L);\n        } catch (Exception e) {\n            logger.error(&quot;alert sender thread error&quot;, e);\n        }\n    }\n}\n\n\nThe key method is this.send(alerts). Iterate over the Alert instances, retrieve the instances of alert plugins, and pass them along with the alert parameters to this.alertResultHandler(instance, alertData). Finally, update the status of the alert message.\n\npublic void send(List&lt;Alert&gt; alerts) {\n    for (Alert alert : alerts) {\n        // get alert group from alert\n        int alertId = Optional.ofNullable(alert.getId()).orElse(0);\n        int alertGroupId = Optional.ofNullable(alert.getAlertGroupId()).orElse(0);\n        List&lt;AlertPluginInstance&gt; alertInstanceList = alertDao.listInstanceByAlertGroupId(alertGroupId);\n        if (CollectionUtils.isEmpty(alertInstanceList)) {\n            logger.error(&quot;send alert msg fail,no bind plugin instance.&quot;);\n            List&lt;AlertResult&gt; alertResults = Lists.newArrayList(new AlertResult(&quot;false&quot;,\n                    &quot;no bind plugin instance&quot;));\n            alertDao.updateAlert(AlertStatus.EXECUTION_FAILURE, JSONUtils.toJsonString(alertResults), alertId);\n            continue;\n        }\n        AlertData alertData = AlertData.builder()\n                .id(alertId)\n                .content(alert.getContent())\n                .log(alert.getLog())\n                .title(alert.getTitle())\n                .warnType(alert.getWarningType().getCode())\n                .alertType(alert.getAlertType().getCode())\n                .build();\n\n        int sendSuccessCount = 0;\n        List&lt;AlertResult&gt; alertResults = new ArrayList&lt;&gt;();\n        for (AlertPluginInstance instance : alertInstanceList) {\n            AlertResult alertResult = this.alertResultHandler(instance, alertData);\n            if (alertResult != null) {\n                AlertStatus sendStatus = Boolean.parseBoolean(String.valueOf(alertResult.getStatus()))\n                        ? AlertStatus.EXECUTION_SUCCESS\n                        : AlertStatus.EXECUTION_FAILURE;\n                alertDao.addAlertSendStatus(sendStatus, JSONUtils.toJsonString(alertResult), alertId,\n                        instance.getId());\n                if (sendStatus.equals(AlertStatus.EXECUTION_SUCCESS)) {\n                    sendSuccessCount++;\n                    AlertServerMetrics.incAlertSuccessCount();\n                } else {\n                    AlertServerMetrics.incAlertFailCount();\n                }\n                alertResults.add(alertResult);\n            }\n        }\n        AlertStatus alertStatus = AlertStatus.EXECUTION_SUCCESS;\n        if (sendSuccessCount == 0) {\n            alertStatus = AlertStatus.EXECUTION_FAILURE;\n        } else if (sendSuccessCount &lt; alertInstanceList.size()) {\n            alertStatus = AlertStatus.EXECUTION_PARTIAL_SUCCESS;\n        }\n        alertDao.updateAlert(alertStatus, JSONUtils.toJsonString(alertResults), alertId);\n    }\n}\n\n\nIn the alertResultHandler method, use alertPluginManager.getAlertChannel(instance.getPluginDefineId()) to retrieve an instance of AlertChannel. Remember when we put the AlertChannel instances into the channelKeyedById map during the registration of alert plugins?\n\npublic Optional&lt;AlertChannel&gt; getAlertChannel(int id) {\n    return Optional.ofNullable(channelKeyedById.get(id));\n}\n\n\nThen, build an AlertInfo object and use CompletableFuture.supplyAsync() to asynchronously execute alertChannel.process(alertInfo). Obtain the returned AlertResult using future.get() and return it.\n\nprivate @Nullable AlertResult alertResultHandler(AlertPluginInstance instance, AlertData alertData) {\n    String pluginInstanceName = instance.getInstanceName();\n    int pluginDefineId = instance.getPluginDefineId();\n    Optional&lt;AlertChannel&gt; alertChannelOptional = alertPluginManager.getAlertChannel(instance.getPluginDefineId());\n    if (!alertChannelOptional.isPresent()) {\n        String message = String.format(&quot;Alert Plugin %s send error: the channel doesn't exist, pluginDefineId: %s&quot;,\n                pluginInstanceName,\n                pluginDefineId);\n        logger.error(&quot;Alert Plugin {} send error : not found plugin {}&quot;, pluginInstanceName, pluginDefineId);\n        return new AlertResult(&quot;false&quot;, message);\n    }\n    AlertChannel alertChannel = alertChannelOptional.get();\n\n    Map&lt;String, String&gt; paramsMap = JSONUtils.toMap(instance.getPluginInstanceParams());\n    String instanceWarnType = WarningType.ALL.getDescp();\n\n    if (paramsMap != null) {\n        instanceWarnType = paramsMap.getOrDefault(AlertConstants.NAME_WARNING_TYPE, WarningType.ALL.getDescp());\n    }\n\n    WarningType warningType = WarningType.of(instanceWarnType);\n\n    if (warningType == null) {\n        String message = String.format(&quot;Alert Plugin %s send error : plugin warnType is null&quot;, pluginInstanceName);\n        logger.error(&quot;Alert Plugin {} send error : plugin warnType is null&quot;, pluginInstanceName);\n        return new AlertResult(&quot;false&quot;, message);\n    }\n\n    boolean sendWarning = false;\n    switch (warningType) {\n        case ALL:\n            sendWarning = true;\n            break;\n        case SUCCESS:\n            if (alertData.getWarnType() == WarningType.SUCCESS.getCode()) {\n                sendWarning = true;\n            }\n            break;\n        case FAILURE:\n            if (alertData.getWarnType() == WarningType.FAILURE.getCode()) {\n                sendWarning = true;\n            }\n            break;\n        default:\n    }\n\n    if (!sendWarning) {\n        logger.info(\n                &quot;Alert Plugin {} send ignore warning type not match: plugin warning type is {}, alert data warning type is {}&quot;,\n                pluginInstanceName, warningType.getCode(), alertData.getWarnType());\n        return null;\n    }\n\n    AlertInfo alertInfo = AlertInfo.builder()\n            .alertData(alertData)\n            .alertParams(paramsMap)\n            .alertPluginInstanceId(instance.getId())\n            .build();\n    int waitTimeout = alertConfig.getWaitTimeout();\n    try {\n        AlertResult alertResult;\n        if (waitTimeout &lt;= 0) {\n            if (alertData.getAlertType() == AlertType.CLOSE_ALERT.getCode()) {\n                alertResult = alertChannel.closeAlert(alertInfo);\n            } else {\n                alertResult = alertChannel.process(alertInfo);\n            }\n        } else {\n            CompletableFuture&lt;AlertResult&gt; future;\n            if (alertData.getAlertType() == AlertType.CLOSE_ALERT.getCode()) {\n                future = CompletableFuture.supplyAsync(() -&gt; alertChannel.closeAlert(alertInfo));\n            } else {\n                future = CompletableFuture.supplyAsync(() -&gt; alertChannel.process(alertInfo));\n            }\n            alertResult = future.get(waitTimeout, TimeUnit.MILLISECONDS);\n        }\n        if (alertResult == null) {\n            throw new RuntimeException(&quot;Alert result cannot be null&quot;);\n        }\n        return alertResult;\n    } catch (InterruptedException e) {\n        logger.error(&quot;send alert error alert data id :{},&quot;, alertData.getId(), e);\n        Thread.currentThread().interrupt();\n        return new AlertResult(&quot;false&quot;, e.getMessage());\n    } catch (Exception e) {\n        logger.error(&quot;send alert error alert data id :{},&quot;, alertData.getId(), e);\n        return new AlertResult(&quot;false&quot;, e.getMessage());\n    }\n}\n\nIn summary, the sequence diagram below illustrates the registration of plugins and the sending of messages.\n\nThis covers the main implementation code for alert plugins. As you can see, the source code doesn't appear to be too complex or difficult to understand. So, take some time to explore the source code. In the future, you can also contribute to open-source projects and create such excellent software.\nReferences\n\n[Feature] Alert Plugin Design Â· Issue #3049 Â· apache/dolphinscheduler (github.com)\n\n\nalert (apache.org)\n\n",
    "title": "How to use Apache DolphinScheduler for targeted alarm plugin development for enterprise applications?",
    "time": "2024-01-19"
  },
  {
    "name": "Introducing-Apache-DolphinScheduler-1.3.9",
    "content": "Introducing Apache DolphinScheduler 1.3.9, StandaloneServer is Available!\nOn October 22, 2021, we are excited to announce the release of Apache DolphinScheduler 1.3.9. After a month and a halfï¼Apache DolphinScheduler 1.3.9 brings StandaloneServer to users with the joint efforts of the community. StandaloneServer is a major update of this version, which means a huge leap in ease of use, and the details will be introduced below. In addition, this upgrade also fixes two critical bugs in 1.3.8.\n1.3.9 Downloadï¼1.3.9 Download Link\nIn 1.3.9, the main updates include:\nNew Features\n[Feature#6480] Add StandaloneServer to make development and operation easier\nStandaloneServer is a service created to allow users to quickly experience the product. The registry and database H2-DataBase and Zk-TestServer are built-in. Users can start StandaloneServer for debugging with only one line of command.\nThe way to start StandaloneServer with one-line command: switch to a user with sudo permission, and run the script\nsh ./bin/dolphinscheduler-daemon.sh start standalone-server\n\nIt shows that 1.3.9 reduces the configuration cost through built-in components. You only need to configure the jdk environment to start the DolphinScheduler system with one click, thereby improving the efficiency of research and development. For detailed usage documents, please refer to: link to be added.\n \nAccess the front page address, interface IP (self-modified) http://localhost:12345/dolphinscheduler, with the default name and password:admin/dolphinscheduler123.\nThe detailed user docs for Standalone, please refer to:1.3.9 standalone-server\nOptimization and Fix\nâ[Fix #6337][task] Sql limit param no default value\nWhen the SqlTask ââis executed, if the limit parameter is not set, the displayed result is empty. Based on this, the default parameters have been added in 1.3.9, and relevant instructions have been made on the log to allow users to track the problem more clearly.\nâ[Bug#6429] [ui] sub_process node open sub_task show empty page #6429\nThe problem that the sub_task node is displayed as empty has been fixed.\nContributor\nThanks to PMC Ke Zhenxu from the SkyWalking community for his contribution to StandaloneServer, Apache DolphinScheduler will persistently optimize the function design and enhance the user experience with the participation of more contributors, so stay tuned.\n1 DolphinScheduler introduction\nApache DolphinScheduler is a distributed and extensible workflow scheduler platform with powerful DAG visual interfaces, dedicated to solving complex job dependencies in the data pipeline and providing various types of jobs available out of box.\nDolphinScheduler assembles Tasks in the form of DAG (Directed Acyclic Graph), which can monitor the running status of tasks in real time. At the same time, it supports operations such as retry, recovery from designated nodes, suspend and Kill tasks, and focuses on the following 6 capabilities :\n\n2 Partial User Cases\nAccording to incomplete statistics, as of October 2020, 600+ companies and institutions have adopted DolphinScheduler in production environments. Partial cases are shown as below (in no particular order).\n3 Participate in Contribution\nWith the prosperity of open source in China, the Apache DolphinScheduler community ushered in vigorous development. In order to make better and easy-to-use scheduling system, we sincerely welcome partners who love open source to join Apache DolphinScheduler Community.\nThere are many ways to participate in and contribute to the Apache DolphinScheduler Community, including:\n\nContribute to the first PR (document, code). We hope it to be simple and a try to get yourself familiar with the submission process and community collaboration.\nWe have compiled a list of issues suitable for novices: Good First Issues\nAnd a list of issues for non-newbie: Volunteer Wanted\nHow to participate in the contribution: Participate in Contributing\n\nApache DolphinScheduler Community needs you! Even if a small piece of tile will make a big differnce.\nIf you are interested in contributing code we created Good First Issues  to get you started. If you have any questions about code, installation, and docs please do not hesitate to reach out to us on slack.\nCommunity Official Website\nhttps://dolphinscheduler.apache.org/\nCode Warehouse Address\nhttps://github.com/apache/dolphinscheduler\n",
    "title": "Introducing Apache DolphinScheduler 1.3.9, StandaloneServer is Available!",
    "time": "2021-10-22"
  },
  {
    "name": "Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2",
    "content": "Apache DolphinScheduler\nBy referring to the official upgrade documentation, it is known that upgrade scripts are provided. If it is a minor version upgrade, executing the script should suffice. However, when upgrading across multiple major versions, various issues can still arise. Therefore, a summary of these issues is provided.\nold versionï¼1.3.4\nnew versionï¼3.1.2\n1. Error &quot;IllegalArgumentException: Failed to specify server's Kerberos principal name&quot; when using Resource Center after the upgrade\nThe Resource Center is configured to use HDFS with Kerberos authentication enabled.\nSolution:\nEdit dolphinscheduler/api-server/conf/hdfs-site.xml and add the following content:\n&lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal.pattern&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n2. Error &quot;Log not found&quot; when viewing task instance logs after the upgrade\nUpon checking the error message and comparing the directory structure and log paths in the new version, it is found that the log path has been changed in the new version.\nThe old log path was located under /logs/, while the new log path is /worker-server/logs/.\nTherefore, the directory needs to be modified accordingly.\nSolution:\nExecute the following SQL statement to modify the log path:\nupdate t_ds_task_instance set log_path=replace(log_path,&#x27;/logs/&#x27;,&#x27;/worker-server/logs/&#x27;);\n\nThen, copy the original log files to the new log path:\ncp -r {old_dolphinscheduler_directory}/logs/[1-9]* {new_dolphinscheduler_directory}/worker-server/logs/*\n\n3. Error when creating workflows after the upgrade\nUpon checking the error message, it is found that the initial values of the primary keys for t_ds_process_definition_log and t_ds_process_definition are inconsistent.\nTo resolve this, the primary keys need to be made consistent.\nSolution:\nExecute the following SQL statement:\n# Retrieve the auto-increment value of the primary key\nselect AUTO_INCREMENT FROM information_schema.TABLES WHERE TABLE_SCHEMA = &#x27;dolphinscheduler&#x27; AND TABLE_NAME = &#x27;t_ds_process_definition&#x27; limit 1\n\n# Replace {max_id} with the above result and execute the statement\nalter table dolphinscheduler_bak1.t_ds_process_definition_log auto_increment = {max_id};\n\n4. Task instance list is empty after the upgrade\nCheck the SQL query in dolphinscheduler-dao/src/main/resources/org/apache/dolphinscheduler/dao/mapper/TaskInstanceMapper.xml file under the select id=&quot;queryTaskInstanceListPaging&quot; section.\n\t\tselect\n        &lt;include refid=&quot;baseSqlV2&quot;&gt;\n            &lt;property name=&quot;alias&quot; value=&quot;instance&quot;/&gt;\n        &lt;/include&gt;\n        ,\n        process.name as process_instance_name\n        from t_ds_task_instance instance\n        left join t_ds_task_definition_log define on define.code=instance.task_code and define.version=instance.task_definition_version\n        left join t_ds_process_instance process on process.id=instance.process_instance_id\n        where define.project_code = #{projectCode}\n        &lt;if test=&quot;startTime != null&quot;&gt;\n            and instance.start_time &lt;![CDATA[ &gt;=]]&gt; #{startTime}\n        &lt;/if&gt;\n\t\t......Omitting redundant code\n\nThe SQL query for querying the task instance list is associated with the t_ds_task_definition_log table. After inspection, it was found that the join condition define.code = instance.task_code cannot be matched.\nConsidering the query condition define.project_code = #{projectCode}, it can be inferred that the purpose of joining the t_ds_task_definition_log table is mainly to filter by projectCode. Let's modify the SQL accordingly.\nSolution:\n    \tselect\n        &lt;include refid=&quot;baseSqlV2&quot;&gt;\n            &lt;property name=&quot;alias&quot; value=&quot;instance&quot;/&gt;\n        &lt;/include&gt;\n        ,\n        process.name as process_instance_name\n        from t_ds_task_instance instance\n--         left join t_ds_task_definition_log define \n--\t\t\t\ton define.code=instance.task_code and \n--\t\t\t\t\tdefine.version=instance.task_definition_version\n        join t_ds_process_instance process\n        \ton process.id=instance.process_instance_id\n        join t_ds_process_definition define\n        \ton define.code=process.process_definition_code\n        where define.project_code = #{projectCode}\n        &lt;if test=&quot;startTime != null&quot;&gt;\n            and instance.start_time &lt;![CDATA[ &gt;=]]&gt; #{startTime}\n        &lt;/if&gt;\n\t\t......Omitting redundant code\n\nModify the SQL query to directly use the t_ds_process_definition table for the association, as it also has the project_code field for filtering.\n5. NullPointerException during the execution of the upgrade script\n5.1 Analysis of the logs led to line 517 in UpgradeDao.java\n513                     if (TASK_TYPE_SUB_PROCESS.equals(taskType)) {\n514                       JsonNode jsonNodeDefinitionId = param.get(&quot;processDefinitionId&quot;);\n515                       if (jsonNodeDefinitionId != null) {\n516                           param.put(&quot;processDefinitionCode&quot;,\n517                                  processDefinitionMap.get(jsonNodeDefinitionId.asInt()).getCode());\n518                            param.remove(&quot;processDefinitionId&quot;);\n519                        }\n520                     }\n\nUpon examining the code, it is evident that processDefinitionMap.get(jsonNodeDefinitionId.asInt()) returns null. Add a null check to skip the null value and print the relevant information for verification after the upgrade.\nSolution:\nAfter modification:\n            if (jsonNodeDefinitionId != null) {\n                if (processDefinitionMap.get(jsonNodeDefinitionId.asInt()) != null) {\n                param.put(&quot;processDefinitionCode&quot;,processDefinitionMap.get(jsonNodeDefinitionId.asInt()).getCode());\n                param.remove(&quot;processDefinitionId&quot;);\n            } else {\n                    logger.error(&quot;*******************error&quot;);\n                logger.error(&quot;*******************param:&quot; + param);\n                logger.error(&quot;*******************jsonNodeDefinitionId:&quot; + jsonNodeDefinitionId);\n            }\n                    }\n\n5.2 Analysis of the logs led to line 675 in UpgradeDao.java\n669 if (mapEntry.isPresent()) {\n670                            Map.Entry&lt;Long, Map&lt;String, Long&gt;&gt; processCodeTaskNameCodeEntry = mapEntry.get();\n671                            dependItem.put(&quot;definitionCode&quot;, processCodeTaskNameCodeEntry.getKey());\n672                            String depTasks = dependItem.get(&quot;depTasks&quot;).asText();\n673                            long taskCode =\n674                                    &quot;ALL&quot;.equals(depTasks) || processCodeTaskNameCodeEntry.getValue() == null ? 0L\n675                                            : processCodeTaskNameCodeEntry.getValue().get(depTasks);\n676                            dependItem.put(&quot;depTaskCode&quot;, taskCode);\n677                        }\n\nUpon examining the code, it is evident that processCodeTaskNameCodeEntry.getValue().get(depTasks) returns null. Modify the logic to assign a value and print the relevant log only if it is not null.\nSolution:\nAfter modification:\n        long taskCode =0;\n        if (processCodeTaskNameCodeEntry.getValue() != null\n                &amp;&amp;processCodeTaskNameCodeEntry.getValue().get(depTasks)!=null){\n        taskCode =processCodeTaskNameCodeEntry.getValue().get(depTasks);\n        }else{\n                logger.error(&quot;******************** depTasks:&quot;+depTasks);\n                logger.error(&quot;******************** taskCode not in &quot;+JSONUtils.toJsonString(processCodeTaskNameCodeEntry));\n        }\n        dependItem.put(&quot;depTaskCode&quot;, taskCode);\n\n6. Login failure after integrating LDAP, unknown field name for email\nLDAP integration can be configured in api-server/conf/application.yaml.\nsecurity:\n  authentication:\n    # Authentication types (supported types: PASSWORD,LDAP)\n    type: LDAP\n    # IF you set type `LDAP`, below config will be effective\n    ldap:\n      # ldap server config\n      urls: xxx\n      base-dn: xxx\n      username: xxx\n      password: xxx\n      user:\n        # admin userId when you use LDAP login\n        admin: xxx\n        identity-attribute: xxx\n        email-attribute: xxx\n        # action when ldap user is not exist (supported types: CREATE,DENY)\n        not-exist-action: CREATE\n\nTo successfully integrate LDAP, the following fields need to be correctly filled in the configuration: urls, base-dn, username, password, identity, and email. If the email field name is unknown, follow the steps below, leaving the email field empty for now:\nStart the service and attempt to log in with an LDAP user.\nSolution:\n** The LDAP authentication code is located in dolphinscheduler-api/src/main/java/org/apache/dolphinscheduler/api/security/impl/ldap/LdapService.java under the ldapLogin() method.**\n        ctx = new InitialLdapContext(searchEnv, null);\n        SearchControls sc = new SearchControls();\n        sc.setReturningAttributes(new String[]{ldapEmailAttribute});\n        sc.setSearchScope(SearchControls.SUBTREE_SCOPE);\n        EqualsFilter filter = new EqualsFilter(ldapUserIdentifyingAttribute, userId);\n        NamingEnumeration&lt;SearchResult&gt; results = ctx.search(ldapBaseDn, filter.toString(), sc);\n        if (results.hasMore()) {\n            // get the users DN (distinguishedName) from the result\n            SearchResult result = results.next();\n            NamingEnumeration&lt;? extends Attribute&gt; attrs = result.getAttributes().getAll();\n            while (attrs.hasMore()) {\n                // Open another connection to the LDAP server with the found DN and the password\n                searchEnv.put(Context.SECURITY_PRINCIPAL, result.getNameInNamespace());\n                searchEnv.put(Context.SECURITY_CREDENTIALS, userPwd);\n                try {\n                    new InitialDirContext(searchEnv);\n                } catch (Exception e) {\n                    logger.warn(&quot;invalid ldap credentials or ldap search error&quot;, e);\n                    return null;\n                }\n                Attribute attr = attrs.next();\n                if (attr.getID().equals(ldapEmailAttribute)) {\n                    return (String) attr.get();\n                }\n            }\n        }\n\nComment out the 3 line that filters based on the field filled.\n// sc.setReturningAttributes(new String[]{ldapEmailAttribute});\n\nAfter executing, the 10 line will return all fields.\nNamingEnumeration&lt;? extends Attribute&gt; attrs = result.getAttributes().getAll();\n\nFind the email field through printing or debugging and fill it in the configuration file.\nUncomment the previously commented line of code.\nRestart the service to enable successful LDAP integration and login.\n7. Authorization of resource files by administrators for ordinary users does not take effect\nAfter multiple tests, it was found that ordinary users can only see resource files that belong to them, even after being granted authorization by administrators.\nSolution:\nModify the listAuthorizedResource() method in the file dolphinscheduler-api/src/main/java/org/apache/dolphinscheduler/api/permission/ResourcePermissionCheckServiceImpl.java to return relationResources instead of the current collection.\n        @Override\n        public Set&lt;Integer&gt; listAuthorizedResource(int userId, Logger logger) {\n            List&lt;Resource&gt; relationResources;\n            if (userId == 0) {\n                relationResources = new ArrayList&lt;&gt;();\n            } else {\n                // query resource relation\n                List&lt;Integer&gt; resIds = resourceUserMapper.queryResourcesIdListByUserIdAndPerm(userId, 0);\n                relationResources = CollectionUtils.isEmpty(resIds) ? new ArrayList&lt;&gt;() : resourceMapper.queryResourceListById(resIds);\n            }\n            List&lt;Resource&gt; ownResourceList = resourceMapper.queryResourceListAuthored(userId, -1);\n            relationResources.addAll(ownResourceList);\n            return relationResources.stream().map(Resource::getId).collect(toSet()); // Resolve the issue of invalid resource file authorization.\n//            return ownResourceList.stream().map(Resource::getId).collect(toSet());\n        }\n\nCheck the change log of the new version and find that this bug has been fixed in version 3.1.3.\nhttps://github.com/apache/dolphinscheduler/pull/13318\n8.Kerberos expiration issue.\nDue to the expiration time set in the Kerberos configuration, the HDFS resources in the Resource Center will become inaccessible after a certain period of time. The best solution is to add relevant logic for scheduled credential update.\nSolution:\nAdd a method in the file dolphinscheduler-service/src/main/java/org/apache/dolphinscheduler/service/utils/CommonUtils.java.```java\n   /**\n    * * Scheduled credential update.\n    */\n   private static void startCheckKeytabTgtAndReloginJob() {\n       // Daily loop, scheduled credential update.\n       Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(() -&gt; {\n           try {\n               UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n               logger.warn(&quot;Check Kerberos Tgt And Relogin From Keytab Finish.&quot;);\n           } catch (IOException e) {\n               logger.error(&quot;Check Kerberos Tgt And Relogin From Keytab Error&quot;, e);\n           }\n       }, 0, 1, TimeUnit.DAYS);\n       logger.info(&quot;Start Check Keytab TGT And Relogin Job Success.&quot;);\n   }\n\nThen, call it before the loadKerberosConf method in that file returns true.\npublic static boolean loadKerberosConf(String javaSecurityKrb5Conf, String loginUserKeytabUsername,\n                                           String loginUserKeytabPath, Configuration configuration) throws IOException {\n        if (CommonUtils.getKerberosStartupState()) {\n            System.setProperty(Constants.JAVA_SECURITY_KRB5_CONF, StringUtils.defaultIfBlank(javaSecurityKrb5Conf,\n                    PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH)));\n            configuration.set(Constants.HADOOP_SECURITY_AUTHENTICATION, Constants.KERBEROS);\n            UserGroupInformation.setConfiguration(configuration);\n            UserGroupInformation.loginUserFromKeytab(\n                    StringUtils.defaultIfBlank(loginUserKeytabUsername,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME)),\n                    StringUtils.defaultIfBlank(loginUserKeytabPath,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH)));\n            startCheckKeytabTgtAndReloginJob();  // call here\n            return true;\n        }\n        return false;\n    }\n\n",
    "title": "Issue documentation and resolution of the Apache DolphinScheduler upgrade from 1.3.4 to 3.1.2",
    "time": "2024-01-19"
  },
  {
    "name": "Json_Split",
    "content": "Why did we split the big json that holds the tasks and relationships in the DolphinScheduler workflow definition?\nThe Background\nCurrently DolphinScheduler saves tasks and relationships in process as big json to the process_definition_json field in the process_definiton table in the database. If a process is large, for example, with 1000 tasks, the json field becomes very large and needs to be parsed when using the json, which is very performance intensive and the tasks cannot be reused, so the community plans to start a json splitting project. Encouragingly, we have now completed most of this work, so a summary is provided for your reference and learning.\nSummarization\nThe json split project was started on 2021-01-12 and the main development was initially completed by 2021-04-25. The code has been merged into the dev branch. Thanks to lenboo, JinyLeeChina, simon824 and wen-hemin for coding.\nThe main changes, as well as the contributions, are as follows:\n\nCode changes 12793 lines\n168 files modified/added\n145 Commits in total\nThere were 85 PRs\n\nReview of the split programme\n\n\n[ ] When the api module performs a save operation\n\n\nThe process definition is saved to process_definition (main table) and process_definition_log (log table), both tables hold the same data and the process definition version is 1\nThe task definition table is saved to task_definition (main table) and task_definition_log (log table), also saving the same data, with task definition version 1\nprocess task relationships are stored in the process_task_relation (main table) and process_task_relation_log (log table), which holds the code and version of the process, as tasks are organised through the process and the dag is drawn in terms of the process. The current node of the dag is also known by its post_task_code and post_task_version, the predecessor dependency of this node is identified by pre_task_code and pre_task_version, if there is no dependency, the pre_task_code and pre_task_version are and pre_task_version are 0 if there is no dependency\n\n\n[ ] When the api module performs an update operation, the process definition and task definition update the main table data directly, and the updated data is inserted into the log table. The main table is deleted and then inserted into the new relationship, and the log table is inserted directly into the new relationship.\n[ ] When the api module performs a delete operation, the process definition, task definition and relationship table are deleted directly from the master table, leaving the log table data unchanged.\n[ ] When the api module performs a switch operation, the corresponding version data in the log table is overwritten directly into the main table.\n\nJson Access Solutions\n\n\n\n[ ] In the current phase of the splitting scheme, the api module controller layer remains unchanged and the incoming big json is still mapped to ProcessData objects in the service layer. insert or update operations are done in the public Service module through the ProcessService. saveProcessDefiniton() entry in the public Service module, which saves the database operations in the order of task_definition, process_task_relation, process_definition. When saving, the task is changed if it already exists and the associated process is not online; if the task is associated with a process that is already online, the task is not allowed to be changed\n\n\n[ ] The data is assembled in the public Service module through the ProcessService.genTaskNodeList() entry, or assembled into a ProcessData object, which in turn generates a json to return\n\n\n[ ] The Server module (Master) also gets the TaskNodeList through the public Service module ProcessService.genTaskNodeList() to generate the dispatch dag, which puts all the information about the current task into the MasterExecThread. readyToSubmitTaskQueue queue in order to generate taskInstance, dispatch to worker\n\n\nPhase 2 Planning\nAPI / UI module transformation\n\n[ ] The processDefinition interface requests a back-end replacement for processDefinitonCode via processDefinitionId\n[ ] Support for separate definition of task, the current task is inserted and modified through the workflow, Phase 2 needs to support separate definition\n[ ] Frontend and backend controller layer json splitting, Phase 1 has completed the api module service layer to dao json splitting, Phase 2 needs to complete the front-end and controller layer json splitting\n\nserver module retrofit\n\n[ ] Replace process_definition_id with process_definition_code in t_ds_command and t_ds_error_commandãt_ds_schedules\n[ ] Generating a taskInstance process transformation\n\nThe current process_instance is generated from the process_definition and schedules and command tables, while the taskInstance is generated from the MasterExecThread. readyToSubmitTaskQueue queue, and the data in the queue comes from the dag object. At this point, the queue and dag hold all the information about the taskInstance, which is very memory intensive. It can be modified to the following data flow, where the readyToSubmitTaskQueue queue and dag hold the task code and version information, and the task_definition is queried before the task_instance is generated\n\n\nAppendix: The snowflake algorithm\nsnowflake: is an algorithm for generating distributed, drama-wide unique IDs called snowflake, which was created by Twitter and used for tweeting IDs.\nA Snowflake ID has 64 bits. the first 41 bits are timestamps, representing the number of milliseconds since the selected period. The next 10 bits represent the computer ID to prevent conflicts. The remaining 12 bits represent the serial number of the generated ID on each machine, which allows multiple Snowflake IDs to be created in the same millisecond. snowflakeIDs are generated based on time and can therefore be ordered by time. In addition, the generation time of an ID can be inferred from itself and vice versa. This feature can be used to filter IDs by time, and the objects associated with them.\n\n\nStructure of the snowflake algorithm:\n\nIt is divided into 5 main parts.\n\nis 1 bit: 0, this is meaningless.\nis 41 bits: this represents the timestamp\nis 10 bits: the room id, 0000000000, as 0 is passed in at this point.\nis 12 bits: the serial number, which is the serial number of the ids generated at the same time during the millisecond on a machine in a certain room, 0000 0000 0000.\n\nNext we will explain the four parts:\n\n\n1 bit, which is meaningless:\nBecause the first bit in binary is a negative number if it is 1, but the ids we generate are all positive, so the first bit is always 0.\n41 bit: This is a timestamp in milliseconds.\n41 bit can represent as many numbers as 2^41 - 1, i.e. it can identify 2 ^ 41 - 1 milliseconds, which translates into 69 years of time.\n10 bit: Record the work machine ID, which represents this service up to 2 ^ 10 machines, which is 1024 machines.\nBut in 10 bits 5 bits represent the machine room id and 5 bits represent the machine id, which means up to 2 ^ 5 machine rooms (32 machine rooms), each of which can represent 2 ^ 5 machines (32 machines), which can be split up as you wish, for example by taking out 4 bits to identify the service number and the other 6 bits as the machine number. This can be combined in any way you like.\n12 bit: This is used to record the different ids generated in the same millisecond.\n12 bit can represent the maximum integer of 2 ^ 12 - 1 = 4096, that is, can be distinguished from 4096 different IDs in the same milliseconds with the numbers of the 12 BIT representative. That is, the maximum number of IDs generated by the same machine in the same milliseconds is 4096\nIn simple terms, if you have a service that wants to generate a globally unique id, you can send a request to a system that has deployed the SnowFlake algorithm to generate the unique id. The SnowFlake algorithm then receives the request and first generates a 64 bit long id using binary bit manipulation, the first bit of the 64 bits being meaningless. This is followed by 41 bits of the current timestamp (in milliseconds), then 10 bits to set the machine id, and finally the last 12 bits to determine how many requests have been made in this millisecond on this machine in this room.\nThe characteristics of SnowFlake are:\n\nthe number of milliseconds is at the high end, the self-incrementing sequence is at the low end, and the entire ID is trended incrementally.\nit does not rely on third-party systems such as databases, and is deployed as a service for greater stability and performance in generating IDs.\nthe bit can be allocated according to your business characteristics, very flexible.\n\n",
    "title": "Why did we split the big json of DAG in workflow definition?",
    "time": "2021-06-03"
  },
  {
    "name": "K8s_Cisco_Hangzhou",
    "content": "Fully Embracing K8s, Cisco Hangzhou Seeks to Support K8s Tasks Based on ApacheDolphinScheduler\n\n\n\nK8s is the future of the cloud and is the only infrastructure platform that connects public and private clouds, making it the choice of more enterprises to modernize their IT.\nBased on Apache DolphinScheduler, Cisco Hangzhou is also exploring K8s support, and some of the features are already running successfully. Recently, Qian Li, a big data engineer from Cisco Hangzhou, will share their results with us.\n\n\n\nQian Li\nBig Data Engineer from Cisco Hangzhou, with years of experience in Big Data solutions, Spark, Flink, scheduling system, ETL, and other projects.\nMy presentation will be related to these parts: Namespace management, K8s tasks running continuously, workflow scheduling of K8s tasks, and our future planning.\n01 Namespace Management\nResource Management\nIn the first part, I will first introduce resource management. The purpose of introducing resource management is to use K8s clusters to run tasks that are not part of the Apache DolphinScheduler concept of scheduling, such as Namespace, which are more akin to a data solution that limits resources in a queue if the CPU has limited memory, and enabling some resource isolation.\nIn the future, we may merge some of the resource management functionality onto Apache DolphinScheduler.\nAdding, deleting, and maintaining management\nWe can add some Type, i.e. the type of tag, e.g. some Namespace only allows certain types of jobs to be run. We can count the number of jobs under the Namespace, the number of pods, the number of resources requested, requests, etc. to see the resource usage of the queue, and the interface is only available to the administrator by default.\n\n\n\nMultiple K8s clusters\nK8s supports multiple clusters, we can connect to multiple K8s clusters via the Apache DolphinScheduler client, batch, PROD, etc to build multiple sets of  K8s clusters and support multiple K8s clusters via Namespace.\nWe can edit the developed clusters and modify all the properties such as memory.\nIn the new version, user permissions are set in user master, authorizing a user to submit tasks to a Namespace and edit resources.\n02 K8s tasks running continuously\nThe second section is about the types of tasks we currently support.\nMirrors that are started without exiting, such as ETL\nETL, for example, is a task that must be done manually before it will exit after being committed. Once a task like this is committed, it sinks the data and theoretically never stops as long as no upgrades.\n\n\n\nThis kind of task may not actually be used for scheduling, as it only has two states, start and stop. So we put it in a live list and made a set of monitors. The POD runs in real-time, interacting mainly through a Fabris operator, and can be dynamically scaled to improve resource utilization.\nFlink tasks\nWe can manage the CPU down to 0.01%, making full use of the K8s virtual CPU.\n\n\n\n\n\n\n\n\n\nWe also use Flink Tasks, an ETL-based extension that includes an interface for editing, viewing status, going online, going offline, deleting, execution history, and monitoring. We designed the Flink UI using a proxy model and developed permission controls to prevent external parties from modifying it.\nFlink starts by default based on a checkpoint, or can be created at a specified time, or submitted and started based on the last checkpoint.\nFlink tasks support multiple mirror versions, as K8s is inherently mirror-running, you can specify mirrors directly to choose a package, or via the file to submit the task.\nAlso, Batch type tasks may be run once and finished or may be scheduled on a cycle basis and exit automatically after execution, which is not quite the same as Flink, so for this type of task, we would deal it based on Apache DolphinScheduler.\n03 Running K8s tasks\nWorkflow scheduling for K8s tasks\nWe added some Flink batch and Spark batch tasks at the bottom and added some configurations such as the resources used, the namespace to be run, and so on. The image information can be started with some custom parameters, and when wrapped up it is equivalent to the plugin model, which is perfectly extended by Apache DolphinScheduler.\n\n\n\nSpark Tasks\nUnder Spark tasks, you can view information such as CPU, files upload supports Spark Jar packages, or can be configured separately.\n\n\n\nThis multi-threaded upper layer can dramatically increase processing speed.\n04 Others and Delineation\nWatch Status\n\n\n\nIn addition to the above changes, we have also optimized the task run state.\nWhen a task is submitted, the runtime may fail and even the parallelism of the task may change based on certain policies in the real run state. In this case, we need to watch and fetch the task status in real-time and synchronize it with the Apache DolphinScheduler system to ensure that the status seen in the interface is always accurate.\nFor batch, we can treat it with or without the watch, as it is not a standalone task that requires fully watch and the namespace resource usage is based on watch mode so that the state is always accurate.\nMultiple environments\nMulti-environment means that the same task can be pushed to different K8s clusters, such as a Flink task.\nIn terms of code, there are two ways to do watch, one is to put some pods separately, for example when using the K8s module, define information about multiple K8s clusters, create some watch pods on each cluster to watch the status of tasks in the cluster and do some proxy functions. Another option is to follow the API or a separate service and start a watch service to track all K8s clusters. However, this does not allow you to do proxying outside of the K8s internal and external networks.\nThere are several options to watch Batch, one of them is by synchronization based on Apache DolphinScheduler, which is more compatible with the latter. We may submit a PR on this work in the future soon. Spark uses the same model, providing a number of pods to interact with, and the internal code we use is the Fabric K8s client.\nGoing forward, we will be working with Apache DolphinScheduler to support the features discussed here and share more information about our progress. Thank you all!\n",
    "title": "Fully Embracing K8s, Cisco Hangzhou Seeks to Support K8s Tasks Based on ApacheDolphinScheduler",
    "time": "2022-3-21"
  },
  {
    "name": "PyDolphinScheduler_Releases_Version_4.0.2_Fixing_Workflow_Submission_Issue_with_DolphinScheduler_3.1.4",
    "content": "PyDolphinScheduler releases version 4.0.2, fixing the problem that workflow cannot be submitted to DolphinScheduler 3.1.4\nPyDolphinScheduler officially releases version 4.0.2, which mainly fixes the problem that version 4.0.1 cannot submit workflows to Apache DolphinScheduler 3.1.4.\nIn addition, the major optimizations of PyDolphinScheduler 4.0.2 include:\n\nPyDolphinScheduler verifies the wrong version of Apache DolphinScheduler\nPython task type adds stmdency dependency\nThe problem of missing dependencies of lower versions of Python\n\nOptimization Details\n01 Fix the problem that the workflow cannot be submitted to DolphinScheduler 3.1.4\nPyDolphinScheduler 4.0.1 cannot submit workflows to Apache DolphinScheduler 3.1.4, because Apache DolphinScheduler 3.1.4 is released later than PyDolphinScheduler 4.0.1, and there are some incompatible updates.\nPyDolphinScheduler 4.0.2 version fixes this problem.\n02 The issue of verifying the version of DolphinScheduler incorrectly\nThis happens only in extreme situations, where the user does not use the official installation package of Apache DolphinScheduler, but modifies the code and packages it by himself, there may be a version problem reported by PyDolphinScheduler that is not supported.\nPyDolphinScheduler 4.0.2 is compatible with this scenario.\n03 Add stmdency dependency to Python task type\nBefore version 4.0.2, only the Python function wrapper introduced stmdency dependency parsing. In 4.0.2 and later versions, we have also added stmdency dependency parsing for the Python task type itself to ensure that the functional dependencies can be obtained.\nModification list\n01 Bugfixes\n\nSupport for submitting workflows to Apache DolphinScheduler 3.1.4\nDetect Apache DolphinScheduler version issues #69\nCI anomaly detection due to dev version #70\nPython task type supports stmdency #72\nAdd missing packaging dependencies #81\n\n02 Optimization\n\nWorkflow start and end time support datetime type, schedule detection #68\nMigrate CI related configuration to setup.cfg #82\n\n03 documents\nModify the release document\nRelease Notes\nhttps://github.com/apache/dolphinscheduler-sdk-Python/releases/tag/4.0.2\nThanks to contributors\nzhongjiajie\n",
    "title": "# PyDolphinScheduler releases version 4.0.2, fixing the problem that workflow cannot be submitted to DolphinScheduler 3.1.4",
    "time": "2023-3-27"
  },
  {
    "name": "Python_API_and_AWS_Support",
    "content": "Apache DolphinScheduler Extends Capabilities Through Python API and AWS Support\nIn the ever-changing world of technology, data is abundant. In fact, trillions of gigabytes of data are generated every day!\nStop and think about that for a second!\nStoring and processing these humongous amounts of data is no easy feat. Big data workflow schedulers such as the Apache DolphinScheduler and Apache Airflow help businesses perform operations on large volumes of data.\nLetâs take a deep dive and look at two prominent features of Apache DolphinScheduler: PyDolphinScheduler and AWS support.\nHappy reading!\nAbout PyDolphinScheduler\nBefore we formally define PyDolphinScheduler, letâs take a deep dive and understand what Apache DolphinScheduler does.\nApache DolphinScheduler is a distributed and extensible workflow scheduler platform that offers powerful DAG visual interfaces, helping data scientists and analysts author, schedule, and monitor batch data pipelines smoothly without needing heavy code.\n\nWhile users of Apache DolphinScheduler can monitor and tunnel Directed Acyclic Graphs (DAG) through its drag-and-drop visual interface (weâll talk more on this later), Apache DolphinScheduler lets users implement workflows through Python code for the best control over their workflows.\nWith this in mind, letâs formally define PyDolphinScheduler.\nAccording to their website,\nâPyDolphin Scheduler is the Python API for Apache DolphinScheduler, which allows you to define your workflows through Python code, aka workflow-as-codes.â\nIn essence, the DolphinScheduler Python API is a wrapper for the workflow scheduler that lets you easily create and define workflows. You can still run and monitor all your workflows through the visual DAG interface, making DolphinScheduler an attractive option compared to Apache Airflow and Apache Azkaban.\n\nYou get the best of both worlds: the innate versatility of the Python code followed by the intuitive ease of using the visual drag-and-drop interface.\nThe rate at which data is generated has blown out of proportion in the 21st century.\nAccording to Cisco, the global internet traffic is estimated to be around one zettabyte per year!\nLetâs stop short and put this into perspective. If each brick of the Great Wall of China measures 1 gigabyte, this hypothetically means that you could build a whopping 258 Great Walls of China with a single zettabyte!\nThe burden of analyzing vast amounts of data can be clearly seen here.\nThatâs what makes Python one of the best programming languages for analysis, maintenance, and deployment. Since Python is typically employed for data analytics and pipelines, PyDolphinScheduler is the perfect way to define your workflows.\nPython is also extensively used by machine learning and artificial intelligence teams and also in schedulers and various libraries.\nBenefits of Visual DAG\nLetâs check out the benefits of having a visual DAG interface.\nSince its release in 2016, Apache Airflow has become quite popular among data scientists and engineers.\nBut there was a caveat.\nWhile developers of Airflowâs code-first philosophy positively impacted the platformâs versatility, it proved disastrous at times. For example, developers often found maintaining and troubleshooting errors through code extremely hard and time-consuming.\nThis is where Apache DolphinScheduler shone through. Users now could create and define workflows easily through PyDolphinSceduler and then utilize the visual DAG interface to monitor the workflows.\nIn addition, Apache DolphinScheduler had one of the best visual suspension and error handling features: something that was unavailable in Apache Airflow, which agonized users a lot.\nTo show you how easy it is to create and run workflows in DolphinScheduler using the visual DAG method, hereâs an example:\nYou can drag-and-drop tasks to create or add them to existing workflows.\n\nIn addition, you can visually check and schedule the timing, including start time, end time, timezone, and Crontab expressions. Furthermore, you can visually set timing in advance for the next, say, 5 times.\n\nIf you wish to manually trigger the workflow, you can do so through the start button. After clicking on the start button, you can see the workflow instance and running status in the âWorkflow Instanceâ page.\n\nIn addition, you can also view the task log for specific workflow instances.\n\nSee how easy it is to handle workflows through the visual DAG interface?\nWhat is No-Code Workflow?\nIn the early days of data engineering, scientists found it extremely hard to merge specific actions with code.\nFor example, letâs say a database is about to run out of storage. Most likely, engineers would have some sort of troubleshooting or monitoring system in place. In addition, they had to verify whether it was a false alarm, then create a bigger instance, move the data, and redirect services to the new one â all the while testing for data integrity.\nSoon engineers realized that many of these steps could be automated through workflows with code.\nA workflow, after all, is a set of tasks with dependencies between them.\nFor big data, powerful schedulers like Apache Airflow and Apache DolphinScheduler helps users create workflows as codes, specifically in the form of Directed Acyclic Graphs. In workflow-as-a-code, the tasks are well-defined through precise inputs and outputs â making this system an excellent leap for data scientists and engineers.\nWhile it has benefits in and of itself, such as optimized performance and modeling, workflow-as-code has its own flaws.\nFirstly, it requires significant expertise in Python coding just to execute the workflow.\nSecondly, since the data comes out in the form of code, deciphering data and valuable information at a higher level is a daunting task.\nThirdly, making changes, monitoring, and handling errors is a nightmare when it comes to the code-only approach.\nThatâs why Apache DolphinScheduler employs a visual DAG interface to address these problems.\nWith the no-code workflow, even beginners and non-technical users can execute workflows and monitor them. The drag-and-drop capability allows users to easily envision data and overall logic â which is not available in Apache Airflow.\nFurthermore, this also reduces errors, which, when paired with Apache DolphinSchedulerâs error handling methods, can vastly improve performance, productivity, and efficiency.\nInstalling PyDolphinScheduler\nTo get started with PyDolphinSchduler, you must have Python and pip installed on your machine.\nInstalling Python\nInstalling Python and pip varies depending on the operating system you use. This Python Wiki offers detailed installation guides for various systems.\nFor PyDolphinScheduler, I recommend using Python 3.6 and above. For smoother usage, ensure that you install stable releases instead of beta versions.\nOnce youâve downloaded and installed Python, verify if your installation is successful by typing:\npython â version.\nYouâll see details about the installed Python version if your installation was successful.\nInstalling PyDolphinScheduler\nThe next step is to install PyDolphinScheduler by using pip.\nHereâs the terminal command:\npython -m pip install apache-dolphinscheduler\nThisâll install the latest version of PyDolphinScheduler.\n\nStarting the Python Gateway Service\nSince PyDolphinScheduler is the Python API for Apache Dolphin Scheduler, you need to install it before you can run workflows. You can define workflows and task structures, though.\nHereâs a detailed guide to help you install Apache DolphinScheduler.\nTask Dependencies and Types\nA task is a basic unit of execution in PyDolphinScheduler.\nIn essence, tasks are the nodes of a Directed Acyclic Graph, i.e., they are arranged into DAGs. Tasks have downstream and upstream dependencies set between them, which helps express the order they should execute in.\nTasks are declared using this command:\npydolphinscheduler.tasks.Shell\nThis command requires a parameter name and command.\nHere are four examples of task declaration:\n\nPyDolphinScheduler has numerous task types:\nPython Function Wrapper\nShell\nSQL\nPython\nHTTP\nSwitch\nCondition\nMap Reduce\nProcedure\nDatax\nSub Process\nFlink\nTask dependencies are the order in which tasks must be performed.\nOne of the critical aspects of using tasks is defining how each task relates to one another, specifically the upstream and downstream tasks.\nFirstly, you declare the tasks, and then you declare their dependencies.\nSetting task dependence is quite easy. You can use the following:\nset_downstream or &gt;&gt;\nset_upstream or &lt;&lt;\nLetâs take an example to illustrate the point.\nIn the following code, task_parent is the leading task of the entire workflow.\ntask_child_one and task_child_two are the two downstream tasks\nThe task called task_union will not run unless both the task_child_one and task_child_two are done, as they are upstream.\n\nUseful PyScheduler Documentation Links\nTo learn more about PythonScheduler, refer to these links:\nPyDolphinScheduler homepage\nInstallation guide for PyDolphinScheduler\nPyDolphinScheduler tutorials\nPyDolphinScheduler tasks\nAbout AWS Support\nLetâs get straight to the point.\nAmazon Web Services offers a fully integrated portfolio of various cloud-computing services to help developers and data engineers build, deploy, and manage big data applications.\nThe best thing about AWS?\nThereâs no need to worry about hardware procurement or fret over infrastructure maintenance. This helps you focus resources and time on building.\n\nSo, why are we talking about AWS here?\nThe success of Apache DolphinSchedulerâs visual DAG interface heralded many international users. While these users loved DolphinSchedulerâs capabilities, most of their projects involved overseas vendors such as AWS.\nWithout dedicated support for AWS, many users were frustrated. This prompted the developers at DolphinScheduler to implement support for the vendor.\nIn the first quarter of 2022, developers added support for some of the most popular tools and services. This proved to be one of the most significant additions to Apache DolphnScheduler.\nApache DolphinScheduler now supports Amazon EMR and Amazon Redshift. In addition, thereâs resource center support for Amazon S3, too.\nLetâs take a deep dive into these task types and look at how they have dramatically changed the way developers use Apache DS.\nAmazon EMR\nAmazon EMR, formerly known as the Amazon Elastic MapReduce, is a managed cluster platform that aims to simplify operating big data frameworks â including Apache Spark and Apache Hadoop â on AWS to analyze copious amounts of data.\n\nFurthermore, Amazon EMR offers the capability to transform and transfer large volumes of data into and out of other AWS services, like Amazon S3 for data stores and Amazon DynamoDB for databases.\nSo how does Amazon EMR support help Apache DolphinScheduler users?\nAWS integration: Having support for Amazon EMR means users can get the best of both worlds, with Amazon EMRâs flexibility, scalability, reliability, and quick deployment. Users who have already used Amazon services can now easily define and schedule their workflows on Apache DolphinScheduler.\nIn addition, Amazon EMR offers robust security features by leveraging other services such as IAM and Amazon VPC to help secure data and clusters.\nCombining Apache DolphinSchedulerâs error detection and handling features with Amazon EMRâs management interfaces and log files can troubleshoot issues and failures quickly.\n\nDS now has support for âRunJobFlowâ feature of Amazon EMR, which allows users to submit multiple steps to the service and specify the number of resources to be used.\nAmazon RedShift\nAmazon Redshift is a data warehouse that âuses SQL to analyze both structured and semi-structured data lakes, operational databases, and data warehouses through AWS-designed hardware and machine learning to deliver the best ROI at all scales.\n\nIt helps collaborate and share data, optimizes business intelligence, improves demand forecast, and above all, increases developer productivity.\nAgain, how does adding Amazon RedShift to DolphinScheduler benefit users?\nAmazon RedShift allows users to focus on getting insights from data within seconds for various needs without worrying about managing the complete data warehouse. It also helps run predictive analytics on complex and scaled data across data lakes, databases, and even third-party data sets.\nIn addition, users can get up to 3X better price performance compared to other cloud data warehouses through RedShiftâs automated optimizations aimed at improving query speed.\nAnd just like Amazon EMR, RedShift also benefits greatly from AWSâs comprehensive security capabilities by providing data security out of the box (without any additional cost).\n\nApache DolphinScheduler has now extended support for Amazon RedShift data sources for SQL task types. Users can run RedShift tasks by selecting the âRedShiftâ data source in the SQL task type. For a quick recap, SQL task type is used to connect to databases and execute SQL commands.\nAmazon S3\nAmazon S3, or Amazon Simple Storage Service, is an object storage service that offers powerful scalability, performance, and security. Users can leverage Amazon S3 to store and protect large amounts of data for multiple use cases, including data lakes, websites, mobile applications, big data analytics, and more.\n\nLetâs pose the same question: How do Apache DolphinScheduler users benefit from this support addition?\nAmazon S3 is one of the cutting-edge technologies in cloud data storage and offers numerous benefits at a very low cost. Coupled with the ease of migration and simplicity in management, users can gather and store copious amounts of data reliably and securely.\n\nApache DolphinSchedulerâs resource center now supports Amazon S3 for uploading resources, adding to its already impressive support for local and HDFS resource storage.\nHereâs a brief guide on how to use DolphinSchedulerâs resource center:\nThe resource center is used for uploading files, UDF functions, and task group management.\n\nCreating files: You can create files â supported file formats include txt, py, sql, xml, and more.\n\nUploading files: Users can click the âUpload fileâ button or even drag-and-drop files.\n\nViewing files: Users can click on files that are viewable and check out the file details.\nFor more information about the resource center, please visit this link.\nFuture Support\nApache DolphinScheduler also has two more future additions in their pipelines: Amazon Athena and AWS DataSync.\nThese exciting additions will add more functionalities to the robust scheduler platform, paving the way for more inclusive projects and business processes.\n\nUseful AWS Support Documentation Links\nTo learn more about AWS in DolphinScheduler, refer to these links:\n\nAmazon EMR\nAmazon RedShift\nAmazon S3\n\nConclusion\nApache DolphinScheduler makes scheduling workflows a breeze with its DAG interface, multiple task types, no-code workflows, and more. PyDolphinScheduler streamlines the process of defining tasks, and the recent support for AWS platforms immensely boosts the schedulerâs capabilities.\nApache DolphinSchedulerâs popularity grew as it is used by tech giants, including Lenovo, IBM China, and Dell.\nThis exhaustive guide covered the basics to help you gain a brief understanding of PyDolphinScheduler and AWS components. We hope this article prompts you to go out and experiment with the features for yourself.\nAdios!\nJoin the Community\nThere are numerous ways and channels to participate and contribute to the growing DolphinScheduler community. Some ways you can do so: Documentation, translation, testing, Q&amp;A, keynote speeches, articles, and more.\nCheck out the comprehensive guide explaining how to participate in the contribution.\nCheck these links for issues compiled by the community:\nFor novices\nFor experts/non-newbies\nUseful links:\nGithub repository\nOfficial website\nYoutube\nSlack\nMailing list: dev@dolphinscheduler@apache.org\nTwitter handle: @DolphinSchedule\n",
    "title": "apache DolphinScheduler Extends Capabilities Through Python API and AWS Support",
    "time": "2022-8-26"
  },
  {
    "name": "Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow",
    "content": "Quick Start with Apache DolphinScheduler Machine Learning Workflow\n\nAbstract\nWith the release of Apache DolphinScheduler 3.1.0, many AI components have been added to help users to build machine learning workflows on Apache DolphinScheduler more efficiently.\nThis article describes in detail how to set up DolphinScheduler with some Machine Learning environments. It also introduces the use of the MLflow component and the DVC component with experimental examples.\nDolphinScheduler and Machine Learning Environment\nTest Program\nAll code can be found at https://github.com/jieguangzhou/dolphinscheduler-ml-tutorial\nGet the code\ngit checkout dev\n\nInstallation environment\nConda\nSimply install it following the official website and add the path to Conda to the environment variables\nAfter installation mlflow and dvc commands will be installed in condaâs bin directory.\npip install mlflow==1.30.0 dvc\n\nJava8 environment\nsudo apt-get install openjdk-8-jdk\njava -version\n\nConfigure the Java environment variable, ~/.bashrc or ~/.zshrc\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\nexport PATH=$PATH:$JAVA_HOME/bin\n\nApache DolphinScheduler 3.1.0\nDownload DolphinScheduler 3.1.0\n# Go to the following directory (you can install in other directories, for the convenience of replication, in this case, the installation is performed in the following directory)\ncd first-example/install_dolphinscheduler\n## install DolphinScheduler\nwget &lt;https://dlcdn.apache.org/dolphinscheduler/3.1.0/apache-dolphinscheduler-3.1.0-bin.tar.gz&gt;\ntar -zxvf apache-dolphinscheduler-3.1.0-bin.tar.gz\nrm apache-dolphinscheduler-3.1.0-bin.tar.gz\n\nConfiguring the Conda environment and Python environment in DolphinScheduler\n## Configure conda environment and default python environment\ncp common.properties apache-dolphinscheduler-3.1.0-bin/standalone-server/conf\necho &quot;export PATH=$(which conda)/bin:\\\\$PATH&quot; &gt;&gt; apache-dolphinscheduler-3.1.0-bin/bin/env/dolphinscheduler_env.sh\necho &quot;export PYTHON_HOME=$(dirname $(which conda))/python&quot; &gt;&gt; apache-dolphinscheduler-3.1.0-bin/bin/env/dolphinscheduler_env.sh\n\n\ndolphinscheduler-mlflow configuration\nWhen using the MLFLOW component, the dolphinscheduler-mlflow project on GitHub will be used as a reference, so if you canât get a proper network connection, you can replace the repository source by following these steps\n\nFirstly execute git clone https://github.com/apache/dolphinscheduler-mlflow.git\nThen change the value of the ml.mlflow.preset_repository field in common.properties to the default path for the download\nStart DolphinScheduler\n## start DolphinScheduler\ncd apache-dolphinscheduler-3.1.0-bin\nbash bin/dolphinscheduler-daemon.sh start standalone-server\n## You can view the log using the following command\n# tail -500f standalone-server/logs/dolphinscheduler-standalone.log\n\nOnce started, wait a moment for the service to boot up and you will be taken to the DolphinScheduler page\nOpen http://localhost:12345/dolphinscheduler/ui and you will see the DolphinScheduler page\nAccount: admin, Password: dolphinscheduler123\n\nMLflow\nThe MLflow Tracking Server is relatively simple to start up, and can simply be started by using the command docker run â name mlflow -p 5000:5000 -d jalonzjg/mlflow:latest\nOpen http://localhost:5000, and you will be able to find the MLflow model and test management page\n\nThe Dockerfile for this mirror image can be found at first-example/docker-mlflow/Dockerfile\nComponents Introduction\nThere are 5 main types of components used in this article\nSHELL component\nThe SHELL component is used to run shell-type tasks\nPYTHON component\nThe PYTHON component is used to run python-type tasks\nCONDITIONS component\nCONDITIONS is a conditional node that determines which downstream task should be run based on the running status of the upstream task.\nMLFLOW component\nMLFLOW component is used to run the MLflow Project on DolphinScheduler based on the dolphinscheduler-mlflow library to implement pre-built algorithms and AutoML functionality for classification scenarios and to deploy models on the MLflow tracking server\nDVC component\nDVC component is used for data versioning in machine learning on DolphinScheduler, such as registering specific data as a specific version and downloading specific versions of data.\nAmong the above five components\n\n\nSHELL component and PYTHON component are the base components, which can run a wide range of tasks.\n\n\nCONDITIONS are logical components that can dynamically control the logic of the workflowâs operation.\n\n\nThe MLFLOW component and DVC component are machine learning type components that can be used to facilitate the ease of use of machine learning scenario feature capabilities within the workflow.\nMachine learning workflow\nThe workflow consists of three parts.\n\n\nThe first part is the preliminary preparation, such as data download, data versioning management repository, etc.; it is a one-time preparation.\n\n\nThe second part is the training model workflow: it includes data pre-processing, training model, and model evaluation\n\n\nThe third part is the deployment workflow, which includes model deployment and interface testing.\n\n\nPreliminary preparation workflow\nCreate a directory to store all the process data mkdir /tmp/ds-ml-example\nAt the beginning of the program, we need to download the test data and initialize the DVC repository for data versioning\nAll the following commands are run in the dolphinscheduler-ml-tutorial/first-example directory\nSince we are submitting the workflow via pydolphinscheduler, letâs install pip install apache-dolphinscheduler==3.1.0\nWorkflow(download-data): Downloading test data\nCommand: pydolphinscheduler yaml -f pyds/download_data.yaml\nExecute the following two tasks in order\n\n\nInstall-dependencies: install the python dependencies packages needed in the download script\n\n\nDownload-data: download the dataset to /tmp/ds-ml-example/raw\n\n\n\nWorkflow(dvc_init_local): Initialize the dvc data versioning management repository\nCommand: pydolphinscheduler yaml -f pyds/init_dvc_repo.yaml\nExecute the following tasks in order\n\n\ncreate_git_repo: Create an empty git repository in the local environment\n\n\ninit_dvc: convert the repository to a dvc-type repository for data versioning\n\n\ncondition: determine the status of the init_dvc task, if successful then execute report_success_message, otherwise execute report_error_message\n\n\n\nTraining model workflow\nIn the training model part, the workflow includes data pre-processing, model training, and model evaluation.\nWorkflow(download-data): data preprocessing\nCommand: pydolphinscheduler yaml -f pyds/prepare_data.yaml\n\nPerform the following tasks in order\n\n\ndata_preprocessing: preprocesses the data, for demo purposes, weâve only perform a simple truncation procedure here\n\n\nupload_data: uploads data to the repository and registers it as a specific version v1\n\n\nThe following image shows the information in the git repository\n\nWorkflow(train_model): Training model\nCommand: pydolphinscheduler yaml -f pyds/train_model.yaml\nPerform the following tasks in order\n\n\nclean_exists_data: Delete the historical data generated by potentially repeated tests /tmp/ds-ml-example/train_data\n\n\npull_data: pull v1 data to /tmp/ds-ml-example/train_data\n\n\ntrain_automl: Uses the MLFLOW componentâs AutoML function to train the classification model and register it with the MLflow Tracking Server, if the current model version F1 is the highest, then register it as the Production version.\n\n\ninference: import a small part of the data for batch inference using the mlflow CLI\n\n\nevaluate: Obtain the results of the inference and perform a simple evaluation of the model again, which includes the metrics of the new data, the projected label distribution, etc.\n\n\n\nThe results of the test and the model can be viewed in the MLflow Tracking Server ( http://localhost:5000 ) after train_automl has completed its operation.\n\nThe operation logs for the evaluation task can be viewed after it has completed its operation.\n\nDeployment Process Workflow\nWorkflow(deploy_model): Deployment model\nRun: pydolphinscheduler yaml -f pyds/deploy.yaml\nRun the following tasks in order.\n\n\nkill-server: Shut down the previous server\n\n\ndeploy-model: Deploy the model\n\n\ntest-server: Test the server\n\n\n\nIf this workflow is started manually, the interface will look as follows, just enter the port number and the model version number.\n\nIntegrate the workflows\nFor practical use, after obtaining stable workflow iterations, the whole process needs to be linked together, for example after getting a new version, then train the model, and if it performs better, then deploy the model.\nFor example, we switch to the production version git checkout first-example-production\nThe differences between the two versions are:\n\n\nthere is an additional workflow definition in train_and_deploy.yaml, which is used to combine the various workflows\n\n\nmodify the pre-processing script to get the v2 data\n\n\nchange the flag in the definition of each sub-workflow to false and let train_and_deploy.yaml run in unison.\n\n\nRun: pydolphinscheduler yaml -f pyds/train_and_deploy.yaml\nEach task in the diagram below is a sub-workflow task, which corresponds to the three workflows described above.\n\nAs below, the new version of the model, version2, is obtained after the operation and has been registered as the Production version\n\n",
    "title": "Quick Start with Apache DolphinScheduler Machine Learning Workflow",
    "time": "2022-12-5"
  },
  {
    "name": "The_most_comprehensive_introductory_tutorial_written_in_a_month",
    "content": "Community Star Series | 1 Donât know how to use Apache DolphinScheduler? A community talent writes the usage tutorial of it in one month!\n\n\n\nAuthor | Ouyang Tao, Big Data Development Engineer\nApache DolphinScheduler (hereinafter referred to as DS) is a distributed and easily scalable visual DAG workflow task scheduling system, dedicated to solving the intricate dependencies in the data processing, thus making it be used out of the box in the data processing. The top open source project, similar to other open-source projects, runs and installs from a script.\nThe scripts are all located in the script folder in the root directory and are executed in the following order:\n\nCheck the start script start-all.sh, you can find the start of the four most important startup services, respectively dolphinscheduler-daemon.sh start master-server/worker-server/alert-server/api-server\nThe dolphinscheduler-daemon.sh script will first execute the dolphinscheduler-env.sh script, which serves to introduce the environment, including the Hadoop, Spark, Flink, Hive environments, etc. As DS needs to schedule these tasks, if these environments are not introduced, the execution will not succeed even if the scheduling is completed.\nImmediately afterward, the dolphinscheduler-daemon.sh script loops through bin/start.sh under the four modules mentioned above, as shown in the following image.\n\n\n\n\nAs shown in the picture below: the execution of dolphinscheduler-daemon.sh start master-server will go to the master moduleâs src/main/bin and execute start.sh. After opening start.sh, you can find that a MasterServer is started. And the other Worker, Alert and API modules are started as the same.\n\n\n\nThis is the end of the scripting process, and we will now describe the main purpose of the four modules: Master is responsible for DAG task slicing, task submission monitoring, and listening to the health status of other Masters and Workers; Worker is responsible for task execution; Alert aims to the alert service; API works for the business logic of DS, i.e. project management, resource management, security management, etc. as seen on the web.\nIf you are familiar with other big data projects, such as Flink, Hdfs, Hbase, etc., you will find that these architectures are similar, such as hdfs is architected as the NameNode and WorkNode; Hbase is architected as the HMasterServer and HRegionServer; Flink adopts the architecture as JobManager and TaskManager, etc., if you can master these frameworks, I think the mastery of DS will be easier.\nMaster and Worker are started through SpringBoot, and the objects created are also hosted by Spring, if you use Spring regularly, I think you will understand DS much easier than other open-source projects.\nNotesï¼\n\n\nThere is a python-gateway-server module in the run script, which is written in python code workflow and is not in the scope of this article, so I ignore it this time, if you understand this module in detail, youâd better turn to the community.\n\n\nStart Alert script executes the Alert module under the alert-server script, because Alert is also a parent module, I do not intend to talk about alert-server much here. I believe that after going through the implementation process of Master and Worker, Alert module should not be difficult to understand.\n\n\nIn addition, if you know about DS for the first time, you will find an alert-api in the Alert module, I want to specify that this alert-api and the aforementioned api-server are totally unrelated, api-server is the ApiApplicationServer script to start the api module, and responsible for the server logic of the entire DS. The api-server is the ApiApplicationServer script that starts the api module and is work for the business logic of the whole DS, while alert-api is the plug-in interface of the spi that is responsible for alerting, and you can open the alert-api module to find that the code inside is all interfaces and definitions and does not deal with any logic. Similarly, the task-api and alert-api under the task module just have the same responsibilities, but deal with different functions.\n\n\nDS is all managed by SpringBoot, so if you have never worked with SpringBoot or Spring, you can refer to the following URLs and other relevant information on the web. https://spring.io/quickstart\n\n\nKnow more about the warning module, please refer to the link below or consult other community members.\nhttps://dolphinscheduler.apache.org/en-us/blog/Hangzhou_cisco\nThe official website of the Apache DolphinScheduler project can be found at: https://github.com/apache/dolphinscheduler\nIn the next chapter, I will introduce the two most important modules of DS, Master, and Worker, and how they communicate with each other, stay tuned!\n",
    "title": "Donât know how to use Apache DolphinScheduler? A community talent writes the usage tutorial of it in one month!ï¼1ï¼",
    "time": "2022-5-23"
  },
  {
    "name": "Twos",
    "content": "Congratulations! Apache DolphinScheduler Has Been Approved As A TWOS Candidate Member\n\n\n\nRecently, TWOS officially announced the approval of 6 full members and 3 candidate members, Apache DolphinScheduler, a cloud-native distributed big data scheduler, was listed by TWOS.\nApache DolphinScheduler is a new-generation workflow scheduling platform that is distributed and easy to expand. It is committed to &quot;solving the intricate dependencies among big data tasks and visualizing the entire data processing&quot;. Its powerful visual DAG interface greatly improves the user experience and can configure workflow without complex code.\nSince it was officially open-sourced in April 2019, Apache DolphinScheduler (formerly known as EasyScheduler) has undergone several architectural evolutions. So far, the relevant open-source codes have accumulated 7100+ Stars, with 280+ experienced code contributors, 110+ non-code contributors participating in the project, which includes PMCs and Committers of other Apache top-level projects. The Apache DolphinScheduler open source community continues to grow, and the WeChat user group has reached 6000+ people, and 600+ companies and institutions have adopted Apache DolphinScheduler in their production environment.\nTWOS\nAt the &quot;2021 OSCAR Open Source Industry Conference&quot;, China Academy of Telecommunication Research of MIIT (CAICT) officially established TWOS. TWOS is composed of open-source projects and open-source communities, which aims to guide the establishment of a healthy, credible, sustainable open source community, and build a communication platform providing a complete set of open source risk monitoring and ecological monitoring services.\nTo help enterprises reduce the risk of using open source software and promote the establishment of a credible open source ecosystem, CAICT has created a credible open-source standard system, which carries authoritative evaluation on enterprise open source governance capabilities, open-source project compliance, open-source community maturity, open-source tool detection capabilities, Open- source risk management capabilities of commercial products.\nAfter being screened by TWOS evaluation criteria, Apache DolphinScheduler was approved to be a candidate member, which shows its recognition of Apache DolphinScheduler's way of open-source operation, maturity, and contribution, and encourages the community to keep active.\nOn September 17, 2021, the first batch of members joined TWOS, including 25 full members such as openEuler, openGauss, MindSpore, openLookeng, etc., and 27 candidate members like Apache RocketMQ, Dcloud, Fluid, FastReID, etc., with a total of 52 members:\n\n\n\nOnly two communities were selected for the second batch of candidate membersâApache DolphinScheduler and PolarDB, an open-source cloud-native ecological distributed database contributed by Alibaba Cloud.\nThe Apache DolphinScheduler community is very honored to be selected as a candidate member of TWOS, which is an affirmation and incentive for the entire industry to build the community a better place. The community will make persistent efforts and strive to become a full member as soon as possible., and provide more value for China's open-source ecological construction together, with all the TWOS members!\n",
    "title": "Congratulations! Apache DolphinScheduler Has Been Approved As A TWOS Candidate Member",
    "time": "2022-1-11"
  },
  {
    "name": "YouZan-case-study",
    "content": "From Airflow to Apache DolphinScheduler, the Roadmap of Scheduling System On Youzan Big Data Development Platform\n\n\n\nAt the recent Apache DolphinScheduler Meetup 2021, Zheqi Song, the Director of Youzan Big Data Development Platform\nshared the design scheme and production environment practice of its scheduling system migration from Airflow to Apache\nDolphinScheduler.\nThis post-90s young man from Hangzhou, Zhejiang Province joined Youzan in September 2019, where he is engaged in the\nresearch and development of data development platforms, scheduling systems, and data synchronization modules. When he\nfirst joined, Youzan used Airflow, which is also an Apache open source project, but after research and production\nenvironment testing, Youzan decided to switch to DolphinScheduler.\nHow does the Youzan big data development platform use the scheduling system? Why did Youzan decide to switch to Apache\nDolphinScheduler? The message below will uncover the truth.\nYouzan Big Data Development Platformï¼DPï¼\nAs a retail technology SaaS service provider, Youzan is aimed to help online merchants open stores, build data products\nand digital solutions through social marketing and expand the omnichannel retail business, and provide better SaaS\ncapabilities for driving merchants' digital growth.\nAt present, Youzan has established a relatively complete digital product matrix with the support of the data center:\n\n\n\nYouzan has established a big data development platform (hereinafter referred to as DP platform) to support the\nincreasing demand for data processing services. This is a big data offline development platform that provides users with\nthe environment, tools, and data needed for the big data tasks development.\n\n\n\nYouzan Big Data Development Platform Architecture\nYouzan Big Data Development Platform is mainly composed of five modules: basic component layer, task component layer,\nscheduling layer, service layer, and monitoring layer. Among them, the service layer is mainly responsible for the job\nlife cycle management, and the basic component layer and the task component layer mainly include the basic environment\nsuch as middleware and big data components that the big data development platform depends on. The service deployment of\nthe DP platform mainly adopts the master-slave mode, and the master node supports HA. The scheduling layer is\nre-developed based on Airflow, and the monitoring layer performs comprehensive monitoring and early warning of the\nscheduling cluster.\n1 Scheduling layer architecture design\n\n\n\nYouzan Big Data Development Platform Scheduling Layer Architecture Design\nIn 2017, our team investigated the mainstream scheduling systems, and finally adopted Airflow (1.7) as the task\nscheduling module of DP. In the design of architecture, we adopted the deployment plan of Airflow + Celery + Redis +\nMySQL based on actual business scenario demand, with Redis as the dispatch queue, and implemented distributed deployment\nof any number of workers through Celery.\nIn the HA design of the scheduling node, it is well known that Airflow has a single point problem on the scheduled node.\nTo achieve high availability of scheduling, the DP platform uses the Airflow Scheduler Failover Controller, an\nopen-source component, and adds a Standby node that will periodically monitor the health of the Active node. Once the\nActive node is found to be unavailable, Standby is switched to Active to ensure the high availability of the schedule.\n2 Worker nodes load balancing strategy\nIn addition, to use resources more effectively, the DP platform distinguishes task types based on CPU-intensive\ndegree/memory-intensive degree and configures different slots for different celery queues to ensure that each machine's\nCPU/memory usage rate is maintained within a reasonable range.\nScheduling System Upgrade and Selection\nSince the official launch of the Youzan Big Data Platform 1.0 in 2017, we have completed 100% of the data warehouse\nmigration plan in 2018. In 2019, the daily scheduling task volume has reached 30,000+ and has grown to 60,000+ by 2021.\nthe platformâs daily scheduling task volume will be reached. With the rapid increase in the number of tasks, DP's\nscheduling system also faces many challenges and problems.\n1 Pain points of Airflow\n\nIn-depth re-development is difficult, the commercial version is separated from the community, and costs relatively\nhigh to upgrade ;\nBased on the Python technology stack, the maintenance and iteration cost higher;\nPerformance issues:\n\n\n\n\nAirflow's schedule loop, as shown in the figure above, is essentially the loading and analysis of DAG and generates DAG\nround instances to perform task scheduling. Before Airflow 2.0, the DAG was scanned and parsed into the database by a\nsingle point. It leads to a large delay (over the scanning frequency, even to 60s-70s) for the scheduler loop to scan\nthe Dag folder once the number of Dags was largely due to business growth. This seriously reduces the scheduling\nperformance.\n\nStability issues:\n\nThe Airflow Scheduler Failover Controller is essentially run by a master-slave mode. The standby node judges whether to\nswitch by monitoring whether the active process is alive or not. If it encounters a deadlock blocking the process\nbefore, it will be ignored, which will lead to scheduling failure. After similar problems occurred in the production\nenvironment, we found the problem after troubleshooting. Although Airflow version 1.10 has fixed this problem, this\nproblem will exist in the master-slave mode, and cannot be ignored in the production environment.\nTaking into account the above pain points, we decided to re-select the scheduling system for the DP platform.\nIn the process of research and comparison, Apache DolphinScheduler entered our field of vision. Also to be Apache's top\nopen-source scheduling component project, we have made a comprehensive comparison between the original scheduling system\nand DolphinScheduler from the perspectives of performance, deployment, functionality, stability, and availability, and\ncommunity ecology.\nThis is the comparative analysis result below:\n\n\n\n\n\n\nAirflow VS DolphinScheduler\n1 DolphinScheduler valuation\n\n\n\nAs shown in the figure above, after evaluating, we found that the throughput performance of DolphinScheduler is twice\nthat of the original scheduling system under the same conditions. And we have heard that the performance of\nDolphinScheduler will greatly be improved after version 2.0, this news greatly excites us.\nIn addition, at the deployment level, the Java technology stack adopted by DolphinScheduler is conducive to the\nstandardized deployment process of ops, simplifies the release process, liberates operation and maintenance manpower,\nand supports Kubernetes and Docker deployment with stronger scalability.\nIn terms of new features, DolphinScheduler has a more flexible task-dependent configuration, to which we attach much\nimportance, and the granularity of time configuration is refined to the hour, day, week, and month. In addition,\nDolphinScheduler's scheduling management interface is easier to use and supports worker group isolation. As a\ndistributed scheduling, the overall scheduling capability of DolphinScheduler grows linearly with the scale of the\ncluster, and with the release of new feature task plug-ins, the task-type customization is also going to be attractive\ncharacter.\nFrom the perspective of stability and availability, DolphinScheduler achieves high reliability and high scalability, the\ndecentralized multi-Master multi-Worker design architecture supports dynamic online and offline services and has\nstronger self-fault tolerance and adjustment capabilities.\nAnd also importantly, after months of communication, we found that the DolphinScheduler community is highly active, with\nfrequent technical exchanges, detailed technical documents outputs, and fast version iteration.\nIn summary, we decided to switch to DolphinScheduler.\nDolphinScheduler Migration Scheme Design\nAfter deciding to migrate to DolphinScheduler, we sorted out the platform's requirements for the transformation of the\nnew scheduling system.\nIn conclusion, the key requirements are as below:\n\nUsers are not aware of migration. There are 700-800 users on the platform, we hope that the user switching cost can\nbe reduced;\nThe scheduling system can be dynamically switched because the production environment requires stability above all\nelse. The online grayscale test will be performed during the online period, we hope that the scheduling system can be\ndynamically switched based on the granularity of the workflow;\nThe workflow configuration for testing and publishing needs to be isolated. Currently, we have two sets of\nconfiguration files for task testing and publishing that are maintained through GitHub. Online scheduling task\nconfiguration needs to ensure the accuracy and stability of the data, so two sets of environments are required for\nisolation.\n\nIn response to the above three points, we have redesigned the architecture.\n1 Architecture design\n\nKeep the existing front-end interface and DP API;\nRefactoring the scheduling management interface, which was originally embedded in the Airflow interface, and will be\nrebuilt based on DolphinScheduler in the future;\nTask lifecycle management/scheduling management and other operations interact through the DolphinScheduler API;\nUse the Project mechanism to redundantly configure the workflow to achieve configuration isolation for testing and\nrelease.\n\n\n\n\nRefactoring Design\nWe entered the transformation phase after the architecture design is completed. We have transformed DolphinScheduler's\nworkflow definition, task execution process, and workflow release process, and have made some key functions to\ncomplement it.\n\nWorkflow definition status combing\n\n\n\n\nWe first combed the definition status of the DolphinScheduler workflow. The definition and timing management of\nDolphinScheduler work will be divided into online and offline status, while the status of the two on the DP platform is\nunified, so in the task test and workflow release process, the process series from DP to DolphinScheduler needs to be\nmodified accordingly.\n\nTask execution process transformation\n\nFirstly, we have changed the task test process. After switching to DolphinScheduler, all interactions are based on the\nDolphinScheduler API. When the task test is started on DP, the corresponding workflow definition configuration will be\ngenerated on the DolphinScheduler. After going online, the task will be run and the DolphinScheduler log will be called\nto view the results and obtain log running information in real-time.\n\n\n\n\n\n\n- Workflow release process transformation\nSecondly, for the workflow online process, after switching to DolphinScheduler, the main change is to synchronize the\nworkflow definition configuration and timing configuration, as well as the online status.\n\n\n\n\n\n\nThe original data maintenance and configuration synchronization of the workflow is managed based on the DP master, and\nonly when the task is online and running will it interact with the scheduling system. Based on these two core changes,\nthe DP platform can dynamically switch systems under the workflow, and greatly facilitate the subsequent online\ngrayscale test.\n2 Function completion\nIn addition, the DP platform has also complemented some functions. The first is the adaptation of task types.\n\nTask type adaptation\n\nCurrently, the task types supported by the DolphinScheduler platform mainly include data synchronization and data\ncalculation tasks, such as Hive SQL tasks, DataX tasks, and Spark tasks. Because the original data information of the\ntask is maintained on the DP, the docking scheme of the DP platform is to build a task configuration mapping module in\nthe DP master, map the task information maintained by the DP to the task on DP, and then use the API call of\nDolphinScheduler to transfer task configuration information.\n\n\n\nBecause some of the task types are already supported by DolphinScheduler, it is only necessary to customize the\ncorresponding task modules of DolphinScheduler to meet the actual usage scenario needs of the DP platform. For the task\ntypes not supported by DolphinScheduler, such as Kylin tasks, algorithm training tasks, DataY tasks, etc., the DP\nplatform also plans to complete it with the plug-in capabilities of DolphinScheduler 2.0.\n3 Transformation schedule\nBecause SQL tasks and synchronization tasks on the DP platform account for about 80% of the total tasks, the\ntransformation focuses on these task types. At present, the adaptation and transformation of Hive SQL tasks, DataX\ntasks, and script tasks adaptation have been completed.\n\n\n\n### 4 Function complement\n\nCatchup mechanism realizes automatic replenishment\n\nDP also needs a core capability in the actual production environment, that is, Catchup-based automatic replenishment and\nglobal replenishment capabilities.\nThe catchup mechanism will play a role when the scheduling system is abnormal or resources is insufficient, causing some\ntasks to miss the currently scheduled trigger time. When the scheduling is resumed, Catchup will automatically fill in\nthe untriggered scheduling execution plan.\nThe following three pictures show the instance of an hour-level workflow scheduling execution.\nIn Figure 1, the workflow is called up on time at 6 o'clock and tuned up once an hour. You can see that the task is\ncalled up on time at 6 o'clock and the task execution is completed. The current state is also normal.\n\n\n\nfigure 1\nFigure 2 shows that the scheduling system was abnormal at 8 o'clock, causing the workflow not to be activated at 7\no'clock and 8 o'clock.\n\n\n\nfigure 2\nFigure 3 shows that when the scheduling is resumed at 9 o'clock, thanks to the Catchup mechanism, the scheduling system\ncan automatically replenish the previously lost execution plan to realize the automatic replenishment of the scheduling.\n\n\n\nFigure 3\nThis mechanism is particularly effective when the amount of tasks is large. When the scheduled node is abnormal or the\ncore task accumulation causes the workflow to miss the scheduled trigger time, due to the system's fault-tolerant\nmechanism can support automatic replenishment of scheduled tasks, there is no need to replenish and re-run manually.\nAt the same time, this mechanism is also applied to DP's global complement.\n\nGlobal Complement across Dags\n\n\n\n\nDP platform cross-Dag global complement process\nThe main use scenario of global complements in Youzan is when there is an abnormality in the output of the core upstream\ntable, which results in abnormal data display in downstream businesses. In this case, the system generally needs to\nquickly rerun all task instances under the entire data link.\nBased on the function of Clear, the DP platform is currently able to obtain certain nodes and all downstream instances\nunder the current scheduling cycle through analysis of the original data, and then to filter some instances that do not\nneed to be rerun through the rule pruning strategy. After obtaining these lists, start the clear downstream clear task\ninstance function, and then use Catchup to automatically fill up.\nThis process realizes the global rerun of the upstream core through Clear, which can liberate manual operations.\nBecause the cross-Dag global complement capability is important in a production environment, we plan to complement it in\nDolphinScheduler.\nCurrent Status &amp; Planning &amp; Outlook\n1 DolphinScheduler migration status\nThe DP platform has deployed part of the DolphinScheduler service in the test environment and migrated part of the\nworkflow.\nAfter docking with the DolphinScheduler API system, the DP platform uniformly uses the admin user at the user level.\nBecause its user system is directly maintained on the DP master, all workflow information will be divided into the test\nenvironment and the formal environment.\n\n\n\nDolphinScheduler 2.0 workflow task node display\nThe overall UI interaction of DolphinScheduler 2.0 looks more concise and more visualized and we plan to directly\nupgrade to version 2.0.\n2 Access planning\nAt present, the DP platform is still in the grayscale test of DolphinScheduler migration., and is planned to perform a\nfull migration of the workflow in December this year. At the same time, a phased full-scale test of performance and\nstress will be carried out in the test environment. If no problems occur, we will conduct a grayscale test of the\nproduction environment in January 2022, and plan to complete the full migration in March.\n\n\n\n3 Expectations for DolphinScheduler\nIn the future, we strongly looking forward to the plug-in tasks feature in DolphinScheduler, and have implemented\nplug-in alarm components based on DolphinScheduler 2.0, by which the Form information can be defined on the backend and\ndisplayed adaptively on the frontend.\n\n\n\n&quot;\nI hope that DolphinScheduler's optimization pace of plug-in feature can be faster, to better quickly adapt to our\ncustomized task types.\nââZheqi Song, Head of Youzan Big Data Development Platform\n&quot;\n",
    "title": "From Airflow to Apache DolphinScheduler, the Roadmap of Scheduling System On Youzan Big Data Development Platform",
    "time": "2021-12-16"
  },
  {
    "name": "Meetup_2022_02_26",
    "content": "Sign Up to Apache DolphinScheduler Meetup Online | We Are Waiting For You to Join the Grand Gathering on 2.26 2022!\n\n\n\nHello the community! After having adjusted ourselves from the pleasure of Spring Festival and re-devote to the busywork, study, and life, Apache DolphinScheduler is here to wish everyone a prosperous and successful journey in the Lunar year of the Tiger.\nIn this early spring, Apache DolphinScheduler's first Meetup in 2022 is also coming soon. I believe that the community has already been looking forward to it!\nIn this Meetup, four guests from our user companies will share their experience when using Apache DolphinScheduler. Whether you are a user of Apache DolphinScheduler or a spectator, you can definitely learn a lot from it.\nWe are looking forward to more friends joining us in the future, and more and more users will share practical experiences with each other.\nThe first show of Apache DolphinScheduler in February 2022 will be broadcast on time at 14:00 pm on February 26th.\nYou can also click https://www.slidestalk.com/m/679 to go directly to the live broadcast reservation interface!\n1. Introduction to the Event\nTopic: Apache DolphinScheduler user practice sharing\nTime: 2022-2-26 14:00-17:00ï¼Beijing Timeï¼\nFormat: Live online\nClick https://www.slidestalk.com/m/679 to register.\nClick https://asf-dolphinscheduler.slack.com/ to join the community on Slack\n2. Event Highlights\nIn this meetup, we invite four big data engineers from 360 Digital Technology,Â CISCO (China) and Tujia to share their development experiences based on Apache DolphinScheduler, which is highly representative and typical. The topics they shared with us refer to the exploration and application on the K8S cluster and the transformation of the Alert module, etc., which are of great significance to solve the difficulties you may encounter in scenario application. We hope these technical exchanges will help to solve your urgent needs.\n3. Event Agenda\n\n\n\n4. Topic introduction\nJianmin Liu/ 360 Digital Technology/ Big Data Engineer\nSpeech Topic: Practice on Apache DolphinScheduler in 360 Digital Technology\nSpeech Outline: The evolution of big data scheduling from Azkaban to DS, and the use and transformation of Apache DolphinScheduler.\nQingwang Li/CISCO (China) /Big Data Engineer\nSpeech Topic: Retrofit of Apache DolphinScheduler Alert module\nSpeech outline:  The exploration of switching to Apache DolphinScheduler, and modifying the Alert module of Apache DolphinScheduler for different alarm scenarios.\nXuchao Zan/Tujia/Big Data Engineer\nSpeech topic: Exploration and Application on Apache DolphinScheduler\nSpeech outline: TujiaÂ introduced Apache DolphinScheduler, built Tujia data scheduling system, and completed the access of data services, including offline tables, emails, and data synchronization. The function development on the scheduling system will be introduced in detail.\nQian Liu/Cisco(China)/Big Data Engineer\nSpeech Topic: Exploration and Application of Apache DolphinScheduler on K8S Cluster\nSpeech outline: The exploration of switching to Apache DolphinScheduler, and submitting tasks to k8s based on DS secondary development support. Currently, tasks such as mirroring, Spark, Flink, etc. can be run on our scheduling system,Â  and the exploration of log monitoring and alarming will also be introduced.\nWe will see you at 14:00 on February 26th!\n",
    "title": "Sign Up to Apache DolphinScheduler Meetup Online | We Are Waiting For You to Join the Grand Gathering on 2.26 2022!",
    "time": "2022-2-18"
  },
  {
    "name": "meetup_2019_10_26",
    "content": "\nApache Dolphin Scheduler(Incubating) Meetup has been held successfully in Shanghai 2019.10.26.\nAddress: Shanghai Changning Yuyuan Road 1107 Chuangyi space (Hongji) 3r20\nThe meetup was begin at 2:00 pm, and close at about 5:00 pm.\nThe Agenda is as following:\n\nIntroduction/overview of DolphinScheduler (William-GuoWei 14:00-14:20). PPT\nDolphinScheduler internals, fairly technical: how DolphinScheduler works and so on (Zhanwei Qiao 14:20-15:00) PPT\nFrom open source users to PPMC --- Talking about Me and DolphinSchedulerï¼Baoqi Wu from guandata 15:10-15:50ï¼PPT\nApplication and Practice of Dolphin Scheduler in cathay-ins ï¼Zongyao Zhang from cathay-ins 15:50-16:30ï¼PPT\nRecently released features and Roadmap (Lidong Dai 16:30-17:00) PPT\nFree discussion\n\n",
    "title": "Apache Dolphin Scheduler(Incubating) Meetup 2019.10 Shanghai",
    "time": "2019-9-27"
  }
]