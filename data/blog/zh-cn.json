[
  {
    "name": "2_The_most_comprehensive_introductory_tutorial_written_in_a_month",
    "content": "【达人专栏】还不会用 Apache Dolphinscheduler 吗，大佬用时一个月写出的最全入门教学【二】\n\n\n\n作者 | 欧阳涛 招联金融大数据开发工程师\n02 Master 启动流程\n2.1 MasterServer 的启动\n在正式开始前，笔者想先鼓励一下大家。我们知道启动 Master 其实就是启动 MasterServer，本质上与其他 SpringBoot 项目相似，即启动里面的 main 函数。但想要开始实操前，肯定有不少的人，尤其是初学者会突然发现这里面有十多个由 bean 注入的 autowired。\n被多个 bean 的注入搞到一头雾水，甚至感觉一脸懵逼的不是少数。但笔者就想说是，这些其实都是吓唬你们的，不用害怕，接下来将带领你们把这些 bean 分别解剖并归类，那么我们就正式开始。\n第一类：MasterConfig、MasterRegistryClient、MasterSchedulerService、Scheduler 这些 bean。从字面意思来说，MasterConfig 就是跟 Master 配置相关的，MasterRegistryClient 就是负责注册相关的内容，MasterSchedulerService 肯定跟 Master 调度有关的，说白了就是 Master 内部的东西。\n第二类：是那些后缀名为一堆 Processor 的，例如 taskExecuteRunningProcessor 等。相同后缀一定处理同样的 task，在以后肯定被某个东西一起加载的。\n第三类：是 EventExecuteService 以及 FailoverExecuteThread，这些根据名字可以大胆猜一下是与事件执行相关以及灾备转换相关的东西，这些肯定也是 Master 内部的东西，理论上应该归到第一类。\n第四类：至于 LoggerRequestProcessor 就是与打印日志相关的了，至于这类具体干的内容，后面会有详细的介绍。\nmain 方法执行完成后，基于 spring 特性，执行 run 方法。在 run 方法中，创建 nettyRemotingServer 对象(这个对象不是 spring 管理的，而是直接 new 创建的)。然后将第二类的一堆 Processor 放到 netty 的 Processor 里面去。从这里就可以推断，Master 和 Worker 的通信一定是通过 netty 的。\n我们可以看看下面的代码，其实就是将第一类的那些 bean 执行 init 以及 start 方法。\n总结其实 Master 这就像一个总司令，这个总司令就调用这里面的 bean 的 start 方法，这些 bean 开始执行自己的功能，至于这些 bean 里面执行啥样的功能，MasterServer 是懒得管，也没必要管了。\n本节总结：\n\n\n\n至此 MasterServer 就运行完了，下一节我们将逐个分析各个 bean 的用途以及功能的了。\n2.2 MasterConfig 的信息以及 MasterRegistry Client 的注册\nMasterConfig 从 application.yml 中获取配置信息加载到 MasterConfig 类中，获取到的具体配置信息如下。\n\n\n\n​\n在 MasterServer 里，MasterRegisterConfig 会执行 init()以及 start()方法。\ninit()方法新建了一个心跳线程池。注意，此时只是建了一个线程池，里面还没有心跳任务。\nstart()方法从 zk 获取了锁(getLock)，注册信息(registry)以及监听注册的信息(subscribe)。\n注册信息做了两件事情：\n第一：构建心跳信息，并丢到线程池中运行心跳任务的。\n第二：在 zk 临时注册该 Master 信息，并移除没用的 Master 信息。\n心跳任务就是检查是否有死亡节点以及每隔 10s(heartbeatInterval)将最新的机器信息，包括机器 CPU，内存，PID 等等信息注册到 zk 上去的。\n监听订阅的信息，只要注册的信息有变化，就会立马感知，如果是增加了机器，则会打印日志。减少了机器，移除并同时打印日志。本节如下图所示:\n\n\n\n2.3 ServerNodeManger 的运行\n前面两节是从 MasterServer 启动过程以及 MasterRegisterConfig 的注册过程的。注册完成之后 Master，Worker 如何管理呢，如何同步保存到数据库的呢。ServerNodeManager 的作用就是负责这一部分的内容。\nServerNodeManager 实现了 InitializingBean 接口的。基于 spring 的特性，构建此对象后，会执行 AfterPropertiesSet()方法。做了三件事情：\n\nload()。从 zk 加载节点信息通过 UpdateMasterNodes()到 MasterPriorityQueue。\n新建线程每十秒钟将 zk 的节点信息同步数据到数据库中。\n监听 zk 节点，实时把最新数据通过 UpdateMasterNodes()方法更新到 MasterPriorityQueue 队列中去。\n\n几乎所有的更新操作都是通过重入锁来实现的，这样就能确保多线程下系统是安全的。此外，还有一个细节是如果是移除节点会发送警告信息。\nMasterProrityQueue 里面有个 HashMap，每台机器对应一个 index，以这样的方式构建了槽位。后面去找 Master 信息的时候就是通过这 index 去找的。\n至于 MasterBlockingQueue 队列的内容，如何同步到数据库的，如何将数据放到队列和队列中移除数据等，这些都是纯 crud 的内容，读者可以自行阅读的。\n2.4 MasterSchedulerService 的启动\n2.1 到 2.3 讲述都是由 zk 管理的节点信息的事情。为什么我们要在 Master 启动之后会先讲节点信息的？\n理由其实很简单，因为不管是 Master 还是 Worker 归根结底都是机器。如果这些机器崩了或者增加了，DS 不知道的话，那这机器岂不是浪费了。只有机器运行正常，配置正常，都管理好了，那 DS 运行才能够顺畅地运行。同样，其他大数据组件也是类似的道理。\n前面 MasterServer 里 MasterRegisterClient 执行完 init()以及 start()方法之后，紧接着 MasterSchedulerService 执行了 init()和 start()方法。从这里开始就真正的进入了 Master 干活的阶段了。\ninit()方法是创建了一个 Master-Pre-Exec-Thread 线程池以及 netty 客户端的。\nPre-exec-Thread 线程池里面有固定的 10 个线程(在 2.1 中对应的是 MasterConfig 配置里面的 pre-exec-threads)。这些线程处理就是从 command 里构建 ProcessInstance(流程实例)过程的。\nstart 方法就是启动了状态轮询执行(StateWheelExecutorThread)的线程，这线程专门就干的是检查 task，process，workflow 超时以及 task 状态的过程，符合条件的都被移除了。\n其中，MasterSchedulerService 本身继承了 thread 类，在 start 方法过后，就立马执行了 run 方法。在 run 方法中确保机器有了足够的 CPU 和内存之后，就会执行 ScheduleProcess 方法。至于 ScheduleProcess 做的事情，将在 2.5 说明。\n2.5 MasterSchedulerService 的执行\nScheduleProcess 方法\nScheduleProcess 是在 MasterSchedulerService 中的 while 死循环里面的，所以它会依次循环执行下面 4 个方法。\n\nFindCommands 方法。从 t_ds_command 表中每次取出 10 条数据，并且这 10 条数据都是根据 slot 查找出来的，查找完成后，可以在 MasterConfig.FetchCommandNum 中进行配置。\nCommandProcessInstance 将这些 command 表中转换成 ProcessInstance。这里用到了 CountdownLatch，目的是全部转换完成才执行以后的方法。\n将转换好的 processInstance 一个一个的构建成 workFlowExecuteThread 对象，将这些对象通过 workFlowExecuteThreadPool 线程池中的线程一个一个执行的，并且将任务实例和工作流在 processInstanceExecCacheManager 缓存起来。\n在这个线程池中运行 StartWorkFlow 方法后，执行 WorkFlowExecuteThread 的 StartProcess 方法的，StartProcess 做了哪些事情将在 2.6 说明的。\n\n这个线程池交给了 spring 管理，而且属于后台线程。它的最大数量以及核心数量的线程池都是 100 个(MasterConfig.getExecThreads)。详细如下图:\n\n\n\n这里有两个细节要说明一下，\n第一：WorkflowExecuteThread 它并不是继承了 Thread 类，而是一个普通类。只是类名字后面有个 Thread，所以阅读的时候不要在此类找 start 或者 run 方法了。\n第二：SchedulerProcess 方法里面如果找到的 ProcessInstance 是超时的话，\n就会交给 2.4 说的状态轮询线程(stateWheelExecuteThread)去执行的，将这个 ProcessInstance 进行移除。\n2.6 WorkflowExecutorThread 里执行 StartProcess 方法\nStartProcess 这个方法就直接先看图的了。\n\n\n\nStartProcess 就干了三件事请，buildFlowDag()构建了 DAG，initTaskQueue()初始化 task 队列以及 submitPostNode()提交节点的。\n构建 DAG 如何干的，初始化队列中又干了什么事情，提交节点后又干了什么事情的，将在 2.7 到 2.9 章节说明。\n2.7 WorkflowExecutorThread 里执行 buildFlowDag 方法\n根据 buildFlowDag 里面的代码，梳理了一下执行过程，分别为下面 9 步:\n\nFindProcessDefinition 获取流程定义，就是要构建哪个流程的 DAG 的。\nGetStartTaskInstanceList 获取流程下有哪些任务实例，一般情况下，一个流程肯定有不止一个任务。\nFindRelationByCode 获取任务关系表(ProcessTaskRelation)中的数据。\nGetTaskDefineLogListByRelation 通过第 3 步获取的任务关系数据确定任务定义日志(TaskDefinitionLog)的数据的。\nTransformTask 就是通过第 3 步和第 4 步获取到 Relation 和 Log 转换成任务节点 TaskNode。\nGetRecoveryNodeCodeList 获取到的是 task 里的 nodeCode。\nParseStartNodeName 获取到的是命令的参数。\n根据第 5、第 6、第 7 获取到数据，构建了流程的 DAG(ProcessDag)。\n将构建好的 ProcessDag 数据转换成 DAG 数据。\n\n基本逻辑就是上面的步骤的。当然，每一步都会有些更多的逻辑，但这些本质上都是数据结构变来变去的。如果读者写过业务方面的代码，这点肯定不陌生的。所以就不详细的说明了。\n可能有读者对于 DAG 是什么，下面是 DAG 的简介链接，阅读之后理解起来应该并不难。\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DAG.md\n这个链接是在理论上介绍 DAG，如果对 DAG 想要在实践上更深入的认识，在 dao 模块的 test 文件夹下搜索 DagHelperTest 类，这里面有 5 个 test 的操作的，大家可以都运行一下(Debug 形式)，就会对 DAG 有着更深入的认识的。\n还有两个链接跟本节有关的。这两个链接是关于 dag 中任务关系的改造的。就是 1.3 版本以前保存任务之间的关系只是以字段的形式进行保存，后来发现数据量很大不可行之后，就把这个字段拆成多个表了。读者可以阅读一下的。\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Json_Split.md\n这构建 DAG(有向无环图)目的就是在前端拖拉拽的任务告诉 Master 任务的执行顺序，也就是告诉 Master 哪些任务先执行，哪些任务后执行。\n2.8 WorkflowExexutorThread 里执行 InitTaskQueue 方法\nInitTaskQueue 里面干了 3 件重要事情：\n\n初始化 4 个 map，分别是 ValidTaskMap，ErrorTaskMap，ActiveTaskProcessorMaps，CompleteTaskMap。就是将找到的 task 和 process 按 valid(有效)，complete(完成)，error(失败)，active(运行)为根据，保存到不同的 map 中(这些 map 都是以 taskCode 作为 key)，这些 map 将在后面的方法中用到的。\n如果 task 是可以重试的，就是通过 addTaskToStandByList 将其放到 readyToSubmitTaskQueue 队列中。\n如果开启了补数状态的话，那就设置具体的补数时间以及全局参数，将其更新到流程实例中。\n\n(笔者觉得这个 InitTaskQueue 方法名字并不是很好，可能觉得 InitTask 或者 InitTaskMap 会更好的。因为 Queue 的话很容易误认为是队列的，这个方法只是构建了 4 个 map 的。而且队列也只是放了可以重试的任务的，这个队列在下面章节中还有更大的用处的。)\n2.9 WorkFlowExecutorThread 里执行 SubmitPostNode 方法\nSubmitPostNode 干了 6 件事情：\n\nDagHelper.ParsePostNodes(dag)把 2.8 最后生成的 DAG 解析出来 TaskNodeList。\n根据 TaskNodeList 生成 TaskInstance 集合。\n如果只有一个任务运行的话,将 TaskInstance 参数配置传递给 ProcessInstance。\n将 TaskInstance 通过 AddTaskToStandByList 方法放到 ReadyToSubmitTaskQueue 队列去。\nSubmitStandByTask 提交这些 task。\nUpdateProcessInstanceState 是更新流程实例状态的。\n\n最重要的就是最后两件事情，就是将 TaskInstance 放到队列里和更新流程实例。更新流程实例纯属数据结构的变化的，这点并不难的。放到队列中的 task 如何处理，接下来将怎么做，\n也就是 SubmitStandByTask 干了哪些事情将在后续章节中说明。\n",
    "title": "还不会用 Apache Dolphinscheduler？大佬用时一个月写出的最全入门教程（2）",
    "time": "2022-5-23"
  },
  {
    "name": "Apache-DolphinScheduler-2.0.1",
    "content": "Apache DolphinScheduler 2.0.1 来了，备受期待的一键升级、插件化终于实现！\n\n\n\n\n编者按：好消息！Apache DolphinScheduler 2.0.1 版本今日正式发布！\n\n\n本版本中，DolphinScheduler 经历了一场微内核+插件化的架构改进，70% 的代码被重构，一直以来备受期待的插件化功能也得到重要优化。此外，本次升级还有不少亮点，如一键升级至最新版本、注册中心“去 ZK 化”、新增任务参数传递功能等。\n\n\nApache DolphinScheduler 2.0.1 下载地址：\nhttps://dolphinscheduler.apache.org/zh-cn/download/2.0.1\n\nApache DolphinScheduler 2.0.1 的工作流执行流程活动如下图所示：\n\n\n\n启动流程活动图\n2.0.1 版本通过优化内核增强了系统处理能力，从而在性能上得到较大提升，全新的 UI 界面也极大地提升了用户体验。更重要的是，2.0.1 版本还有两个重大变化：插件化和重构。\n01 插件化\n此前，有不少用户反馈希望 Apache DolphinScheduler 可以优化插件化，为响应用户需求，Apache DolphinScheduler 2.0.1 在插件化上更进了一步，新增了告警插件、注册中心插件和任务插件管理功能。 利用插件化，用户可以更加灵活地实现自己的功能需求，更加简单地根据接口自定义开发任务组件，也可以无缝迁移用户的任务组件至 DolphinScheduler 更高版本中。\nDolphinScheduler 正在处于微内核 + 插件化的架构改进之中，所有核心能力如任务、告警组件、数据源、资源存储、注册中心等都将被设计为扩展点，我们希望通过 SPI 来提高 Apache DolphinScheduler 本身的灵活性和友好性。\n相关代码可以参考 dolphinscheduler-spi 模块，相关插件的扩展接口也皆在该模块下。用户需要实现相关功能插件化时，建议先阅读此模块代码。当然，也建议大家阅读文档以节省时间。\n我们采用了一款优秀的前端组件 form-create，它支持基于 json 生成前端 UI 组件，如果插件开发涉及到前端，我们会通过 json 来生成相关前端 UI 组件。\norg.apache.dolphinscheduler.spi.params 里对插件的参数做了封装，它会将相关参数全部转化为对应的 json。这意味着，你完全可以通过 Java 代码的方式完成前端组件的绘制（这里主要是表单）。\n1 告警插件\n以告警插件为例，我们实现了在 alert-server 启动时加载相关插件。alert 提供了多种插件配置方法，目前已经内置了 Email、DingTalk、EnterpriseWeChat、Script 等告警插件。当插件模块开发工作完成后，通过简单的配置即可启用。\n2 多注册中心组件\n在 Apache DolphinScheduler 1.X 中，Zookeeper 组件有着非常重要的意义，包括 master/worker 服务的监控发现、失联告警、通知容错等功能。在 2.0.1 版本中，我们在注册中心逐渐“去 ZK 化”，弱化了 Zookeeper 的作用，新增了插件管理功能。\n在插件管理中，用户可以增加 ETCD 等注册中心的支持，使得 Apache Dolphinscheduler 的灵活性更高，能适应更复杂的用户需求。\n3 任务组件插件\n新版本还新增了任务插件功能，增强了不同的任务组件的隔离功能。用户开发自定义插件时，只需要实现插件的接口即可。主要包含创建任务（任务初始化、任务运行等方法）和任务取消。\n如果是 Yarn 任务，则需要实现 AbstractYarnTask。目前，任务插件的前端需要开发者自己使用 Vue 开发部署，在后续版本中，我们将实现由 Java 代码的方式完成前端组件的自动绘制。\n02 重构\n迄今为止，Apache DolphinScheduler 已经重构了约 70% 的代码，实现了全面的升级。\n1 Master 内核优化\n2.0.1 版本升级包括重构了 Master 的执行流程，将之前状态轮询监控改为事件通知机制，大幅减轻了数据库的轮询压力；去掉全局锁，增加了 Master 的分片处理机制，将顺序读写命令改为并行处理，增强了 Master 横向扩展能力；优化工作流处理流程，减少了线程池的使用，大幅提升单个 Master 处理的工作流数量；增加缓存机制，优化数据库连接方式，以及简化处理流程，减少处理过程中不必要的耗时操作等。\n2 工作流和任务解耦\n在 Apache DolphinScheduler 1.x 版本中，任务及任务关系保存是以大 json 的方式保存到工作流定义表中的，如果某个工作流很大，比如达到 100 至 1000 个任务规模，这个 json 字段会非常大，在使用时需要解析 json。这个过程比较耗费性能，且任务无法重用；另一方面，基于大 json，在工作流版本及任务版本上也没有很好的实现方案。\n因此，在新版本中，我们针对工作流和任务做了解耦，新增了任务和工作流的关系表，并新增了日志表，用来保存工作流定义和任务定义的历史版本，大幅提高工作流运行的效率。\n下图为 API 模块下工作流和任务的操作流程图：\n\n\n\n03 版本自动升级功能\n2.0.1 增加了版本自动升级功能，用户可以从 1.x 版本自动升级到 2.0.1 版本。只需要运行一个使用脚本，即可无感知地使用新版本运行以前的工作流：\nsh ./script/create-dolphinscheduler.sh\n具体升级文档请参考： https://dolphinscheduler.apache.org/zh-cn/docs/2.0.1/guide/upgrade\n另外，Apache DolphinScheduler 将来的版本均可实现自动升级，省去手动升级的麻烦。\n04 新功能列表\nApache DolphinScheduler 2.0.1 新增功能详情如下：\n1 新增 Standalone 服务\nStandAloneServer 是为了让用户快速体验产品而创建的服务，其中内置了注册中心和数据库 H2-DataBase、Zk-TestServer，在修改后一键启动 StandAloneServer 即可进行调试。\n如果想快速体验，在解压安装包后，用户只需要配置 jdk 环境等即可一键启动 Apache DolphinScheduler 系统，从而减少配置成本，提高研发效率。\n详细的使用文档请参考：https://dolphinscheduler.apache.org/zh-cn/docs/2.0.1/guide/installation/standalone\n或者使用 Docker 一键部署所有的服务：https://dolphinscheduler.apache.org/zh-cn/docs/2.0.1/guide/installation/docker\n2 任务参数传递功能\n目前支持 shell 任务和 sql 任务之间的传递。\n\nshell 任务之间的传参：\n\n在前一个&quot;create_parameter&quot;任务中设置一个 out 的变量”trans“: echo '${setValue(trans=hello trans)}'\n\n\n\n当前置任务中的任务日志中检测到关键字：”${setValue(key=value)}“, 系统会自动解析变量传递值，在后置任务中，可以直接使用”trans“变量：\n\n\n\n\nSQL 任务的参数传递：\n\nSQL 任务的自定义变量 prop 的名字需要和字段名称一致，变量会选择 SQL 查询结果中的列名中与该变量名称相同的列对应的值。输出用户数量：\n\n\n\n在下游任务中使用变量”cnt“:\n\n\n\n新增 switch 任务和 pigeon 任务组件：\n\nswitch 任务\n\n在 switch 任务中设置判断条件，可以实现根据不同的条件判断结果运行不同的条件分支的效果。例如：有三个任务，其依赖关系是 A -&gt; B -&gt; [C, D] ，其中 task_a 是 shell 任务，task_b 是 switch 任务。\n任务 A 中通过全局变量定义了名为 id 的全局变量，声明方式为echo '${setValue(id=1)}' 。\n任务 B 增加条件，使用上游声明的全局变量实现条件判断（注意 Switch 运行时存在的全局变量就行，意味着可以是非直接上游产生的全局变量）。下面我们想要实现当 id 为 1 时，运行任务 C，其他运行任务 D。\n配置当全局变量 id=1 时，运行任务 C。则在任务 B 的条件中编辑 ${id} == 1，分支流转选择 C。对于其他任务，在分支流转中选择 D。\n\n\n\n\npigeon 任务\n\npigeon 任务，是一个可以和第三方系统对接的一种任务组件，可以实现触发任务执行、取消任务执行、获取任务状态，以及获取任务日志等功能。pigeon 任务需要在配置文件中配置上述任务操作的 API 地址，以及对应的接口参数。在任务组件里输入一个目标任务名称，即可对接第三方系统，实现在 Apache DolphinScheduler 中操作第三方系统的任务。\n3 新增环境管理功能\n默认环境配置为 dolphinscheduler_env.sh。\n在线配置 Worker 运行环境，一个 Worker 可以指定多个环境，每个环境等价于 dolphinscheduler_env.sh 文件。\n\n\n\n在创建任务的时候，选择 worker 分组和对应的环境变量，任务在执行时，worker 会在对应的执行环境中执行任务。\n05 优化项\n1 优化 RestApi\n我们更新了新的 RestApi 规范，并且按照规范，重新优化了 API 部分，使得用户在使用 API 时更加简单。\n2 优化工作流版本管理\n优化了工作流版本管理功能，增加了工作流和任务的历史版本。\n3 优化 worker 分组管理功能\n在 2.0 版本中，我们新增了 worker 分组管理功能，用户可以通过页面配置来修改 worker 所属的分组信息，无需到服务器上修改配置文件并重启 worker，使用更加便捷。\n优化 worker 分组管理功能后，每个 worker 节点都会归属于自己的 Worker 分组，默认分组为 default。在任务执行时，可以将任务分配给指定 worker 分组，最终由该组中的 worker 节点执行该任务。\n修改 worker 分组有两种方法：\n\n打开要设置分组的 worker 节点上的&quot;conf/worker.properties&quot;配置文件，修改 worker.groups 参数。\n可以在运行中修改 worker 所属的 worker 分组，如果修改成功，worker 就会使用这个新建的分组，忽略 worker.properties 中的配置。修改步骤为&quot;安全中心 -&gt; worker 分组管理 -&gt; 点击 '新建 worker 分组' -&gt; 输入'组名称' -&gt; 选择已有 worker -&gt; 点击'提交'&quot;\n\n其他优化事项：\n\n增加了启动工作流的时候，可以修改启动参数；\n新增了保存工作流时，自动上线工作流状态；\n优化了 API 返回结果，加快了创建工作流时页面的加载速度；\n加快工作流实例页面的加载速度；\n优化工作流关系页面的显示信息；\n优化了导入导出功能，支持跨系统导入导出工作流；\n优化了一些 API 的操作，如增加了若干接口方法，增加任务删除检查等。\n\n06 变更日志\n另外 Apache DolphinScheduler 2.0.1 也修复了一些 bug，主要包括：\n\n修复了 netty 客户端会创建多个管道的问题；\n修复了导入工作流定义错误的问题；\n修复了任务编码会重复获取的问题；\n修复使用 Kerberos 时，Hive 数据源连接失败的问题；\n修复 Standalone 服务启动失败问题；\n修复告警组显示故障的问题；\n修复文件上传异常的问题；\n修复 Switch 任务运行失败的问题；\n修复工作流超时策略失效的问题；\n修复 sql 任务不能发送邮件的问题。\n\n07 致谢\n感谢 289 位参与 2.0.1 版本优化和改进的社区贡献者（排名不分先后）！\n\n\n\n\n\n\n8 加入社区\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n参与 DolphinScheduler 社区有非常多的参与贡献的方式，包括：\n\n\n\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n\n\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n\n\n进阶问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n\n\n如何参与贡献链接：https://dolphinscheduler.apache.org/zh-cn/community\n\n\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n参与开源可以近距离与各路高手切磋，迅速提升自己的技能，如果您想参与贡献，我们有个贡献者种子孵化群，可以添加社区小助手微信(Leonard-ds) 手把手教会您( 贡献者不分水平高低，有问必答，关键是有一颗愿意贡献的心 )。\n",
    "title": "Apache DolphinScheduler 2.0.1 来了，备受期待的一键升级、插件化终于实现！",
    "time": "2021-12-17"
  },
  {
    "name": "Apache_DolphinScheduler_in_XWBank",
    "content": "金融任务实例实时、离线跑批Apache DolphinScheduler在新网银行的三大场景与五大优化\n\n\n\n在新网银行，每天都有大量的任务实例产生，其中实时任务占据多数。为了更好地处理任务实例，新网银行在综合考虑之后，选择使用 Apache DolphinScheduler 来完成这项挑战。如今，新网银行多个项目已经完成了实时与准实时的跑批，指标管理系统的离线跑批，应用于离线数据开发和任务调度、准实时数据开发和任务调度，以及其他非 ETL 用户定义数据跑批三类场景中。\n为了更好地适应业务需求，新网银行是如何基于Apache DolphinScheduler 做改造的呢？在 Apache DolphinScheduler 4 月Meetup上，来自新网银行大数据中心的高级大数据工程师 陈卫，为我们带来了《 Apache DolphinScheduler 在新网银行的实践应用》。\n本次分享分为四个环节：\n\n\n新网银行引入 Apache DolphinScheduler 的背景介绍\n\n\nApache DolphinScheduler 的应用场景\n\n\n对新网银行的优化与改造\n\n\n新网银行使用 Apache DolphinScheduler 的后续计划\n\n\n\n\n\n陈卫\n新网银行 大数据中心 高级大数据工程师\n11 年工作经验，早期从事数据仓库建设，后转向大数据基础平台、调度系统等建设，有传统金融行业、互联网数据仓库、数据集市建设经验，多年的调度系统建设经验，咪咕文化分析云调度系统设计，报表平台设计，目前主要负责新网银行 DataOps 体系相关系统建设(离线开发，指标系统，标签系统)。\n01背景介绍\n我们选择使用 Apache DolphinScheduler 主要基于三大需求：研发场景的统一、测试场景的优化，以及投产部署场景优化。\n01研发场景\n过去，我们在数据开发过程中无统一的开发工具，因此新网银行在开发工作过程中，需要在多个工具间来回切换，导致过高的开发成本；\n另一方面，我们在开发过程中的参数替换需求无法满足，无法进行即席调试，无现成工具支持开发态与生产态离线任务。\n02测试场景\n在测试场景的部署的过程中，当我们的开发人员将脚本提供给测试，返回的文档却相当不友好。尤其是需要在多个版本多个场景中部署的时候，测试人员的任务量骤增，可视化的部署也相对较弱，无法进行较友好的自动化测试。\n03投产部署\n\n\n当前调度系统配置复杂，可视化效果差；\n\n\n开发与生产环境网络物理隔离，因此开发环境代码部署至生产环境流程长，易出错。测试环境无法充分体现生产环境配置，手动配置文件易出错，易漏配；\n\n\n运维监控能力不足，可视化效果差，无法在线查看日志，故障排除进入监控机房须登录物理机器，流程复杂。\n\n\n02应用场景\n我们应用 Apache DolphinScheduler 的场景主要有以下离线数据开发以及任务调度、准实时数据开发以及任务调度以及其他非 ETL 用户定义数据跑批三类。\n01离线数据开发以及任务调度\n在离线数据开发以及任务调度中，主要应用于我们的银行业的数据仓库、数据集市等，数据包括一些离线数据，按日按月的离线加工的数据等。\n02准实时数据开发以及任务调度\n在新网银行中准实时的数据是通过 Flink 从上游的消息队列数据库的日志里面进行融合计算，补全相关维度信息后，把数据推送到 Clickhouse 内进行处理。但按分钟级进行跑批计算，但相对于日常的按日跑批的调度，会有一些特殊的需求。\n03其他非ETL用户定义数据跑批\n我们有这部分的应用是通过一些内部的低代码平台来实现功能，我们将应用系统开放给业务人员，他们可以自助分析应用数据，不需要开发人员处理。业务人员定义好后，可以自助对这部分数据进行跑批。\n1、离线数据开发以及任务调度\n其中，我们在离线数据开发和任务调度场景中应用 Apache DolphinScheduler ，主要涉及任务开发调式、历史的任务集成、工作流与任务分离、项目环境变量、数据源查找五个板块。\n1、任务开发调式（SQL,SHELL,PYTHON,XSQL等），在线开发调式（在下查看日志，在线查看 SQL 查询返回结果）。WEBIDE 可以自动对弹窗变量替换，会根据用户的设置以及默认的处理进行动态替换。\n2、历史的任务集成\n银行业大部分数仓已经建立了四五年，有很多的历史任务，因此，我们不希望我们新的系统上线的时候，用户需要自主改造代码，因为这样会导致用户的使用成本相对过高。\n3、工作流与任务分离\n开发直接开发任务并调式、测试，工作流直接引用已开发任务，这样我们的任务开发与我们的任务编排就进行了相应的切割。\n4、项目环境变量\n新增项目环境变量，项目环境变量默认适配项目内的所有作业，这样我们不需要在每一个工作流内配置，每个项目可以直接引用。\n5、数据源\n我们按数据源名称查找数据源，支持 phoenix 等数据源。后续我们希望任务可以导入导出，但在导入导出的过程中，我们任务中的参数定义，数据源等不能进行改变，这样从测试就可以直接导向直接投产，在生产方面就会较为简单。\n2、准实时的任务\n\n\n任务开发调式（SQL），在线开发调式（在线查看日志，在线查看 SQL 查询返回结果），WEBIDE 中弹窗替换脚本变量。\n\n\nClickhouse 数据源 HA 配置集成支持。但在离线跑批中会出现一个小问题，即如果当前端口不可用，可能直接报错，在这一块，需要进行额外的处理。\n\n\n准实时工作流单实例运行，如已有初始化实例，或存在正在进行的工作流实例，即使触发了下一批次，也不会触发工作流的运行。\n\n\n3、其他非ETL用户定义数据跑批\n1、我们目前有来自指标管理平台推送的模型数据计算任务，这些用户自定的简单报表，平台会动态生成 SQL ，随后直接推送到离线调度中。未来这一过程将不会有开发人员参与。\n2、在标签管理系统中，我们主要通过生成特殊的插件任务来配适。\n03优化改造\n1、新网银行现状\n在新网银行，每天都有大约 9000+ 的任务实例产生，其中实时任务占据多数。如今，我们已经使用 Apache DolphinScheduler ，在很多项目中完成实时与准实时的跑批，指标管理系统的离线跑批等，包括对集成的支持 XSQL 内部 SQL 工具进行跑批。\n\n\n\n在右侧的截图中我们可以看到，我们其实完成了任务独立，将参数进行二次替换。另外，在任务血缘方面，尤其是 SQL 类的任务，我们可以做到自动解析，也可以手动增加。这主要用于我们工作流的自动编排，如公司内部的任务地图等。\n为了满足以上的业务需求，我们对 Apache DolphinScheduler 进行了如下五大优化，同时也列出了相应的在改造过程中必须要注意的修改。\n\n\n项目通过环境进行认为在不同场景下的各类（开发、测试）；\n\n\n环境变量与项目、环境进行隔离，但不同环境环境变量名称保持一致；\n\n\n数据源通过项目、环境进行隔离，但不同环境数据源名称保持一致；\n\n\n新增非 JDBC 数据源，ES，Livy等。因为在内部透明的应用中，需要 Livy 作为数据服务框架，对接 Spark job 进行数据脱敏。\n\n\n2、独立任务\n\n\n开发独立的任务开发，调试，配置页面，能够支持项目环境变量\n\n\nJDBC，XSQL 的任务能够通过数据源名称引用数据源\n\n\n开发交互式 WEBIDE 调试开发\n\n\n完成参数优化，支持用户${参数}并引用系统内置时间函数\n\n\n完成独立 SQL、XQSL 自动血缘解析\n\n\n完成 SQL 自动参数解析\n\n\n3、工作流启动逻辑优化\n\n\n准实时工作流单实例运行，如已存在正在运行的工作流实例，则忽略本次运行\n\n\n增加环境控制策略，工作流根据不同的环境引用不同的环境变量、数据源访问连接，比如如果提前配置了灾备环境和生产环境，一旦生产环境出现问题，可以一键切换到灾备环境中。\n\n\n优化由于工作流、任务分离带来的调度问题，主要包括异常的检测\n\n\n4、导入导出优化\n\n\n新增导入导出任务、任务配置及其资源文件等\n\n\n由于银行业和金融业有许多开发测试环境网络和生产网络是不一致的，所以需要在多个环境中处理数据时，导出一个相对友好的资源脚本工作流以及资源文件信息。\n\n\n新增工作流导入导出逻辑，处理由于不同数据库实例自增ID存在的数据冲突问题\n\n\n导航式导入导出，版本管理，主要应对紧急情况时，部分代码的回退等等\n\n\n5、告警体系改进与优化\n\n\n对接新网银行内部告警系统，默认对任务创建人员订阅告警组用户进行告警\n\n\n增加策略告警（启动延迟、完成延迟），对重点任务进行启动、完成延迟告警\n\n\n6、对接内部系统\n\n\n模型类任务运行以及监控\n\n\n报表推送类任务运行以及监控\n\n\n对接内部 IAM SSO 统一登录认证系统\n\n\n按网络不同，限定特定功能（代码编辑，工作流运行，任务运行等）\n\n\n金融行业有一个特殊的现象，就是我们的投产需要在特定的机房去做，我们必须限定某些操作只能在机房中完成，但我们也需要减少修改一次的成本，我们希望开发在看到日志以后，直接在办公网络中进行修复，修复完成后再去机房进行投产。\n\n\n\n如上图所示，我们主要基于这种维度模型理论自动创建报表。配置后，我们根据配置报表逻辑，进行多个表的代码合并计算。聚合计算完成后推送到报表服务器。这样业务用户可以按照我们提供的一些基础功能。直接进行数据聚合，不需要去写 SQL ，也避免了业务端用户不安给我们提出临时的需求。\n04后续计划\n\n\n向更多的项目组推广离线数据研发平台\n\n\n逐步替换行内已有调度系统，实现所有离线任务平稳迁移\n\n\n调度系统下沉，对接行数据研发管理系统\n\n\n技术目标\n\n\n更加智能化、自动化的任务调度、编排系统，降低调度系统在用户侧的使用门槛\n\n\n运行监控、预测，面相与运维人员提供更加友好的运维监控，任务完成时间预测等功能\n\n\n全局视图功能，面向开发、运维人员提供离线任务的全局视图，提供数据血缘、影响分析功能\n\n\n进一步集成行内定制的配置模块化，降低开发人员的开发成本\n\n\n与数据质量管理平台进行整合集成\n\n\n用户定义木板任务支持\n\n\n谢谢大家，我今天的分享就到这里。\n",
    "title": "金融任务实例实时、离线跑批Apache DolphinScheduler在新网银行的三大场景与五大优化",
    "time": "2022-5-23"
  },
  {
    "name": "Apache_DolphinScheduler_s_Graduation_From_ASF_Incubator",
    "content": "Apache DolphinScheduler ASF 孵化器毕业一周年，汇报来了！\n\n\n\n不知不觉，Apache DolphinScheduler 已经从 Apache 软件基金会（以下简称 ASF）孵化器毕业一年啦！\n北京时间 ​2021 年 4 月 9 日，ASF 官方宣布 Apache DolphinScheduler 毕业成为 Apache 顶级项目，让首个由国人主导并贡献到 Apache 的大数据工作流调度领域的顶级项目进入更多人的视野。\n如今一年过去，Apache DolphinScheduler 也在众人瞩目和基金会的帮助之下，加快了奔跑的步伐，力争在 DataOps 领域更好地发光发热。\n时值 Apache DolphinScheduler 从 ASF 孵化器毕业一周年，我们在这里向大家汇报一下在这段有限的时间内，项目在 ASF 和社区帮助下取得的成绩。\n保持快速迭代，健康状态良好\n根据 ASF Project Statistics 显示，Apache DolphinScheduler 社区健康评分为 9.19，这说明社区运行状态良好。\n目前，社区共有 45 为 Committer 和 19 位 PMC，Committer-to-PMC 比例为 2:1。\n项目活动：\n软件开发：\n2021 年至今，我们发布了 11 个版本，完成了 70% 的代码重构，使得性能提升20 倍。新增 Python SDK 支持，上线了 WorkflowAsCode 功能，并实现了插件化、一键升级等社区呼声极高的功能优化。目前最新版本为 2.0.5。\nMeetup 和会议：\n\n2021 年 11 月 27 日举行的 Apache DolphinScheduler 线上 meetup，约 4000 人次观看；\n2022 年 2 月 26 日举行的 Apache DolphinScheduler 线上 meetup，约 5000 人次观看；\n2022 年 3 月 26 日和 Apache ShenYu(Incubating)联合举办线上 Meetup，约 6000 人次观看；\n2022 年 4 月之后，将定期举办一次 Meetup（包括海外联合 Meetup）......\n\n社区健康状态：\n\ndev@dolphinscheduler.apache.org 流量较上季度增加了 64%\n\n\n\n\n\n297 封电子邮件，上季度为 181 封\n上季度共 972 次 commits（增长 123%）\n上季度有 88 位代码贡献者（增长 25%）\n上季度在 GitHub 上新开 824 个 PR（增长 89%）\n上季度在 GitHub 上关闭了 818 个 PR（增长 100%）\n上季度在 GitHub 上新开 593 个 issue（增长 90%）\n上季度在 GitHub 上关闭了 608 个 issue（增长 155%）\n\n最活跃的 GitHub issues/PRs:\n\ndolphinscheduler/issues/8790[Bug] [Process Definition] Duplicate key TaskDefinition\t(31 comments)\ndolphinscheduler/issues/9068[Bug] [API server] could not get flow in exists project after upgrade from 2.0.1 to 2.0.5\t(27 comments)\ndolphinscheduler/pull/8340[Feature-8222][python] move examples into the scope of source package\t(17 comments)\ndolphinscheduler/pull/8246[Feature-8245][Alert] Add Alert Plugin Telegram\t(14 comments)\ndolphinscheduler/pull/9246[Fix-9221] [alert-server] optimization and gracefully close\t(14 comments)\ndolphinscheduler-website/pull/713[Feature-8023][Document] Add example and notice about task type Python\t(13 comments)\ndolphinscheduler/pull/8747[Fix-8744][standalone-server] start standalone server failed\t(13 comments)\ndolphinscheduler-website/pull/667[Feature-8020][Document] Add example and notice about task type SQL\t(12 comments)\ndolphinscheduler/issues/7992[Feature][Alert] Support PagerDuty Plugin &amp;&amp; Alert module judging strategy\t(11 comments)\ndolphinscheduler/pull/9336[Improvement-9338][API] show more create datasource exception message\t(11 comments)\n自成立以来，Apache DolphinScheduler 历经过次迭代，功能不断完善，性能持续提升，不断优化以符合开发者习惯的开发方式，为用户提供了经过生产实践环境检验的成熟工作流调度解决方案。\n\n目前，Apache DolphinScheduler 也开启了国际化的步伐，尝试增加了 Python，AWS，以及时区支持等，以接轨国际化的开发使用方式。\n见证中国开源奔跑的一年\n2021 年是 Apache DolphinScheduler 成长的一年，也是见证中国开源项目飞速发展的一年。\n\nCNCF 超过 20% 的开源项目来自中国，贡献度跃升至世界第二。\n2021 年，首次有华人（吴晟）当选为 Apache 软件基金会董事会董事。\n2021 年，来自中国的 5 个项目顺利进入 Apache 孵化器。截至目前，共有 14 个源自中国的 ASF 项目。\n还有 1 个孵化项目顺利毕业成为 Apache 顶级项目，那就是 Apache DolphinScheduler。\n开源大有可为，在此我们呼唤更多有志于开源的同伴，能够与 Apache DolphinScheduler 携手，在共同成长的过程中推进中国开源走向更高的世界舞台！\n\n",
    "title": "Apache DolphinScheduler ASF 孵化器毕业一周年，汇报来了！",
    "time": "2022-4-14"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.2",
    "content": "WorkflowAsCode 来了，Apache DolphinScheduler 2.0.2 惊喜发布！\n\n\n\n千呼万唤中，WorkflowAsCode 功能终于在 2.0.2 版本中如约上线，为有动态、批量创建和更新工作流需求的用户带来福音。\n此外，新版本还新增企业微信告警群聊会话消息推送，简化了元数据初始化流程，并修复了旧版本中强制终止后服务重启失败，添加 Hive 数据源失败等问题。\n01 新功能\n1 WorkflowAsCode\n首先在新功能上，2.0.2 版本重磅发布了 PythonGatewayServer， 这是一个 Workflow-as-code 的服务端，与 apiServer 等服务的启动方式相同。\n启用 PythonGatewayServer 后，所有 Python API 的请求都会发送到 PythonGatewayServer。Workflow-as-code 让用户可以通过 Python API 创建工作流，对于有动态、批量地创建和更新工作流的用户来说是一个好消息。通过 Workflow-as-code 创建的工作流与其他工作流一样，都可以在 web UI 查看。\n以下为一个 Workflow-as-code 测试用例：\n\n# 定义工作流属性，包括名称、调度周期、开始时间、使用租户等信息\nwith ProcessDefinition(\n    name=&quot;tutorial&quot;,\n    schedule=&quot;0 0 0 * * ? *&quot;,\n    start_time=&quot;2021-01-01&quot;,\n    tenant=&quot;tenant_exists&quot;,\n) as pd:\n    # 定义4个任务，4个都是 shell 任务，shell 任务的必填参数为任务名、命令信息，这里都是 echo 的 shell 命令\n    task_parent = Shell(name=&quot;task_parent&quot;, command=&quot;echo hello pydolphinscheduler&quot;)\n    task_child_one = Shell(name=&quot;task_child_one&quot;, command=&quot;echo &#x27;child one&#x27;&quot;)\n    task_child_two = Shell(name=&quot;task_child_two&quot;, command=&quot;echo &#x27;child two&#x27;&quot;)\n    task_union = Shell(name=&quot;task_union&quot;, command=&quot;echo union&quot;)\n\n    # 定义任务间依赖关系\n    # 这里将 task_child_one，task_child_two 先声明成一个任务组，通过 python 的 list 声明\n    task_group = [task_child_one, task_child_two]\n    # 使用 set_downstream 方法将任务组 task_group 声明成 task_parent 的下游，如果想要声明上游则使用 set_upstream\n    task_parent.set_downstream(task_group)\n\n    # 使用位操作符 &lt;&lt; 将任务 task_union 声明成 task_group 的下游，同时支持通过位操作符 &gt;&gt; 声明\n    task_union &lt;&lt; task_group\n\n\n上面的代码运行后，可以在 web UI 看到的工作流如下：\n                /                    \\\ntask_parent --&gt;                        --&gt;  task_union\n                \\                   /\n                  --&gt; task_child_two\n\n2 企业微信告警方式支持群聊消息推送\n在此前版本中，微信告警方式仅支持消息通知方式；在 2.0.2 版本中，用户在使用企业微信的告警时，支持进行应用内以群聊会话消息推送的方式推送给用户。\n02 优化\n1 简化元数据初始化流程\n首次安装 Apache DolphinScheduler 时，运行 create-dolphinscheduler.sh 需要从最早的版本逐步升级到当前版本。为了更方便快捷地初始化元数据流程，2.0.2 版本让用户可以直接安装当前版本的数据库脚本，提升安装速度。\n2 删除补数日期中的“+1”（天）\n删除了补数日期中的“+1”天，以避免补数时 UI 日期总显示 +1 给用户造成的困惑。\n03 Bug 修复\n[#7661] 修复 logger 在 worker 中的内存泄漏\n[#7750] 兼容历史版本数据源连接信息\n[#7705] 内存限制导致从 1.3.5 升级到 2.0.2 出现错误\n[#7786] 强制终止后服务重启失败\n[#7660] 流程定义版本创建时间错误\n[#7607] 执行 PROCEDURE 节点失败\n[#7639] 在通用配置项中添加 quartz 和 zookeeper 默认配置\n[#7654] 在依赖节点中，出现不属于当前项目的选项时报错\n[#7658] 工作流复制错误\n[#7609] worker sendResult 成功但 master 未收到错误时，工作流始终在运行\n[#7554] Standalone Server 中的 H2 会在数分钟后自动重启，导致数据异常丢失\n[#7434] 执行 MySQL 建表语句报错\n[#7537] 依赖节点重试延迟不起作用\n[#7392] 添加 Hive 数据源失败\n下载：https://dolphinscheduler.apache.org/zh-cn/download\nRelease Note：https://github.com/apache/dolphinscheduler/releases/tag/2.0.2\n04 致谢\n一如既往地，感谢所有为 2.0.2 版本建言献策并付诸行动的 Contributor（排名不分先后），是你们的智慧和付出让 Apache DolphinScheduler 更加符合用户的使用需求。\n\n\n\n05 参与贡献\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n参与 DolphinScheduler 社区有非常多的参与贡献的方式，包括：\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n非新手问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n如何参与贡献链接：https://dolphinscheduler.apache.org/zh-cn/community\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n社区官网\nhttps://dolphinscheduler.apache.org/\n代码仓地址\nhttps://github.com/apache/dolphinscheduler\n您的 Star，是 Apache DolphinScheduler 为爱发电的动力 ❤️ ～\n",
    "title": "WorkflowAsCode 来了，Apache DolphinScheduler 2.0.2 惊喜发布！",
    "time": "2022-1-13"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.3",
    "content": "Apache DolphinScheduler 2.0.3 发布，支持钉钉告警签名校验，数据源可从多个会话获取链接\n\n\n\n\n今天，Apache DolphinScheduler 宣布 2.0.3 版本正式发布。本版本支持钉钉告警签名校验，以及数据源从多个会话获取链接。此外，2.0.3 还对缓存管理、补数时间、日志中的数据源密码显示等进行优化，并修复了若干关键 Bug。\n\n新增功能\n钉钉告警支持加签名校验\n2.0.3 支持通过签名方式实现钉钉机器人报警的功能。\n\n\n\n钉钉的参数配置\n\nWebhook\n\n格式如下：https://oapi.dingtalk.com/robot/send?access_token=XXXXXX\n\nKeyword\n\n安全设置的自定义关键词\n\nSecret\n\n安全设置的加签\n自定义机器人发送消息时，可以通过手机号码指定“被@人列表”。在“被@人列表”中的人员收到该消息时，会有@消息提醒。设置为免打扰模式，会话仍然会有通知提醒，在首屏出现“有人@你”提示。\n\n@Mobiles\n\n被@人的手机号\n\n@UserIds\n\n被@人的用户 userid\n\n@All\n\n是否@所有人\n支持数据源从多个会话获取链接\n此前，使用 JdbcDataSourceProvider.createOneSessionJdbcDataSource() 方法 hive/impala 创建连接池设置了 MaximumPoolSize=1，但是调度任务中，如果 hive/impala 多任务同时运行，会出现 getConnection=null 的情况，SqlTask.prepareStatementAndBind() 方法会抛出空指针异常。\n2.0.3 优化了这一点，支持数据源从多个会话获取链接。\n优化\n缓存管理优化，减少 Master 调度过程中的 DB 查询次数\n由于主服务器调度进程，中会出现大量的数据库读操作，如 tenant、user、processDefinition 等，这一方面会给 DB 带来巨大压力，另一方面会减慢整个核心调度过程。\n考虑到这部分业务数据是多读少写的场景，2.0.3 引入了缓存模块，主要作用于 Master 节点，将业务数据如租户、工作流定义等进行缓存，降低数据库查询压力，加快核心调度进程，详情可查看官网文档：https://dolphinscheduler.apache.org/en-us/docs/3.1.2/architecture/cache\n补数时间区间从 “左闭右开” 改为 “左闭右闭”\n此前，补数时间为“左闭右开”(startDate &lt;= N &lt; endDate)，不利于用户理解。优化之后，部署时间区间改为“左闭右闭”。\n对日志中的数据源密码进行加密显示\n数据源中的密码进行加密，加强隐私保护。\nBug 修复\n\nzkRoot 配置不起作用\n修复修改管理员账号的用户信息引起的错误\n增加删除工作流定义同时删除工作流实例\nUDF 编辑文件夹对话框不能取消\n修复因为 netty 通讯没有失败重试，worker 和 master 通讯失败，导致工作流一直运行中的问题\n删除运行中的工作流，Master 会一直打印失败日志\n修复环境变量中选择 workerGroup 的问题\n修复依赖任务中告警设置不起作用的问题\n工作流历史版本查询信息出错\n解决高并发下任务日志输出影响性能的问题\nsub_process 节点的全局参数未传递给关联的工作流任务\nK8S 上 Master 任务登录时，查询日志无法显示内容\n进程定义列表中存在重复进程\n当流程实例 FailureStrategy.END 时任务失败，流程实例一直在运行\nt_ds_resources 表中的“is_directory”字段在 PostgreSQL 数据库中出现类型错误\n修复 Oracle 的 JDBC 连接\nDag 中有禁止节点时，执行流程异常\nquerySimpleList 返回错误的项目代码\n\nRelease Note: https://github.com/apache/dolphinscheduler/releases/tag/2.0.3\n下载地址： https://dolphinscheduler.apache.org/zh-cn/download\n感谢贡献者\n感谢社区 Contributor 对本版本的积极贡献，以下为 Contributor 名单，排名不分先后：\n\n\n\n",
    "title": "Apache DolphinScheduler 2.0.3 发布，支持钉钉告警签名校验，数据源可从多个会话获取链接",
    "time": "2022-1-27"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.5",
    "content": "Apache DolphinScheduler 2.0.5 发布，Worker 容错流程优化\n\n\n\n今天，Apache DolphinScheduler 宣布 2.0.5 版本正式发布。此次版本进行了一些功能优化，如 Worker 的容错流程优化，在资源中心增加了重新上传文件的功能，并进行了若干 Bug 修复。\n优化\nWorker 容错流程\n2.0.5 版本优化了 worker 的容错流程，使得服务器由于压力过大导致 worker 服务中断时，可以正常将任务转移至其他 worker 上继续执行，避免任务中断。\n禁止运行任务页面标志\n优化禁止运行任务的页面显示标志，区别于正常执行的任务显示，以免用户混淆工作状态。\n\n\n\n任务框增加提示语\n2.0.5 版本在任务框上增加了提示语，可以显示出全部的长任务名字，方便用户查看。\n\n\n\n资源中心增加重新上传文件功能\n在资源中心增加了重新上传文件的功能，当用户需要修改执行脚本时，无需再重新配置任务参数，可实现自动更新执行脚本功能。\n修改工作流后跳转到列表页\n改变了此前修改工作流以后页面仍然留在 DAG 页面的现状，优化后可跳转到列表页，便于用户后续操作。\n钉钉告警插件新增 Markdown 信息类型\n在钉钉告警插件的告警内容中新增 Markdown 信息类型，丰富信息类型支持。\nBug 修复\n[#8213] 修复了当 worker 分组包含大写字母时，任务运行错误的问题；\n[#8347] 修复了当任务失败重试时，工作流不能被停止的问题；\n[#8135] 修复了 jdbc 连接参数不能输入‘@’的问题；\n[#8367] 修复了补数时可能不会正常结束的问题；\n[#8170] 修复了从页面上不能进入子工作流的问题。\n2.0.5 下载地址：\nhttps://dolphinscheduler.apache.org/zh-cn/download\nRelease Note：https://github.com/apache/dolphinscheduler/releases/tag/2.0.5\n感谢贡献者\n感谢 Apache DolphinScheduler 2.0.5 版本的贡献者，贡献者 GitHub ID 列表如下（排名不分先后）：\n\n\n\n",
    "title": "Apache DolphinScheduler 2_0_5 发布，Worker 容错流程优化",
    "time": "2022-3-7"
  },
  {
    "name": "Apache_dolphinScheduler_3.0.0_alpha",
    "content": "3.0.0 alpha 重磅发布！九大新功能、全新 UI 解锁调度系统新能力\n\n\n\n\n2022 年 4 月 22 日，Apache DolphinScheduler 正式宣布 3.0.0 alpha 版本发布！此次版本升级迎来了自发版以来的最大变化，众多全新功能和特性为用户带来新的体验和价值。\n3.0.0-alpha 的关键字，总结起来是 “更快、更现代化、更强、更易维护”。\n\n\n更快、更现代化： 重构了 UI 界面，新 UI 不仅用户响应速度提高数十倍，开发者构建速度也提高数百倍，且页面布局、图标样式都更加现代化；\n更强： 带来了许多振奋人心的新功能，如数据质量评估、自定义时区、支持 AWS，并新增多个任务插件和多个告警插件；\n更易维护： 后端服务拆分更加符合容器化和微服务化的发展趋势，还能明确各个服务的职责，让维护更加简单。\n\n新功能和新特性\n全新 UI，前端代码更健壮，速度更快\n3.0.0-alpha 最大的变化是引入了新的 UI，切换语言页面无需重新加载，并且新增了深色主题。新 UI 使用了 Vue3、TSX、Vite 相关技术栈。对比旧版 UI，新 UI 不仅更加现代化，操作也更加人性化，前端的鲁棒性也更强，使用户在编译时一旦发现代码中的问题，可以对接口参数进行校验，从而使前端代码更加健壮。\n此外，新架构和新技术栈不仅能让用户在操作 Apache DolphinScheduler 时响应速度有数十倍的提升，同时开发者本地编译和启动 UI 的速度有了数百倍的提升，这将大大缩短开发者调试和打包代码所需的时间。\n新 UI 使用体验：\n\n\n\n本地启动耗时对比\n\n\n\n首页\n\n\n\n工作流实例页面\n\n\n\nShell 任务页面\n\n\n\nMySQL  数据源页面\n支持  AWS\n随着 Apache DolphinScheduler 用户群体越来越丰富，吸引了很多海外用户。但在海外业务场景下，用户在调研过程中发现有两个影响用户便捷体验 Apache DolphinScheduler 的点，一个是时区问题，另一个则是对海外云厂商，尤其是对 AWS 的支持不足。为此，我们决定对 AWS 较为重要的组件进行支持，这也是此版本的最重大的变化之一。\n目前，Apache DolphinScheduler 对 AWS 的支持已经涵盖  Amazon EMR  和  Amazon Redshift两个 AWS 的任务类型，并实现了资源中心支持  Amazon S3 存储。\n\n针对 Amazon EMR，我们创建了一个新的任务类型，并提供了其 Run Job Flow 的功能，允许用户向 Amazon EMR 提交多个 steps 作业，并指定使用的资源数量。详情可见：https://dolphinscheduler.apache.org/zh-cn/docs/3.1.2/guide/task/emr\n\n\n\n\nAmazon EMR 任务定义\n\n对于 Amazon Redshift，我们目前在 SQL 任务类型中扩展了对 Amazon Redshift 数据源的支持，现在用户可以在 SQL 任务中选择 Redshift 数据源来运行 Amazon Redshift 任务。\n\n\n\n\nAmazon Redshift 支持\n对于 Amazon S3，我们扩展了 Apache DolphinScheduler 的资源中心，使其不仅能支持本地资源、HDFS 资源存储，同时支持 Amazon S3 作为资源中心的储存。详情可见：https://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/guide/resource/configuration.md中的\n`resource.storage.type`\n\n后续我们将用户的实际需求支持更多 AWS 任务，敬请期待。\n服务拆分\n全新的 UI 是 3.0.0-alpha 前端的最大变化，而后端最大的变化就是对服务进行拆分。考虑到容器和微服务的概念越来越火热，Apache DolphinScheduler 开发者做出了重大决定：对后端服务进行拆分。按照职能，我们将服务拆分成了以下几部分：\n\nmaster-server: master 服务\nworker-server: worker 服务\napi-server: API 服务\nalert-server: 告警服务\nstandalone-server: standalone 用于快速体验 dolphinscheduler 功能\nui: UI 资源\nbin: 快速启动脚本，主要是启动各个服务的脚本\ntools: 工具相关脚本，主要包含数据库创建，更新脚本\n所有的服务都可以通过执行下面的命令进行启动或者停止。\n\n`bin/dolphinscheduler-daemon.sh &lt;start|stop&gt; &lt;server-name&gt;`\n\n\n数据质量校验\n此版本中，用户期待已久的数据质量校验应用功能上线，解决了从源头同步的数据条数准确性，单表或多表周均、月均波动超过阈值告警等数据质量问题。Apache DolphinScheduler 此前版本解决了将任务以特定顺序和时间运行的问题，但数据运行完之后对数据的质量一直没有较为通用的衡量标准，用户需要付出额外的开发成本。\n现在，3.0.0-alpha 已经实现了数据质量原生支持，支持在工作流运行前进行数据质量校验过程，通过在数据质量功能模块中，由用户自定义数据质量的校验规则，实现了任务运行过程中对数据质量的严格控制和运行结果的监控。\n\n\n\n\n\n\n任务组\n任务组主要用于控制任务实例并发并明确组内优先级。用户在新建任务定义时，可配置当前任务对应的任务组，并配置任务在任务组内运行的优先级。当任务配置了任务组后，任务的执行除了要满足上游任务全部成功外，还需要满足当前任务组正在运行的任务小于资源池的大小。当大于或者等于资源池大小时，任务会进入等待状态等待下一次检查。当任务组中多个任务同时进到待运行队列中时，会先运行优先级高的任务。\n详见链接：https://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/guide/resource/configuration.md\n\n\n\n\n\n\n自定义时区\n在 3.0.0-alpha 之前版本，Apache DolphinScheduler 默认的时间是 UTC+8 时区，但随着用户群体扩大，海外用户和在海外开展跨时区业务的用户在使用中经常被时区所困扰。3.0.0-alpha 支持时区切换后，时区问题迎刃而解，满足了海外用户和出海业务伙伴的需求。例如，如当企业业务涉及的时区包含东八区和西五区，如果想要使用同一个 DolphinScheduler 集群，可以分别创建多个用户，每个用户使用自己当地的时区，对应 DolphinScheduler 对象显示的时间均会切换为对应时区的当地时间，更加符合当地开发者的使用习惯。\n\n\n\n详见链接：https://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/guide/howto/general-setting.md\n任务定义列表\n使用 Apache DolphinScheduler 3.0.0-alpha 此前版本，用户如果想要操作任务，需要先找到对应的工作流，并在工作流中定位到任务的位置之后才能编辑。然而，当工作流数量变多或单个工作流有较多的任务时，找到对应任务的过程将会变得非常痛苦，这不符合 Apache DolphinScheduler 所追求的 easy to use 理念。所以，我们在 3.0.0-alpha 中增加了任务定义页面，让用户可以通过任务名称快速定位到任务，并对任务进行操作，轻松实现批量任务变更。\n详见链接：https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/project/task-definition\n新增告警类型\n与此同时，3.0.0-alpha 告警类型也增加了对 Telegram、Webexteams 告警类型的支持。\nPython API 新功能\n3.0.0-alpha 中，Python API 最大的变化是将对应的 PythonGatewayServer 集成到了 API-Server 服务，使得开启对外服务更加规整，且缓解了因服务拆分导致的二进制包变大的问题。同时，Python API 还增加了 CLI 和 configuration 模块，让用户可以自定义配置文件，修改配置更加便捷。\n其他新功能\n除了上述功能外，3.0.0-alpha 版本还进行了很多细节功能增强，如重构任务插件、数据源插件模块，让扩展更简单；恢复了对 Spark SQL 的支持；E2E 测试已经完美兼容新 UI 等。\n主要优化项\n[#8584] 任务后端插件优化，新插件只需要修改插件自带的模块\n[#8874] 在工作流下提交/创建 cron 时验证结束时间和开始时间\n[#9016] Dependent 添加依赖时可以选择全局项目\n[#9221] AlertSender 优化及关闭优化，如 MasterServer\n[#9228] 实现使用 slot 扫描数据库\n[#9230] python gateway server 集成到 apiserver 来减少二进制包大小\n[#9372] [python] 将 pythonGatewayServer 迁移到 api 服务器\n[#9443] [python] 添加缺失的配置和连接远程服务器文档\n[#8719] [Master/Worker] 将任务 ack 更改为运行回调\n[#9293] [Master] 添加任务事件线程池\n主要 Bug 修复\n[#7236] 修复使用 S3a Minio 创建租户失败的问题\n[#7416] 修复文本文件 busy 的问题\n[#7896] 修复项目授权时生成一个重复授权项目的问题\n[#8089] 修复因无法连接到 PostgreSQL  而启动服务器失败的问题\n[#8183] 修复消息显示找不到数据源插件“Spark”的问题\n[#8202] 修复 MapReduce 生成的命令内置参数位置错误的问题\n[#8751] 解决更改参数用户，队列在 ProcessDefinition 中失效的问题\n[#8756] 解决使用依赖组件的进程无法在测试和生产环境之间迁移\n[#8760] 解决了资源文件删除条件的问题\n[#8791] 修复编辑复制节点的表单时影响原始节点数据的问题\n[#8951] 解决了 Worker 资源耗尽并导致停机的问题\n[#9243] 解决了某些类型的警报无法显示项目名称的问题\nRelease Note\nhttps://github.com/apache/dolphinscheduler/releases/tag/3.0.0-alpha\n感谢贡献者\n按首字母排序\nAaron Lin, Amy0104, Assert, BaoLiang, Benedict Jin, BenjaminWenqiYu, Brennan Fox, Devosend, DingPengfei, DuChaoJiaYou, EdwardYang, Eric Gao, Frank Chen, GaoTianDuo, HanayoZz, Hua Jiang, Ivan0626, Jeff Zhan, Jiajie Zhong, JieguangZhou, Jiezhi.G, JinYong Li, J·Y, Kerwin, Kevin.Shin, KingsleyY, Kirs, KyoYang, LinKai, LiuBodong, Manhua, Martin Huang, Maxwell, Molin Wang, OS, QuakeWang, ReonYu, SbloodyS, Shiwen Cheng, ShuiMuNianHuaLP, ShuoTiann, Sunny Lei, Tom, Tq, Wenjun Ruan, X&amp;Z, XiaochenNan, Yanbin Lin, Yao WANG, Zonglei Dong, aCodingAddict, aaronlinv, caishunfeng, calvin, calvinit, cheney, chouc, gaojun2048, guoshupei, hjli, huangxiaohai, janeHe13, jegger, jon-qj, kezhenxu94, labbomb, lgcareer, lhjzmn, lidongdai, lifeng, lilyzhou, lvshaokang, lyq, mans2singh, mask, mazhong, mgduoduo, myangle1120, nobolity, ououtt, ouyangyewei, pinkhello, qianli2022, ronyang1985, seagle, shuai hou, simsicon, songjianet, sparklezzz, springmonster, uh001, wangbowen, wangqiang, wangxj3, wangyang, wangyizhi, wind, worry, xiangzihao, xiaodi wang, xiaoguaiguai, xuhhui, yangyunxi, yc322, yihong, yimaixinchen, zchong, zekai-li, zhang, zhangxinruu, zhanqian, zhuangchong, zhuxt2015, zixi0825, zwZjut,\n天仇, 小张, 时光, 王强,  百岁, 弘树, 张俊杰, 罗铭涛\n",
    "title": "3.0.0 alpha 重磅发布！九大新功能、全新 UI 解锁调度系统新能力",
    "time": "2022-4-25"
  },
  {
    "name": "Awarded_most_popular_project_in_2021",
    "content": "Apache DolphinScheduler 获评 2021 年度「最受欢迎项目」！\n\n\n\n\n近日，由 OSCHINA 举办的「2021 OSC 中国开源项目」评选活动公布了评选结果。\n在广大用户和开源社区的喜爱和支持下，云原生分布式大数据调度系统 Apache DolphinScheduler 获评 2021 年度「OSCHINA 人气指数 Top 50 开源项目」和「最受欢迎项目」。\n\n获评「最受欢迎项目」\n\n\n\n\n\n\n今年，「2021 OSC 中国开源项目」活动设置了两轮投票环节，第一轮投票根据票数选出了「OSCHINA 人气指数 TOP 50 开源项目」。第二轮投票基于第一轮选出的 TOP 50 项目进行，并在此基础上通过投票选出了 30 个「最受欢迎项目」。\n在第一轮投票中，OSCHINA 根据票数选出「组织」项目 7 大分类（基础软件、云原生、大前端、DevOps、开发框架与工具、AI &amp; 大数据、IoT &amp; 5G）中每个分类的 TOP 5，Apache DolphinScheduler 在「云原生」大类中脱颖而出，凭借优秀的云原生能力入选。\n之后，经过第二轮投票的激烈角逐，Apache DolphinScheduler 再次胜出，获得「最受欢迎项目」奖项。\n\n\n\n中国开源软件生态蓬勃发展，近年来涌现出了一大批优秀的开源软件创企，他们不忘初心，深耕开源，回馈社区，为中国开源软件事业添砖加瓦，成为全球开源软件生态中不可忽视的重要力量。「OSC 中国开源项目评选」是开源中国（OSCHINA，OSC 开源社区）举办的国内最权威、最盛大的开源项目评选活动，旨在更好地展示国内开源现状，探讨国内开源趋势，激励国内开源人才，促进国内开源生态完善。\nApache DolphinScheduler：分布式工作流任务调度系统\nApache DolphinScheduler 是 Apache 基金会孵化的顶级项目。作为新一代大数据任务调度系统，致力于“解决大数据任务之间错综复杂的依赖关系，使整个数据处理流程直观可见”。DolphinScheduler 以 DAG(有向无环图)  的方式将 Task 组装起来，可实时监控任务的运行状态，同时支持重试、从指定节点恢复失败、暂停及 Kill 任务等操作，并专注于可视化 DAG、调用高可用、丰富的任务类型、以来、任务日志/告警机制和补数 6 大能力。\n迄今为止，Apache DolphinScheduler 社区已经有 280+ 经验丰富的代码贡献者，110+ 非代码贡献者，其中也不乏其他 Apache 顶级项目的 PMC 或者 Committer。自创建以来，Apache DolphinScheduler 开源社区在不断发展壮大，微信用户群已达 6000+ 人。截止 2022 年 1 月，已经有 600 + 家公司及机构在生产环境中采用 Apache DolphinScheduler。 \n最后，感谢 OSCHINA 对 Apache DolphinScheduler 以及其背后商业公司白鲸开源科技的认可，更感谢社区每一位参与者，开源之力滴水成河，Apache DolphinScheduler 社区的壮大离不开每一位贡献者的参与。社区将以此为鞭策，希望在更多小伙伴的助力之下，社区可以再上一层楼！\n参与贡献\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n参与 DolphinScheduler 社区有非常多的参与贡献的方式，包括：\n文档、翻译、答疑、测试、代码、实践文章、原理文章、会议分享等\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n\n\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n\n\n非新手问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n\n\n如何参与贡献链接：https://dolphinscheduler.apache.org/en-us/community\n\n\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n参与开源可以近距离与各路高手切磋，迅速提升自己的技能，如果您想参与贡献，我们有个贡献者种子孵化群，可以添加社区小助手微信(Leonard-ds) 手把手教会您( 贡献者不分水平高低，有问必答，关键是有一颗愿意贡献的心 )。添加小助手微信时请说明想参与贡献。\n",
    "title": "Apache DolphinScheduler 获评 2021 年度「最受欢迎项目」",
    "time": "2022-1-7"
  },
  {
    "name": "Board_of_Directors_Report",
    "content": "Apache DolphinScheduler 董事会报告：社区健康运行，Commit 增长 123%\n\n\n\n\n自 2021 年 3 月 17 日从 Apache 孵化器毕业以来，Apache DolphinScheduler 不知不觉已经和社区一起经过了十个月的成长。在社区的共同参与下，Apache DolphinScheduler 在数次版本迭代后，蜕变为一个经过数百家企业生产环境检验的成熟调度系统产品。\n\n\n在将近一年的时间里，Apache DolphinScheduler 有了哪些进步？今天我们将通过这篇 Apache 报告，一起回顾这段时间发生在 Apache DolphinScheduler 及其社区中的变化。\n\n基本数据：\n成立： 2021 年 03 月 17 日(十个月前)\nChair： 代立冬\n下次报告日期: 2022 年 1 月 19 日(星期三)\n社区健康评分( Chi ): 7.55（健康）\n项目组成：\n\n目前该项目中有 39 个 committer 和 16 个 PMC 成员。\ncommitter 与 PMC 的比例大约是 5 ： 2 。\n\n与上季度相比，社区的变化：\n\n无新的 PMC 成员加入。最新加入的 PMC 成员为 Calvin Kirs，加入时间 2021 - 05 - 07 。\nShunFeng Cai 于 2021 年 12 月 18 日新晋 committer。\nZhenxu Ke 于 2021 年 12 月 12 日新晋 committer。\nWang Xingjie 于 2021 年 11 月 24 日新晋 committer。\nYi zhi Wang 于 2021 年 12 月 15 日新晋 committer。\nJiajie Zhong于 2021 年 12 月 12 日新晋 committer。\n\n社区健康指标:\n\n邮件列表趋势\ncommit 数量\nGitHub PR 数量\nGitHub issue\n最活跃的 GitHub issues/ PR\n\n邮件列表趋势:\nDev@DolphinScheder.apache.org 在过去的一个季度，流量增长 64%（ 297 封电子邮件，上季度为 181 封）：\n\n\n\ncommit 数量：\n\n上季度共 972 个 commit(增长 123 %)\n上季度共新增 88 个代码贡献者(增长 25 %)\n\n\n\n\nGitHub PR 数量:\n\nGitHub 上新开 824 个 PR ，较上季度(增长 89 %)\nGitHub 上关闭 818 个 PR ，较上季度（增长 100 %）\n\n\n\n\nGitHub issues:\nGitHub 上新开 593 个 issues， 较上季度(增长 90 %)\nGitHub 上关闭 608 个 issue，较上季度(增长 155 %)\n\n\n\n讨论最热烈的 GitHub issues/ PR :\n\ndolphinscheduler/pull/6894[Improvement][Logger]Logger server integrate into worker server(15 comments)\ndolphinscheduler/pull/6744[Bug][SnowFlakeUtils] fix snowFlake bug(15 comments)\ndolphinscheduler/pull/6674[Feature][unittest] Recover UT in AlertPluginManagerTest.java [closes: #6619](15 comments)\ndolphinscheduler/issues/7039[Bug] [Task Plugin] hive sql execute failed(14 comments)\ndolphinscheduler/pull/6782[improvement] improve install.sh if then statement(13 comments)\ndolphinscheduler/issues/7485[Bug] [dolphinscheduler-datasource-api] Failed to create hive datasource using ZooKeeper way in 2.0.1(13 comments)\ndolphinscheduler/pull/7214[DS-7016][feat] Auto create workflow while import sql script with specific hint(12 comments)\ndolphinscheduler/pull/6708[FIX-#6505][Dao] upgrade the MySQL driver package for building MySQL jdbcUrl(12 comments)\ndolphinscheduler/pull/7515[6696/1880][UI] replace node-sass with dart-sass(12 comments)\ndolphinscheduler/pull/6913Use docker.scarf.sh to track docker user info(12 comments)\n\n",
    "title": "Apache DolphinScheduler 董事会报告：社区健康运行，Commit 增长 123%",
    "time": "2022-1-13"
  },
  {
    "name": "China_Unicom_revamps_Apache_DolphinScheduler",
    "content": "中国联通改造 Apache DolphinScheduler 资源中心，实现计费环境跨集群调用与数据脚本一站式访问\n\n\n\n截止2022年，中国联通用户规模达到4.6亿，占据了全中国人口的30%，随着5G的推广普及，运营商IT系统普遍面临着海量用户、海量话单、多样化业务、组网模式等一系列变革的冲击。\n当前，联通每天处理话单量超过400亿条。在这样的体量基础上，提高服务水平，为客户提供更有针对性的服务，也成为了联通品牌追求的终极目标。而中国联通在海量数据汇集、加工、脱敏、加密等技术与应用方面已崭露头角，在行业中具有一定的先发优势，未来势必成为大数据赋能数字经济发展的重要推动者。\n在 Apache DolphinScheduler 4月 Meetup 上，我们邀请到了联通软件研究院的柏雪松，他为我们分享了《DolphinScheduler在联通计费环境中的应用》。\n本次演讲主要包括三个部分：\n\n\nDolphinScheduler在联通的总体使用情况\n\n\n联通计费业务专题分享\n\n\n下一步的规划\n\n\n\n\n\n柏雪松 联通软研院 大数据工程师\n毕业于中国农业大学，从事于大数据平台构建和 AI 平台构建，为 Apache DolphinScheduler 贡献 Apache SeaTunnel(Incubating) 插件，并为 Apache SeaTunnel(Incubating) 共享 alluxio 插件\n01  总体使用情况\n首先给大家说明一下联通在DolphinScheduler的总体使用情况：\n\n\n现在我们的业务主要运行在3地4集群\n\n\n总体任务流数量大概在300左右\n\n\n日均任务运行差不多5000左右\n\n\n我们使用到的DolphinScheduler组件包括Spark、Flink、SeaTunnel（原Waterdrop），以及存储过程中的Presto和一些Shell脚本，涵盖的业务则包含稽核，收入分摊，计费业务，还有其他一些需要自动化的业务等。\n\n\n\n02 业务专题分享\n​01 跨集群双活业务调用\n上文说过，我们的业务运行在3地4集群上，这样就免不了集群之间的互相的数据交换和业务调用。如何统一管理和调度这些跨集群的数据传输任务是一个重要的问题，我们数据在生产集群，对于集群网络带宽十分敏感，必须有组织地对数据传输进行管理。\n另一方面，我们有一些业务需要跨集群去调用，例如A集群数据到位后B集群要启动统计任务等，我们选择 Apache DolphinScheduler作为调度和控制，来解决这两个问题。\n首先说明下我们跨集群数据传输的流程在AB两个集群上进行，我们均使用HDFS进行底层的数据存储，在跨集群的HDFS数据交换上，根据数据量大小和用途，我们将使用的数据分为小批量和大批量数据，向结构表，配置表等。\n对于小批量数据，我们直接将其挂载到同一个Alluxio上进行数据共享，这样不会发生数据同步不及时导致的版本问题。\n\n\n像明细表和其他大文件，我们使用Distcp和Spark混合进行处理；\n\n\n对于结构表数据，使用SeaTunnel on Spark的方式；\n\n\n通过Yarn队列的方式进行限速设置；\n\n\n非结构数据使用Distcp传输，通过自带的参数Bandwidth进行速度限制；\n\n\n这些传输任务都是运行在DolphinScheduler平台上面，我们整体的数据流程主要是A集群的数据到位检测，A集群的数据完整性校验，AB集群之间的数据传输，B集群的数据稽核和到位通知。\n强调一点：其中我们重点用到了DolphinScheduler自带的补数重跑，对失败的任务或者不完整的数据进行修复。\n\n\n\n在完成了跨集群的数据同步和访问，我们还会使用DolphinScheduler进行跨地域和集群的任务调用。\n我们在A地有两个集群，分别是测试A1和生产A2，在B地有生产B1集群，我们会在每个集群上拿出两台具有内网IP的机器作为接口机，通过在6台接口机上搭建DolphinScheduler建立一个虚拟集群，从而可以在统一页面上操作三个集群的内容；\nQ：如何实现由测试到生产上线？\nA：在A1测试上进行任务开发，并且通过测试之后，直接将worker节点改动到A2生产上；\nQ：遇到A2生产出了问题，数据未到位等情况怎么办？\nA：我们可以直接切换到B1生产上，实现手动的双活容灾切换；\n\n\n\n最后我们还有些任务比较大，为满足任务时效性，需要利用两个集群同时计算，我们会将数据拆分两份分别放到A2和B1上面，之后同时运行任务，最后将运行结果传回同一集群进行合并，这些任务流程基本都是通过DolphinScheduler来进行调用的。\n请大家注意，在这个过程中，我们使用DolphinScheduler解决了几个问题：\n\n\n项目跨集群的任务依赖校验；\n\n\n控制节点级别的任务环境变量；\n\n\n02 AI开发同步任务运行\n1、统一数据访问方式\n我们现在已经有一个简易的AI开发平台，主要为用户提供一些Tensorflow和Spark ML的计算环境。在业务需求下，我们需要将用户训练的本地文件模型和集群文件系统打通，并且能够提供统一的访问方式和部署方法，为解决这个问题，我们使用了Alluxio-fuse和DolphinScheduler这两个工具。\n\n\nAlluxio-fuse打通本地和集群存储\n\n\nDolphinScheduler共享本地和集群存储\n\n\n由于我们搭建的AI平台集群和数据集群是两个数据集群，所以在数据集群上我们进行一个数据的存储，利用Spark SQL或者Hive进行一些数据的预加工处理，之后我们将处理完的数据挂载到Alluxio上，最后通过Alluxio fuse跨级群映射到本地文件，这样我们基于Conda的开发环境，就可以直接访问这些数据，这样就可以做到统一数据的访问方式，以访问本地数据的方法访问集群的数据。\n\n\n\n2、数据脚本一站式访问\n分离资源之后，通过预处理大数据内容通过数据集群，通过我们的AI集群去处理训练模型和预测模型，在这里，我们使用Alluxio-fuse对DolphinScheduler的资源中心进行了二次改动，我们将DolphinScheduler资源中心连接到Alluxio上，再通过Alluxio-fuse同时挂载本地文件和集群文件，这样在DolphinSchedule上面就可以同时访问在本地的训练推理脚本，又可以访问到存储在hdfs上的训练推理数据，实现数据脚本一站式访问。\n\n\n\n03 业务查询逻辑持久化\n第三个场景是我们用Presto和Hue为用户提供了一个前台的即时查询界面，因为有些用户通过前台写完SQL，并且测试完成之后，需要定时运行一些加工逻辑和存储过程，所以这就需要打通从前台SQL到后台定时运行任务的流程。\n\n\n\n另一个问题是Presto原生没有租户间的资源隔离问题。我们也是对比了几个方案之后，最后结合实际情况选择了Presto on Spark方案。\n因为我们是一个多租户平台，最开始给用户提供的方案是前端用Hue界面，后端直接使用原生的Presto跑在物理集群上，这导致了用户资源争抢占的问题。当有某些大查询或者大的加工逻辑存在时，会导致其他租户业务长时间处于等待状态。\n为此，我们对比了Presto on Yarn和Presto on Spark，综合对比性能之后发现Presto on Spark资源使用效率会更高一些，这里大家也可以根据自己的需求选择对应的方案。\n\n\n\n另一方面，我们使用了原生Presto和Presto on spark共存的方式，对于一些数据量较小，加工逻辑较为简单的SQL，我们直接将其在原生Presto上运行，而对于一些加工逻辑比较复杂，运行时间比较长的SQL，则在Presto on spark上运行，这样用户用一套SQL就可以切换到不同的底层引擎上。\n此外，我们还打通了Hue到DolphinScheduler定时任务调度流程。我们在Hue上进行SQL开发调制后，通过存储到本地Serve文件，连接到Git进行版本控制。\n我们将本地文件挂载到Alluxio fuse上，作为SQL的同步挂载，最后我们使用Hue，通过DolphinScheduler的API创建任务和定时任务，实现从SQL开发到定时运行的流程控制。\n\n\n\n04 数据湖数据统一治理\n最后一个场景是数据湖数据统一管理，在我们自研的数据集成平台上，使用分层治理的方式对数据湖数据进行统一的管理和访问，其中使用了DolphinScheduler作为入湖调度和监控引擎。\n在数据集成平台上，对于数据集成、数据入湖、数据分发这些批量的和实时的任务的，我们使用DolphinScheduler进行调度。\n底层运行在Spark和Flink上，对于数据查询和数据探索这些需要即时反馈的业务需求，我们使用嵌入Hue接入Spark和Presto的方法，对数据进行探索查询；对于数据资产登记同步和数据稽核等，直接对数据源文件信息进行查询，直接同步底层数据信息。\n最后一个场景是数据湖数据统一管理，在我们自研的数据集成平台上，使用分层治理的方式对数据湖数据进行统一的管理和访问，其中使用了DolphinScheduler作为入湖调度和监控引擎。\n在数据集成平台上，对于数据集成、数据入湖、数据分发这些批量的和实时的任务的，我们使用DolphinScheduler进行调度。\n底层运行在Spark和Flink上，对于数据查询和数据探索这些需要即时反馈的业务需求，我们使用嵌入Hue接入Spark和Presto的方法，对数据进行探索查询；对于数据资产登记同步和数据稽核等，直接对数据源文件信息进行查询，直接同步底层数据信息。\n\n\n\n目前我们集成平台基本上管理着460张数据表的质量管理，对数据准确性和准时性提供统一的管理。\n03 下一步计划与需求\n\n\n\n01 资源中心\n在资源中心层面，为了方便用户之间的文件共享，我们计划为全用户提供资源授权，同时根据它的归属租户，分配租户级别的共享文件，使得对于一个多租户的平台更为友善。\n02 用户管理\n其次与用户传权限相关，我们只提供租户级别的管理员账账户，后续的用户账户由租户管理员账户创建，同时租户组内的用户管理也是由租户管理员去控制，以方便租户内部的管理。\n03 任务节点\n最后是我们的任务节点相关的计划，现在已在进行之中：一方面是完成SQL节点的优化，让用户能够选择一个资源中心的SQL文件，而不需要手动复制SQL；另一方面是HTTP节点对返回的json自定义解析提取字段判断，对复杂返回值进行更为友好的处理。\n",
    "title": "中国联通改造 Apache DolphinScheduler 资源中心，实现计费环境跨集群调用与数据脚本一站式访问",
    "time": "2022-5-07"
  },
  {
    "name": "DS-2.0-alpha-release",
    "content": "重构、插件化、性能提升 20 倍，Apache DolphinScheduler 2.0 alpha 发布亮点太多！\n\n社区的小伙伴们，好消息！经过 100 多位社区贡献者近 10 个月的共同努力，我们很高兴地宣布 Apache DolphinScheduler 2.0 alpha 发布。这是 DolphinScheduler 自进入 Apache 以来的首个大版本，进行了多项关键更新和优化，是 DolphinScheduler 发展中的里程碑。\nDolphinScheduler 2.0 alpha 主要重构了 Master 的实现，大幅优化了元数据结构和处理流程，增加了 SPI 插件化等能力，在性能上提升 20 倍。同时，新版本设计了全新的 UI 界面，带来更好的用户体验。另外，2.0 alpha 还新添加和优化了一些社区呼声极高的功能，如参数传递、版本控制、导入导出等功能。\n注意：当前 alpha 版本还未支持自动升级，我们将在下个版本中支持这一功能。\n2.0 alpha 下载地址：https://dolphinscheduler.apache.org/en-us/download\n优化内核，性能提升 20 倍\n相较于 DolphinScheduler 1.3.8，同等硬件配置下(3 台 8 核 16 G)，2.0 alpha 吞吐性能提升 20 倍，这主要得益于 Master 的重构，Master 执行流程和优化了工作流处理流程等，包括：\n\n重构 Master 的执行流程，将之前状态轮询监控改为事件通知机制，大幅减轻了数据库的轮询压力；\n去掉全局锁，增加了 Master 的分片处理机制，将顺序读写命令改为并行处理，增强了 Master 横向扩展能力；\n优化工作流处理流程，减少了线程池的使用，大幅提升单个 Master 处理的工作流数量；\n增加缓存机制，大幅减少数据库的操作次数；\n优化数据库连接方式，极大地缩减数据库操作耗时；\n简化处理流程，减少处理过程中不必要的耗时操作。\n\n优化 UI 组件，全新的 UI 界面\n\n\n \n  UI 界面对比：1.3.9（上） VS. 2.0 alpha（下）\n\n\n2.0 UI 重要优化在以下几个方面：\n\n\n优化组件显示：界面更简洁，流程显示更清晰，一目了然；\n突出重点内容：鼠标点击任务框，显示任务详情信息；\n增强可识别性：左侧工具栏标注名称，使工具更易识别，便于操作；\n调整组件顺序：调整组件排列顺序，更符合用户习惯。\n\n除了性能与 UI 上的变化外，DolphinScheduler 也新增和优化了 20 多项功能\n及 BUG 修复。\n新功能列表\n\n优化项\n\nBug 修复\n\n感谢贡献者\nDolphinScheduler 2.0 alpha 的发布凝聚了众多社区贡献者的智慧和力量，是他们的积极参与和极大的热情开启了 DolphinScheduler 2.0 时代！\n非常感谢 100+ 位（GitHub ID）社区小伙伴的贡献，期待更多人能够加入 DolphinScheduler 社\n区共建，为打造一个更好用的大数据工作流调度平台贡献自己的力量！\n\n2.0 alpha 贡献者名单\n加入我们\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n进阶问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n如何参与贡献链接：https://dolphinscheduler.apache.org/zh-cn/community\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n参与开源可以近距离与各路高手切磋，迅速提升自己的技能，如果您想参与贡献，我们有个贡献者种子孵化群，可以添加社区小助手微信(Leonard-ds) 手把手教会您( 贡献者不分水平高低，有问必答，关键是有一颗愿意贡献的心 )。添加小助手微信时请说明想参与贡献。\n来吧，开源社区非常期待您的参与。\n",
    "title": "重构、插件化、性能提升 20 倍，Apache DolphinScheduler 2.0 alpha 发布亮点太多!",
    "time": "2021-10-29"
  },
  {
    "name": "DS_architecture_evolution",
    "content": "Apache DolphinScheduler 架构演进及开源经验分享\n引言\n来自 eBay 的文俊同学在近期的上海开源大数据 Meetup 上做了十分精彩的 “Apache DolphinScheduler 的架构演进” 分享。本次分享有近 200 人参与，在线观看次数超过 2,500 次\n演讲者介绍\n阮文俊，eBay 开发工程师，DolphinScheduler 贡献者。\n视频分享参见\n\nApache DolphinScheduler介绍\nApache DolphinScheduler是一个分布式去中心化，易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。DolphinScheduler以有向无环图的方式将任务连接起来，可实时监控任务的运行状态，同时支持取消、暂停、恢复、从指定任务节点重跑等操作。\nDolphinScheduler具有以下几个优良功能特性：\n\n\nCloud Native — 支持多云/数据中心工作流管理，也支持 Kubernetes、Docker 部署和自定义任务类型，分布式调度，整体调度能力随集群规模线性增长\n\n\n高可靠与高可扩展性 — 去中心化的多 Master 多 Worker 设计架构，支持服务动态上下线，自我容错与调节能力\n\n\n支持多租户\n\n\n丰富的使用场景 — 包括流、暂停、恢复操作，以及额外的任务类型，如 Spark、Hive、MR、Shell、Python、Flink 以及 DS 独有的子工作流、任务依赖设计，扩展点采用插件化的实现方式\n\n\n简单易用 — 所有流程定义操作可视化编排，定义关键信息一目了然，一键部署\n\n\n关于DolphinSheduler更多功能介绍和开发文档请查阅官网详细信息 https://dolphinscheduler.apache.org\n架构演进过程\n1.2.x架构\nDolphinScheduler最初进入Apache孵化器的版本是1.2，在这一版本中采用的架构由以下几个重要部分组成：\n\n去中心化的master节点，负责工作流调度、DAG任务切分、任务提交监控和监听其它节点健康状态等任务\n去中心化的worker节点，负责执行任务和维护任务的生命周期等\n数据库，存储工作流元数据，运行实例数据\nZookeeper，主要负责注册中心、分布式锁、任务队列等工作任务\n\n1.2版本基本实现了高可靠的工作流调度系统，但是也存在多个问题：\n\n重量级的worker，worker节点需要负责多种任务\n异步派发任务会导致任务执行延迟\n由于masker和worker都需要依赖数据库，导致数据库压力大\n\n[]\n1.3.x架构\n针对1.2版本存在的问题，1.3架构进行了如下改进：\n\n去任务队列，保证master节点同步派发任务，降低任务执行延迟\n轻量级worker，worker节点只负责执行任务，单一化worker职责\n减小数据库压力，worker不再连接数据库\n采用多任务负载均衡策略，master根据worker节点资源使用情况分配任务，提高worker资源利用率\n\n这些改进有效改进了1.2版本的缺陷，但仍存在一些问题，例如：\n\nmaster调度工作流时需要依赖分布式锁，导致工作流吞吐量难以提升\n因为需要创建大量线程池，多数线程处于轮询数据库，导致master资源利用率低\nmaster轮询数据库，仍然导致数据库压力大\n各组件存在耦合情况\n\n\n2.0架构\n针对1.3版本的缺陷，2.0架构进一步做出改进：\n\n去分布式锁，对master进行分区编号，实现错位查询数据库，避免多个节点同时访问同一个工作流造成的冲突问题\n重构master线程模型，对所有工作流使用统一的线程池\n重构数据库中DAG元数据模型\n彻底的插件化，所有扩展点都采用插件化实现\n数据血缘关系分析\n\n1 去分布式锁\n\n2 重构 master 中的线程模型\n\nSchedulerThread 负责从数据库中查询 Command 并提交到 Command Queue\nDagExecuteThreadPool 从 Command Queue 中取 command，并构造 DAG实例添加到 DAG 队列，进行处理，当前 DAG 没有未执行的任务，则当前 DAG 执行结束\nTaskExecuteThreadPool 提交任务给 Worker\nTaskEventThread 监听任务事件队列，修改任务状态\n3 彻底的插件化\n\n所有扩展点都采用插件化实现\n告警SPI\n注册中心SPI\n资源存储SPI\n任务插件SPI\n数据源SPI\n……\nApache DolphinScheduler发展方向\n开发者阮文俊针对dolphinsheduler的未来发展方向，也分享了一些看法：\n\n系统更稳、速度更快（高吞吐、低延迟、智能化运维、高可用）\n支持更多的任务集成（深度学习任务、CI/CD等其它系统集成、存储过程和数据质量任务、容器调度任务、复杂调度场景等）\n轻量化dolphinscheduler内核，提供基础调度服务\n\n\n如何参与开源贡献\n最后，开发者阮文俊针对入门新手如何参与开源贡献的问题，提出了宝贵的指导意见：\n\n从小事做起，积累开发经验\n关注社区动态，积极参与讨论，更好融入社区\n坚持开源精神，乐于帮助他人\n持之以恒\n\n\n",
    "title": "Apache DolphinScheduler 架构演进及开源经验分享",
    "time": "2021-07-18"
  },
  {
    "name": "DS_run_in_windows",
    "content": "DolphinScheduler 在 Windows 本地搭建开发环境，源码启动\n如果您对本地开发的视频教程感兴趣的话，也可以跟着视频来一步一步操作:\n\n\n\n下载源码\n官网 ：https://dolphinscheduler.apache.org/zh-cn\n地址 ：https://github.com/apache/dolphinscheduler.git\n这里选用 1.3.6-release 分支。\n\n\nwindows 安装 zk\n\n\n下载 zk https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.6.3/apache-zookeeper-3.6.3-bin.tar.gz\n\n\n解压 apache-zookeeper-3.6.3-bin.tar.gz\n\n\n在 zk 的目录下新建 data、log 文件夹\n\n\n将 conf 目录下的 zoo_sample.cfg 文件，复制一份，重命名为 zoo.cfg，修改其中数据和日志的配置，如：\ndataDir=D:\\\\code\\\\apache-zookeeper-3.6.3-bin\\\\data\ndataLogDir=D:\\\\code\\\\apache-zookeeper-3.6.3-bin\\\\log\n\n\n\n在 bin 中运行 zkServer.cmd，然后运行 zkCli.cmd 查看 zk 运行状态，可以查看 zk 节点信息即代表安装成功。\n\n\n\n\n搭建后端环境\n\n\n新建一个自我调试的 mysql 库，库名可为 ：dolphinschedulerKou\n\n\n把代码导入 idea，修改根项目中 pom.xml，将 mysql-connector-java 依赖的 scope 修改为 compile\n\n\n修改 dolphinscheduler-dao 模块的 datasource.properties\n# mysql\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://192.168.2.227:3306/dolphinschedulerKou?useUnicode=true&amp;characterEncoding=UTF-8\nspring.datasource.username=root\nspring.datasource.password=Dm!23456\n\n\n\n刷新 dao 模块，运行 org.apache.dolphinscheduler.dao.upgrade.shell.CreateDolphinScheduler 的 main 方法，自动插入项目所需的表和数据\n\n\n修改 dolphinscheduler-service 模块的 zookeeper.properties\nzookeeper.quorum=localhost:2181\n\n\n\n在 logback-worker.xml、logback-master.xml、logback-api.xml 中添加控制台输出\n&lt;root level=&quot;INFO&quot;&gt;\n    &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;  &lt;!-- 添加控制台输出 --&gt;\n    &lt;appender-ref ref=&quot;APILOGFILE&quot;/&gt;\n    &lt;appender-ref ref=&quot;SKYWALKING-LOG&quot;/&gt;\n&lt;/root&gt;\n\n\n\n启动 MasterServer，执行 org.apache.dolphinscheduler.server.master.MasterServer 的 main 方法，需要设置 VM Options:\n-Dlogging.config=classpath:logback-master.xml -Ddruid.mysql.usePingMethod=false\n\n\n\n启动 WorkerServer，执行 org.apache.dolphinscheduler.server.worker.WorkerServer 的 main 方法，需要设置 VM Options:\n-Dlogging.config=classpath:logback-worker.xml -Ddruid.mysql.usePingMethod=false\n\n\n\n启动 ApiApplicationServer，执行 org.apache.dolphinscheduler.api.ApiApplicationServer 的 main 方法，需要设置 VM Options:\n-Dlogging.config=classpath:logback-api.xml -Dspring.profiles.active=api\n\n\n\n如果需要用到日志功能，执行 org.apache.dolphinscheduler.server.log.LoggerServer 的 main 方法。\n\n\n后端 swagger 地址 ：http://localhost:12345/dolphinscheduler/doc.html?language=zh_CN&amp;lang=cn\n\n\n\n\n搭建前端环境\n\n\n本机安装 node（不再赘述）\n\n\n进入 dolphinscheduler-ui，运行\nnpm install\nnpm run start\n\n\n\n访问 http://localhost:8888\n\n\n登录管理员账号\n\n用户：admin\n密码：dolphinscheduler123\n\n\n\n\n\n",
    "title": "DolphinScheduler在Windows环境启动源码",
    "time": "2021-07-05"
  },
  {
    "name": "DolphinScheduler_Kubernetes_Technology_in_action",
    "content": "Apache DolphinScheduler 在 Kubernetes 体系中的技术实战\n作者 | 杨滇，深圳交通中心 数据和算法平台架构师\nKubernetes 技术体系给 Apache DolphinScheduler 带来的技术新特性\nApache DolphinScheduler 是当前非常优秀的分布式易扩展的可视化工作流任务调度平台。\n基于笔者所在公司业务的特性，阐述我们使用 Kubernetes 作为 Apache DolphinScheduler 的技术底座的原因：\n\n各类独立部署项目，需要快速建立开发环境和生产环境；\n项目环境互联网访问受限，服务器只能使用离线的安装方式；\n尽可能统一的安装配置的信息，减少多个项目配置的异常；\n与对象存储技术的结合，统一非结构化数据的技术；\n便捷的监控体系，与现有监控集成；\n多种调度器的混合使用；\n全自动的资源调整能力；\n快速的自愈能力；\n\n本文的案例都是基于 Apache DolphinScheduler1.3.9 版本为基础。Hadoop\n基于 helm 工具的自动化高效部署方式\n首先，我们介绍基于官网提供的 helm 的安装方式。Helm 是查找、分享和使用软件构建 Kubernetes 的最优方式。也是云原生 CNCF 的毕业项目之一。\n\n\n\n海豚的官网和 GitHub 上有非常详细的配置文件和案例。这里我们重点介绍一些社区中经常出现的咨询和问题。\n官网文档地址 https://dolphinscheduler.apache.org/zh-cn/docs/1.3.9/kubernetes-deployment\nGitHub 文件夹地址 https://github.com/apache/dolphinscheduler/tree/1.3.9/docker/kubernetes/dolphinscheduler/\n\n\n在 value.yaml 文件中修改镜像，以实现离线安装（air-gap install）；\nhttps://about.gitlab.com/topics/gitops/\nimage:\n  repository: &quot;apache/dolphinscheduler&quot;\n  tag: &quot;1.3.9&quot;\n  pullPolicy: &quot;IfNotPresent&quot;\n\n针对公司内部安装好的 harbor，或者其他公有云的私仓，进行 pull，tag，以及 push。这里我们假定私仓地址是 harbor.abc.com，你所在构建镜像的主机已经进行了 docker login harbor.abc.com， 且已经建立和授权私仓下面新建 apache 项目。\n执行 shell 命令\ndocker pull apache/dolphinscheduler:1.3.9\ndock tag apache/dolphinscheduler:1.3.9 harbor.abc.com/apache/dolphinscheduler:1.3.9\ndocker push apache/dolphinscheduler:1.3.9\n\n再替换 value 文件中的镜像信息，这里我们推荐使用 Always 的方式拉取镜像，生产环境中尽量每次去检查是否是最新的镜像内容，保证软件制品的正确性。此外，很多同学会把 tag 写成 latest（制作镜像不写 tag 信息，这样在生产环境非常危险，任何人 push 了镜像，就相当于改变了 latest 的 tag 的镜像，而且也无法判断 latest 是什么版本，所以建议要明确每次发版的 tag，并且使用 Always。\nimage:\n  repository: &quot;harbor.abc.com/apache/dolphinscheduler&quot;\n  tag: &quot;1.3.9&quot;\n  pullPolicy: &quot;Always&quot;GitHub\n\n把 https://github.com/apache/dolphinscheduler/tree/1.3.9/docker/kubernetes/dolphinscheduler/ 整个目录 copy 到可以执行 helm 命令的主机，然后按照官网执行\nkubectl create ns ds139Git\nMySQL install dolphinscheduler . -n ds139\n\n即可实现离线安装。\n\n\n集成 DataX MySQL Oracle 客户端组件，首先下载以下组件\n\n\nhttps://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.49/mysql-connector-java-5.1.49.jar\nhttps://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/\nhttps://GitHub.com/alibaba/DataX/blob/master/userGuid.md 根据提示进行编译构建，文件包位于 {DataX_source_code_home}/target/DataX/DataX/\n基于以上 plugin 组件新建 dockerfile，基础镜像可以使用已经 push 到私仓的镜像。\nFROM harbor.abc.com/apache/dolphinscheduler:1.3.9\nCOPY *.jar /opt/dolphinscheduler/lib/\nRUN mkdir -p /opt/soft/DataX\nCOPY DataX /opt/soft/DataX\n\n保存 dockerfile，执行 shell 命令\ndocker build -t harbor.abc.com/apache/dolphinscheduler:1.3.9-MySQL-Oracle-DataX .  #不要忘记最后一个点\ndocker push harbor.abc.com/apache/dolphinscheduler:1.3.9-MySQL-Oracle-DataX\n\n修改 value 文件\nimage:\n  repository: &quot;harbor.abc.com/apache/dolphinscheduler&quot;\n  tag: &quot;1.3.9-MySQL-Oracle-DataX&quot;\n  pullPolicy: &quot;Always&quot;\n\n执行 helm install dolphinscheduler . -n ds139，或者执行 helm upgrade dolphinscheduler -n ds139，也可以先 helm uninstall dolphinscheduler -n ds139，再执行 helm install dolphinscheduler . -n ds139。\n\n\n通常生产环境建议使用独立外置 postgresql 作为管理数据库，并且使用独立安装的 zookeeper 环境（本案例使用了 zookeeper operator https://GitHub.com/pravega/zookeeper-operator ，与 Apache DolphinScheduler 在同一个 Kubernetes 集群中）。\n## If not exists external database, by default, Dolphinscheduler&#x27;s database will use it.\npostgresql:\n  enabled: false\n  postgresqlUsername: &quot;root&quot;\n  postgresqlPassword: &quot;root&quot;\n  postgresqlDatabase: &quot;dolphinscheduler&quot;\n  persistence:\n    enabled: false\n    size: &quot;20Gi&quot;\n    storageClass: &quot;-&quot;\n\n## If exists external database, and set postgresql.enable value to false.\n## external database will be used, otherwise Dolphinscheduler&#x27;s database will be used.\nexternalDatabase:\n  type: &quot;postgresql&quot;\n  driver: &quot;org.postgresql.Driver&quot;\n  host: &quot;192.168.1.100&quot;\n  port: &quot;5432&quot;\n  username: &quot;admin&quot;\n  password: &quot;password&quot;\n  database: &quot;dolphinscheduler&quot;\n  params: &quot;characterEncoding=utf8&quot;\n\n## If not exists external zookeeper, by default, Dolphinscheduler&#x27;s zookeeper will use it.\nzookeeper:\n  enabled: false\n  fourlwCommandsWhitelist: &quot;srvr,ruok,wchs,cons&quot;\n  persistence:\n    enabled: false\n    size: &quot;20Gi&quot;\n    storageClass: &quot;storage-nfs&quot;\n  zookeeperRoot: &quot;/dolphinscheduler&quot;\n\n## If exists external zookeeper, and set zookeeper.enable value to false.\n## If zookeeper.enable is false, Dolphinscheduler&#x27;s zookeeper will use it.\nexternalZookeeper:\n  zookeeperQuorum: &quot;zookeeper-0.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.zookeeper.svc.cluster.local:2181&quot;\n  zookeeperRoot: &quot;/dolphinscheduler&quot;\n\n\n\n基于 argo-cd 的 Gitops 部署方式\nargo-cd 是基于 Kubernetes 的声明式 Gitops 持续交付工具。argo-cd 是 CNCF 的孵化项目，Gitops 的最佳实践工具。关于 Gitops 的解释可以参考https://about.gitlab.com/topics/gitops/\n\n\n\nGitops 可以为 Apache DolphinScheduler 的实施带来以下优点。\n\n图形化安装集群化的软件，一键安装；\nGit 记录全发版流程，一键回滚；\n便捷的海豚工具日志查看；\n\n使用 argo-cd 的实施安装步骤：\n\n\n从 GitHub 上下载 Apache DolphinScheduler 源码，修改 value 文件，参考上个章节 helm 安装需要修改的内容；\n\n\n把修改后的源码目录新建 Git 项目，并且 push 到公司内部的 Gitlab 中，GitHub 源码的目录名为 docker/Kubernetes/dolphinscheduler；\n\n\n在 argo-cd 中配置 Gitlab 信息，我们使用 https 的模式；\n\n\n\n\n\n\nargo-cd 新建部署工程，填写相关信息\n\n\n\n\n\n\n\n\n对 Git 中的部署信息进行刷新和拉取，实现最后的部署工作；可以看到 pod，configmap，secret，service 等等资源全自动拉起。\n\n\n\n\n\n\n\n\n\n通过 kubectl 命令可以看到相关资源信息；\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME                                     READY   STATUS    RESTARTS   AGE\ndolphinscheduler-alert-96c74dc84-72cc9   1/1     Running   0          22m\ndolphinscheduler-api-78db664b7b-gsltq    1/1     Running   0          22m\ndolphinscheduler-master-0                1/1     Running   0          22m\ndolphinscheduler-master-1                1/1     Running   0          22m\ndolphinscheduler-master-2                1/1     Running   0          22m\ndolphinscheduler-worker-0                1/1     Running   0          22m\ndolphinscheduler-worker-1                1/1     Running   0          22m\ndolphinscheduler-worker-2                1/1     Running   0          22m\n\n[root@tpk8s-master01 ~]# kubectl get statefulset -n ds139\nNAME                      READY   AGE\ndolphinscheduler-master   3/3     22m\ndolphinscheduler-worker   3/3     22m\n\n[root@tpk8s-master01 ~]# kubectl get cm -n ds139\nNAME                      DATA   AGE\ndolphinscheduler-alert    15     23m\ndolphinscheduler-api      1      23m\ndolphinscheduler-common   29     23m\ndolphinscheduler-master   10     23m\ndolphinscheduler-worker   7      23m\n\n[root@tpk8s-master01 ~]# kubectl get service -n ds139\nNAME                               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)              AGE\ndolphinscheduler-api               ClusterIP   10.43.238.5   &lt;none&gt;        12345/TCP            23m\ndolphinscheduler-master-headless   ClusterIP   None          &lt;none&gt;        5678/TCP             23m\ndolphinscheduler-worker-headless   ClusterIP   None          &lt;none&gt;        1234/TCP,50051/TCP   23m\n\n[root@tpk8s-master01 ~]# kubectl get ingress -n ds139\nNAME               CLASS    HOSTS           ADDRESS\ndolphinscheduler   &lt;none&gt;   ds139.abc.com\n\n\n\n\n可以看到所有的 pod 都分撒在 Kubernetes 集群中不同的 host 上，例如 worker1 和 2 都在不同的节点上。\n\n\n\n\n\n\n\n\n\n我们配置了 ingress，公司内部配置了泛域名就可以方便的使用域名进行访问；\n\n\n\n\n可以登录域名进行访问。\n\n\n\n具体配置可以修改 value 文件中的内容：\ningress:\n  enabled: true\n  host: &quot;ds139.abc.com&quot;\n  path: &quot;/dolphinscheduler&quot;\n  tls:\n    enabled: false\n    secretName: &quot;dolphinscheduler-tls&quot;\n\n\n方便查看 Apache DolphinScheduler 各个组件的内部日志：\n\n\n\n\n\n对部署好的系统进行检查，3 个 master，3 个 worker，zookeeper 都配置正常；\n\n\n\n\n\n\n\n\n\n\n\n\n使用 argo-cd 可以非常方便的进行修改 master，worker，api，alert 等组件的副本数量，海豚的 helm 配置也预留了 cpu 和内存的设置信息。这里我们修改 value 中的副本值。修改后，提交公司内部 Gitlab。\nmaster:\n  ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\n  podManagementPolicy: &quot;Parallel&quot;\n  ## Replicas is the desired number of replicas of the given Template.\n  replicas: &quot;5&quot;\n\nworker:\n  ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\n  podManagementPolicy: &quot;Parallel&quot;\n  ## Replicas is the desired number of replicas of the given Template.\n  replicas: &quot;5&quot;\n\nalert:\n  ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\n  replicas: &quot;3&quot;\n\napi:\n  ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\n  replicas: &quot;3&quot;\n\n\n\n\n只需要在 argo-cd 点击 sync 同步，对应的 pods 都按照需求进行了增加\n\n\n\n\n\n\n\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME                                     READY   STATUS    RESTARTS   AGE\ndolphinscheduler-alert-96c74dc84-72cc9   1/1     Running   0          43m\ndolphinscheduler-alert-96c74dc84-j6zdh   1/1     Running   0          2m27s\ndolphinscheduler-alert-96c74dc84-rn9wb   1/1     Running   0          2m27s\ndolphinscheduler-api-78db664b7b-6j8rj    1/1     Running   0          2m27s\ndolphinscheduler-api-78db664b7b-bsdgv    1/1     Running   0          2m27s\ndolphinscheduler-api-78db664b7b-gsltq    1/1     Running   0          43m\ndolphinscheduler-master-0                1/1     Running   0          43m\ndolphinscheduler-master-1                1/1     Running   0          43m\ndolphinscheduler-master-2                1/1     Running   0          43m\ndolphinscheduler-master-3                1/1     Running   0          2m27s\ndolphinscheduler-master-4                1/1     Running   0          2m27s\ndolphinscheduler-worker-0                1/1     Running   0          43m\ndolphinscheduler-worker-1                1/1     Running   0          43m\ndolphinscheduler-worker-2                1/1     Running   0          43m\ndolphinscheduler-worker-3                1/1     Running   0          2m27s\ndolphinscheduler-worker-4                1/1     Running   0          2m27s\n\nApache DolphinScheduler 与 s3 对象存储技术集成\n许多同学在海豚的社区中提问，如何配置 s3 minio 的集成。这里给出基于 Kubernetes 的 helm 配置。\n\n\n修改 value 中 s3 的部分，建议使用 ip+端口指向 minio 服务器。\ncommon:\n  ## Configmap\n  configmap:\n    DOLPHINSCHEDULER_OPTS: &quot;&quot;\n    DATA_BASEDIR_PATH: &quot;/tmp/dolphinscheduler&quot;\n    RESOURCE_STORAGE_TYPE: &quot;S3&quot;\n    RESOURCE_UPLOAD_PATH: &quot;/dolphinscheduler&quot;\n    FS_DEFAULT_FS: &quot;s3a://dfs&quot;\n    FS_S3A_ENDPOINT: &quot;http://192.168.1.100:9000&quot;\n    FS_S3A_ACCESS_KEY: &quot;admin&quot;\n    FS_S3A_SECRET_KEY: &quot;password&quot;\n\n\n\nminio 中存放海豚文件的 bucket 名字是 dolphinscheduler，这里新建文件夹和文件进行测试。\n\n\n\n\n\n\n\n\nApache DolphinScheduler 与 Kube-prometheus 的技术集成\n\n\n我们在 Kubernetes 使用 kube-prometheus operator 技术，实现了在部署海豚后，自动实现了对海豚各个组件的资源监控。\n\n\n请注意 kube-prometheus 的版本，需要对应 Kubernetes 主版本。https://GitHub.com/prometheus-operator/kube-prometheus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache DolphinScheduler 与 Service Mesh 的技术集成\n\n\n通过 Service Mesh 技术可以实现对海豚内部的服务调用，以及海豚 api 外部调用的可观测性分析，以实现 Apache DolphinScheduler 产品的自身服务优化。\n\n\n我们使用 linkerd 作为 Service Mesh 的产品进行集成，linkerd 也是 CNCF 优秀的毕业项目。\n\n\n \n\n\n\n\n只需要在海豚 helm 的 value 文件中修改 annotations，重新部署，就可以快速实现 mesh proxy sidecar 的注入。可以对 master，worker，api，alert 等组件都注入。\nannotations: #{}\n  linkerd.io/inject: enabled\n\n\n\n可以观察组件之间的服务通信质量，每秒请求的次数等等。\n\n\n\n\n\n\n未来 Apache DolphinScheduler 基于云原生技术的展望\nApache DolphinScheduler 作为面向新一代云原生大数据工具，未来可以在 Kubernetes 生态集成更多的优秀工具和特性，满足更多的用户群体和场景。\n\n\n和 argo-workflow 的集成，可以通过 api，cli 等方式在 Apache DolphinScheduler 中调用 argo-workflow 单个作业，dag 作业，以及周期性作业；\n\n\n使用 hpa 的方式，自动扩缩容 worker，实现无人干预的水平扩展方式；\n\n\n集成 Kubernetes 的 spark operator 和 Hadoopoperator 工具，全面的云原生化；\n\n\n实现多云和多集群的分布式作业调度；\n\n\n采用 sidecar 实现定期删除 worker 作业日志；\n\n\n\n\n\n",
    "title": "Apache DolphinScheduler 在 Kubernetes 体系中的技术实战",
    "time": "2022-2-18"
  },
  {
    "name": "DolphinScheduler漏洞情况说明",
    "content": "【安全通报】【影响程度：低】DolphinScheduler 漏洞情况说明\nApache DolphinScheduler 社区邮件列表最近通告了 1 个漏洞，考虑到有很多用户并未订阅此邮 件列表，我们特地在此进行情况说明：\nCVE-2021-27644\n重要程度： 低\n影响范围： 暴露服务在外网中、且内部账号泄露。如果无上述情况，用户可根据实际情况决定是否需要升级。\n影响版本： &lt;1.3.6\n漏洞说明：\n此问题是由于 mysql connectorj 漏洞引起的，DolphinScheduler 登陆用户（未登录用户无法执行此操作，建议企业做好账号安全规范）可在数据源管理页面-Mysql 数据源填写恶意参数，导致安全隐患。（未使用 Mysql 数据源的不影响）\n修复建议： 升级到&gt;=1.3.6 版本\n特别感谢\n特别感谢漏洞报告者：来自蚂蚁安全非攻实验室的锦辰同学，他提供了漏洞的还原过程以及对应的解决方案。整个过程呈现了专业安全人员的技能和高素质，感谢他们为开源项目的安全守护所作出的贡献。\n建议\n十分感谢广大用户选择 Apache DolphinScheduler 作为企业的大数据任务调度系统，但必须要提醒的是调度系统属于大数据建设中核心基础设施，请不要将其暴露在外网中。此外应该对企业内部人员账号做好安全措施，降低账号泄露的风险。\n贡献\n迄今为止，Apache DolphinScheduler 社区已经有近 200+ 位代码贡献者，70+位非代码贡献者。其中也不乏其他 Apache 顶级项目的 PMC 或者 Committer，非常欢迎更多伙伴也能参与到开源社区建设中来，为建造一个更加稳定安全可靠的大数据任务调度系统而努力，同时也为中国开源崛起献上自己的一份力量！\nWebSite ：https://dolphinscheduler.apache.org/zh-cn\nMailList ：dev@dolphinscheduler@apache.org\nTwitter ：@DolphinSchedule\nYouTube ：https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\nSlack ：https://s.apache.org/dolphinscheduler-slack\nContributor Guide：https://dolphinscheduler.apache.org/zh-cn/community\n如果对漏洞有任何疑问，欢迎参与讨论，竭诚解决大家的疑虑：\n",
    "title": "DolphinScheduler漏洞情况说明",
    "time": "2021-10-26"
  },
  {
    "name": "Eavy_Info",
    "content": "亿云基于 DolphinScheduler 构建资产数据管理平台服务，助力政务信息化生态建设 | 最佳实践\n\n\n\n作者| 孙浩\n\n编 者 按：基于 Apache Dolphinscheduler 调度平台，云计算和大数据提供商亿云信息已经服务公司多个项目部的地市现场平稳运行一年之久。\n\n\n结合政务信息化生态建设业务，亿云信息基于 DolphinScheduler 构建了资产数据管控平台的数据服务模块。他们是如何进行探索和优化的？亿云信息研发工程师 孙浩 进行了详细的用户实践交流分享。\n\n01 研发背景\n亿云主要的业务主要是 ToG 的业务，而业务前置的主要工作，在于数据的采集和共享，传统 ETL 工具，例如 kettle 等工具对于一线的实施人员的来说上手难度还是有的，再就是类似 kettle 的工具本身做为独立的部分，本身就增加了现场项目运维的使用难度。因此，如何实现一套数据采集（同步）—数据处理—数据管理的平台，就显得尤为重要。\n出于这样的考虑，我们开发了数据资产管理平台，而管理平台的核心就是我们基于DolphinSchduler（简称 DS）实现的数据服务模块。\nDolphinScheduler 是一个分布式去中心化，易扩展的可视化 DAG 调度系统，支持包括 Shell、Python、Spark、Flink 等多种类型的 Task 任务，并具有很好的扩展性。其整体架构如下图所示：\n\n\n\n典型的 master-slave 架构，横向扩展能力强，调度引擎是 Quartz，本身作为 Spring Boot 的 java 开源项目，对于熟悉 Spring Boot 开发的人，集成使用更加的简单上手。\nDS 作为调度系统支持以下功能：\n调度方式：系统支持基于 cron 表达式的定时调度和手动调度。命令类型支持：启动工作流、从当前节点开始执行、恢复被容错的工作流、恢复暂停流程、从失败节点开始执行、补数、定时、重跑、暂停、停止、恢复等待线程。其中恢复被容错的工作流和恢复等待线程两种命令类型是由调度内部控制使用，外部无法调用。\n定时调度：系统采用 quartz 分布式调度器，并同时支持 cron 表达式可视化的生成。\n依赖：系统不单单支持 DAG 简单的前驱和后继节点之间的依赖，同时还提供任务依赖节点，支持流程间的自定义任务依赖。\n优先级 ：支持流程实例和任务实例的优先级，如果流程实例和任务实例的优先级不设置，则默认是先进先出。\n邮件告警：支持 SQL任务 查询结果邮件发送，流程实例运行结果邮件告警及容错告警通知。\n失败策略：对于并行运行的任务，如果有任务失败，提供两种失败策略处理方式，继续是指不管并行运行任务的状态，直到流程失败结束。结束是指一旦发现失败任务，则同时Kill掉正在运行的并行任务，流程失败结束。\n补数：补历史数据，支持区间并行和串行两种补数方式。\n我们基于 Dolphinscheduler 与小伙伴一起进行如下的实践。\n02 基于DS构建数据同步工具\n回归业务本身，我们的业务场景，数据同步的业务需要是类型多，但数据量基本不会特别大，对实时要求并不高。所以在架构选型之初，我们就选择了 datax+ds 的组合，并进行对应业务的改造实现，现在作为服务产品融合在各个项目中，提供离线同步服务。\n\n\n\n同步任务分为了周期任务和一次性任务，在配置完成输入输出源的配置任务之后，周期任务的话，需要配置 corn 表达式，然后调用保存接口，将同步任务发送给DS 的调度平台。\n\n\n\n我们这里综合考虑放弃了之前 DS 的 UI 前端（第二部分在自助开发模块会给大家解释），复用 DS 后端的上线、启停、删除、日志查看等接口。\n\n\n\n\n\n\n整个同步模块的设计思路，就是重复利用 datax 组件的输入输出 plugin 多样性，配合 DS 的优化，来实现一个离线的同步任务，这个是当前我们的同步的一个组件图，实时同步这块不再赘述。\n\n\n\n03 基于DS的自助开发实践\n熟悉 datax的人都知道它本质上是一个 ETL 工具，而其 Transform 的属性体现在，它提供了一个支持 grovy 语法的 transformer 模块，同时可以在 datax 源码中进一步丰富 transformer 中用到工具类，例如替换、正则匹配、筛选、脱敏、统计等功能。而 Dolphinscheduler 的任务，是可以用 DAG 图来实现，那么我们想到，是否存在一种可能，针对一张表或者几张表，把每个 datax 或者 SQL 抽象成一个数据治理的小模块，每个模块按照 DAG 图去设计，并且在上下游之间可以实现数据的传递，最好还是和 DS 一样的可以拖拽式的实现。于是，我们基于前期对 datax 与 ds 的使用，实现了一个自助开发的模块。\n\n\n\n每个组件可能是一个模块，每个模块功能之间的依赖关系，我们利用 ds 的depend 来处理，而对应组件与组件传递数据，我们利用前端去存储，也就是我们在引入 input（输入组件）之后，让前端来进行大部分组件间的传递和逻辑判断，因为每个组件都可以看作一个 datax 的(输出/输出)，所有参数在输入时，最终输出的全集基本就确定了，这也是我们放弃 DS 的 UI 前端的原因。之后，我们将这个 DAG 图组装成 DS 的定义的类型，同样交付给 ds 任务中心。\nPS：因为我们的业务场景可能存在跨数据库查询的情况（不同实例的 mysql 组合查询），我们的 SQL 组件底层使用 Presto 来实现一个统一 SQL 层，这样即使是不同 IP 实例下的数据源（业务上有关联意义），也可以通过 Presto 来支持组合检索。\n04 其他的一些简单尝试\n熟悉治理流程的人都知道，如果能够做到简单的治理流程化，那么必然可以产出一份质量报告。我们在自助开发的基础上进行优化，将部分治理的记录写入 ES 中，再利用 ES 的聚合能力来实现了一个质量报告。\n\n\n\n\n\n\n\n\n\n以上便是我们使用 DS 结合 datax 等中间件，并结合业务背景所做的一些符合自身需求的实践。\n05 感谢\n从 EasyScheduler 到现在的 DolphinScheduler 2.0，我们更多时候还是作为旁观者，或者是追随者，而这次更多地是从这一年来使用 Dolphinscheduler 构建我们数据资产管控平台数据服务模块的实践来进行交流分享。当前基于Dolphinscheduler 调度平台，我们已经服务了公司多个项目部的地市现场运行。随着 DolphinScheduler 2.0 发版，我们也和 DolphinScheduler 一起在不断进步的社区环境中共同成长。\n06 欢迎更多实践分享\n如果你也是 Apache DolphinScheduler 的用户或实践者，欢迎投稿或联系社区分享你们的实践经验，开源共享，互帮互助！\n",
    "title": "亿云基于 DolphinScheduler 构建资产数据管理平台服务，助力政务信息化生态建设",
    "time": "2021-12-30"
  },
  {
    "name": "Exploration_and_practice_of_Tujia_Big_Data_Platform_Based",
    "content": "途家大数据平台基于 Apache DolphinScheduler 的探索与实践\n\n\n\n\n途家在 2019 年引入 Apache DolphinScheduler，在不久前的 Apache DolphinScheduler 2 月份的 Meetup上，途家大数据工程师 昝绪超 详细介绍了途家接入 Apache DolphinScheduler 的历程，以及进行的功能改进。\n\n\n\n\n途家大数据工程师数据开发工程师，主要负责大数据平台的开发，维护和调优。\n本次演讲主要包括4个部分，第一部分是途家的平台的现状，介绍途家的数据的流转过程，如何提供数据服务，以及Apache DolphinScheduler在平台中扮演的角色。第二部分，调度选型，主要介绍调度的一些特性，以及接入的过程。第三部分主要介绍我们对系统的的一些改进和功能扩展，包括功能表依赖的支持，邮件任务扩展，以及数据同步的功能，第四部分是根据业务需求新增的一些功能，如Spark jar包支持发布系统，调度与数据质量打通，以及表血缘展示。\n途家数据平台现状\n01 数据架构\n首先介绍一下途家数据平台的架构以及Apache DolphinScheduler在数据平台扮演的角色。\n\n\n\n途家数据平台架构\n上图为我司数据平台的架构，主要包括数据源，数据采集，数据存储，数据管理，最后提供服务。\n数据源主要来源包括三个部分：业务库MySQL API数据同步，涉及到Dubbo接口、http 接口，以及web页面的埋点数据。\n数据采集采用实时和离线同步，业务数据是基于Canal的增量同步，日志是Flume，Kafka实时收集，落到HDFS上。\n数据存储过程，主要涉及到一些数据同步服务，数据落到HDFS 后经过清洗加工，推送到线上提供服务。\n数据管理层面，数据字典记录业务的元数据信息，模型的定义，各层级之间的映射关系，方便用户找到自己关心的数据；日志记录任务的运行日志，告警配置故障信息等。调度系统，作为大数据的一个指挥中枢，合理分配调度资源，可以更好地服务于业务。指标库记录了维度和属性，业务过程指标的规范定义，用于更好的管理和使用数据。Abtest记录不同指标和策略对产品功能的影响；数据质量是数据分析有效性和准确性的基础。\n最后是数据服务部分，主要包括数据的即席查询，报表预览，数据下载上传分析，线上业务数据支持发布等。\n02 Apache DolphinScheduler在平台的作用\n下面着重介绍调度系统在平台扮演的角色。数据隧道同步，每天凌晨定时拉去增量数据。数据清洗加工后推送到线上提供服务。数据的模型的加工，界面化的配置大大提高了开发的效率。定时报表的服务，推送邮件，支持附件，正文table 以及折线图的展示。报表推送功能，数据加工后，分析师会配置一些数据看板，每天DataX把计算好的数据推送到MySQL，做报表展示。\n接入DS\n第二部分介绍我们接入Apache DolphinScheduler做的一些工作。\nApache DolphinScheduler具有很多优势，作为大数据的一个指挥中枢，系统的可靠性毋庸置疑，Apache DolphinScheduler 去中心化的设计避免了单点故障问题，以及节点出现问题，任务会自动在其他节点重启，大大提高了系统的可靠性。\n此外，调度系统简单实用，减少了学习成本，提高工作效率，现在公司很多人都在用我们的调度系统，包括分析师、产品运营，开发。\n调度的扩展性也很重要，随着任务量的增加，集群能及时增添资源，提供服务。应用广泛也是我们选择它的一个重要原因，它支持丰富的任务类型：Shell、MR、Spark、SQL(MySQL、PostgreSQL、Hive、SparkSQL)，Python，Sub_Process，Procedure等，支持工作流定时调度、依赖调度、手动调度、手动暂停/停止/恢复，同时支持失败重试/告警、从指定节点恢复失败、Kill任务等操作等。它的优势很多，一时说不完，大家都用起来才知道。\n接下就是我们定时调度的升级。\n在采用 Apache DolphinScheduler之前，我们的调度比较混乱，有自己部署本地的Crontab，也有人用Oozie做调度，还有部分是在系统做定时调度。管理起来比较混乱，没有统一的管理调度平台，时效性，和准确性得不到保障，管理任务比较麻烦，找不到任务的情况时有发生。此外，自建调度稳定性不足，没有配置依赖，数据产出没有保障，而且产品功能单一，支持的任务调度有限。\n\n\n\n2019年，我们引入Apache DolphinScheduler ，到现在已经接近三年时间，使用起来非常顺手。\n下面是我们迁移系统的一些数据。\n我们搭建了DS集群 ，共4台实体机 ，目前单机并发支持100个任务调度。\n算法也有专门的机器，并做了资源隔离。\nOozie 任务居多 ，主要是一些Spark和Hive 任务 。还有Crontab 上的一些脚本，一些邮件任务以及报表系统的定时任务。\n基于DS的调度系统构建\n在此之前，我们也对系统做过优化，如支持表级别的依赖，邮件功能的扩展等。接入Apache DolphinScheduler后，我们在其基础之上进行了调度系统构建，以能更好地提供服务。\n**第一． 支持表依赖的同步，**当时考虑到任务迁移，会存在并行的情况，任务一时没法全部同步，需要表任务运行成功的标记，于是我们开发了一版功能，解决了任务迁移中的依赖问题。然而，每个人的命名风格不太一样，导致配置依赖的时候很难定位到表的任务，我们也无法识别任务里面包含哪些表，无法判断表所在的任务，这给我们使用造成不小的麻烦。\n\n\n\n\n\n\n第二．邮件任务支持多table。 调度里面自带邮件推送功能，但仅支持单个table ,随着业务要求越来越多，我们需要配置多个 table和多个sheet，要求正文和附件的展示的的条数不一样，需要配置，另外还需要支持折线图的功能，丰富正文页面。此外，用户还希望能在正文或者每个表格下面加注释，进行指标的说明等。我们使用Spark jar包实现了邮件推送功能，支持异常预警、表依赖缺失等。\n\n\n\n\n\n\n**第三．支持丰富的数据源同步。**由于在数据传输方面存在一些问题，在以前迁移的过程中，我们需要修改大量的配置代码，编译打包上传，过程繁琐，经常出现漏改，错该，导致线上故障，数据源不统一，测试数据和线上数据无法分开；在开发效率方面，代码有大量重复的地方，缺少统一的配置工具，参数配置不合理，导致MySQL压力大，存在宕机的风险；数据传输后，没有重复值校验，数据量较大的时候，全量更新，导致MySQL压力比较大。MySQL 传输存在单点故障问题，任务延迟影响线上服务。\n\n\n\n\n\n\n我们在此过程中简化了数据开发的流程，使得MySQL支持 pxc/mha的高可用，提升了数据同步的效率。\n我们支持输入的数据源支持关系型数据库，支持FTP同步，Spark作为计算引擎，输出的数据源支持各种关系型数据库，以及消息中间件Kafka、MQ和Redis。\n接下来讲一下我们实现的过程。\n我们对Apache DolphinScheduler 的数据源做了扩展，支持kafka mq和namespace 的扩展，MySQL 同步之前首先在本地计算一个增量，把增量数据同步到MySQL，Spark 也支持了MySQL pxc/qmha 的高可用。另外，在推送MQ和Redis时会有qps 的限制，我们根据数据量控制Spark的分区数和并发量。\n改进\n第四部分主要是对系统新增的一些功能，来完善系统。主要包含以下三点：\n\nSpark支持发布系统\n数据质量打通\n数据血缘的展示\n\n01Spark 任务支持发布系统\n由于我们平时的调度80%以上都是Spark jar 包任务，但任务的发布流程缺少规范，代码修改随意，没有完整的流程规范，各自维护一套代码。这就导致代码不一致的情况时有发生，严重时还会造成线上问题。\n这要求我们完善任务的发布流程。我们主要使用发布系统，Jenkens 打包功能，编译打包后生成btag，在测试完成后再发布生成rtag ，代码合并到master 。这就避免了代码不一致的问题，也减少了jar包上传的步骤。在编译生成jar 包后，系统会自动把jar包推送到Apache DolphinScheduler的资源中心，用户只需配置参数，选择jar包做测试发布即可。在运行Spark任务时，不再需要把文件拉到本地，而是直接读取HDFS上的jar包。\n02 数据质量打通\n数据质量是保障分析结论的有效性和准确性的基础。我们需要要完整的数据监控产出流程才能让数据更有说服力。质量平台从四个方面来保证数据准确性，完整性一致性和及时性，并支持电话、企业微信和邮件等多种报警方式来告知用户。\n接下来将介绍如何将数据质量和调度系统打通。调度任务运行完成后，发送消息记录，数据质量平台消费消息，触发数据质量的规则监控 根据监控规则来阻断下游运行或者是发送告警消息等。\n03 数据血缘关系展示\n数据血缘是元数据管理、数据治理、数据质量的重要一环，其可以追踪数据的来源、处理、出处，为数据价值评估提供依据，描述源数据流程、表、报表、即席查询之间的流向关系，表与表的依赖关系、表与离线ETL任务，调度平台、计算引擎之间的依赖关系。数据仓库是构建在Hive之上，而Hive的原始数据往往来自生产DB，也会把计算结果导出到外部存储，异构数据源的表之间是有血缘关系的。\n\n追踪数据溯源：当数据发生异常，帮助追踪到异常发生的原因；影响面分析，追踪数据的来源，追踪数据处理过程。\n评估数据价值：从数据受众、更新量级、更新频次等几个方面给数据价值的评估提供依据。\n生命周期：直观地得到数据整个生命周期，为数据治理提供依据。\n血缘的收集过程主要是 ：Spark 通过监控Spark API来监听SQL和插入的表，获取Spark的执行计划 ，并解析Spark 执行计划。\n\n",
    "title": "途家大数据平台基于 Apache DolphinScheduler 的探索与实践",
    "time": "2022-3-10"
  },
  {
    "name": "Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial",
    "content": "极速开发扩充 Apache DolphinScheduler Task 类型 | 实用教程\n\n\n\n背景简介\n目前在大数据生态中，调度系统是不可或缺的一个重要组件。Apache DolphinScheduler 作为一个顶级的 Apache 项目，其稳定性和易用性也可以说是名列前茅的。而对于一个调度系统来说，能够支持的可调度的任务类型同样是一个非常重要的因素，在调度、分布式、高可用、易用性解决了的情况下，随着业务的发展或者各种需求使用到的组件增多，用户自然而然会希望能够快速、方便、简洁地对 Apache Dolphinscheduler 可调度的任务类型进行扩充。本文便带大家了解如何方便、极速扩充一个 Apache DolphinScheduler Task。\n作者简介\n\n\n\n张柏强，大数据开发工程师，主要研究方向为实时计算、元数据治理、大数据基础组件。\n1 什么是 SPI 服务发现(What is SPI)？\nSPI 全称为 (Service Provider Interface) ，是 JDK 内置的一种服务提供发现机制。大多数人可能会很少用到它，因为它的定位主要是面向开发厂商的，在 java.util.ServiceLoader 的文档里有比较详细的介绍，其抽象的概念是指动态加载某个服务实现。\n2 为什么要引入 SPI(Why did we introduce SPI)?\n不同的企业可能会有自己的组件需要通过 task 去执行，大数据生态中最为常用数仓工具 Apache Hive 来举例，不同的企业使用 Hive 方法各有不同。有的企业通过 HiveServer2 执行任务，有的企业使用 HiveClient 执行任务，而 Apache DolphinScheduler 提供的开箱即用的 Task 中并没有支持 HiveClient 的 Task，所以大部分使用者都会通过 Shell 去执行。然而，Shell 哪有天然的TaskTemplate 好用呢？所以，Apache DolphinScheduler 为了使用户能够更好地根据企业需求定制不同的 Task，便支持了 TaskSPI 化。\n我们首先要了解一下 Apache DolphinScheduler 的 Task 改版历程，在 DS 1.3.x 时，扩充一个 Task 需要重新编译整个 Apache DolphinScheduler，耦合严重，所以在 Apache DolphinScheduler 2.0.x 引入了 SPI。前面我们提到了 SPI 的抽象概念是动态加载某个服务的实现，这里我们具象一点，将 Apache DolphinScheduler 的 Task 看成一个执行服务，而我们需要根据使用者的选择去执行不同的服务，如果没有的服务，则需要我们自己扩充，相比于 1.3.x 我们只需要完成我们的 Task 具体实现逻辑，然后遵守 SPI 的规则，编译成 Jar 并上传到指定目录，即可使用我们自己编写的 Task。\n3 谁在使用它(Who is using it)?\n1、Apache DolphinScheduler\n\n\ntask\n\n\ndatasource\n2、Apache Flink\n\n\nflink sql connector，用户实现了一个flink-connector后，Flink也是通过SPI来动态加载的\n3、Spring boot\n\n\nspring boot spi\n4、Jdbc\n\n\njdbc4。0以前， 开发人员还需要基于Class。forName(&quot;xxx&quot;)的方式来装载驱动，jdbc4也基于spi的机制来发现驱动提供商了，可以通过META-INF/services/java。sql。Driver文件里指定实现类的方式来暴露驱动提供者\n5、更多\n\n\ndubbo\n\n\ncommon-logging\n\n\n4 Apache DolphinScheduler SPI Process?\n\n\n\n剖析一下上面这张图，我给 Apache DolphinScheduler 分为逻辑 Task 以及物理 Task，逻辑 Task 指 DependTask，SwitchTask 这种逻辑上的 Task；物理 Task 是指 ShellTask，SQLTask 这种执行任务的 Task。而在 Apache DolphinScheduler中，我们一般扩充的都是物理 Task，而物理 Task 都是交由 Worker 去执行，所以我们要明白的是，当我们在有多台 Worker 的情况下，要将自定义的 Task 分发到每一台有 Worker 的机器上，当我们启动 Worker 服务时，worker 会去启动一个 ClassLoader 来加载相应的实现了规则的 Task lib，可以看到 HiveClient 和 SeatunnelTask 都是用户自定义的，但是只有 HiveTask 被 Apache DolphinScheduler TaskPluginManage 加载了，原因是 SeatunnelTask 并没有去遵守 SPI 的规则。SPI 的规则图上也有赘述，也可以参考 java.util.ServiceLoader 这个类，下面有一个简单的参考(摘出的一部分代码，具体可以自己去看看）\npublic final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt; {\n    //scanning dir prefix\n    private static final String PREFIX = &quot;META-INF/services/&quot;;\n\n    //The class or interface representing the service being loaded\n    private final Class&lt;S&gt; service;\n\n    //The class loader used to locate, load, and instantiate providers\n    private final ClassLoader loader;\n\n    //Private inner class implementing fully-lazy provider lookup\n    private class LazyIterator implements Iterator&lt;S&gt; {\n        Class&lt;S&gt; service;\n        ClassLoader loader;\n        Enumeration&lt;URL&gt; configs = null;\n        String nextName = null;\n\n        //......\n        private boolean hasNextService() {\n            if (configs == null) {\n                try {\n                    //get dir all class\n                    String fullName = PREFIX + service.getName();\n                    if (loader == null)\n                        configs = ClassLoader.getSystemResources(fullName);\n                    else\n                        configs = loader.getResources(fullName);\n                } catch (IOException x) {\n                    //......\n                }\n                //......\n            }\n        }\n    }\n}\n\n\n5 如何扩展一个 data sourceTask or DataSource (How to extend a task or datasource)?\n5.1 创建 Maven 项目\n\nmvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.dolphinscheduler \\\n    -DarchetypeArtifactId=dolphinscheduler-hive-client-task \\\n    -DarchetypeVersion=1.10.0 \\\n    -DgroupId=org.apache.dolphinscheduler \\\n    -DartifactId=dolphinscheduler-hive-client-task \\\n    -Dversion=0.1 \\\n    -Dpackage=org.apache.dolphinscheduler \\\n    -DinteractiveMode=false \n    \n    \n\n5.2 Maven 依赖\n\n &lt;!--dolphinscheduler spi basic core denpendence--&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-spi&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version&gt;\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-task-api&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version&gt;\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency&gt;\n    \n    \n\n5.3 创建 Task 通道工厂(TaskChannelFactory)\n首先我们需要创建任务服务的工厂，其主要作用是帮助构建 TaskChannel 以及 TaskPlugin 参数，同时给出该任务的唯一标识，ChannelFactory 在 Apache DolphinScheduler 的 Task 服务组中，其作用属于是在任务组中的承上启下，交互前后端以及帮助 Worker 构建 TaskChannel。\npackage org.apache.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.spi.params.base.PluginParams;\nimport org.apache.dolphinscheduler.spi.task.TaskChannel;\nimport org.apache.dolphinscheduler.spi.task.TaskChannelFactory;\n\nimport java.util.List;\n\npublic class HiveClientTaskChannelFactory implements TaskChannelFactory {\n    /**\n     *  创建任务通道,基于该通道执行任务\n     * @return 任务通道\n     */\n    @Override\n    public TaskChannel create() {\n        return new HiveClientTaskChannel();\n    }\n\n    /**\n     *  返回当前任务的全局唯一标识\n     * @return 任务类型名称\n     */\n    @Override\n    public String getName() {\n        return &quot;HIVE CLIENT&quot;;\n    }\n\n    /**\n     * 前端页面需要用到的渲染,主要分为\n     \n     * @return\n     */\n    @Override\n    public List&lt;PluginParams&gt; getParams() {\n        List&lt;PluginParams&gt; pluginParams = new ArrayList&lt;&gt;();\n        InputParam nodeName = InputParam.newBuilder(&quot;name&quot;, &quot;$t('Node name')&quot;)\n                .addValidate(Validate.newBuilder()\n                        .setRequired(true)\n                        .build())\n                .build();\n        PluginParams runFlag = RadioParam.newBuilder(&quot;runFlag&quot;, &quot;RUN_FLAG&quot;)\n                .addParamsOptions(new ParamsOptions(&quot;NORMAL&quot;, &quot;NORMAL&quot;, false))\n                .addParamsOptions(new ParamsOptions(&quot;FORBIDDEN&quot;, &quot;FORBIDDEN&quot;, false))\n                .build();\n\n        PluginParams build = CheckboxParam.newBuilder(&quot;Hive SQL&quot;, &quot;Test HiveSQL&quot;)\n                .setDisplay(true)\n                .setValue(&quot;-- author: \\n --desc:&quot;)\n                .build();\n\n        pluginParams.add(nodeName);\n        pluginParams.add(runFlag);\n        pluginParams.add(build);\n\n        return pluginParams;\n    }\n}\n\n5.4 创建 TaskChannel\n有了工厂之后，我们会根据工厂创建出 TaskChannel，TaskChannel 包含如下两个方法，一个是取消，一个是创建，目前不需要关注取消，主要关注创建任务。\n    void cancelApplication(boolean status);\n\n    /**\n     * 构建可执行任务\n     */\n    AbstractTask createTask(TaskRequest taskRequest);\n    public class HiveClientTaskChannel implements TaskChannel {\n    @Override\n    public void cancelApplication(boolean b) {\n        //do nothing\n    }\n\n    @Override\n    public AbstractTask createTask(TaskRequest taskRequest) {\n        return new HiveClientTask(taskRequest);\n    }\n}\n\n\n5.5 构建 Task 实现\n通过 TaskChannel 我们得到了可执行的物理 Task，但是我们需要给当前 Task 添加相应的实现，才能够让Apache DolphinScheduler 去执行你的任务，首先在编写 Task 之前我们需要先了解一下 Task 之间的关系：\n\n\n\n通过上图我们可以看到，基于 Yarn 执行任务的 Task 都会去继承 AbstractYarnTask，不需要经过 Yarn 执行的都会去直接继承 AbstractTaskExecutor，主要是包含一个 AppID，以及 CanalApplication setMainJar 之类的方法，想知道的小伙伴可以自己去深入研究一下，如上可知我们实现的 HiveClient 就需要继承 AbstractYarnTask，在构建 Task 之前，我们需要构建一下适配 HiveClient 的 Parameters 对象用来反序列化JsonParam。\n  package com.jegger.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.ResourceInfo;\n\nimport java.util.List;\n\npublic class HiveClientParameters extends AbstractParameters {\n    /**\n     * 用HiveClient执行,最简单的方式就是将所有SQL全部贴进去即可,所以我们只需要一个SQL参数\n     */\n    private String sql;\n\n    public String getSql() {\n        return sql;\n    }\n\n    public void setSql(String sql) {\n        this.sql = sql;\n    }\n\n    @Override\n    public boolean checkParameters() {\n        return sql != null;\n    }\n\n    @Override\n    public List&lt;ResourceInfo&gt; getResourceFilesList() {\n        return null;\n    }\n}\n\n\n实现了 Parameters 对象之后，我们具体实现 Task，例子中的实现比较简单，就是将用户的参数写入到文件中，通过 Hive -f 去执行任务。\n package org.apache.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.request.TaskRequest;\nimport org.apache.dolphinscheduler.spi.utils.JSONUtils;\n\nimport java.io.BufferedWriter;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\n\n\npublic class HiveClientTask extends AbstractYarnTask {\n\n    /**\n     * hive client parameters\n     */\n    private HiveClientParameters hiveClientParameters;\n\n    /**\n     * taskExecutionContext\n     */\n    private final TaskRequest taskExecutionContext;\n\n\n\n    public HiveClientTask(TaskRequest taskRequest) {\n        super(taskRequest);\n        this.taskExecutionContext = taskRequest;\n    }\n\n    /**\n     * task init method\n     */\n    @Override\n    public void init() {\n        logger.info(&quot;hive client task param is {}&quot;, JSONUtils.toJsonString(taskExecutionContext));\n        this.hiveClientParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), HiveClientParameters.class);\n\n        if (this.hiveClientParameters != null &amp;&amp; !hiveClientParameters.checkParameters()) {\n            throw new RuntimeException(&quot;hive client task params is not valid&quot;);\n        }\n    }\n\n    /**\n     * build task execution command\n     *\n     * @return task execution command or null\n     */\n    @Override\n    protected String buildCommand() {\n        String filePath = getFilePath();\n        if (writeExecutionContentToFile(filePath)) {\n            return &quot;hive -f &quot; + filePath;\n        }\n        return null;\n    }\n\n    /**\n     * get hive sql write path\n     *\n     * @return file write path\n     */\n    private String getFilePath() {\n        return String.format(&quot;%s/hive-%s-%s.sql&quot;, this.taskExecutionContext.getExecutePath(), this.taskExecutionContext.getTaskName(), this.taskExecutionContext.getTaskInstanceId());\n    }\n\n    @Override\n    protected void setMainJarName() {\n        //do nothing\n    }\n\n    /**\n     * write hive sql to filepath\n     *\n     * @param filePath file path\n     * @return write success?\n     */\n    private boolean writeExecutionContentToFile(String filePath) {\n        Path path = Paths.get(filePath);\n        try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {\n            writer.write(this.hiveClientParameters.getSql());\n            logger.info(&quot;file:&quot; + filePath + &quot;write success.&quot;);\n            return true;\n        } catch (IOException e) {\n            logger.error(&quot;file:&quot; + filePath + &quot;write failed.please path auth.&quot;);\n            e.printStackTrace();\n            return false;\n        }\n\n    }\n\n    @Override\n    public AbstractParameters getParameters() {\n        return this.hiveClientParameters;\n    }\n}\n\n\n5.6 遵守 SPI 规则\n # 1,Resource下创建META-INF/services文件夹,创建接口全类名相同的文件\nzhang@xiaozhang resources % tree ./\n./\n└── META-INF\n    └── services\n        └── org.apache.dolphinscheduler.spi.task.TaskChannelFactory\n# 2,在文件中写入实现类的全限定类名\nzhang@xiaozhang resources % more META-INF/services/org.apache.dolphinscheduler.spi.task.TaskChannelFactory \norg.apache.dolphinscheduler.plugin.task.hive.HiveClientTaskChannelFactory\n\n\n5.7 打包和部署\n## 1,打包\nmvn clean install\n## 2,部署\ncp ./target/dolphinscheduler-task-hiveclient-1.0.jar $DOLPHINSCHEDULER_HOME/lib/\n## 3,restart dolphinscheduler server\n\n\n以上操作完成后，我们查看 worker 日志 tail -200f $Apache DolphinScheduler_HOME/log/Apache DolphinScheduler-worker.log\n\n\n\nApache DolphinScheduler 的插件开发就到此完成~涉及到前端的修改可以参考：\nApache DolphinScheduler-ui/src/js/conf/home/pages/dag/_source/formModel/\n",
    "title": "极速开发扩充 Apache DolphinScheduler Task 类型 | 实用教程",
    "time": "2022-4-14"
  },
  {
    "name": "Hangzhou_cisco",
    "content": "杭州思科对 Apache DolphinScheduler Alert 模块的改造\n\n\n\n杭州思科已经将 Apache DolphinScheduler 引入公司自建的大数据平台。目前，杭州思科大数据工程师   李庆旺  负责 Alert 模块的改造已基本完成，以更完善的 Alert 模块适应实际业务中对复杂告警的需求。\n\n\n\n李庆旺\n杭州思科 大数据工程师，主要负责 Spark、调度系统等大数据方向开发。\n我们在使用原有的调度平台处理大数据任务时，在操作上多有不便。比如一个对数据进行处理聚合分析的任务，首先由多个前置 Spark 任务对不同数据源数据进行处理、分析。最后的 Spark 任务对这期间处理的结果进行再次聚合、分析，得到我们想要的最终数据。但遗憾的是当时的调度平台无法串行执行多个任务，需要估算任务处理时间来设置多个任务的开始执行时间。同时其中一个任务执行失败，需要手动停止后续任务。这种方式既不方便，也不优雅。\n而 Apache DolphinScheduler 的核心功能——工作流定义可以将任务串联起来，完美契合我们的需求。于是，我们将 Apache DolphinScheduler 引入自己的大数据平台，而我主要负责 Alert 模块改造。目前我们其他同事也在推进集成 K8s，希望未来任务在 K8s 中执行。\n今天分享的是 Alert 模块的改造。\n01 Alert 模块的设计\n\n\n\nDolphinScheduler Alert 模块的设计\nApache DolphinScheduler 1.0 版本的 Alert 模式使用配置 alert.properties 的方式，通过配置邮箱、短信等实现告警，但这样的方式已经不适用于当前的场景了。官方也进行过告警模块重构，详情设计思路参考官方文档：\nhttps://github.com/apache/dolphinscheduler/issues/3049\nhttps://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/contribute/backend/spi/alert.md\nApache DolphinScheduler 告警模块是一个独立启动的服务，核心之一是 AlertPluginManager 类。告警模块集成了很多插件，如钉钉、微信、飞书、邮件等，以独立的形式写在源码中，启动服务时会解析插件并将配置的参数格式化成 JSON 形式，前端通过 JSON 自动渲染出页面。AlertPluginManager 在启动时会缓存插件到内存中。AlertServer 类会启动线程池，定时扫描 DB。\n当工作流配置了通知策略，同时 Worker 执行工作流结束，执行结果匹配通知策略成功后，DB 插入告警数据后，线程池扫描 DB，调用 AlertSender 类的 send 方法传入告警数据。告警数据绑定的是告警组，一个告警组对应了多个告警实例。AlertSender 类遍历告警实例，通过 AlertPluginManager 类获取插件实例，调用实例的发送方法，最后更新结果。这是 Apache DolphinScheduler 的整个告警流程。\n需要注意的是，Alert server 启动的同时也启动了 RPC 服务，这是一种针对特殊类型任务，如 SQL 查询报表而设计的告警方式，可以让 Worker 通过 RPC 直接访问 Alert  Server，利用 Alert 模块完成告警，这个数据不写入 DB。但从整体上来说，Apache DolphinScheduler 的告警模式还是以写 DB，异步交互的方式为主。\n\n\n\n定义工作流之后，可以在启动前设置通知策略，绑定告警组。\n\n\n\n在任务维度，可以配置超时告警，当任务超时可以触发报警。这里没有告警组配置，任务和工作流共用一个告警组，当任务超时，会推送到工作流设置的告警组。\n\n\n\n上图为系统告警配置的流程图。可以看到，一个工作流可以配置多个任务实例，任务可以配置超时触发告警，工作流成功或者失败可以触发告警。一个告警组可以绑定多个告警实例。这样的配置不太合理，我们希望告警实例也可以匹配工作流/任务实例的状态，也就是工作流成功和失败调用同一个告警组，但是触发不同的告警实例。这样使用起来更符合真实场景。\n\n\n\n创建告警组，一个告警组可以绑定多个告警实例。\n02 大数据任务告警场景\n\n\n\n以下是我们日常工作中的一些 常见的大数据任务告警场景。\n对于定时任务，在开始执行前、任务上线、下线或修改参数，以及任务执行成功或失败时都发送通知。区别是，对于同一任务不同结果，我们希望触发不同的通知，比如成功发短信通知或者钉钉微信群通知即可，而任务失败了需要在第一时间通知对应的研发人员，以得到更快的响应，这时候钉钉微信群中@对应研发人员或者电话通知会更及时。目前，公司的任务调度平台是任务中调用 API 进行通知，这种与代码强耦合的方式极其不方便，实际上可以抽象成一个更为通用的模块来实现。\nApache DolphinScheduler 的架构虽然符合实际场景需求，但问题在于告警模块页面配置只能选择成功触发通知，或失败触发通知，绑定的是同一个告警组，即无论成功还是失败，告警的途径是相同的，这一点并不满足我们在实际生产环境中需要不同结果以不同方式通知的需求。因此，我们对 Alert 模块进行了一些改造。\n03 Alert 模块的改造\n\n\n\n改造的第一步是告警实例。此前，新增一个告警实例，触发告警就会触发该实例的 send 方法，我们希望在定义告警实例时可以绑定一个告警策略，有三个选项，成功发、失败发，以及成功和失败都发。\n在任务定义维度，有一个超时告警的功能，实际上对应失败的策略。\n\n\n\n上图为改造完成的配置页面，在创建告警实例页面，我们添加了一个告警类型字段，选择是在成功、失败，或者无论成功或失败时调用插件。\n\n\n\n上图为改造后 Apache DolphinScheduler 告警模块的架构，我们对其中进行了两点改造。\n其一，在执行完工作流或任务时，如果触发告警，在写入 DB 时，会保存本次工作流或者任务的执行结果，具体是成功还是失败。\n第二，调用告警实例发送方法添加了一个逻辑判断，将告警实例与任务状态进行匹配，匹配则执行该告警实例发送逻辑，不匹配则过滤。\n改造后告警模块支持场景如下：\n\n\n\n详细设计请参考 issue：https://github.com/apache/dolphinscheduler/issues/7992\n代码详见：https://github.com/apache/dolphinscheduler/pull/8636\n此外，我们还针对 Apache DolphinScheduler 的告警模块向社区提出几点优化的建议，感兴趣的小伙伴可以跟进 issue，一起来做后续的工作：\n\n工作流启动或上下线或参数修改时，可以触发通知；\n告警场景针对 worker 的监控，如果 worker 挂掉或和 ZK 断开失去心跳，会认为 worker 宕机，会触发告警，但会默认匹配 ID 为 1 的告警组。这样的设置是在源码中写明的，但不看源码不知道其中的逻辑，不会专门设置 ID 为 1 的告警组，无法第一时间得到 worker 宕机的通知；\n告警模块目前支持飞书、钉钉、微信、邮件等多种插件，这些插件适用于国内用户，但国外用户可能使用不同的插件，如思科使用的 Webex Teams，国外常用告警插件 PagerDuty，我们也都进行开发并贡献给了社区。同时还有一些比较常用的比如 Microsoft Teams 等，感兴趣的小伙伴也可以提个 PR，贡献到社区。\n最后一点，可能大数据领域的小伙伴对于前端不太熟悉，想要开发并贡献告警插件，但是想到需要开发前端就不想进行下去了。开发 Apache DolphinScheduler 告警插件是不需要写前端代码的，只需要在新建告警实例插件时，在 Java 代码中配置好页面中需要输入的参数或者需要选择的按钮（源码详见 org.apache.dolphinscheduler.spi.params），系统会自动格式化成 JSON 格式，前端通过 form-create 可以通过 JSON 自动渲染成页面。因此，完全不用担心写前端的问题。\n\n",
    "title": "杭州思科对 Apache DolphinScheduler Alert 模块的改造",
    "time": "2022-3-16"
  },
  {
    "name": "How_Does_360_DIGITECH_process_10_000+_workflow_instances_per_day",
    "content": "日均处理 10000+ 工作流实例，Apache DolphinScheduler 在 360 数科的实践\n\n\n\n\n从 2020 年起，360 数科全面将调度系统从 Azkaban 迁移到 Apache DolphinScheduler。作为 DolphinScheduler 的资深用户，360 数科如今每天使用 DolphinScheduler 日均处理 10000+ 工作流实例。为了满足大数据平台和算法模型业务的实际需求，360 数科在 DolphinScheduler 上进行了告警监控扩展、worker增加维护模式、多机房等改造，以更加方便运维。他们具体是如何进行二次开发的呢？360 数科大数据工程师 刘建敏 在不久前的 Apache DolphinScheduler 2 月份的 Meetup 上进行了详细的分享。\n\n\n\n\n刘建敏\n360 数科 大数据工程师，主要从事 ETL 任务与调度框架的研究，大数据平台以及实时计算平台的开发。\n从 Azkaban 迁移到 DolphinScheduler\n2019 年之前，360 数科采用 Azkaban 调度进行大数据处理。\nAzkaban 是由 Linkedin 开源的批量工作流任务调度器。其安装简单，只需安装 web 与 executor server，就可以创建任务，并通过上传 zip 包实现工作流调度。\nAzkaban 的 web-executor 架构如下图所示：\n\n\n\nAzkaban 的缺点\nAzkaban 适用于场景简单的调度，经过三年时间的使用，我们在发现它存在三个重要缺陷：\n\n\n体验性差\n没有可视化创建任务功能，创建与修改任务都需要通过上传 zip 包来实现，这并不方便；另外，Azkaban 没有管理资源文件功能。\n\n\n功能不够强大\nAzkaban 缺乏一些生产环境中不可或缺的功能，比如补数、跨任务依赖功能；用户与权限管理太弱，调度配置不支持按月，在生产上我们需要用很多其他方式进行弥补。\n\n\n稳定性不够好\n最重要的一点是 Azkaban 稳定性不够，当 executor 负载过高时，任务经常会出现积压；小时级别或分钟级别的任务容易出现漏调度；没有超时告警，虽然我们自己开发了有限的短信告警，但还是容易出现生产事故。\n\n\n\n\n\n针对这些缺陷，我们在 2018 曾进行过一次改造，但由于 Azkaban 源码复杂，改造的过程很是痛苦，因此我们决定重新选型。当时，我们测试了 Airflow、DolphinScheduler 和 XXL-job，但 Airflow Python 的技术栈与我们不符，而 XXL-job 功能又过于简单，显然，DolphinScheduler 是更好的选择。\n2019 年，我们 Folk 了 EasyScheduler 1.0 的代码，在 2020 年进行改造与调度任务的部分迁移，并上线运行至现在。\nDolphinScheduler 选型调研\n为什么我们选择 DolphinScheduler？因为其有四点优势：\n\n去中心化结构，多 Master 多 Worker；\n调度框架功能强大，支持多种任务类型，具备跨项目依赖、补数功能；\n用户体验性好，可视化编辑 DAG 工作流，操作方便；\nJava 技术栈，扩展性好。\n\n\n改造过程非常流畅，我们顺利地将调度系统迁移到了 DolphinScheduler。\nDolphinScheduler 的使用\n在 360 数科，DolphinScheduler 不仅用于大数据部门，算法部门也在使用其部分功能。为了让算法模型部们更方便地使用 DolphinScheduler 的功能，我们将其整合进了我们自己的毓数大数据平台。\n毓数大数据平台\n\n\n\n毓数是一个由基础组件、毓数平台、监控运维和业务支撑层组成的大数据平台，可实现查询、数据实时计算、消息队列、实时数仓、数据同步、数据治理等功能。其中，离线调度部分便是通过 DolphinScheduler 调度数据源到 Hive 数仓的 ETL 任务，以及支持 TiDB 实时监控，以实时数据报表等功能。\nDolphinScheduler 嵌套到毓数\n为了支持公司算法建模的需求，我们抽取了常用的一些节点，并嵌套了一层 UI，并调用 API。\n\n\n\n算法部门多用 Python 脚本和 SQL 节点，用框表逻辑进行定时，再配置机器学习算法进行训练，组装数据后用 Python 调用模型生成模型分。我们封装了一些 Kafka 节点，通过 Spark 读取 Hive 数据并推送到 Kafka。\n任务类型\n\n\n\nDolphinScheduler 支持的任务类型有 Shell、SQL、Python 和 Jar。其中，Shell 支持 Sqoop DataX mr 同步任务和 Hive-SQL、Spark-SQL；SQL 节点主要是支持 TiDB SQL（处理上游分库分表的监控） 和 Hive SQL；Python 任务类型支持离线调用模型脚本等；Jar 包主要支持 Spark Jar 离线管理。\n任务场景\n\n\n\nDolphinScheduler 的任务场景主要是将各种数据源，如 MySQL、Hbase 等数据源调度同步至 Hive，再通过 ETL 工作流直接生成 DW。通过 Python 脚本组装或调用，生成模型和规则结果，再推送到 Kafka。Kafka 会给出风控系统调额、审批和分析，并将结果反馈给业务系统。这就是 DolphinScheduler 调度流程的一个完整工作流示例。\nDolphinScheduler 的运维\n目前 ，DolphinScheduler 日均处理工作流已达到 10000+ 的规模，很多离线报表依赖于 DolphinScheduler，因此运维非常重要。\n\n在 360 数科，对 DolphinScheduler 的运维主要分为三部分：\n\nDS 依赖组件运维\n\n\n\n\nDolphinScheduler 依赖组件的运维主要是针对 MySQL 监控和 Zookeeper 监控。\n因为工作流定义元信息、工作流实例和任务实例、Quartz 调度、Command 都依赖 MySQL，因此 MySQL 监控至关重要。曾经有一次机房网络中断，导致很多工作流实例出现漏调度，之后通过 MySQL 监控才得以排查。\nZookeeper 监控的重要性也不言而喻，Master worker 状态和 task 队列都依赖 Zookeeper，但运行至今，Zookeeper 都比较稳定，还未有问题发生。\n\nMaster 与 Worker 状态监控\n我们都知道，Master 负责的是任务切分，实际上对业务的影响并不是很大，因此我们采用邮件监控即可；但 Worker 挂掉会导致任务延迟，增加集群压力。另外，由于之前 Yarn 运行任务不一定被成功地 kill，任务容错还可能导致 Yarn 任务重复运行，因此我们对  Worker 状态采用电话告警。\n\nGrafana 大盘监控工作流实例状态\n\n\n\n此外，我们还为运维创建了 Grafana 监控看板，可以实时监控工作流实例的状态，包括工作流实例的数量，各项目工作流实例运行状态，以及超时预警设置等。\nDolphinScheduler 改造\n体验性改造\n\n增加个人可以授权资源文件与项目，资源文件区分编辑与可读权限，方便授权；\n扩展全局变量(流程定义id, 任务id等放到全局变量中),提交到yarn上任务可追踪到调度的任务，方便集群管理，统计工作流实例锁好资源，利于维护\n工作流复制、任务实例查询接口优化，提高查询速度，以及 UI 优化。\n\n增加短信告警改造\n因为原有的邮箱告警不足以保证重要任务的监控，为此我们在 UI 上增加了 SMS 告警方式，保存工作流定义。另外，我们还把告警 receivers 改造成用户名，通过 AlertType 扩展成短信、邮箱等告警方式关联到用户信息，如手机号等，保证重要性任务失败后可以及时收到告警。\nWorker 增加维护模式改造\n此外，当 worker 机器需要维护时，需要这台 worker 上不会有新的运行任务提交过来。为此，我们对 worker 增加了维护模式改造，主要包括 4 点：\n\nUI 设置 worker 进行维护模式，调用 API 写入worker 指定的 zk 路径；\nWorkerServer 进行定时调度轮询是否维护模式；\nWorkerServer 进行维护模式，FetchTaskThread 不获取新增任务；\nworker 任务运行结束，可进行重启。\n经过以上改造，我们运维的压力大大减轻。\n\n多机房改造\n最后，我们还进行了多机房改造。因为我们之前有 2 套集群，分布在不同的机房，我们的改造目标是在调度中设置多机房，这样当某一个机房出现故障后，可一键切换到其他机房使用，实现多机房使用同一套调度框架。\n\n\n\n改造的切入点就是在调度中设置多机房，以保证某一任务可以在多机房启动。改造流程如下：\n\n在各机房分别部署 ZK，Master 与 Worker 注册到对应的机房，Master 负责任务切分，Worker 负责任务处理，各自处理各机房的任务；\nschedule 与 command 带上 datacenter 信息\n为保证双机房任务切换，资源文件进行上机房任务上传，同时改动任务接口，任务依赖、Master容错都需要改造过滤对应机房。\n\n",
    "title": "日均处理 10000+ 工作流实例，Apache DolphinScheduler 在 360 数科的实践",
    "time": "2022-3-15"
  },
  {
    "name": "How_Does_Live-broadcasting_Platform_Adapt_to_Apache_DolphinScheduler",
    "content": "论语音社交视频直播平台与 Apache DolphinScheduler 的适配度有多高\n\n\n\n在 Apache DolphinScheduler Meetup 上，YY 直播 软件工程师 袁丙泽为我们分享了《YY直播基于Apache DolphinScheduler的适配与探索》。\n本次演讲主要包括四个部分：\n\nYY直播引入Apache DolphinScheduler的背景\nApache DolphinScheduler的引入过程\nApache DolphinScheduler应用的适配\nYY直播未来的规划\n\n\n\n\n袁丙泽YY直播 软件工程师，10 余年工作经验，主要从事风控大数据平台开发工作，对常用大数据组件深感兴趣，研发经验丰富。\n背景\nYY直播是中国领先的语音社交视频直播企业，目前我们团队的主要职责是保障公司的业务安全。\n01技术现状\n目前我们采用分层的技术架构，最底层是数据源层，其次从下往上依次是采集层、存储层和管理层和计算层与应用层。\n\n在数据源层，我们目前会去拉取各个业务方的一个关系型数据库数据，以及通过API向我们传输的数据，还有一些数据是通过Kafka这种流的方式来传输给我们。\n\n采集层采用了我们自己研发的一套数据采集系统。\n\n存储层中，我们目前将数据主要放在了关系型数据库中，如Clickhouse，还有一小部分会放在一些非关系型数据库中，如Redis和图库。当然大部分数据都存储在大数据系统中。\n\n管理层我们主要有大数据管理系统，结合自己研发的一个计算调度以及任务管理系统和服务治理平台。\n02调度Apache DolphinScheduler之前的问题\n1、调度平台复杂：团队除了有基于Xxl-job的任务调度外，部分老项目中有使用Crontab、Springboot、Scheduler、Quartz等管理任务的启动。\n2、任务依赖需求强烈：目前我们所使用的的调度，仅能设置单个任务的执行，无法通过任务依赖形成工作流，任务依赖设置严重依赖于个人经验设定定时时间。实际上很多任务都需要有依赖关系。\n3、任务复杂多样：目前任务有基于大数据系统的Spark、Flink任务，服务治理平台中各种Java服务任务、Shell、Java application、Python等。\n引入过程\n在需求调研中，我们实际上需要一款调度平台，需要满足如下条件：\n1、统一管理任务及依赖关系\n随着业务计算的需求越来越多，特别是各种各样的画像计算和任务，这些任务分散在各个系统当中，管理起来非常困难，部分任务之间有一定的依赖关系，但配置其时间依靠的是个人经验。急需一款能够统一配置管理依赖的产品。\n2、兼容公司内部各平台系统\n我们需要调度任务平台管理我们的任务，同时为了快速投入使用，调度平台需要兼容我们公司其他的平台系统，如内部的Datax和Crontab服务。\n3、高可用、高性能、高并发，容易使用\n最后为了保证业务的稳定性，我们也需要这种调度平台能够高可用、高性能、高并发，并且容易使用。\n\n通过调研我们发现，Apache DolphinScheduler几乎就是为我们设计的，适配过程中无需太多修改，就能满足我们需求。\n应用适配\nApache DolphinScheduler 是一个分布式去中心化，易扩展的可视化DAG工作流任务调度系统，致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用，这非常符合我们的需求。\n首先了解下Apache DolphinScheduler的架构，便于理解接下来的适配案例。\n\n\n\nApache DolphinScheduler主要有API、master、 worker、 log以及 alert这5个模块。\nAPI接口层，主要负责处理前端UI层的请求。该服务统一提供RESTful api向外部提供请求服务。接口包括工作流的创建、定义、查询、修改、发布、下线、手工启动、停止、暂停、恢复、从该节点开始执行等等。\n\nMasterServer采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交监控，并同时监听其它MasterServer和WorkerServer的健康状态。MasterServer服务启动时向Zookeeper注册临时节点，通过监听Zookeeper临时节点变化来进行容错处理。\nWorkerServer也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务。WorkerServer服务启动时向Zookeeper注册临时节点，并维持心跳。workServer还提供有logger服务。\n\nAlert提供告警相关接口，接口主要包括两种类型的告警数据的存储、查询和通知功能。其中通知功能又有邮件通知和**SNMP(暂未实现)**两种。\n\n目前我们部署的是2.0版本，主要使用了4台物理机，在这4台物理机上部署了2个master实例，2个API实例和3个worker与logger实例，一个alert实例。\n\n接下来分享3个具体的适配案例。\n\n首先是与我们服务治理平台的适配，该适配主要目的是用于任务监控；尽管Apache DolphinScheduler本身提供有任务监控模块，我们同事早已经习惯利用服务治理平台统一管理监控。所以我们需要把Apache DolphinScheduler任务运行状态及时上报至服务治理平台。\n01服务治理适配—MasterServer服务说明\n在适配之前，再次详细了解下MasterServer服务，MasterServer提供有：\n\nDistributed Quartz分布式调度组件，主要负责定时任务的启停操作，当quartz挑起任务后，Master内部会有线程池具体负责处理任务的后续操作；\nMasterSchedulerThread是一个扫描线程，定时扫描数据库中的command表，根据不同的命令类型进行不同的业务操作；\nMasterExecThread（WorkflowExecutThread.java）主要负责DAG任务切分、任务提交监控、各种不同命令类型的逻辑处理；\nMasterTaskExecThread主要负责任务的持久化。\n\n02服务治理适配-code\n我们的需求是监控任务，通过代码分析，我们发现任务提交与监听主要在WorkflowExecuteThread类中的方法中实现，该类会启动多个实例线程。分别负责任务执行与监听。其流程图如下：\n\n\n\n任务提交及监控流程图\n我们的需求是监控任务，通过分析代码后发现，WorkflowExecuteThread主要有startprocess和handle events两个方法分别实现了任务执行与监听。其实我们主要在handleEvents方法中注入我们的服务治理平台数据收集代码，这样就能把任务监听情况及时上报到我们服务治理平台了。\n\n其修改部分如下：\n\n\n\n在服务治理平台中具体的效果图如下：\n\n\n\n除了监控我们的具体任务状况外，我们还会分 project去做一些监控，最后都通过服务治理平台来做监控操作，比如像一些任务如果比较重要，我们就会配置一些电话报警，即一旦这个任务失败或者未按时执行完毕，便会进行电话通知。\n03Datax服务适配过程\n第2个案例是关于Datax服务的适配过程。我们在研究Apache DolphinScheduler的时候，发现其已经集成了Datax类型的任务，这个对我们非常友好。因为我们也有数量相当多的任务是通过Datax来实现的，并且我们也开发了一部分Datax的插件，来去适配内部各个系统与存储的数据读写。\nDatax适配的时候主要分为两部分，一部分是通过这种自定义模板来去实现，这部分其实就是我们将之前的一些Datax的服务拷贝过来，稍加修改，就能够实现了，主要涉及到的是一些非关型数据库之间的一些数据交互。\n\n而纯粹的关型数据库之间的交互，我们还是需要通过配置方式实现。\n\n首先我们在配置Clickhouse读写任务时，就遇见了一个小bug。\n04Datax服务适配—Clickhouse兼容#8092\n我们在使用Datax来读取Clickhouse数据源的数据时，发现在sql当中，只要引用参数，无论时间参数还是其他参数，在提交的时都会失败，我们就怀疑其中可能有一些bug，阅读错误日志的时候，也发现在Apache DolphinScheduler提交 SQL时，是参数并未被替换就直接提交给了Clickhouse去执行，由于clickhouse并不能识别我们的Apache DolphinScheduler参数,所以就直接抛出异常了。我们梳理了一下Apache DolphinScheduler在执行datax任务时读取clickhouse的流程。其中在将我们在Apache DolphinScheduler配置转为datax配置流程如下：\n\n\n \n系统首先要做的就是先去解析sql的所有语法，然后通过语法拿到一些列的信息，这时它要去调用sql解析器。在这个过程当中，如果Apache DolphinScheduler没有对我们的这个参数去做替换，在执行这个 circle的时候就会发生错误，最后导致整个任务失败。\n\n因此在解决的过程中，既然可能获取不到Clickhouse的解析器，最好的方法就是直接加入一个解析器。首先构建一个Json文件，然后格式化解析出来的所有的链，最后对语法去做一次解析，层层调用，最后能够调用到目标解析器。\n05Time参数适配Apache DolphinScheduler现状\n最后的案例是关于时间参数适配。\n\nApache DolphinScheduler虽然提供有时间参数，但是我们自己的数据大部分都需要精确到毫秒级别的unixtime时间。通过阅读Apache DolphinScheduler的文档，我们遗憾地发现其并未提供该类型时间参数的实现。翻阅源码过程中，我们发现Apache DolphinScheduler提供有timestamp函数，实际上能够提供unixtime时间值。\n\n在使用timestamp的时候，我们发现有两个小问题，首先timestamp直接表达unixtime有一些歧义，其次timestamp仅支持到秒级别，而我们大部分数据需要毫秒级别。为了方便使用，我们对此部分做了一些修改进行适配。\n\n\n\n适配过程\n首先我们做的第一件事情就是消除歧义，在Apache DolphinScheduler中，Timestamp是表达时间的方式，从Wiki百科获得的关于Timestamp和Unix time时间表达的解释能看出，Timestamp通常是通过日期加时间来表示的，但是Unix time时间采用的是格林威治时间，从1970年1月1日零时零分零秒至今，并且不考虑微秒的时间表达，采用的是整数。\n明确了需求，接下来就需要了解如何实现了。我们通过分析代码发现，时间参数函数的实现是通过api方式层层调用，最终主要函数均通过在TimePlaceHolderUtils类中calculateTime的方法实现。该方法实现过程中，也会调用TaskConstants类中的表达时间函数名称的常量。于是我们对其中 TaskConstants类的一些常量进行了修改。又因为我们需要毫秒级别的函数，加入了一个 milli_unixtime函数，最后为了满足设备用户的需求，我们加入了一些更精度更高的函数，如微秒和纳秒的函数。\n\n\n\n\n\n\n在补数功能上，在使用Apache DolphinScheduler之后，我们只需要在手动执行任务的时候选中补数的功能，再填充上我们要调度的日期，就可以直接进行补充了，同时我们还可以填写并行度。这个功能对我们这来说非常实用的，在Apache DolphinScheduler 2.0版本以后，时间的配置和执行的时间有日绩差的问题也被解决，在使用上带来了很大的便利。\n未来规划\n在使用的过程中，我们发现通过Apache DolphinScheduler配置的任务在使用数据源方面，目前还不支持高可用的方案，这个需求在我们这里是比较强烈的，因此目前我们也正在做高可用的适配。\n其次，我们目前使用的是Apache DolphinScheduler的2.0版本，因为社区比较活跃，版本升级也比较快，即使是一个小版本的升级，也会带来一些很大的功能和设计上的一些变化。比如在新版本当中，告警功能已经插件化，也解决了一些补数日期换算的问题。这也驱动着我们团队升级到新的版本去体验一些新的功能。虽然目前Apache DolphinScheduler只是在我们自己的小团队内部使用，但我们也正在思考让整个公司普遍使用的可行性方案。\n\n尽管Apache DolphinScheduler非常完美地解决我们的大部分问题，并且大幅度提高我们的工作效率。但在各种复杂的情况下，我们还是会遇见一些小的Bug，我们未来也会在修复后提交给官方，当然我们自己在使用过程中也尝试了一些小Future，未来也会提交给官方共同讨论。\n",
    "title": "论语音社交视频直播平台与 Apache DolphinScheduler 的适配度有多高",
    "time": "2022-4-16"
  },
  {
    "name": "How_Does_Ziru_Build_A_Job_Scheduling_System_Popular_Among_Data_Analysts",
    "content": "数据分析师干了专业数仓工程师的活，自如是怎么做到的？\n\n\n\n数据分析师作为企业数据资产的缔造者之一，具有一定的维度与指标体系管理、血缘分析、ETL 调度平台等技能。能够灵活使用调度平台会为数据分析师带来很大的便利，然而对于编程技能水平参差不齐的数据分析师来说，一个操作简单，使用成本低的调度平台才能让他们如虎添翼，而不是增加额外的学习成本。\n与大多企业相比，自如大数据平台的独特之处在于，大量的数仓加工并非由专业的数仓工程师完成，而是由数据分析师所做。而自如的数据分析师之所以能够做到专业团队才能完成的复杂的数据处理、分析工作，与其调度系统迁移到 Apache DolphinScheduler 分不开。\n在不久前的 Apache DolphinScheduler&amp; Apache ShenYu(Incubating) Meetup 上，自如大数据研发经理 刘涛，为我们分享了受数据分析师们欢迎的调度系统是什么样的。\n\n\n\n刘涛自如大数据研发经理，负责自如大数据基础平台构建，建设一站式大数据开发平台。\n01 自如大数据平台现状\n\n\n\n自如大数据平台\n上图是自如大数据离线平台的简单图示，数据源包括 MySQL、Oracle 等业务库数据，以及各种日志数据，通过 Hive 离线 T 加 1 采集、另外使用Hive acid加上Flink实现了一个10分钟级别的业务库数据更新。\n数据加工是分析师关心的部分，这个过程可以配置调度、配置依赖和 SQL 开发。而在数据落地上，我们采用了 ClickHouse 的 OLAP 引擎，数据应用层使用网易有数提供报表平台。\n自如的大数据平台与业界大多数平台相差不大，但独特之处在于除了支持专业数仓开发工程师外，大量的数据分析师参与到了数仓加工之中。这就要求大数据平台要足够简化。\n02 分析师的期望\n\n\n\n由于数据分析师的编码水平参差不齐，有些分析师会写 SQL，而有些分析师根本不会写 SQL。即使是对于会写 SQL 的分析师，在面对任务依赖概念的理解上，也会觉得难度很大。\n因此，分析师群体对于调度的期望是要简单，上手成本低。\n03 Airflow的实现方式\n\n\n\n\n\n\n一开始，自如选用的是 Airflow，使用Airflow 可视化插件Airflow DAG createmanager plug-in来供分析师用，底层使用hivepartitionsensor，用数据依赖的方式配置调度，便于分析师理解和使用，这套解决方案，对于分析师来说体验尚可，但是面临几个较大的问题：\n\n数据依赖的底层实现导致的任务重跑非常复杂；\n任务量比较多后，调度性能较差，有些任务调起延迟较大；\n与一站式大数据开发平台集成二开成本比较高；\n原生不支持多租户。\n\n04 Apache DolphinScheduler改造与 Airflow任务迁移\n以上几个比较重要的挑战，促使我们重新进行调度选型。经过对比分析后，我们选择了 Apache DolphinScheduler。\n对于分析师来说，数据依赖是一个好理解的概念，但任务依赖就比较让人费解。\n比较理想的方案是对分析师展示的是数据依赖，底层实现是任务依赖，并且这数据依赖是自动生产的，不需要分析师手动输入依赖表。\n做到这一点，首先需要解决一个问题，如何根据一段 SQL，判断出这段 SQL 的输入输出表？\n\n\n\n\n\n\n由于是在 Hive 的环境中，所以需要看下 Hive  sql 的解析过程。\n如上图所示hive利用antlr 进行语法和语义解析，生成抽象语法树。举例，如下一段 sql 语句：\n\n\n\n解析成的语法树：\n\n\n\n遍历这棵抽象语法树就可以准确获得输入输出，我们发现并不需要从头来做，Hive 147 中就实现了这个功能。\nhttps://issues.apache.org/jira/browse/HIVE-147\n\n\n\n我们解析了输入输出之后，就可以把输入输出表和对应的 Apache DolphinScheduler调度任务关联起来，这样就完成了对分析师看到的是数据依赖，底层实现是任务依赖。当然这种实现就会让每个任务都很小，大部分任务都是只最终产出一张表，调度数量会比较多，但目前来看，没有带来性能问题。\n\n\n\n这之后就是面临的如何把Airflow中的任务平滑的迁移到 Apache DolphinScheduler 中，Airflow的任务都是一个个Python文件，Airflow 的调度器不停地扫描Pyhton文件所在文件目录，生成调度任务。核心实现类就是上图中的 DagFileProcessorManager，我们就可以参考这个类的实现来解析Python任务，生成Apache DolphinScheduler 任务定义需要的 Json 串，从而完成调度任务的迁移。\n最后是做个广告，我们是自如大数据基础平台，负责大数据的部署、 运维、 监控、优化、二开，并且在此之上构建一站式的大数据开发平台，欢迎加入我们。\n我的分享就到这里，感谢大家！\n05 特别感谢\n联合主办方\nApache ShenYu(Incubating)\n合作方\n示说网、开源中国、CSDN、稀土掘金、开源社、SeaTunnel 社区、思否 和 ALC 北京\n礼品赞助\nYY 直播\nApache ShenYu(Incubating)\n感谢主持人，低代码无代码平台 Treelab  张德通，以及活动志愿者 曹海洋 对本场活动的大力支持！\n",
    "title": "数据分析师干了专业数仓工程师的活，自如是怎么做到的？",
    "time": "2022-4-16"
  },
  {
    "name": "K8s_Cisco_Hangzhou",
    "content": "全面拥抱 K8s，ApacheDolphinScheduler 应用与支持 K8s 任务的探索\n\n\n\n\nK8s 打通了主流公私云之间的壁垒，成为唯一连通公私云的基础架构平台。K8s 是未来云端的趋势，全面拥抱 K8s 成为更多企业推动 IT 现代化的选择。\n\n杭州思科基于 Apache DolphinScheduler，也在进行支持 K8s 的相关探索，且部分功能已经成功上线运行。今天，来自杭州思科的大数据工程师 李千，将为我们分享他们的开发成果。\n\n\n\n\n\n李千，杭州思科 大数据工程师，多年大数据解决方案经验，有 Spark，Flink，以及调度系统，ETL 等方面的项目经验。\n正文：\n本次我的分享主要分为这几部分，Namespace 管理，持续运行的 K8s 任务，K8s 任务的工作流调度，以及未来的规划。\nNamespace 管理\n资源管理\n第一部分中，我首先介绍一下资源管理。我们引入资源管理目的，是为了利用 K8s 集群运行不属于 Apache DolphinScheduler 所属的调度概念上的任务，比如 Namespace，更类似于一个数据解决方案，如果 CPU 的 memory 有限，就可以限制队列中的资源，实现一定的资源隔离。\n以后我们可能会把一部分资源管理功能合并到 Apache DolphinScheduler 上。\n增删维护管理\n我们可以加一些 Type，即标记的类型，比如某些 Namespace 只允许跑一些特定类型的 job。我们可以统计Namespace 下面的任务数量、pod 数量、请求资源量、请求等，查看队列的资源使用情况，界面默认只有管理员才可以操作。\n\n\n\n多 K8s 集群\nK8s 支持多个集群，我们通过 Apache DolphinScheduler 客户端连接到多个 K8s 集群，batch、PROD 等可以搭建多套这K8s 集群，并通过 Namespace 支持多套 K8s 集群。\n我们可以编辑所开发的集群，修改所有的属性，如内存等。\n在新版中，用户权限的管理位于 user master 中，可以给某个用户授权，允许用户可以向某个 Namespace 上提交任务，并编辑资源。\n02 持续运行的 K8s 任务\n第二部分是关于我们目前已经支持的任务类型。\n启动不退出的普通镜像，如 ETL 等\n比如 ETL 这种提交完之后必须要手动操作才会退出的任务。这种任务一旦提交，就会把数据 sink，这种任务理论上只要不做升级，它永远不会停。\n\n\n\n这种任务其实调度可能用不到，因为它只有启停这两种状态。所以，我们把它放在一个实时列表中，并做了一套监控。POD是实时运行的状态，主要是通过一个 Fabris operator 进行交互，可以进行动态进行扩展，以提高资源利用率。\nFlink 任务\n我们对于 CPU 的管理可以精确到 0.01%，充分利用了 K8s 虚拟 CPU。\n\n\n\n\n\n\n\n\n\n另外，我们也常用 Flink 任务，这是一种基于 ETL 的扩展。Flink 任务界面中包含编辑、查看状态、上线、下线、删除、执行历史，以及一些监控的设计。我们用代理的模式来设计 Flink UI，并开发了权限管控，不允许外部的人随意修改。\nFlink 默认了基于 checkpoint 启动，也可以指定一个时间创建，或基于上一次 checkpoint 来提交和启动。\nFlink 任务支持多种模式镜像版本，因为 K8s 本身就是运行镜像的，可以直接指定一些镜像来选择使用包，或通过文件上传的方式提交任务。\n另外，Batch 类型的任务可能一次运行即结束，或是按照周期来调度，自动执行完后退出，这和 Flink 不太一样，所以对于这种类型的任务，我们还是基于 Apache DolphinScheduler 做。\n03 K8s 任务的运行\nK8s 任务的工作流调度\n我们在最底层增加了一些 Flink 的 batch 和 Spark 的 batch 任务，添加了一些配置，如使用的资源，所运行的 namespace 等。镜像信息可以支持一些自定义参数启动，封装起来后就相当于插件的模式，Apache DolphinScheduler 完美地扩展了它的功能。\n\n\n\nSpark 任务\nSpark 任务下可以查看 CPU 等信息，上传文件支持 Spark Jar 包，也可以单独上传配置文件。\n\n\n\n这种多线程的上层，可以大幅提高处理速度。\n04 其他和规划\nWatch 状态\n\n\n\n除了上述改动，我们还对任务运行状态进行了优化。\n当提交任务后，实际情况下运行过程中可能会出现失败，甚至任务的并行度也会基于某些策略发生改变。这时，我们就需要一种 watch 的方式来动态实时地来获取任务状态，并同步给 Apache DolphinScheduler 系统，以保证界面上看到的状态一定是最准确的。\nBatch 做不做 watch 都可以，因为这不是一个需要全量监听的独立任务而且 namespace 的资源使用率也是基于 watch 模式，这样就可以保证状态都是准确的。\n多环境\n多环境是指，同一个任务可以推送到不同的 K8s 集群上，比如同一个Flink 任务。\n从代码上来说，watch 有两种方式，一种是单独放一些 pod，比如当使用了 K8s 模块时，定义多个 K8s 集群的信息，在每个集群上创建一些watch pod 来监听集群中的任务状态，并做一些代理的功能。另一种是跟随api或单独服务，启动一个监听服务监听所有k8s集群。但这样无法而外做一些k8s内外网络的代理。\nBatch 有多种方案，一种是可以基于 Apache DolphinScheduler 自带功能，通过同步的方式进行 watch，这和 Apache DolphinScheduler 比较兼容。关于这方面的工作我们未来可能很快会提交 PR。Spark 使用相同的模式，提供一些 pod 来进行交互，而内部代码我们使用的是 Fabric K8s 的 client。\n今后，我们将与 Apache DolphinScheduler 一起共建，陆续支持这里讨论的功能，并和大家分享更多关于我们的工作进展。谢谢大家！\n",
    "title": "全面拥抱 K8s，ApacheDolphinScheduler 应用与支持 K8s 任务的探索",
    "time": "2022-3-21"
  },
  {
    "name": "Lizhi-case-study",
    "content": "荔枝机器学习平台与大数据调度系统“双剑合璧”，打造未来数据处理新模式！\n\n\n\n\n编 者 按：在线音频行业在中国仍是蓝海一片。根据 CIC 数据显示，中国在线音频行业市场规模由 2016 年的 16 亿元增长至 2020 年的 131 亿元，复合年增长率为 69.4%。随着物联网场景的普及，音频场景更是从移动端扩展至车载端、智能硬件端、家居端等各类场景，从而最大限度发挥音频载体的伴随性优势。\n\n\n近年来，国内音频社区成功上市的案例接踵而至。其中，于 2020 年在纳斯达克上市的“在线音频”第一股荔枝，用户数早已超过 2 亿。进入信息化时代，音频行业和所有行业一样，面对平台海量 UGC 数据的产生，音频用户和消费者对于信息传递的效率也有着高要求，互联网音频平台也希望信息能够精准、快速地推送给用户，同时保证平台 UGC 内容的安全性。 为了最高效地达到这一目标，利用日益成熟的机器学习技术与大数据调度系统相结合，以更好地海量数据处理任务，成为各大音频平台探索的方向。荔枝的机器学习智能推荐平台就走在探索的最前沿，尝试将机器学习平台与大数据调度系统 DolphinScheduler 相结合。\n\n01 背景\n\n\n\n荔枝是一个快速发展的UGC音频社区公司，非常注重 AI 和数据分析两个领域的技术。因为 AI 可以在海量碎片化的声音中找到每个用户喜欢的声音，并将其打造成一个可持续发展的生态闭环。而数据分析可以为公司快速发展的业务提供指引。两者都需要对海量数据做处理，所以需要使用大数据调度系统。荔枝机器学习平台团队尝试利用大数据调度平台的调度能力与机器学习平台的能力相结合，处理每天涌进荔枝平台的海量 UGC 音频内容。\n在 2020 年初之前，荔枝机器学习平台使用的是 Azkaban 调度系统，但是对于 AI 来说，虽然大数据调度的 sql/shell/python 脚本和其他大数据相关组件也能完成整个AI 流程，但不够易用，也很难复用。机器学习平台是专门为 AI 构建的一套系统，它将 AI 开发范式，即获取数据、数据预处理、模型训练、模型预测、模型评估和模型发布过程抽象成组件，每个组件提供多个实现，用 DAG 串联，使用拖拽和配置的方式，就可以实现低代码开发。\n目前，荔枝每天有 1600+ 流程和 12000+ 任务（IDC）在 DolphinScheduler 上顺利运行。\n02 机器学习平台开发中的挑战\n荔枝在进行机器学习平台开发时，对于调度系统有一些明确的需求：\n1、需要对海量数据存算，比如筛选样本、生成画像、特征预处理、分布式模型训练等；\n2、需要 DAG 执行引擎，将获取数据-&gt;数据预处理-&gt;模型训练-&gt;模型预测-&gt;模型评估-&gt;模型发布等流程用 DAG 串联起来执行。\n而使用之前的 Azkaban 调度系统进行开发时，他们遇到了一些挑战：\n挑战1：开发模式繁琐，需要自己打包脚本和构建 DAG 依赖，没有 DAG 拖拽的实现；\n挑战2：组件不够丰富，脚本编写的 AI 组件不够通用，需要重复开发，且易出错；\n挑战3：单机部署，系统不稳定，容易出故障，任务卡顿容易导致下游任务都无法进行。\n03 切换到DolphinScheduler\n在旧系统中踩坑无数后，荔枝机器学习团队，包括推荐系统开发工程师喻海斌、林燕斌、谢焕杰和郭逸飞，决定采用 DolphinScheduler。喻海斌表示，他们团队调度系统的主要使用人员为推荐算法 &gt; 数据分析 &gt; 风控算法 &gt; 业务开发（依次减弱），他们对调度系统了解不多，所以对于一个简单易用，可拖拉拽实现的调度系统的需求比较大。相比之下，DolphinScheduler 完美切合了他们的需求：\n1、分布式去中心化架构及容错机制，保障系统高可用；\n2、可视化 DAG 拖拽 UI，使用简洁方便，迭代效率高；\n3、拥有丰富的组件，且可以轻松开发集成自己的组件；\n4、社区活跃，无后顾之忧；\n5、与机器学习平台运行模式非常接近，使用 DAG 拖拽 UI 编程。\n04 应用案例\n选型 DolphinScheduler 之后，荔枝机器学习平台在其之上进行了二次开发，并将成果应用于实际业务场景汇总，目前主要是在推荐和风控场景。其中，推荐场景包括声音推荐、主播推荐、直播推荐、播客推荐、好友推荐等，风控场景包括支付、广告、评论等场景中的风险管控。\n在平台底层，荔枝针对机器学习的五大范式，获取训练样本、数据预处理、模型训练、模型评估、模型发布过程，进行了扩展组件优化。\n一个简单的xgboost案例：\n\n\n\n1、获取训练样本\n目前还没有实现像阿里 PAI 那样直接从 Hive 选取数据，再对样本进行 join，union，拆分等操作，而是直接使用 shell 节点把样本处理好。\n2. 数据预处理\nTransformer&amp;自定义预处理配置文件，训练和线上采用同一份配置，获取特征后进行特征预处理。里面包含了要预测的 itemType 及其特征集，用户 userType 及其特征集，关联和交叉的 itemType 及其特征集。定义每个特征预处理的 transformer 函数，支持自定义transformer 和热更新，xgboost 和 tf 模型的特征预处理。经过此节点后，才是模型训练真正要的数据格式。这个配置文件在模型发布时也会带上，以便保持训练和线上预测是一致的。这个文件维护在 DolphinScheduler 的资源中心。\n\n\n\n3. Xgboost 训练\n支持 w2v、xgboost、tf模型训练组件，训练组件先使用 TensorFlow或 PyTorch封装，再封装成 DolphinScheduler 组件。\n例如，xgboost 训练过程中，使用 Python封装好 xgboost训练脚本，包装成 DolphinScheduler 的 xgboost 训练节点，在界面上暴露训练所需参数。经过“训练集数据预处理”输出的文件，经过 hdfs 输入到训练节点。\n\n\n\n4. 模型发布\n发布模型会把模型和预处理配置文件发到 HDFS，同时向模型发布表插入记录，模型服务会自动识别新模型，进而更新模型，对外提供在线预测服务。\n\n\n\n喻海斌表示，基于历史和技术原因，目前荔枝还没有做到像阿里 PAI 那样真正的机器学习平台，但其实践经验已经证明基于 DolphinScheduler 可以做到类似的平台效果。\n此外，荔枝还基于 DolphinScheduler 进行了很多二次开发，让调度系统更加符合实际业务需求，如：\n\n上线工作流定义的时候弹窗是否上线定时\n增加所有工作流定义的显示页面，以方便查找\na) 增加工作流定义筛选并跳转到工作流实例页面，并用折线图表示其运行时长的变化\nb) 工作流实例继续下潜到任务实例\n运行时输入参数，还可以进行配置禁用任务节点\n\n05 基于调度的机器学习平台实现或将成未来趋势\n深度学习是未来的大趋势，荔枝也针对深度学习模型开发了新的组件，目前已完成 tf 整个流程，后续在开发的还有 LR 和 GBDT 模型相关组件。后两者相对简单，上手更快，一般的推荐场景都可以使用，迭代更快，实现之后可以让荔枝机器学习平台更加完善。\n荔枝认为，如果调度系统可以在内核稳定性，支持拖拽 UI，方便扩展的组件化，任务插件化，以及和任务参数传递方向上更加出色，基于调度系统实现机器学习平台，未来有可能会成为业界的普遍做法之一。\n06 期待就是我们前进的动力\n用户的期待就是 DolphinScheduler 前进的动力，荔枝也对 DolphinScheduler 的未来发展提出了很多具有指导性意义的建议，如期待 DolphinScheduler 未来可以优化插件机制，精减成一个专注于调度的“微内核”，并集成组件插件化，同时支持 UI 插件化，后台开发可以定制插件的 UI 界面。此外，荔枝还希望 DolphinScheduler 能够优化MySQL 表维护的“字典管理”，包含 key，value，desc 等主要字段，方便用户把一些易变量维护在字典表中，而不是维护在 .properties 文件里，这使得配置可以运行时更新，比如某个阈值，避免新增组件带来的配置参数放在 .properties 文件里导致的臃肿和不易维护等问题。\n最后，荔枝也表达了对 DolphinScheduler 寄语的期待，希望社区发展越来越好，探索更加广阔的领域，在海外也能取得更好的成绩，实现可持续的商业化开源！\n",
    "title": "荔枝机器学习平台与大数据调度系统“双剑合璧”，打造未来数据处理新模式!",
    "time": "2021-11-23"
  },
  {
    "name": "The_most_comprehensive_introductory_tutorial_written_in_a_month",
    "content": "达人专栏 | 还不会用 Apache Dolphinscheduler？大佬用时一个月写出的最全入门教程\n\n\n\n作者 | 欧阳涛 招联金融大数据开发工程师\n海豚调度(Apache DolphinScheduler，下文简称 DS)是分布式易扩展的可视化 DAG 工作流任务调度系统，致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。Apache DolphinScheduler 作为 Apache 的顶级开源项目，与其他开源项目相似的地方在于，其运行以及安装都是从脚本开始的。\n脚本的位置都是根目录的 script 文件夹下的，脚本执行顺序如下:\n1、查看启动的脚本 start-all.sh，可以发现启动 4 个最重要的启动服务，分别是 dolphinscheduler-daemon.sh start master-server/worker-server/alert-server/api-server\n2、在 dolphinscheduler-daemon.sh 脚本中会首先执行 dolphinscheduler-env.sh 脚本，这个脚本作用是引入环境，包括 Hadoop、Spark、Flink、Hive 环境等。由于 DS 需要调度这些任务，如果不引入这些环境，即使调度成功，执行也无法成功。\n3、紧接着在 dolphinscheduler-daemon.sh 脚本中循环执行上述 4 个模块下的 bin/start.sh.如下图所示：\n\n\n\n如下图所示：执行 dolphinscheduler-daemon.sh start master-server 时会去 master 模块的 src/main/bin 执行 start.sh，打开 start.sh 后，可以发现启动了一个 MasterServer，其他 Worker，Alert 以及 API 模块等同理。\n\n\n\n至此，从脚本如何运行代码这块就已经结束了，接下来我们将详细介绍一下这 4 个模块的主要用途。Master 主要负责 DAG 任务切分、任务提交监控，并同时监听其它 Master 和 Worker 的健康状态等；Worker 主要负责任务的执行；Alert 是负责警告服务；API 负责 DS 的增删改查业务逻辑，即网页端看到的项目管理、资源管理、安全管理等等。\n其实，如果大家接触过其他大数据项目，例如 Flink、Hdfs、Hbase 等，就会发现这些架构都是类似的，像 hdfs 是 NameNode 和 WorkNode 的架构；Hbase 是 HMasterServer 和 HRegionServer 的架构；Flink 是 JobManager 和 TaskManager 的架构等，如果你能够熟练掌握这些框架，想必对于 DS 的掌握也会更容易的了。\nMaster，Worker 这些都是通过 SpringBoot 的启动，创建的对象也都是由 Spring 托管，如果大家平常接触 Spring 较多的话，那么笔者认为您理解 DS 一定会比其他的开源项目更容易。\n备注:\n1、运行脚本中还有一个 python-gateway-server 模块，这个模块是用 python 代码编写工作流的，并不在本文考虑范围之内，所以就暂时忽略，如果详细了解此模块的话，在社区请教其他同学的了。\n2、启动 Alert 脚本是执行 Alert 模块下的 alert-server 的脚本，因为 Alert 也是个父模块的，笔者不打算讲 alert-server。相信在看完 Master 和 Worker 的执行流程之后，Alert 模块应该不难理解。\n3、另外，初次接触 DS 的同学会发现 Alert 模块有个 alert-api 模块，笔者想说的是这 alert-api 和前面所说的 api-server 没有一丁点关系，api-server 是启动 api 模块的 ApiApplicationServer 脚本，负责整个 DS 的业务逻辑的，而 alert-api 则是负责告警的 spi 的插件接口，打开 alert-api 模块可以发这里面的代码全是接口和定义，没有处理任何逻辑的，所以还是很好区分的了。同理 task 模块下的 task-api 与 alert-api 只是职责相同，处理的是不同功能而已。\n4、DS 的全都是 SpringBoot 管理的，如果有同学没搞过 SpringBoot 或者 Spring 的话，可以参考下列网址以及网上的其他相关资料等。\nhttps://spring.io/quickstart\n如果想详细了解警告模块，请参考下方链接以及咨询其他同学。\nhttps://dolphinscheduler.apache.org/zh-cn/blog/Hangzhou_cisco\nApache DolphinScheduler 项目官网的地址为:https://github.com/apache/dolphinscheduler\n下一章，笔者将介绍 DS 最重要的两个模块 Master 和 Worker，以及它们如何进行通信的，敬请期待。\n",
    "title": "还不会用 Apache Dolphinscheduler？大佬用时一个月写出的最全入门教程（1）",
    "time": "2022-5-23"
  },
  {
    "name": "Twos",
    "content": "恭喜 Apache DolphinScheduler 入选可信开源社区共同体（TWOS）预备成员！\n\n\n\n近日，可信开源社区共同体正式宣布批准 6 位正式成员和 3 位预备成员加入。其中，云原生分布式大数据调度系统 Apache DolphinScheduler 入选，成为可信开源社区共同体预备成员。\n\n\n\nApache DolphinScheduler 是一个分布式易扩展的新一代工作流调度平台，致力于“解决大数据任务之间错综复杂的依赖关系，使整个数据处理过程直观可见”，其强大的可视化 DAG 界面极大地提升了用户体验，配置工作流程无需复杂代码。\n自 2019 年 4 月正式对外开源以来，Apache DolphinScheduler 经过数代架构演进，迄今相关开源相关代码累积已获得 7100+ 个 Star，280+ 经验丰富的代码贡献者，110+ 非代码贡献者参与其中，其中也不乏其他 Apache 顶级项目的 PMC 或者 Committer。Apache DolphinScheduler 开源社区不断发展壮大，微信用户群已达 6000+ 人，600 + 家公司及机构已在生产环境中采用 Apache DolphinScheduler。\nTWOS\n在“2021OSCAR 开源产业大会”上，中国信通院正式成立可信开源社区共同体（TWOS）。可信开源社区共同体由众多开源项目和开源社区组成，目的是引导建立健康可信且可持续发展的开源社区，旨在搭建交流平台，提供全套的开源风险监测与生态监测服务。\n\n\n\n为帮助企业降低开源软件的使用风险，推动建立可信开源生态，中国信通院建立了可信开源标准体系，对企业开源治理能力、开源项目合规性、开源社区成熟度、开源工具检测能力、商业产品开源风险管理能力开展测评。其中，对于开源社区的评估，是开源社区=人+项目+基础设施平台，一个好的开源社区有助于开源项目营造良好的开源生态并扩大影响力。可信开源社区评估从基础设施、社区治理、社区运营与社区开发等角度，梳理开源社区应关注的内容及指标，聚焦于如何构建活跃的开发者生态与可信的开源社区。\n经过可信开源社区共同体（TWOS）的重重评估标准的筛选，批准 Apache DolphinScheduler 入选预备成员，证明了其对 Apache DolphinScheduler 的开源运营方式、成熟度及贡献的认可，激励社区提升活跃度。\n\n\n\n可信开源标准体系 开源社区评估标准\n2021 年 9 月 17 日，可信开源社区共同体（TWOS）第一批成员加入。目前，可信开源社区共同体包括 openEuler、openGauss、MindSpore、openLookeng 等在内的 25 名正式成员，以及 Apache RocketMQ、Dcloud、Fluid、FastReID 等在内的 27 名预备成员，总数为 52 名：\n\n\n\n第二批预备成员仅有两个项目——Apache DolphinScheduler 与阿里云开源云原生态分布式数据库 PolarDB 入选。\nApache DolphinScheduler 社区十分荣幸能够入选可信开源社区共同体（TWOS）预备成员，这是整个行业对 Apache DolphinScheduler 社区建设的肯定与激励，社区将再接再厉，争取早日成为正式成员，与中国信息通信研究院领导的可信开源社区共同体一起，为中国开源生态建设提供更多价值！\n",
    "title": "恭喜 Apache DolphinScheduler 入选可信开源社区共同体（TWOS）预备成员！",
    "time": "2022-1-11"
  },
  {
    "name": "dolphinscheduler_json",
    "content": "dolphinscheduler json拆解\n1、为什么拆解json\n在dolphinscheduler 1.3.x及以前的工作流中的任务及任务关系保存时是以大json的方式保存到数据库中process_definiton表的process_definition_json字段，如果某个工作流很大比如有100或者1000个任务，这个json字段也就非常大，在使用时需要解析json，非常耗费性能，且任务没法重用；基于大json，在工作流版本及任务版本上也没有好的实现方案，否则会导致数据大量冗余。\n故社区计划启动json拆解项目，实现的需求目标：\n\n大json完全拆分\n新增工作流及任务版本\n引入全局唯一键(code)\n\n2、如何设计拆解后的表\n1、1.3.6版本工作流\n1、比如在当前1.3.6版本创建个a--&gt;b的工作流\n\n以下是processDefiniton save 接口在controller入口打印的入参日志\ncreate  process definition, project name: hadoop, process definition name: ab, process_definition_json: {&quot;globalParams&quot;:[],&quot;tasks&quot;:[{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-77643&quot;,&quot;name&quot;:&quot;a&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[{&quot;prop&quot;:&quot;yesterday&quot;,&quot;direct&quot;:&quot;IN&quot;,&quot;type&quot;:&quot;VARCHAR&quot;,&quot;value&quot;:&quot;${system.biz.date}&quot;}],&quot;rawScript&quot;:&quot;echo ${yesterday}&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[]},{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-99814&quot;,&quot;name&quot;:&quot;b&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[{&quot;prop&quot;:&quot;today&quot;,&quot;direct&quot;:&quot;IN&quot;,&quot;type&quot;:&quot;VARCHAR&quot;,&quot;value&quot;:&quot;${system.biz.curdate}&quot;}],&quot;rawScript&quot;:&quot;echo ${today}&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;a&quot;]}],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}, desc:  locations:{&quot;tasks-77643&quot;:{&quot;name&quot;:&quot;a&quot;,&quot;targetarr&quot;:&quot;&quot;,&quot;nodenumber&quot;:&quot;1&quot;,&quot;x&quot;:251,&quot;y&quot;:166},&quot;tasks-99814&quot;:{&quot;name&quot;:&quot;b&quot;,&quot;targetarr&quot;:&quot;tasks-77643&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:533,&quot;y&quot;:161}}, connects:[{&quot;endPointSourceId&quot;:&quot;tasks-77643&quot;,&quot;endPointTargetId&quot;:&quot;tasks-99814&quot;}]\n\n2、依赖节点的工作流，dep是依赖节点\n\n以下是processDefiniton save 接口在controller入口打印的入参日志\n create  process definition, project name: hadoop, process definition name: dep_c, process_definition_json: {&quot;globalParams&quot;:[],&quot;tasks&quot;:[{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-69503&quot;,&quot;name&quot;:&quot;c&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 11&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;dep&quot;]},{&quot;type&quot;:&quot;DEPENDENT&quot;,&quot;id&quot;:&quot;tasks-22756&quot;,&quot;name&quot;:&quot;dep&quot;,&quot;params&quot;:{},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{&quot;relation&quot;:&quot;AND&quot;,&quot;dependTaskList&quot;:[{&quot;relation&quot;:&quot;AND&quot;,&quot;dependItemList&quot;:[{&quot;projectId&quot;:1,&quot;definitionId&quot;:1,&quot;depTasks&quot;:&quot;b&quot;,&quot;cycle&quot;:&quot;day&quot;,&quot;dateValue&quot;:&quot;today&quot;}]}]},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[]}],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}, desc:  locations:{&quot;tasks-69503&quot;:{&quot;name&quot;:&quot;c&quot;,&quot;targetarr&quot;:&quot;tasks-22756&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:597,&quot;y&quot;:166},&quot;tasks-22756&quot;:{&quot;name&quot;:&quot;dep&quot;,&quot;targetarr&quot;:&quot;&quot;,&quot;nodenumber&quot;:&quot;1&quot;,&quot;x&quot;:308,&quot;y&quot;:164}}, connects:[{&quot;endPointSourceId&quot;:&quot;tasks-22756&quot;,&quot;endPointTargetId&quot;:&quot;tasks-69503&quot;}]\n\n3、条件判断的工作流\n\n以下是processDefiniton save 接口在controller入口打印的入参日志\ncreate  process definition, project name: hadoop, process definition name: condition_test, process_definition_json: {&quot;globalParams&quot;:[],&quot;tasks&quot;:[{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-68456&quot;,&quot;name&quot;:&quot;d&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 11&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[]},{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-58183&quot;,&quot;name&quot;:&quot;e&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 22&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;cond&quot;]},{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-43996&quot;,&quot;name&quot;:&quot;f&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 33&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;cond&quot;]},{&quot;type&quot;:&quot;CONDITIONS&quot;,&quot;id&quot;:&quot;tasks-38972&quot;,&quot;name&quot;:&quot;cond&quot;,&quot;params&quot;:{},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;e&quot;],&quot;failedNode&quot;:[&quot;f&quot;]},&quot;dependence&quot;:{&quot;relation&quot;:&quot;AND&quot;,&quot;dependTaskList&quot;:[{&quot;relation&quot;:&quot;AND&quot;,&quot;dependItemList&quot;:[{&quot;depTasks&quot;:&quot;d&quot;,&quot;status&quot;:&quot;SUCCESS&quot;}]}]},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;d&quot;]}],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}, desc:  locations:{&quot;tasks-68456&quot;:{&quot;name&quot;:&quot;d&quot;,&quot;targetarr&quot;:&quot;&quot;,&quot;nodenumber&quot;:&quot;1&quot;,&quot;x&quot;:168,&quot;y&quot;:158},&quot;tasks-58183&quot;:{&quot;name&quot;:&quot;e&quot;,&quot;targetarr&quot;:&quot;tasks-38972&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:573,&quot;y&quot;:82},&quot;tasks-43996&quot;:{&quot;name&quot;:&quot;f&quot;,&quot;targetarr&quot;:&quot;tasks-38972&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:591,&quot;y&quot;:288},&quot;tasks-38972&quot;:{&quot;name&quot;:&quot;cond&quot;,&quot;targetarr&quot;:&quot;tasks-68456&quot;,&quot;nodenumber&quot;:&quot;2&quot;,&quot;x&quot;:382,&quot;y&quot;:175}}, connects:[{&quot;endPointSourceId&quot;:&quot;tasks-68456&quot;,&quot;endPointTargetId&quot;:&quot;tasks-38972&quot;},{&quot;endPointSourceId&quot;:&quot;tasks-38972&quot;,&quot;endPointTargetId&quot;:&quot;tasks-58183&quot;},{&quot;endPointSourceId&quot;:&quot;tasks-38972&quot;,&quot;endPointTargetId&quot;:&quot;tasks-43996&quot;}]\n\n从以上三个案例中，我们知道controller的入口参数的每个参数都可以在t_ds_process_definition表中找到对应，故表中数据如下图\n\n2、拆解后的表设计思路\n工作流只是dag的展现形式，任务通过工作流进行组织，组织的同时存在了任务之间的关系，也就是依赖。就好比一个画板，画板上有些图案，工作流就是画板，图案就是任务，图案之间的关系就是依赖。而调度的核心是调度任务，依赖只是表述调度的先后顺序。当前定时还是对整个工作流进行的定时，拆解后就方便对单独任务进行调度。正是基于这个思想设计了拆解的思路，所以这就需要三张表，工作流定义表、任务定义表、任务关系表。\n\n工作流定义表：描述工作流的基本信息，比如全局参数、dag中节点的位置信息\n任务定义表：描述任务的详情信息，比如任务类别、任务容错信息、优先级等\n任务关系表：描述任务的关系信息，比如当前节点、前置节点等\n\n基于这个设计思想再扩展到版本，无非是对于这三张表，每张表新增个保存版本的日志表。\n工作流定义表\n现在看案例中save接口日志，现有字段(project、process_definition_name、desc、locations、connects)，对于json中除了task之外的还剩下\n{&quot;globalParams&quot;:[],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}\n\n所以可知工作流定义表：\nCREATE TABLE `t_ds_process_definition` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;self-increasing id&#x27;,\n  `code` bigint(20) NOT NULL COMMENT &#x27;encoding&#x27;,\n  `name` varchar(200) DEFAULT NULL COMMENT &#x27;process definition name&#x27;,\n  `version` int(11) DEFAULT NULL COMMENT &#x27;process definition version&#x27;,\n  `description` text COMMENT &#x27;description&#x27;,\n  `project_code` bigint(20) NOT NULL COMMENT &#x27;project code&#x27;,\n  `release_state` tinyint(4) DEFAULT NULL COMMENT &#x27;process definition release state：0:offline,1:online&#x27;,\n  `user_id` int(11) DEFAULT NULL COMMENT &#x27;process definition creator id&#x27;,\n  `global_params` text COMMENT &#x27;global parameters&#x27;,\n  `flag` tinyint(4) DEFAULT NULL COMMENT &#x27;0 not available, 1 available&#x27;,\n  `locations` text COMMENT &#x27;Node location information&#x27;,\n  `connects` text COMMENT &#x27;Node connection information&#x27;,\n  `warning_group_id` int(11) DEFAULT NULL COMMENT &#x27;alert group id&#x27;,\n  `timeout` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;time out, unit: minute&#x27;,\n  `tenant_id` int(11) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;tenant id&#x27;,\n  `create_time` datetime NOT NULL COMMENT &#x27;create time&#x27;,\n  `update_time` datetime DEFAULT NULL COMMENT &#x27;update time&#x27;,\n  PRIMARY KEY (`id`,`code`),\n  UNIQUE KEY `process_unique` (`name`,`project_code`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_process_definition_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;self-increasing id&#x27;,\n  `code` bigint(20) NOT NULL COMMENT &#x27;encoding&#x27;,\n  `name` varchar(200) DEFAULT NULL COMMENT &#x27;process definition name&#x27;,\n  `version` int(11) DEFAULT NULL COMMENT &#x27;process definition version&#x27;,\n  `description` text COMMENT &#x27;description&#x27;,\n  `project_code` bigint(20) NOT NULL COMMENT &#x27;project code&#x27;,\n  `release_state` tinyint(4) DEFAULT NULL COMMENT &#x27;process definition release state：0:offline,1:online&#x27;,\n  `user_id` int(11) DEFAULT NULL COMMENT &#x27;process definition creator id&#x27;,\n  `global_params` text COMMENT &#x27;global parameters&#x27;,\n  `flag` tinyint(4) DEFAULT NULL COMMENT &#x27;0 not available, 1 available&#x27;,\n  `locations` text COMMENT &#x27;Node location information&#x27;,\n  `connects` text COMMENT &#x27;Node connection information&#x27;,\n  `warning_group_id` int(11) DEFAULT NULL COMMENT &#x27;alert group id&#x27;,\n  `timeout` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;time out,unit: minute&#x27;,\n  `tenant_id` int(11) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;tenant id&#x27;,\n  `operator` int(11) DEFAULT NULL COMMENT &#x27;operator user id&#x27;,\n  `operate_time` datetime DEFAULT NULL COMMENT &#x27;operate time&#x27;,\n  `create_time` datetime NOT NULL COMMENT &#x27;create time&#x27;,\n  `update_time` datetime DEFAULT NULL COMMENT &#x27;update time&#x27;,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n从表字段可以看出 日志表仅仅比主表多了两个字段operator、operate_time\n任务定义表\n案例中ab工作流task的json\n\t&quot;tasks&quot;: [{\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-77643&quot;,\n\t\t&quot;name&quot;: &quot;a&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [{\n\t\t\t\t&quot;prop&quot;: &quot;yesterday&quot;,\n\t\t\t\t&quot;direct&quot;: &quot;IN&quot;,\n\t\t\t\t&quot;type&quot;: &quot;VARCHAR&quot;,\n\t\t\t\t&quot;value&quot;: &quot;${system.biz.date}&quot;\n\t\t\t}],\n\t\t\t&quot;rawScript&quot;: &quot;echo ${yesterday}&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: []\n\t}, {\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-99814&quot;,\n\t\t&quot;name&quot;: &quot;b&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [{\n\t\t\t\t&quot;prop&quot;: &quot;today&quot;,\n\t\t\t\t&quot;direct&quot;: &quot;IN&quot;,\n\t\t\t\t&quot;type&quot;: &quot;VARCHAR&quot;,\n\t\t\t\t&quot;value&quot;: &quot;${system.biz.curdate}&quot;\n\t\t\t}],\n\t\t\t&quot;rawScript&quot;: &quot;echo ${today}&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;a&quot;]\n\t}]\n\ndep_c工作流task的json\n\t&quot;tasks&quot;: [{\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-69503&quot;,\n\t\t&quot;name&quot;: &quot;c&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 11&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;dep&quot;]\n\t}, {\n\t\t&quot;type&quot;: &quot;DEPENDENT&quot;,\n\t\t&quot;id&quot;: &quot;tasks-22756&quot;,\n\t\t&quot;name&quot;: &quot;dep&quot;,\n\t\t&quot;params&quot;: {},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {\n\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t&quot;dependTaskList&quot;: [{\n\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t&quot;dependItemList&quot;: [{\n\t\t\t\t\t&quot;projectId&quot;: 1,\n\t\t\t\t\t&quot;definitionId&quot;: 1,\n\t\t\t\t\t&quot;depTasks&quot;: &quot;b&quot;,\n\t\t\t\t\t&quot;cycle&quot;: &quot;day&quot;,\n\t\t\t\t\t&quot;dateValue&quot;: &quot;today&quot;\n\t\t\t\t}]\n\t\t\t}]\n\t\t},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: []\n\t}]\n\ncondition_test工作流task的json\n\t&quot;tasks&quot;: [{\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-68456&quot;,\n\t\t&quot;name&quot;: &quot;d&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 11&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: []\n\t}, {\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-58183&quot;,\n\t\t&quot;name&quot;: &quot;e&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 22&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;cond&quot;]\n\t}, {\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-43996&quot;,\n\t\t&quot;name&quot;: &quot;f&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 33&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;cond&quot;]\n\t}, {\n\t\t&quot;type&quot;: &quot;CONDITIONS&quot;,\n\t\t&quot;id&quot;: &quot;tasks-38972&quot;,\n\t\t&quot;name&quot;: &quot;cond&quot;,\n\t\t&quot;params&quot;: {},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;e&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;f&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {\n\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t&quot;dependTaskList&quot;: [{\n\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t&quot;dependItemList&quot;: [{\n\t\t\t\t\t&quot;depTasks&quot;: &quot;d&quot;,\n\t\t\t\t\t&quot;status&quot;: &quot;SUCCESS&quot;\n\t\t\t\t}]\n\t\t\t}]\n\t\t},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;d&quot;]\n\t}]\n\n从案例中可以知道SHELL/DEPENDENT/CONDITIONS类型的节点的json构成（其他任务类似SHELL），preTasks标识前置依赖节点。conditionResult结构比较固定，而dependence结构复杂，DEPENDENT和CONDITIONS类型任务的dependence结构还不一样，所以为了统一，我们将conditionResult和dependence整体放到params中，params对应表字段的task_params。\n这样我们就确定了t_ds_task_definition表\nCREATE TABLE `t_ds_task_definition` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `code` bigint(20) NOT NULL COMMENT 'encoding',\n  `name` varchar(200) DEFAULT NULL COMMENT 'task definition name',\n  `version` int(11) DEFAULT NULL COMMENT 'task definition version',\n  `description` text COMMENT 'description',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `user_id` int(11) DEFAULT NULL COMMENT 'task definition creator id',\n  `task_type` varchar(50) NOT NULL COMMENT 'task type',\n  `task_params` text COMMENT 'job custom parameters',\n  `flag` tinyint(2) DEFAULT NULL COMMENT '0 not available, 1 available',\n  `task_priority` tinyint(4) DEFAULT NULL COMMENT 'job priority',\n  `worker_group` varchar(200) DEFAULT NULL COMMENT 'worker grouping',\n  `fail_retry_times` int(11) DEFAULT NULL COMMENT 'number of failed retries',\n  `fail_retry_interval` int(11) DEFAULT NULL COMMENT 'failed retry interval',\n  `timeout_flag` tinyint(2) DEFAULT '0' COMMENT 'timeout flag:0 close, 1 open',\n  `timeout_notify_strategy` tinyint(4) DEFAULT NULL COMMENT 'timeout notification policy: 0 warning, 1 fail',\n  `timeout` int(11) DEFAULT '0' COMMENT 'timeout length,unit: minute',\n  `delay_time` int(11) DEFAULT '0' COMMENT 'delay execution time,unit: minute',\n  `resource_ids` varchar(255) DEFAULT NULL COMMENT 'resource id, separated by comma',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`,`code`),\n  UNIQUE KEY `task_unique` (`name`,`project_code`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_task_definition_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `code` bigint(20) NOT NULL COMMENT 'encoding',\n  `name` varchar(200) DEFAULT NULL COMMENT 'task definition name',\n  `version` int(11) DEFAULT NULL COMMENT 'task definition version',\n  `description` text COMMENT 'description',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `user_id` int(11) DEFAULT NULL COMMENT 'task definition creator id',\n  `task_type` varchar(50) NOT NULL COMMENT 'task type',\n  `task_params` text COMMENT 'job custom parameters',\n  `flag` tinyint(2) DEFAULT NULL COMMENT '0 not available, 1 available',\n  `task_priority` tinyint(4) DEFAULT NULL COMMENT 'job priority',\n  `worker_group` varchar(200) DEFAULT NULL COMMENT 'worker grouping',\n  `fail_retry_times` int(11) DEFAULT NULL COMMENT 'number of failed retries',\n  `fail_retry_interval` int(11) DEFAULT NULL COMMENT 'failed retry interval',\n  `timeout_flag` tinyint(2) DEFAULT '0' COMMENT 'timeout flag:0 close, 1 open',\n  `timeout_notify_strategy` tinyint(4) DEFAULT NULL COMMENT 'timeout notification policy: 0 warning, 1 fail',\n  `timeout` int(11) DEFAULT '0' COMMENT 'timeout length,unit: minute',\n  `delay_time` int(11) DEFAULT '0' COMMENT 'delay execution time,unit: minute',\n  `resource_ids` varchar(255) DEFAULT NULL COMMENT 'resource id, separated by comma',\n  `operator` int(11) DEFAULT NULL COMMENT 'operator user id',\n  `operate_time` datetime DEFAULT NULL COMMENT 'operate time',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n注意：dev版本和1.3.6版本区别，dev版本已将description换成desc，并且新增了delayTime\n{\n\t&quot;globalParams&quot;: [],\n\t&quot;tasks&quot;: [{\n\t\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-18200&quot;,\n\t\t\t&quot;name&quot;: &quot;d&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {\n\t\t\t\t&quot;resourceList&quot;: [],\n\t\t\t\t&quot;localParams&quot;: [],\n\t\t\t\t&quot;rawScript&quot;: &quot;echo 5&quot;\n\t\t\t},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [],\n\t\t\t&quot;depList&quot;: null\n\t\t},\n\t\t{\n\t\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-55225&quot;,\n\t\t\t&quot;name&quot;: &quot;e&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {\n\t\t\t\t&quot;resourceList&quot;: [],\n\t\t\t\t&quot;localParams&quot;: [],\n\t\t\t\t&quot;rawScript&quot;: &quot;echo 6&quot;\n\t\t\t},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [\n\t\t\t\t&quot;def&quot;\n\t\t\t],\n\t\t\t&quot;depList&quot;: null\n\t\t},\n\t\t{\n\t\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-67639&quot;,\n\t\t\t&quot;name&quot;: &quot;f&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {\n\t\t\t\t&quot;resourceList&quot;: [],\n\t\t\t\t&quot;localParams&quot;: [],\n\t\t\t\t&quot;rawScript&quot;: &quot;echo 7&quot;\n\t\t\t},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [\n\t\t\t\t&quot;def&quot;\n\t\t\t],\n\t\t\t&quot;depList&quot;: null\n\t\t},\n\t\t{\n\t\t\t&quot;type&quot;: &quot;CONDITIONS&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-67387&quot;,\n\t\t\t&quot;name&quot;: &quot;def&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;e&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;f&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {\n\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t&quot;dependTaskList&quot;: [{\n\t\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t\t&quot;dependItemList&quot;: [{\n\t\t\t\t\t\t\t&quot;depTasks&quot;: &quot;d&quot;,\n\t\t\t\t\t\t\t&quot;status&quot;: &quot;SUCCESS&quot;\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t&quot;depTasks&quot;: &quot;d&quot;,\n\t\t\t\t\t\t\t&quot;status&quot;: &quot;FAILURE&quot;\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}]\n\t\t\t},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [\n\t\t\t\t&quot;d&quot;\n\t\t\t],\n\t\t\t&quot;depList&quot;: null\n\t\t}\n\t],\n\t&quot;tenantId&quot;: 1,\n\t&quot;timeout&quot;: 0\n}\n\n任务关系表\npreTasks标识前置依赖节点，当前节点在关系表中使用postTask标识。由于当前节点肯定存在而前置节点不一定存在，所以post不可能为空，而preTask有可能为空\nCREATE TABLE `t_ds_process_task_relation` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `name` varchar(200) DEFAULT NULL COMMENT 'relation name',\n  `process_definition_version` int(11) DEFAULT NULL COMMENT 'process version',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `process_definition_code` bigint(20) NOT NULL COMMENT 'process code',\n  `pre_task_code` bigint(20) NOT NULL COMMENT 'pre task code',\n  `pre_task_version` int(11) NOT NULL COMMENT 'pre task version',\n  `post_task_code` bigint(20) NOT NULL COMMENT 'post task code',\n  `post_task_version` int(11) NOT NULL COMMENT 'post task version',\n  `condition_type` tinyint(2) DEFAULT NULL COMMENT 'condition type : 0 none, 1 judge 2 delay',\n  `condition_params` text COMMENT 'condition params(json)',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_process_task_relation_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `name` varchar(200) DEFAULT NULL COMMENT 'relation name',\n  `process_definition_version` int(11) DEFAULT NULL COMMENT 'process version',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `process_definition_code` bigint(20) NOT NULL COMMENT 'process code',\n  `pre_task_code` bigint(20) NOT NULL COMMENT 'pre task code',\n  `pre_task_version` int(11) NOT NULL COMMENT 'pre task version',\n  `post_task_code` bigint(20) NOT NULL COMMENT 'post task code',\n  `post_task_version` int(11) NOT NULL COMMENT 'post task version',\n  `condition_type` tinyint(2) DEFAULT NULL COMMENT 'condition type : 0 none, 1 judge 2 delay',\n  `condition_params` text COMMENT 'condition params(json)',\n  `operator` int(11) DEFAULT NULL COMMENT 'operator user id',\n  `operate_time` datetime DEFAULT NULL COMMENT 'operate time',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n对于依赖关系复杂的场景\n\n3、API模块如何改造\n\n\n[ ] api模块进行save操作时\n\n\n通过雪花算法生成13位的数字作为process_definition_code，工作流定义保存至process_definition（主表）和process_definition_log（日志表），这两个表保存的是同样的数据，工作流定义版本为1\n通过雪花算法生成13位的数字作为task_definition_code，任务定义表保存至task_definition（主表）和task_definition_log（日志表），也是保存同样的数据，任务定义版本为1\n工作流任务关系保存在 process_task_relation（主表）和process_task_relation_log（日志表），该表保存的code和version是工作流的code和version，因为任务是通过工作流进行组织，以工作流来画dag。也是通过post_task_code和post_task_version标识dag的当前节点，这个节点的前置依赖通过pre_task_code和pre_task_version来标识，如果没有依赖，pre_task_code和pre_task_version为0\n\n\n\n[ ] api模块进行update操作时，工作流定义和任务定义直接更新主表数据，更新后的数据insert到日志表。关系表主表先删除然后再插入新的关系，关系表日志表直接插入新的关系\n\n\n[ ] api模块进行delete操作时，工作流定义、任务定义和关系表直接删除主表数据，日志表数据不变动\n\n\n[ ] api模块进行switch操作时，直接将日志表中对应version数据覆盖到主表\n\n\n4、数据交互如何改造\n\n\n\n[ ] 在json拆分一期api模块controller层整体未变动，传入的大json还是在service层映射为ProcessData对象。insert或update操作在公共Service模块通过ProcessService.saveProcessDefiniton()入口完成保存数据库操作，按照task_definition、process_task_relation、process_definition的顺序保存。保存时，如果该任务已经存在并且关联的工作流未上线，则更改任务；如果任务关联的工作流已上线，则不允许更改任务\n\n\n[ ] api查询操作时，当前还是通过工作流id来查询，在公共Service模块通过ProcessService.genTaskNodeList()入口完成数据组装，还是组装为ProcessData对象，进而生成json返回\n\n\n[ ] Server模块（Master）也是通过公共Service模块ProcessService.genTaskNodeList()获得TaskNodeList生成调度dag，把当前任务所有信息放到 MasterExecThread.readyToSubmitTaskQueue队列，以便生成taskInstance，dispatch给worker\n\n\n5、当前json还需做什么\n\ncontroller对外restAPI接口改造\nui模块dag改造\nui模块新增task操作页面\n\nprocessDefinition\n\n\n\n接口名称\n参数名称\n参数说明\n数据类型\n\n\n\n\n\nprojectName\n项目名称\nstring\n\n\n\nname\n流程定义名称\nstring\n\n\n\ndescription\n流程定义描述信息\nstring\n\n\n\nglobalParams\n全局参数\nstring\n\n\nsave\nconnects\n流程定义节点图标连接信息(json格式)\nstring\n\n\n\nlocations\n流程定义节点坐标位置信息(json格式)\nstring\n\n\n\ntimeout\n超时分钟数\nint\n\n\n\ntenantId\n租户id\nint\n\n\n\ntaskRelationJson\n任务关系(json格式)\nstring\n\n\n--\n--\n--\n--\n\n\n\nprojectName\n项目名称\nstring\n\n\n\ncode\n流程定义code\nlong\n\n\n\nname\n流程定义名称\nstring\n\n\n\ndescription\n流程定义描述信息\nstring\n\n\nupdate\nreleaseState\n发布流程定义,可用值:OFFLINE,ONLINE\nref\n\n\n\nconnects\n流程定义节点图标连接信息(json格式)\nstring\n\n\n\nlocations\n流程定义节点坐标位置信息(json格式)\nstring\n\n\n\ntimeout\n超时分钟数\nint\n\n\n\ntenantId\n租户id\nint\n\n\n\ntaskRelationJson\n任务关系(json格式)\nstring\n\n\n\n--\n--\n--\n\n\n\ncode\n流程定义code\nlong\n\n\nswitch/deleteCode\nprojectName\n项目名称\nstring\n\n\n\nversion\n版本号\nstring\n\n\n\n备注：taskRelationJson格式：[{&quot;name&quot;:&quot;&quot;,&quot;pre_task_code&quot;:0,&quot;pre_task_version&quot;:0,&quot;post_task_code&quot;:123456789,&quot;post_task_version&quot;:1,&quot;condition_type&quot;:0,&quot;condition_params&quot;:{}},{&quot;name&quot;:&quot;&quot;,&quot;pre_task_code&quot;:123456789,&quot;pre_task_version&quot;:1,&quot;post_task_code&quot;:123451234,&quot;post_task_version&quot;:1,&quot;condition_type&quot;:0,&quot;condition_params&quot;:{}}]\n同理其他接口请求参数processDefinitionId换成code\nschedule\n\n\n\n接口名称\n参数名称\n参数说明\n数据类型\n\n\n\n\n\ncode\n流程定义code\nlong\n\n\n\nprojectName\n项目名称\nstring\n\n\n\nfailureStrategy\n失败策略,可用值:END,CONTINUE\nstring\n\n\ncreateSchedule\nprocessInstancePriority\n流程实例优先级,可用值:HIGHEST,HIGH,MEDIUM,LOW,LOWEST\nstring\n\n\n\nschedule\n定时\nstring\n\n\n\nwarningGroupId\n发送组ID\nint\n\n\n\nwarningType\n发送策略,可用值:NONE,SUCCESS,FAILURE,ALL\nstring\n\n\n\nworkerGroup\nworkerGroup\nstring\n\n\n--\n--\n--\n--\n\n\n\ncode\n流程定义code\nlong\n\n\n\nprojectName\n项目名称\nstring\n\n\nqueryScheduleListPaging\npageNo\n页码号\nint\n\n\n\npageSize\n页大小\nint\n\n\n\nsearchVal\n搜索值\nstring\n\n\n\ntaskDefinition(新增)\n\n\n\n接口名称\n参数名称\n参数说明\n数据类型\n\n\n\n\nsave\nprojectName\n项目名称\n\n\n\n\ntaskDefinitionJson\ntask信息\nstring\n\n\n--\n--\n--\n--\n\n\n\nprojectName\n项目名称\nstring\n\n\nupdate\ncode\ntask code\nlong\n\n\n\ntaskDefinitionJson\ntask信息\nstring\n\n\n--\n--\n--\n--\n\n\n\nprojectName\n项目名称\nstring\n\n\nswitch/deleteCode\ntask_code\ntaskCode\nlong\n\n\n\nversion\n版本号\nint\n\n\n--\n--\n--\n--\n\n\nquery/delete\nprojectName\n项目名称\nstring\n\n\n\ntask_code\ntask code\nlong\n\n\n--\n--\n--\n--\n\n\nqueryTaskListPaging\n\n\n\n\n\n\ntaskDefinitionJson：[{&quot;name&quot;:&quot;test&quot;,&quot;description&quot;:&quot;&quot;,&quot;task_type&quot;:&quot;SHELL&quot;,&quot;task_params&quot;:[],&quot;flag&quot;:0,&quot;task_priority&quot;:0,&quot;worker_group&quot;:&quot;default&quot;,&quot;fail_retry_times&quot;:0,&quot;fail_retry_interval&quot;:0,&quot;timeout_flag&quot;:0,&quot;timeout_notify_strategy&quot;:0,&quot;timeout&quot;:0,&quot;delay_time&quot;:0,&quot;resource_ids&quot;:&quot;&quot;}]\n对应需求issue\n[Feature][JsonSplit-api] api module controller design #5498 \n1. [Feature][JsonSplit-api]processDefinition save/update interface  #5499 \n2. [Feature][JsonSplit-api]processDefinition switch interface #5501 \n3. [Feature][JsonSplit-api]processDefinition delete interface #5502 \n4. [Feature][JsonSplit-api]processDefinition copy interface #5503 \n5. [Feature][JsonSplit-api]processDefinition export interface #5504 \n6. [Feature][JsonSplit-api]processDefinition list-paging interface #5505 \n7. [Feature][JsonSplit-api]processDefinition move interface #5506 \n8. [Feature][JsonSplit-api]processDefinition queryProcessDefinitionAllByProjectId interface #5507 \n9. [Feature][JsonSplit-api]processDefinition select-by-id interface #5508 \n10. [Feature][JsonSplit-api]processDefinition view-tree interface #5509 \n11. [Feature][JsonSplit-api]schedule create interface #5510 \n12. [Feature][JsonSplit-api]schedule list-paging interface #5511 \n13. [Feature][JsonSplit-api]schedule update interface #5512 \n14. [Feature][JsonSplit-api]taskDefinition save interface #5513 \n15. [Feature][JsonSplit-api]taskDefinition update interface #5514 \n16. [Feature][JsonSplit-api]taskDefinition switch interface #5515 \n17. [Feature][JsonSplit-api]taskDefinition query interface #5516 \n18. [Feature][JsonSplit-api]taskDefinition delete interface #5517 \n19. [Feature][JsonSplit-api]WorkFlowLineage interface #5518 \n20. [Feature][JsonSplit-api]analysis interface #5519 \n21. [Feature][JsonSplit-api]executors interface #5520 \n22. [Feature][JsonSplit-api]processInstance interface #5521 \n23. [Feature][JsonSplit-api]project interface #5522 \n\n",
    "title": "DolphinScheduler 核心之 DAG 大 JSON 拆分详解",
    "time": "2021-05-29"
  },
  {
    "name": "ipalfish_tech_platform",
    "content": "伴鱼数据质量平台实践\n伴鱼少儿英语是目前飞速成长的互联网在线英语教育品牌，期望打造更创新、更酷、让学英语更有效的新一代互联网产品。博客官网\n日常工作中，数据开发、数仓开发工程师开发上线完一个任务后并不是就可以高枕无忧了，时常会因为上游链路数据异常或者自身处理逻辑的 BUG 导致产出的数据结果不可信。而这个问题的发现可能会经历一个较长的周期（尤其是离线场景），往往是业务方通过上层数据报表发现数据异常后 push 数据方去定位问题（对于一个较冷的报表，这个周期可能会更长）。同时，由于数据加工链路较长需要借助数据的血缘关系逐个任务排查，也会导致问题的定位难度增大，严重影响开发人员的工作效率。更有甚者，如果数据问题没有被及时发现，可能导致业务方作出错误的决策。此类问题可统一归属为大数据领域数据质量的问题。\n本文将向大家介绍伴鱼基础架构数据团队在应对该类问题时推出的平台化产品 - 数据质量中心（Data Quality Center, DQC）的设计与实现，但这与 DolphinScheduler 有什么关系呢？且听伴鱼的伙伴细细道来。\n调研\n业内关于数据质量平台化的产品介绍不多，我们主要对两个开源产品和一个云平台产品进行了调研，下面将主要介绍开源产品。\nApache Griffin\nApache Griffin是 eBay 开源的一款基于 Apache Hadoop 和 Apache Spark 的数据质量服务平台。其架构图如下：\n\n架构图从 High Level 层面清晰地展示了数据质量平台的三个核心流程：\n\n\nDefine：数据质检规则（指标）的定义。\n\n\nMeasure：数据质检任务的执行，基于 Spark 引擎实现。\n\n\nAnalyze：数据质检结果量化及可视化展示。同时，平台对数据质检规则进行了分类（这也是目前业内普遍认可的数据质量的六大标准）:\n\n\nAccuracy：准确性。如是否符合表的加工逻辑。\n\n\nCompleteness：完备性。如数据是否存在丢失。\n\n\nTimeliness：及时性。如表数据是否按时产生。\n\n\nUniqueness：唯一性。如主键字段是否唯一。\n\n\nValidity：合规性。如字段长度是否合规、枚举值集合是否合规。\n\n\nConsistency：一致性。如表与表之间在某些字段上是否存在矛盾。\n\n\n目前该开源项目仅在 Accuracy 类的规则上进行了实现。\nGriffin 是一个完全闭环的平台化产品。其质检任务的执行依赖于内置定时调度器的调度，调度执行时间由用户在 UI 上设定。任务将通过 Apache Livy 组件提交至配置的 Spark 集群。这也就意味着质检的实时性难以保障，我们无法对产出异常数据的任务进行强行阻断，二者不是在同一个调度平台被调度，时序上也不能保持串行。\nQualitis\nQualitis  是微众银行开源的一款数据质量管理系统。同样，它提供了一整套统一的流程来定义和检测数据集的质量并及时报告问题。从整个流程上看我们依然可以用 Define、Measure 和 Analyze 描述。它是基于其开源的另一款组件 Linkis 进行计算任务的代理分发，底层依赖 Spark 引擎，同时可以与其开源的 DataSphereStudio 任务开发平台无缝衔接，也就实现了在任务执行的工作流中嵌入质检任务，满足质检时效性的要求。可见，Qualitis 需要借助微众银行开源的一系列产品才能达到满意的效果。\nDataWorks 数据质量\nDataWorks 是阿里云上提供的一站式大数据工场，其中就包括了数据质量在内的产品解决方案。同样，它的实现依赖于阿里云上其他产品组件的支持。不过不得不说 DataWorks 数据质量部分的使用介绍从产品形态上给了我们很大的帮助，对于我们的产品设计非常具有指导性的作用。\n设计目标\n经过一番调研，我们确定了 DQC 的设计目标，主要包括以下几点：\n\n目前暂且只支持离线部分的数据质量管理。\n支持通用的规则描述和规则管理。\n质检任务由公司内部统一的调度引擎调度执行，可支持对质检结果异常的任务进行强阻断。同时，尽量降低质检功能对调度引擎的代码侵入。\n支持质检结果的可视化。\n\nDQC 系统设计\n背景补充\n伴鱼离线调度开发平台是基于 Apache DolphinScheduler 实现的。它是一个分布式去中心化，易扩展的可视化 DAG 调度系统，支持包括 Shell、Python、Spark、Flink 等多种类型的 Task 任务，并具有很好的扩展性。架构如下图所示：\n\nMaster 节点负责任务的监听和调度，Worker 节点则负责任务的执行。值得注意的是，每一个需要被调度的任务必然需要设置一个调度时间的表达式（cron 表达式），由 Quartz 定时为任务生成待执行的 DAG Command，有且仅有一个 Master 节点获得执行权，掌管该 DAG 各任务节点的调度执行。\n数据质量平台整体架构\n以下是数据质量平台整体的架构图：\n\n由以下几部分组成：\n\nDQC Web UI：质检规则等前端操作页面。\nDQC(GO)：简单的实体元数据管理后台。主要包括：规则、规则模板、质检任务和质检结果几个实体。\nDS(数据质量部分)：质检任务依赖 DolphinScheduler 调度执行，需要对 DolphinScheduler 进行一定的改造。\nDQC SDK(JAR)：DolphinScheduler 调度执行任务时，检测到任务绑定了质检规则，将生成一类新的任务 DQC Task （与 DolphinScheduler 中其他类型的 Task 同级，DolphinScheduler 对于 Task 进行了很好的抽象可以方便扩展），本质上该 Task 将以脚本形式调用执行 DQC SDK 的逻辑。DQC SDK 涵盖了规则解析、执行的全部逻辑。\n\n下文主要阐述我们在各模块设计上的一些思考和权衡。\n规则表述\n标准与规则\n前文在调研部分提及了业内普遍认可的数据质量的六大标准。那么问题来了：\n\n如何将标准与平台的规则对应起来？\n标准中涉及到的现实场景是否我们可以一一枚举？\n即便我们可以将标准一一细化，数据开发人员是否可以轻松的理解？\n\n可以将这些问题统一归类为：平台在规则设定上是否需要和业界数据质量标准所抽象出来的概念进行绑定。很遗憾我们并没有找到有关数据质量标准更加细化和指导性的描述，事实上作为一个开发人员这些概念对于我来说是比较费解的，而更贴近程序员视角的方式是「show me the code」，因此我们决定将这一层概念弱化。未来更深入的实践过程后再做更细化的思考。\n标量化\n接下来我们着重讨论下另一个问题：如何对规则提供一种通用的描述（or maybe a kind of DSL）？\n其实当我们跳脱出前文所描述的一切背景和概念，仔细思考下数据质检的过程，会发现本质上就是通过一次真实的任务执行产出结果，然后对比输出结果与期望是否满足，以验证任务逻辑的正确性。这个过程可形象得和 Unit Testing 进行类比，只不过 Unit Testing 是通过模拟数据构造的一次代码逻辑的执行。另外数据任务执行产生的结果是一张二维结构的 Hive 表，需要进行加工方能获取到想要的统计结果，这也是两者的区别之一。\n顺着这个思路，我们可以利用 Unit Testing 的概念从以下三方面继续深入：\nActual Value\n数据任务执行产出的结果是一张 Hive 表，我们需要对这张 Hive 表的数据进行加工、提取以获得需要的 Actual Value。涉及到对 Hive 表的加工，必然想到是以 SQL 的方式来实现，通过 Query 和 一系列 Aggregation 操作拿到结果，此结果的结构又可分为以下三类：\n\n\n二维数组\n\n\n单行或者单列的一维数组\n\n\n单行且单列的标量\n\n\n显然单行且单列的标量是我们期望得到的，因为它更易于结果的比较（事实上就目前我们所能想到的规则，都可以通过 SQL 方式提取为一个标量结果）。因此，在规则设计中，需要规则创建者输入一段用于结果提取的 SQL，该段 SQL 的执行结果需要为一个标量。\nExpected Value\n既然 Actual Value 是一个标量，那么 Expected Value 同样也是一个标量，需要规则创建者在平台输入。\nAssert\n上述标量的类型决定了断言的比较方式。目前我们只支持了数值型标量的比较方式，包含「大于」、「等于」及「小于」三种比较算子。如出现其他类型标量，需要扩充比较的方式。\n以上三要素即可完整的描述规则想要表达的核心逻辑。如我们想要表述「字段为空异常」的规则（潜在含义：字段为空的行数大于 0 时判定异常），就可以通过以下设定满足：\n\n\nActual Value ：出现字段为空的行数\ncount(case when ${field} is null then 1 else null end)\n\n\n\nExpected Value : 0\n\n\nAssert：「大于」\n\n\n规则管理\n规则模板\n规则模板是为了规则复用抽象出的一个概念，模板中包含规则的 SQL 定义、规则的比较方式、参数定义（注：SQL 中包含一些占位符，这些占位符将以参数的形式被定义，在规则实体定义时需要用户明确具体含义）以及其他的一些元信息。下图为「字段空值的行数」模板的示例：\n\n规则实体\n规则实体是基于规则模板构建的，是规则的具象表达。在规则实体中将明确规则的 Expected Value、比较方式中具体的比较算子、参数的含义以及其他的一些元信息。基于同一个规则模板，可以构造多个规则实体。下图为「某表 user_id 唯一性校验」规则的示例：\n\n值得一提的是，规则可能不仅仅只是针对单表的校验，对于多表的情况我们这套规则模板同样是适用的，只要我们可以将逻辑使用 SQL 表达。\n规则绑定\n在 DolphinScheduler 的前端交互上支持为任务直接绑定校验规则，规则列表通过 API 从 DQC 获取，这种方式在用户的使用体验上存在一定的割裂（规则创建和绑定在两个平台完成）。同时，在 DQC 的前端亦可以直接设置关联调度，为已有任务绑定质检规则，任务列表通过 API 从 DolphinScheduler 获取。同一个任务可绑定多个质检规则，这些信息将存储至 DolphinScheduler 的 DAG 元信息中。那么这里需要考虑几个问题：\n\n规则的哪些信息应该存储至 DAG 的元信息中？\n规则的更新 DAG 元信息是否可以实时同步？\n\n主要有两种方式：\n\n以大 Json 方式将规则信息打包存储，计算时解析 Json 逐个执行校验。在规则更新时，需要同步调用修改 Json 信息。\n以 List 方式存储规则 ID，计算时需执行一次 Pull 操作获取规则具体信息然后执行校验。规则更新，无须同步更新 List 信息。\n\n我们选择了后者，ID List 方式可以使对 DolphinScheduler 的侵入降到最低。\n\n规则执行\n规则的强弱性质由用户为任务绑定规则时设定，此性质决定了规则执行的方式。\n强规则\n和当前所执行的任务节点同步执行，一旦规则检测失败整个任务节点将置为执行失败的状态，后续任务节点的执行会被阻断。对应 DolphinScheduler 中的执行过程表述如下：\n\n某一个 Master 节点获取 DAG 的执行权，将 DAG 拆分成不同的 Job Task 先后下发给 Worker 节点执行。\n执行 Job Task 逻辑，并设置 Job Task 的 ExitStatusCode。\n判断 Job Task 是否绑定了强规则。若是，则生成 DQC Task 并触发执行，最后根据执行结果修正 Job Task 的 ExitStatusCode。\nMaster 节点根据 Job Task 的 ExitStatusCode 判定任务是否成功执行，继续进入后续的调度逻辑。\n\n弱规则\n和当前所执行的任务节点异步执行，规则检测结果对于原有的任务执行状态无影响，从而也就不能阻断后续任务的执行。对应 DolphinScheduler 中的执行过程表述如下：\n\n某一个 Master 节点获取 DAG 的执行权，将 DAG 拆分成不同的 Job Task 先后下发给 Worker 节点执行。\n执行 Job Task 逻辑，并设置 Job Task 的 ExitStatusCode。\n判断 Job Task 是否绑定了弱规则。若是，则在 Job Task 的 Context 中设置弱规则的标记 。\nMaster 节点根据 Job Task 的 ExitStatusCode 判定任务是否成功执行，若成功执行再判定是否 Context 中带有弱规则标记，若有则生成一个新的 DAG（有且仅有一个 DQC Task，且新生成的 DAG 与 当前执行的 DAG 没有任何的关联） 然后继续进入后续的调度逻辑。\n各 Master 节点竞争新生成的 DAG 的执行权。\n\n可以看出在强弱规则的执行方式上，对 DolphinScheduler 调度部分的代码有一定的侵入，但这个改动不大，成本是可以接受的。\nDQC Task &amp; DQC SDK\n上文提及到一个 Job Task 绑定的规则（可能有多个）将被转换为一个 DQC Task 被 DolphinScheduler 调度执行，接下来我们就讨论下 DQC Task 的实现细节以及由此引出的 DQC SDK 的设计和实现。\nDQC Task 继承自 DolphinScheduler 中的抽象类 AbstractTask，只需要实现抽象方法 handle（任务执行的具体实现）即可。那么对于我们的质检任务，实际上执行逻辑可以拆分成以下几步：\n\n提取 Job Task 绑定的待执行的 Rule ID List。\n拉取各个 Rule ID 对应的详情信息。\n构建完整的执行 Query 语句（将规则参数填充至模板 SQL 中）。\n执行 Query。\n执行 Asset。\n\n最核心的步骤为 Query 的执行。Query 的实现方式又可分为两种：\nSpark 实现\n\n优点：实现可控，灵活性更高。\n缺点：配置性要求较高。\n\nPresto SQL 实现\n\n优点：不需要额外配置，开发量少，拼接 SQL 即可。\n缺点：速度没有 Spark 快。\n\n我们选择了后者，这种方式最易实现，离线场景这部分的计算耗时也可以接受。同时由于一个 DQC Task 包含多条规则，在拼接 SQL 时将同表的规则聚合以减少 IO 次数。不同的 SQL 交由不同的线程并行执行。\n上述执行逻辑其实是一个完整且闭环的功能模块，因此我们想到将其作为一个单独的 SDK 对外提供，并以 Jar 包的形式被 DolphinScheduler 依赖，后续即便是更换调度引擎，这部分的逻辑可直接迁移使用（当然概率很低）。那么 DolphinScheduler 中 DQC Task 的 handle 逻辑也就变得异常简单，直接以 Shell 形式调用 SDK ，进一步降低对 DolphinScheduler 代码的侵入。\n执行结果\n单条规则的质检结果将在平台上直接展现，目前我们还未对任务级的规则进行聚合汇总，这是接下来需要完善的。对于质检失败的任务将向报警接收人发送报警。\n\n实践中的问题\n平台解决了规则创建、规则执行的问题，而在实践过程中，对用户而言更关心的问题是：\n\n一个任务应该需要涵盖哪些的规则才能有效地保证数据的质量？\n我们不可能对全部的表和字段都添加规则，那么到底哪些是需要添加的？\n\n这些是很难通过平台自动实现的，因为平台理解不了业务的信息，平台能做的只能是通过质量检测报告给与用户反馈。因此这个事情需要具体的开发人员对核心场景进行梳理，在充分理解业务场景后根据实际情况进行设定。话又说回来，平台只是工具，每一个数据开发人员应当提升保证数据质量的意识，这又涉及到组织内规范落地的问题了。\n未来工作\n数据质量管理是一个长期的过程，未来在平台化方向我们还有几个关键的部分有待继续推进：\n\n基于血缘关系建立全链路的数据质量监控。当前的监控粒度是任务级的，如果规则设置的是弱规则，下游对于数据问题依旧很难感知。\n数据质量的结果量化。需要建立起一套指标用于定量地衡量数据的质量。\n支持实时数据的质量检测。\n\n",
    "title": "伴鱼数据质量平台实践",
    "time": "2021-07-06"
  },
  {
    "name": "json_split",
    "content": "为什么要把 DolphinScheduler 工作流定义中保存任务及关系的大 json 给拆了?\n背景\n当前 DolphinScheduler 的工作流中的任务及关系保存时是以大 json 的方式保存到数据库中 process_definiton 表的 process_definition_json 字段，如果某个工作流很大比如有 1000 个任务，这个 json 字段也就随之变得非常大，在使用时需要解析 json，非常耗费性能，且任务没法重用，故社区计划启动 json 拆分项目。可喜的是目前我们已经完成了这个工作的大部分，因此总结一下，供大家参考学习。\n总结\njson split 项目从 2021-01-12 开始启动，到 2021-04-25 初步完成主要开发。代码已合入 dev 分支。感谢 lenboo、JinyLeeChina、simon824、wen-hemin 四位伙伴参与 coding。\n主要变化以及贡献如下：\n\n代码变更 12793 行\n修改/增加 168 个文件\n共 145 次 Commits\n有 85 个 PR\n\n拆分方案回顾\n\n\n[ ] api 模块进行 save 操作时\n\n\n通过雪花算法生成 13 位的数字作为 process_definition_code，工作流定义保存至 process_definition（主表）和 process_definition_log（日志表），这两个表保存的是同样的数据，工作流定义版本为 1\n通过雪花算法生成 13 位的数字作为 task_definition_code，任务定义表保存至 task_definition（主表）和 task_definition_log（日志表），也是保存同样的数据，任务定义版本为 1\n工作流任务关系保存在 process_task_relation（主表）和 process_task_relation_log（日志表），该表保存的 code 和 version 是工作流的 code 和 version，因为任务是通过工作流进行组织，以工作流来画 dag。也是通过 post_task_code 和 post_task_version 标识 dag 的当前节点，这个节点的前置依赖通过 pre_task_code 和 pre_task_version 来标识，如果没有依赖，pre_task_code 和 pre_task_version 为 0\n\n\n\n[ ] api 模块进行 update 操作时，工作流定义和任务定义直接更新主表数据，更新后的数据 insert 到日志表。关系表主表先删除然后再插入新的关系，关系表日志表直接插入新的关系\n\n\n[ ] api 模块进行 delete 操作时，工作流定义、任务定义和关系表直接删除主表数据，日志表数据不变动\n\n\n[ ] api 模块进行 switch 操作时，直接将日志表中对应 version 数据覆盖到主表\n\n\nJson 存取方案\n\n\n\n[ ] 当前一期拆分方案，api 模块 controller 层整体未变动，传入的大 json 还是在 service 层映射为 ProcessData 对象。insert 或 update 操作在公共 Service 模块通过 ProcessService.saveProcessDefiniton() 入口完成保存数据库操作，按照 task_definition、process_task_relation、process_definition 的顺序保存。保存时，如果该任务已经存在并且关联的工作流未上线，则更改任务；如果任务关联的工作流已上线，则不允许更改任务\n\n\n[ ] api 查询操作时，当前还是通过工作流 id 来查询，在公共 Service 模块通过 ProcessService.genTaskNodeList() 入口完成数据组装，还是组装为 ProcessData 对象，进而生成 json 返回\n\n\n[ ] Server 模块（Master）也是通过公共 Service 模块 ProcessService.genTaskNodeList() 获得 TaskNodeList 生成调度 dag，把当前任务所有信息放到 MasterExecThread.readyToSubmitTaskQueue 队列，以便生成 taskInstance、dispatch 给 worker\n\n\n二期规划\napi/ui 模块改造\n\n[ ] processDefinition 接口通过 processDefinitionId 请求后端的替换为 processDefinitonCode\n[ ] 支持 task 的单独定义，当前 task 的插入及修改是通过工作流来操作的，二期需要支持单独定义\n[ ] 前端及后端 controller 层 json 拆分，一期已完成 api 模块 service 层到 dao 的 json 拆分，二期需要完成前端及 controller 层 json 拆分\n\nserver 模块改造\n\n\n[ ] t_ds_command、t_ds_error_command、t_ds_schedules 中 process_definition_id 替换为 process_definition_code\n\n\n[ ] 生成 taskInstance 流程改造\n\n\n当前生成 process_instance，是通过 process_definition 和 schedules 及 command 表生成，而生成 taskInstance 还是来源于 MasterExecThread.readyToSubmitTaskQueue 队列，并且队列中数据来源于 dag 对象。此时，该队列及 dag 中保存 taskInstance 的所有信息，这种方式非常占用内存。可改造为如下图的数据流转方式，readyToSubmitTaskQueue 队列及 dag 中保存任务 code 和版本信息，在生成 task_instance 前，查询 task_definition\n\n\n附录：雪花算法（snowflake）\n雪花算法（snowflake）： 是一种生成分布式全剧唯一 ID 的算法，生成的 ID 称为 snowflake，这种算法是由 Twitter 创建，并用于推文的 ID。\n一个 Snowflake ID 有 64 bit。前 41 bit 是时间戳，表示了自选定的时期以来的毫秒数。 接下来的 10 bit 代表计算机 ID，防止冲突。 其余 12 bit 代表每台机器上生成 ID 的序列号，这允许在同一毫秒内创建多个 Snowflake ID。SnowflakeID 基于时间生成，故可以按时间排序。此外，一个 ID 的生成时间可以由其自身推断出来，反之亦然。该特性可以用于按时间筛选 ID，以及与之联系的对象。\n雪花算法的结构：\n\n主要分为 5 个部分：\n\n是 1 个 bit：0，这个是无意义的；\n是 41 个 bit：表示的是时间戳；\n是 10 个 bit：表示的是机房 id，0000000000，因为此时传入的是 0；\n是 12 个 bit：表示的序号，就是某个机房某台机器上这一毫秒内同时生成的 id 的序号，0000 0000 0000。\n\n接下去我们来解释一下四个部分：\n1 bit，是无意义的：\n因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。\n41 bit：表示的是时间戳，单位是毫秒。\n41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2 ^ 41 - 1 个毫秒值，换算成年就是表示 69 年的时间。\n10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。\n但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2 ^ 5 个机房（32 个机房），每个机房里可以代表 2 ^ 5 个机器（32 台机器），这里可以随意拆分，比如拿出 4 位标识业务号，其他 6 位作为机器号。可以随意组合。\n12 bit：这个是用来记录同一个毫秒内产生的不同 id。\n12 bit 可以代表的最大正整数是 2 ^ 12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。也就是同一毫秒内同一台机器所生成的最大 ID 数量为 4096\n简单来说，你的某个服务假设要生成一个全局唯一 id，那么就可以发送一个请求给部署了 SnowFlake 算法的系统，由这个 SnowFlake 算法系统来生成唯一 id。这个 SnowFlake 算法系统首先肯定是知道自己所在的机器号，（这里姑且讲 10bit 全部作为工作机器 ID）接着 SnowFlake 算法系统接收到这个请求之后，首先就会用二进制位运算的方式生成一个 64 bit 的 long 型 id，64 个 bit 中的第一个 bit 是无意义的。接着用当前时间戳（单位到毫秒）占用 41 个 bit，然后接着 10 个 bit 设置机器 id。最后再判断一下，当前这台机房的这台机器上这一毫秒内，这是第几个请求，给这次生成 id 的请求累加一个序号，作为最后的 12 个 bit。\nSnowFlake 的特点是：\n\n毫秒数在高位，自增序列在低位，整个 ID 都是趋势递增的。\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成 ID 的性能也是非常高的。\n可以根据自身业务特性分配 bit 位，非常灵活。\n\n",
    "title": "为什么要把 DolphinScheduler 工作流定义中保存任务及关系的大 json 给拆了?",
    "time": "2021-05-25"
  },
  {
    "name": "Meetup_2022_02_26",
    "content": "直播报名火热启动 | 2022 年 Apache DolphinScheduler Meetup 首秀！\n\n\n\n各位关注 Apache DolphinScheduler 的小伙伴们大家好呀！相信大家都已经从热闹的春节里回过神来，重新投入到忙碌的工作学习生活中去。海豚调度在这里祝福大家虎年大吉，万事顺利。\n在这早春时节， Apache DolphinScheduler 于 2022 年的第一场 Meetup 也是即将到来，相信社区里的小伙伴早就已经翘首以盼了吧！\n在此次的Meetup中，四位来自不同公司的嘉宾将会为我们讲述他们在使用 Apache DolphinScheduler 时的心得体会，相信不论你是 Apache Dolphin Scheduler 的使用者抑或是正在观望的朋友都能从中收获到许多。\n我们相信未来将会有更多的小伙伴加入我们，也会有越来越多的使用者互相分享实践经验使用和体会， Apache DolphinScheduler 的未来离不开大家的贡献与参与，愿我们一起越变越好。\n此次 Apache DolphinScheduler 2月首秀将于2月26日下午14:00准时开播，大家别忘了扫码预约哦~\n\n\n\n扫码预约直播\n小伙伴们也可以点击 https://www.slidestalk.com/m/679 直达直播预约界面！\n1 活动简介\n主题：Apache DolphinScheduler 用户实践分享\n时间：2022年2月26日 14：00-17：00\n形式：线上直播\n02 活动亮点\n此次 Apache DolphinScheduler 的活动我们将迎来四位分享嘉宾，他们都是来自于360数科、杭州思科以及途家的大咖，相信他们对于 Apache DolphinScheduler的实际体验将会有高度的代表性以及典型性。他们与我们分享的实践经验包含但不限于在 K8S 集群上的探索与应用以及对Alert 模块的改造等，对于解决大家在实际使用中遇到的困难具有很大的意义，希望大家都可以从中得到独属于自己的宝贵财富。\n03 活动议程\n\n\n\n04 议题介绍\n\n\n\n刘建敏/360数科/大数据工程师\n演讲主题：Apache DolphinScheduler 在360数科的实践\n演讲概要：大数据调度从 Azkaban 到 Apache DolphinScheduler 的演变， Apache  DolphinScheduler 的使用与改造。\n\n\n\n李庆旺/杭州思科/大数据工程师\n演讲主题：Apache DolphinScheduler Alert 模块的改造\n演讲概要：切换到 Apache DolphinScheduler 探索，针对于不同的告警场景，对Apache DolphinScheduler 的 Alert 模块进行一些改造和探索。\n\n\n\n昝绪超/途家/大数据工程师\n演讲主题：Apache DolphinScheduler 的探索与应用\n演讲概要：引入 Apache DolphinScheduler ，搭建了途家数据调度系统，完成数据服务的接入，具体包括离线表，邮件，以及数据同步等。详细的介绍了我们对调度系统做了一些功能的开发。\n\n\n\n刘千/杭州思科/大数据工程师\n演讲主题：Apache DolphinScheduler 在 K8S 集群上的探索与应用\n演讲概要：切换到 Apache DolphinScheduler 探索，以及在基于 Apache DolphinScheduler二次开发支持任务提交在k8s上。目前可以运行镜像，spark ， flink 等任务，以及日志监控报警的探索。\nApache DolphinScheduler 的小伙伴们，我们2月26日下午14:00不见不散！\n",
    "title": "直播报名火热启动 | 2022 年 Apache DolphinScheduler Meetup 首秀！",
    "time": "2022-2-18"
  },
  {
    "name": "meetup_2019_10_26",
    "content": "\nApache Dolphin Scheduler(Incubating) Meetup 会议 2019 年 10 月 26 日在上海成功举行。\n地址：上海长宁愚园路 1107 号创邑 SPACE(弘基)3R20\n会议时间：下午 2 点开始，5 点结束.\n议程如下：\n\nDolphinScheduler 简介/概述（易观 CTO-郭炜 14:00-14:20）PPT\nDolphinScheduler 内部原理和架构设计（易观-乔占卫 14:20-15:00）PPT\n从开源用户到 PPMC——谈我和 DolphinScheduler（观远数据-吴宝琪 15:10-15:50）PPT\nDolphinScheduler 在国泰产险的应用与实践 （张宗耀 15:50-16:30）PPT\n即将发布的特性和路线图（易观-代立冬 16:30-17:00）PPT\n自由讨论\n\n",
    "title": "Apache Dolphin Scheduler(Incubating)将于2019.10在上海举行见面会",
    "time": "2019-9-27"
  }
]