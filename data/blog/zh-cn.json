[
  {
    "name": "2_The_most_comprehensive_introductory_tutorial_written_in_a_month",
    "content": "【达人专栏】还不会用 Apache Dolphinscheduler 吗，大佬用时一个月写出的最全入门教学【二】\n\n\n\n作者 | 欧阳涛 招联金融大数据开发工程师\n02 Master 启动流程\n2.1 MasterServer 的启动\n在正式开始前，笔者想先鼓励一下大家。我们知道启动 Master 其实就是启动 MasterServer，本质上与其他 SpringBoot 项目相似，即启动里面的 main 函数。但想要开始实操前，肯定有不少的人，尤其是初学者会突然发现这里面有十多个由 bean 注入的 autowired。\n被多个 bean 的注入搞到一头雾水，甚至感觉一脸懵逼的不是少数。但笔者就想说是，这些其实都是吓唬你们的，不用害怕，接下来将带领你们把这些 bean 分别解剖并归类，那么我们就正式开始。\n第一类：MasterConfig、MasterRegistryClient、MasterSchedulerService、Scheduler 这些 bean。从字面意思来说，MasterConfig 就是跟 Master 配置相关的，MasterRegistryClient 就是负责注册相关的内容，MasterSchedulerService 肯定跟 Master 调度有关的，说白了就是 Master 内部的东西。\n第二类：是那些后缀名为一堆 Processor 的，例如 taskExecuteRunningProcessor 等。相同后缀一定处理同样的 task，在以后肯定被某个东西一起加载的。\n第三类：是 EventExecuteService 以及 FailoverExecuteThread，这些根据名字可以大胆猜一下是与事件执行相关以及灾备转换相关的东西，这些肯定也是 Master 内部的东西，理论上应该归到第一类。\n第四类：至于 LoggerRequestProcessor 就是与打印日志相关的了，至于这类具体干的内容，后面会有详细的介绍。\nmain 方法执行完成后，基于 spring 特性，执行 run 方法。在 run 方法中，创建 nettyRemotingServer 对象(这个对象不是 spring 管理的，而是直接 new 创建的)。然后将第二类的一堆 Processor 放到 netty 的 Processor 里面去。从这里就可以推断，Master 和 Worker 的通信一定是通过 netty 的。\n我们可以看看下面的代码，其实就是将第一类的那些 bean 执行 init 以及 start 方法。\n总结其实 Master 这就像一个总司令，这个总司令就调用这里面的 bean 的 start 方法，这些 bean 开始执行自己的功能，至于这些 bean 里面执行啥样的功能，MasterServer 是懒得管，也没必要管了。\n本节总结：\n\n\n\n至此 MasterServer 就运行完了，下一节我们将逐个分析各个 bean 的用途以及功能的了。\n2.2 MasterConfig 的信息以及 MasterRegistry Client 的注册\nMasterConfig 从 application.yml 中获取配置信息加载到 MasterConfig 类中，获取到的具体配置信息如下。\n\n\n\n​\n在 MasterServer 里，MasterRegisterConfig 会执行 init()以及 start()方法。\ninit()方法新建了一个心跳线程池。注意，此时只是建了一个线程池，里面还没有心跳任务。\nstart()方法从 zk 获取了锁(getLock)，注册信息(registry)以及监听注册的信息(subscribe)。\n注册信息做了两件事情：\n第一：构建心跳信息，并丢到线程池中运行心跳任务的。\n第二：在 zk 临时注册该 Master 信息，并移除没用的 Master 信息。\n心跳任务就是检查是否有死亡节点以及每隔 10s(heartbeatInterval)将最新的机器信息，包括机器 CPU，内存，PID 等等信息注册到 zk 上去的。\n监听订阅的信息，只要注册的信息有变化，就会立马感知，如果是增加了机器，则会打印日志。减少了机器，移除并同时打印日志。本节如下图所示:\n\n\n\n2.3 ServerNodeManger 的运行\n前面两节是从 MasterServer 启动过程以及 MasterRegisterConfig 的注册过程的。注册完成之后 Master，Worker 如何管理呢，如何同步保存到数据库的呢。ServerNodeManager 的作用就是负责这一部分的内容。\nServerNodeManager 实现了 InitializingBean 接口的。基于 spring 的特性，构建此对象后，会执行 AfterPropertiesSet()方法。做了三件事情：\n\nload()。从 zk 加载节点信息通过 UpdateMasterNodes()到 MasterPriorityQueue。\n新建线程每十秒钟将 zk 的节点信息同步数据到数据库中。\n监听 zk 节点，实时把最新数据通过 UpdateMasterNodes()方法更新到 MasterPriorityQueue 队列中去。\n\n几乎所有的更新操作都是通过重入锁来实现的，这样就能确保多线程下系统是安全的。此外，还有一个细节是如果是移除节点会发送警告信息。\nMasterProrityQueue 里面有个 HashMap，每台机器对应一个 index，以这样的方式构建了槽位。后面去找 Master 信息的时候就是通过这 index 去找的。\n至于 MasterBlockingQueue 队列的内容，如何同步到数据库的，如何将数据放到队列和队列中移除数据等，这些都是纯 crud 的内容，读者可以自行阅读的。\n2.4 MasterSchedulerService 的启动\n2.1 到 2.3 讲述都是由 zk 管理的节点信息的事情。为什么我们要在 Master 启动之后会先讲节点信息的？\n理由其实很简单，因为不管是 Master 还是 Worker 归根结底都是机器。如果这些机器崩了或者增加了，DS 不知道的话，那这机器岂不是浪费了。只有机器运行正常，配置正常，都管理好了，那 DS 运行才能够顺畅地运行。同样，其他大数据组件也是类似的道理。\n前面 MasterServer 里 MasterRegisterClient 执行完 init()以及 start()方法之后，紧接着 MasterSchedulerService 执行了 init()和 start()方法。从这里开始就真正的进入了 Master 干活的阶段了。\ninit()方法是创建了一个 Master-Pre-Exec-Thread 线程池以及 netty 客户端的。\nPre-exec-Thread 线程池里面有固定的 10 个线程(在 2.1 中对应的是 MasterConfig 配置里面的 pre-exec-threads)。这些线程处理就是从 command 里构建 ProcessInstance(流程实例)过程的。\nstart 方法就是启动了状态轮询执行(StateWheelExecutorThread)的线程，这线程专门就干的是检查 task，process，workflow 超时以及 task 状态的过程，符合条件的都被移除了。\n其中，MasterSchedulerService 本身继承了 thread 类，在 start 方法过后，就立马执行了 run 方法。在 run 方法中确保机器有了足够的 CPU 和内存之后，就会执行 ScheduleProcess 方法。至于 ScheduleProcess 做的事情，将在 2.5 说明。\n2.5 MasterSchedulerService 的执行\nScheduleProcess 方法\nScheduleProcess 是在 MasterSchedulerService 中的 while 死循环里面的，所以它会依次循环执行下面 4 个方法。\n\nFindCommands 方法。从 t_ds_command 表中每次取出 10 条数据，并且这 10 条数据都是根据 slot 查找出来的，查找完成后，可以在 MasterConfig.FetchCommandNum 中进行配置。\nCommandProcessInstance 将这些 command 表中转换成 ProcessInstance。这里用到了 CountdownLatch，目的是全部转换完成才执行以后的方法。\n将转换好的 processInstance 一个一个的构建成 workFlowExecuteThread 对象，将这些对象通过 workFlowExecuteThreadPool 线程池中的线程一个一个执行的，并且将任务实例和工作流在 processInstanceExecCacheManager 缓存起来。\n在这个线程池中运行 StartWorkFlow 方法后，执行 WorkFlowExecuteThread 的 StartProcess 方法的，StartProcess 做了哪些事情将在 2.6 说明的。\n\n这个线程池交给了 spring 管理，而且属于后台线程。它的最大数量以及核心数量的线程池都是 100 个(MasterConfig.getExecThreads)。详细如下图:\n\n\n\n这里有两个细节要说明一下，\n第一：WorkflowExecuteThread 它并不是继承了 Thread 类，而是一个普通类。只是类名字后面有个 Thread，所以阅读的时候不要在此类找 start 或者 run 方法了。\n第二：SchedulerProcess 方法里面如果找到的 ProcessInstance 是超时的话，\n就会交给 2.4 说的状态轮询线程(stateWheelExecuteThread)去执行的，将这个 ProcessInstance 进行移除。\n2.6 WorkflowExecutorThread 里执行 StartProcess 方法\nStartProcess 这个方法就直接先看图的了。\n\n\n\nStartProcess 就干了三件事请，buildFlowDag()构建了 DAG，initTaskQueue()初始化 task 队列以及 submitPostNode()提交节点的。\n构建 DAG 如何干的，初始化队列中又干了什么事情，提交节点后又干了什么事情的，将在 2.7 到 2.9 章节说明。\n2.7 WorkflowExecutorThread 里执行 buildFlowDag 方法\n根据 buildFlowDag 里面的代码，梳理了一下执行过程，分别为下面 9 步:\n\nFindProcessDefinition 获取流程定义，就是要构建哪个流程的 DAG 的。\nGetStartTaskInstanceList 获取流程下有哪些任务实例，一般情况下，一个流程肯定有不止一个任务。\nFindRelationByCode 获取任务关系表(ProcessTaskRelation)中的数据。\nGetTaskDefineLogListByRelation 通过第 3 步获取的任务关系数据确定任务定义日志(TaskDefinitionLog)的数据的。\nTransformTask 就是通过第 3 步和第 4 步获取到 Relation 和 Log 转换成任务节点 TaskNode。\nGetRecoveryNodeCodeList 获取到的是 task 里的 nodeCode。\nParseStartNodeName 获取到的是命令的参数。\n根据第 5、第 6、第 7 获取到数据，构建了流程的 DAG(ProcessDag)。\n将构建好的 ProcessDag 数据转换成 DAG 数据。\n\n基本逻辑就是上面的步骤的。当然，每一步都会有些更多的逻辑，但这些本质上都是数据结构变来变去的。如果读者写过业务方面的代码，这点肯定不陌生的。所以就不详细的说明了。\n可能有读者对于 DAG 是什么，下面是 DAG 的简介链接，阅读之后理解起来应该并不难。\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DAG.md\n这个链接是在理论上介绍 DAG，如果对 DAG 想要在实践上更深入的认识，在 dao 模块的 test 文件夹下搜索 DagHelperTest 类，这里面有 5 个 test 的操作的，大家可以都运行一下(Debug 形式)，就会对 DAG 有着更深入的认识的。\n还有两个链接跟本节有关的。这两个链接是关于 dag 中任务关系的改造的。就是 1.3 版本以前保存任务之间的关系只是以字段的形式进行保存，后来发现数据量很大不可行之后，就把这个字段拆成多个表了。读者可以阅读一下的。\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md\nhttps://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Json_Split.md\n这构建 DAG(有向无环图)目的就是在前端拖拉拽的任务告诉 Master 任务的执行顺序，也就是告诉 Master 哪些任务先执行，哪些任务后执行。\n2.8 WorkflowExexutorThread 里执行 InitTaskQueue 方法\nInitTaskQueue 里面干了 3 件重要事情：\n\n初始化 4 个 map，分别是 ValidTaskMap，ErrorTaskMap，ActiveTaskProcessorMaps，CompleteTaskMap。就是将找到的 task 和 process 按 valid(有效)，complete(完成)，error(失败)，active(运行)为根据，保存到不同的 map 中(这些 map 都是以 taskCode 作为 key)，这些 map 将在后面的方法中用到的。\n如果 task 是可以重试的，就是通过 addTaskToStandByList 将其放到 readyToSubmitTaskQueue 队列中。\n如果开启了补数状态的话，那就设置具体的补数时间以及全局参数，将其更新到流程实例中。\n\n(笔者觉得这个 InitTaskQueue 方法名字并不是很好，可能觉得 InitTask 或者 InitTaskMap 会更好的。因为 Queue 的话很容易误认为是队列的，这个方法只是构建了 4 个 map 的。而且队列也只是放了可以重试的任务的，这个队列在下面章节中还有更大的用处的。)\n2.9 WorkFlowExecutorThread 里执行 SubmitPostNode 方法\nSubmitPostNode 干了 6 件事情：\n\nDagHelper.ParsePostNodes(dag)把 2.8 最后生成的 DAG 解析出来 TaskNodeList。\n根据 TaskNodeList 生成 TaskInstance 集合。\n如果只有一个任务运行的话,将 TaskInstance 参数配置传递给 ProcessInstance。\n将 TaskInstance 通过 AddTaskToStandByList 方法放到 ReadyToSubmitTaskQueue 队列去。\nSubmitStandByTask 提交这些 task。\nUpdateProcessInstanceState 是更新流程实例状态的。\n\n最重要的就是最后两件事情，就是将 TaskInstance 放到队列里和更新流程实例。更新流程实例纯属数据结构的变化的，这点并不难的。放到队列中的 task 如何处理，接下来将怎么做，\n也就是 SubmitStandByTask 干了哪些事情将在后续章节中说明。\n",
    "title": "还不会用 Apache Dolphinscheduler？大佬用时一个月写出的最全入门教程（2）",
    "time": "2022-5-23"
  },
  {
    "name": "Apache-DolphinScheduler-2.0.1",
    "content": "Apache DolphinScheduler 2.0.1 来了，备受期待的一键升级、插件化终于实现！\n\n\n\n\n编者按：好消息！Apache DolphinScheduler 2.0.1 版本今日正式发布！\n\n\n本版本中，DolphinScheduler 经历了一场微内核+插件化的架构改进，70% 的代码被重构，一直以来备受期待的插件化功能也得到重要优化。此外，本次升级还有不少亮点，如一键升级至最新版本、注册中心“去 ZK 化”、新增任务参数传递功能等。\n\n\nApache DolphinScheduler 2.0.1 下载地址：\nhttps://dolphinscheduler.apache.org/zh-cn/download/2.0.1\n\nApache DolphinScheduler 2.0.1 的工作流执行流程活动如下图所示：\n\n\n\n启动流程活动图\n2.0.1 版本通过优化内核增强了系统处理能力，从而在性能上得到较大提升，全新的 UI 界面也极大地提升了用户体验。更重要的是，2.0.1 版本还有两个重大变化：插件化和重构。\n01 插件化\n此前，有不少用户反馈希望 Apache DolphinScheduler 可以优化插件化，为响应用户需求，Apache DolphinScheduler 2.0.1 在插件化上更进了一步，新增了告警插件、注册中心插件和任务插件管理功能。 利用插件化，用户可以更加灵活地实现自己的功能需求，更加简单地根据接口自定义开发任务组件，也可以无缝迁移用户的任务组件至 DolphinScheduler 更高版本中。\nDolphinScheduler 正在处于微内核 + 插件化的架构改进之中，所有核心能力如任务、告警组件、数据源、资源存储、注册中心等都将被设计为扩展点，我们希望通过 SPI 来提高 Apache DolphinScheduler 本身的灵活性和友好性。\n相关代码可以参考 dolphinscheduler-spi 模块，相关插件的扩展接口也皆在该模块下。用户需要实现相关功能插件化时，建议先阅读此模块代码。当然，也建议大家阅读文档以节省时间。\n我们采用了一款优秀的前端组件 form-create，它支持基于 json 生成前端 UI 组件，如果插件开发涉及到前端，我们会通过 json 来生成相关前端 UI 组件。\norg.apache.dolphinscheduler.spi.params 里对插件的参数做了封装，它会将相关参数全部转化为对应的 json。这意味着，你完全可以通过 Java 代码的方式完成前端组件的绘制（这里主要是表单）。\n1 告警插件\n以告警插件为例，我们实现了在 alert-server 启动时加载相关插件。alert 提供了多种插件配置方法，目前已经内置了 Email、DingTalk、EnterpriseWeChat、Script 等告警插件。当插件模块开发工作完成后，通过简单的配置即可启用。\n2 多注册中心组件\n在 Apache DolphinScheduler 1.X 中，Zookeeper 组件有着非常重要的意义，包括 master/worker 服务的监控发现、失联告警、通知容错等功能。在 2.0.1 版本中，我们在注册中心逐渐“去 ZK 化”，弱化了 Zookeeper 的作用，新增了插件管理功能。\n在插件管理中，用户可以增加 ETCD 等注册中心的支持，使得 Apache Dolphinscheduler 的灵活性更高，能适应更复杂的用户需求。\n3 任务组件插件\n新版本还新增了任务插件功能，增强了不同的任务组件的隔离功能。用户开发自定义插件时，只需要实现插件的接口即可。主要包含创建任务（任务初始化、任务运行等方法）和任务取消。\n如果是 Yarn 任务，则需要实现 AbstractYarnTask。目前，任务插件的前端需要开发者自己使用 Vue 开发部署，在后续版本中，我们将实现由 Java 代码的方式完成前端组件的自动绘制。\n02 重构\n迄今为止，Apache DolphinScheduler 已经重构了约 70% 的代码，实现了全面的升级。\n1 Master 内核优化\n2.0.1 版本升级包括重构了 Master 的执行流程，将之前状态轮询监控改为事件通知机制，大幅减轻了数据库的轮询压力；去掉全局锁，增加了 Master 的分片处理机制，将顺序读写命令改为并行处理，增强了 Master 横向扩展能力；优化工作流处理流程，减少了线程池的使用，大幅提升单个 Master 处理的工作流数量；增加缓存机制，优化数据库连接方式，以及简化处理流程，减少处理过程中不必要的耗时操作等。\n2 工作流和任务解耦\n在 Apache DolphinScheduler 1.x 版本中，任务及任务关系保存是以大 json 的方式保存到工作流定义表中的，如果某个工作流很大，比如达到 100 至 1000 个任务规模，这个 json 字段会非常大，在使用时需要解析 json。这个过程比较耗费性能，且任务无法重用；另一方面，基于大 json，在工作流版本及任务版本上也没有很好的实现方案。\n因此，在新版本中，我们针对工作流和任务做了解耦，新增了任务和工作流的关系表，并新增了日志表，用来保存工作流定义和任务定义的历史版本，大幅提高工作流运行的效率。\n下图为 API 模块下工作流和任务的操作流程图：\n\n\n\n03 版本自动升级功能\n2.0.1 增加了版本自动升级功能，用户可以从 1.x 版本自动升级到 2.0.1 版本。只需要运行一个使用脚本，即可无感知地使用新版本运行以前的工作流：\nsh ./script/create-dolphinscheduler.sh\n具体升级文档请参考： https://dolphinscheduler.apache.org/zh-cn/docs/2.0.1/guide/upgrade\n另外，Apache DolphinScheduler 将来的版本均可实现自动升级，省去手动升级的麻烦。\n04 新功能列表\nApache DolphinScheduler 2.0.1 新增功能详情如下：\n1 新增 Standalone 服务\nStandAloneServer 是为了让用户快速体验产品而创建的服务，其中内置了注册中心和数据库 H2-DataBase、Zk-TestServer，在修改后一键启动 StandAloneServer 即可进行调试。\n如果想快速体验，在解压安装包后，用户只需要配置 jdk 环境等即可一键启动 Apache DolphinScheduler 系统，从而减少配置成本，提高研发效率。\n详细的使用文档请参考：https://dolphinscheduler.apache.org/zh-cn/docs/2.0.1/guide/installation/standalone\n或者使用 Docker 一键部署所有的服务：https://dolphinscheduler.apache.org/zh-cn/docs/2.0.1/guide/installation/docker\n2 任务参数传递功能\n目前支持 shell 任务和 sql 任务之间的传递。\n\nshell 任务之间的传参：\n\n在前一个&quot;create_parameter&quot;任务中设置一个 out 的变量”trans“: echo '${setValue(trans=hello trans)}'\n\n\n\n当前置任务中的任务日志中检测到关键字：”${setValue(key=value)}“, 系统会自动解析变量传递值，在后置任务中，可以直接使用”trans“变量：\n\n\n\n\nSQL 任务的参数传递：\n\nSQL 任务的自定义变量 prop 的名字需要和字段名称一致，变量会选择 SQL 查询结果中的列名中与该变量名称相同的列对应的值。输出用户数量：\n\n\n\n在下游任务中使用变量”cnt“:\n\n\n\n新增 switch 任务和 pigeon 任务组件：\n\nswitch 任务\n\n在 switch 任务中设置判断条件，可以实现根据不同的条件判断结果运行不同的条件分支的效果。例如：有三个任务，其依赖关系是 A -&gt; B -&gt; [C, D] ，其中 task_a 是 shell 任务，task_b 是 switch 任务。\n任务 A 中通过全局变量定义了名为 id 的全局变量，声明方式为echo '${setValue(id=1)}' 。\n任务 B 增加条件，使用上游声明的全局变量实现条件判断（注意 Switch 运行时存在的全局变量就行，意味着可以是非直接上游产生的全局变量）。下面我们想要实现当 id 为 1 时，运行任务 C，其他运行任务 D。\n配置当全局变量 id=1 时，运行任务 C。则在任务 B 的条件中编辑 ${id} == 1，分支流转选择 C。对于其他任务，在分支流转中选择 D。\n\n\n\n\npigeon 任务\n\npigeon 任务，是一个可以和第三方系统对接的一种任务组件，可以实现触发任务执行、取消任务执行、获取任务状态，以及获取任务日志等功能。pigeon 任务需要在配置文件中配置上述任务操作的 API 地址，以及对应的接口参数。在任务组件里输入一个目标任务名称，即可对接第三方系统，实现在 Apache DolphinScheduler 中操作第三方系统的任务。\n3 新增环境管理功能\n默认环境配置为 dolphinscheduler_env.sh。\n在线配置 Worker 运行环境，一个 Worker 可以指定多个环境，每个环境等价于 dolphinscheduler_env.sh 文件。\n\n\n\n在创建任务的时候，选择 worker 分组和对应的环境变量，任务在执行时，worker 会在对应的执行环境中执行任务。\n05 优化项\n1 优化 RestApi\n我们更新了新的 RestApi 规范，并且按照规范，重新优化了 API 部分，使得用户在使用 API 时更加简单。\n2 优化工作流版本管理\n优化了工作流版本管理功能，增加了工作流和任务的历史版本。\n3 优化 worker 分组管理功能\n在 2.0 版本中，我们新增了 worker 分组管理功能，用户可以通过页面配置来修改 worker 所属的分组信息，无需到服务器上修改配置文件并重启 worker，使用更加便捷。\n优化 worker 分组管理功能后，每个 worker 节点都会归属于自己的 Worker 分组，默认分组为 default。在任务执行时，可以将任务分配给指定 worker 分组，最终由该组中的 worker 节点执行该任务。\n修改 worker 分组有两种方法：\n\n打开要设置分组的 worker 节点上的&quot;conf/worker.properties&quot;配置文件，修改 worker.groups 参数。\n可以在运行中修改 worker 所属的 worker 分组，如果修改成功，worker 就会使用这个新建的分组，忽略 worker.properties 中的配置。修改步骤为&quot;安全中心 -&gt; worker 分组管理 -&gt; 点击 '新建 worker 分组' -&gt; 输入'组名称' -&gt; 选择已有 worker -&gt; 点击'提交'&quot;\n\n其他优化事项：\n\n增加了启动工作流的时候，可以修改启动参数；\n新增了保存工作流时，自动上线工作流状态；\n优化了 API 返回结果，加快了创建工作流时页面的加载速度；\n加快工作流实例页面的加载速度；\n优化工作流关系页面的显示信息；\n优化了导入导出功能，支持跨系统导入导出工作流；\n优化了一些 API 的操作，如增加了若干接口方法，增加任务删除检查等。\n\n06 变更日志\n另外 Apache DolphinScheduler 2.0.1 也修复了一些 bug，主要包括：\n\n修复了 netty 客户端会创建多个管道的问题；\n修复了导入工作流定义错误的问题；\n修复了任务编码会重复获取的问题；\n修复使用 Kerberos 时，Hive 数据源连接失败的问题；\n修复 Standalone 服务启动失败问题；\n修复告警组显示故障的问题；\n修复文件上传异常的问题；\n修复 Switch 任务运行失败的问题；\n修复工作流超时策略失效的问题；\n修复 sql 任务不能发送邮件的问题。\n\n07 致谢\n感谢 289 位参与 2.0.1 版本优化和改进的社区贡献者（排名不分先后）！\n\n\n\n\n\n\n8 加入社区\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n参与 DolphinScheduler 社区有非常多的参与贡献的方式，包括：\n\n\n\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n\n\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n\n\n进阶问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n\n\n如何参与贡献链接：https://dolphinscheduler.apache.org/zh-cn/community\n\n\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n参与开源可以近距离与各路高手切磋，迅速提升自己的技能，如果您想参与贡献，我们有个贡献者种子孵化群，可以添加社区小助手微信(Leonard-ds) 手把手教会您( 贡献者不分水平高低，有问必答，关键是有一颗愿意贡献的心 )。\n",
    "title": "Apache DolphinScheduler 2.0.1 来了，备受期待的一键升级、插件化终于实现！",
    "time": "2021-12-17"
  },
  {
    "name": "Apache_DolphinScheduler's_multi-cluster_unified_construction_and_management_practice_in_the_communications_industry",
    "content": "\n背景介绍\n为什么我们考虑构建统一的调度平台？ 主要原因是：我们公司的大数据中心目前拥有七个大数据集群，这些集群分布在不同的机房，例如内蒙、南京、苏州和广州。而且，这些机房之间的网络并不互通。如果每个集群都独立部署调度系统，将会有多套调度服务管理入口，这对于运维和开发者，无论维护和使用上来说都非常不便。因此，我们决定构建一个统一调度平台，集中管理多个集群的调度任务，也为我们后续深度平台集成提供契机。\n\n构建经验\n网络通讯: 之前我们的DolphinScheduler是基于单一机房的内网通讯。然而，考虑到我们的集群遍布多个省份，我们需要对其进行改造，使其支持跨机房通过公网通讯，而考虑降低网络延迟的影响，同一机房内的节点仍然希望服务之间内网通讯。为了保证数据的安全性，我们还为公网通讯配置了TLS加密。\n权限管理: 由于我们需要管理多套集群，因此会遇到多集群权限的问题。我们优化DolphinScheduler的工作组功能来管理不同的集群环境，并为不同集群环境和租户进行权限隔离。\n任务资源共享: DolphinScheduler本身支持对象存储。我们决定将所有集群的任务资源统一上传到同一个对象存储桶中，从而实现资源的统一管理和调度。\n服务架构\n我们的新架构基于DolphinScheduler的3.1.4版本。为了实现公网和内网通讯的混合部署，我们做了以下调整：\n\n同一机房内的服务节点通过内网通讯。\n不同机房之间的节点通过公网通讯。\nmaster节点和zookeeper部署在统一机房内并与其他节点进行通讯。\n\n\n为了实现上述设计，我们修改了DolphinScheduler的源码，使其可以基于主机名（Hostname）来识别服务节点，而不仅仅是IP。然后，我们通过配置hosts文件来映射内网IP和公网IP，从而实现内外网IP的动态切换。\n多权限集群管理的挑战与实施\n面对众多的项目基于单集群架构（单机房）的实际情况，我们实施了一个独特的集群标识系统，借由在数据库表格中新增字段的方法实现。为识别各集群身份，我们采用字符标识，以便在项目中明确标明集群归属。\n集群标识的引入是基于以下几点：\n\n部署决策：我们将不同的账户节点部署至不同的数据中心。\n环境获取：在用户配置过程中，我们需要确定他们如何获得环境信息。为此，我们将不同集群的环境配置集中处理，并分配至不同的组。\n授权策略：在进行授权时，我们只需将相应集群环境授权给用户即可。用户在配置任务时，仅需选择我们授权给他们的集群环境。\n\n在部署不同账户节点至多机房的过程中，一个值得探讨的问题是：在配置用户时，如何有效获取其运行环境？\n\n我们将各集群环境信息配置至平台，进一步配置至不同分组。在执行授权时，简化操作——仅需将授权的集群环境授权给相应用户，通过集群环境实现集群的隔离与任务分配。\n调度逻辑的升级与改造\n在实际使用过程中，我们从DolphinScheduler 3.0升级至3.1.4，经历了多次版本升级。经历了跨机房施工与临时停服的挑战后，我们开发了一个自动连续调度系统，解决了由于非计划性停机导致的人工数据补充问题。\n优化方案\n逻辑统一：我们统一了DS的调度逻辑。之前的逻辑在调度一条任务时，会在表中插入一条记录，调度完成后再删除。现在的优化方案直接预生成未来需要调度的50条记录。\n资源文件的缓存处理\n我们面临一个挑战——在跨机房调度任务的过程中，资源文件需从S3下载，受到机房带宽的限制，这一过程变得异常缓慢。因此，我们实施了一个资源文件缓存机制，当从S3下载资源后，通过本地缓存与时间戳判断来避免不必要的重复下载，并通过软链接的方式快速引导执行目录。\n缓存优化的必要性源于以下几点：\n\n跨网调度：我们跨网络调度任务时需从S3下载资源。\n带宽限制：由于数据中心的带宽限制（千兆带宽，而业界通常为万兆），下载速度较慢。\n\n实施细节\n\n缓存逻辑：简要来说，每次从S3下载的资源会被缓存至本地。通过检查资源的时间戳来判断其是否被更新，未更新的资源将直接链接至本地文件。\n\n调度连续性的直观展示\n通过具体图表的展示，我们详细说明了调度自动连续的效果和相关的恢复容错机制。\n\n例如，一项工作流作业每20秒执行一次，在DS不可用的近三分钟后，我们恢复了服务。调度能继续执行停服期间未调度的实例，并避免了发布或服务重启时需要数据开发团队进行手动补数。\n未来计划：开发作业调度分析页面\n我们注意到，当前的海豚调度器尚不具备一个集中分析多项目作业的页面。我们计划开发一个作业调度分析页面，从而简化多集群项目的作业故障日志分析和作业调度跟进处理。\n这个页面将基于任务维度展示相关作业，允许查看日志、重运作业，并提供一定的筛选功能。这将辅助开发和运维团队更快速地定位和分析问题，并高效地处理例如作业重运等操作。\n\n通过上述的一系列策略和改进，我们在多集群管理、调度逻辑和资源缓存等多个方面实现了技术的优化和提升。我们会继续深入研究和开发，希望为社区提供更多的便利和支持。\n我的演讲到此结束，非常感谢大家的聆听！\n",
    "title": " Apache DolphinScheduler在某通信行业的多集群统一建设与管理实践",
    "time": "2023-11-17"
  },
  {
    "name": "Apache_DolphinScheduler_3.0.0_to_3.1.8_tutorial",
    "content": "\ntitle: Apache DolphinScheduler 3.0.0 升级到 3.1.8 教程\nkeywords: Apache DolphinScheduler, 升级, 教程\ndescription: 安装部署的流程可参考官网的文档Version 3.1.8/部署指南/伪集群部署安装部署的流程可参考官网的文档\n背景介绍\nVersion 3.1.8/部署指南/伪集群部署(Pseudo-Cluster)\nhttps://dolphinscheduler.apache.org/zh-cn/docs/3.1.8/guide/installation/pseudo-cluster\n本文开始之前，我先补充说明一下升级 Apache DolphinScheduler 的几个关键点\n元数据备份\n做好已有元数据做备份（以 MySQL 为例，元数据存储在 Mysql 的 dolphinscheduler 数据库里）\n伪代码\nmysqldump -u root -p dolphinscheduler &gt; dolphinscheduler_backup_XXX.sql\n\napache-dolphinscheduler-3.1.8-bin/bin/env/install_env.sh\napache-dolphinscheduler-3.1.8-bin/bin/env/dolphinscheduler_env.sh\n这两个文件的配置项，可以从旧版本的相应文件里直接复制粘贴\n需要将使用到的数据库（比如 MySQL）的 connector (本人亲测 mysql-connector-java-8.0.30.jar 适用，其他 connector 版本未测试)放到相应的 libs 库里\napi-server/libs\nalert-server/libs\nmaster-server/libs\nworker-server/libs\ntools/libs\n伪代码如下\ncp XXX/mysql-connector-java-8.0.30.jar XXX/apache-dolphinscheduler-3.1.8-bin/api-server/libs/\ncp XXX/mysql-connector-java-8.0.30.jar XXX/apache-dolphinscheduler-3.1.8-bin/alert-server/libs\ncp XXX/mysql-connector-java-8.0.30.jar XXX/apache-dolphinscheduler-3.1.8-bin/master-server/libs\ncp XXX/mysql-connector-java-8.0.30.jar XXX/apache-dolphinscheduler-3.1.8-bin/worker-server/libs\ncp XXX/mysql-connector-java-8.0.30.jar XXX/apache-dolphinscheduler-3.1.8-bin/tools/libs\n\ncommon.properties 文件，需要修改相应的参数\nalert-server/conf/common.properties\napi-server/conf/common.properties\nmaster-server/conf/common.properties\nworker-server/conf/common.properties\n本人升级时，只修改了\napi-server/conf/common.properties\nworker-server/conf/common.properties\n这两个文件的三个参数\n（\nresource.storage.type\nresource.storage.upload.base.path\nresource.hdfs.fs.defaultFS\n）\n需要注意的是，3.1.8 此文件的有些参数名和 3.0.0 此文件中的参数名有变化。\n3.0.0 参数 resource.upload.path\n对应\n3.1.8 参数 resource.storage.upload.base.path\n3.0.0 参数 fs.defaultFS\n对应\n3.1.8 参数 resource.hdfs.fs.defaultFS\nCDH集群升级\n如果是 CDH 集群（比如 CDH 6.2.1）升级使用  Apache DolphinScheduler 3.1.8\n还需要替换相应的 Zookeeper jar 包， **否则会有  Zookeeper Failed to delete registry key 报错提示\n**\n参考文章\nDolphin3.0在伪集群模式下总是会报zookeeper Failed to delete registry key，我该怎么排查这个问题：\nhttps://github.com/apache/dolphinscheduler/discussions/11948\n看看是不是 Zookeeper 的版本不兼容，DolphinScheduler3.1.8 版本的 zookeeper 和 curator 是下面这个:\ncurator-client-5.3.0.jar\ncurator-recipes-5.3.0.jar\ncurator-framework-5.3.0.jar\nzookeeper-3.8.0.jar\n将/api-server/libs，master-server/libs，worker-server/libs，/tools/libs 下的 zookeeper，curator 的版本替换成 Zookeeper 服务版本一致才行。\n注意：zookeeper-3.4.x 对应的 curator-*版本必须是 4.2.0\nCDH6.2.1 使用的是\nZooKeeper\n3.4.5+cdh6.2.1\n即 Zookeeper-3.4.5 版本，需要下载相应的 jar 包  zookeeper-3.4.5.jar,可从如下网站上搜索下载 jar 包:https://nowjava.com/jar/\n此外还需要下载下面三个 jar 包\n\ncurator-client-4.2.0.jar\ncurator-recipes-4.2.0.jar\ncurator-framework-4.2.0.jar\n\n即：\n\nzookeeper-3.4.5.jar\ncurator-client-4.2.0.jar\ncurator-recipes-4.2.0.jar\ncurator-framework-4.2.0.jar\n\n将这 4 个 jar 包放到如下\n\napi-server/libs\nmaster-server/libs\nworker-server/libs\ntools/libs\n\n这四个目录，并替换原先的版本 jar 包，伪代码如下\n####################### \ncd XXX/apache-dolphinscheduler-3.1.8-bin/api-server/libs/ \n\nrm -f zookeeper-3.8.0.jar\ncp XXXX/zookeeper-3.4.5.jar . \n\nrm -f curator-*\ncp XXXX/curator-*4.2* . \n\n####################### \n\ncd XXX/apache-dolphinscheduler-3.1.8-bin/master-server/libs \n\nrm -f zookeeper-3.8.0.jar\ncp XXXX/zookeeper-3.4.5.jar . \n\nrm -f curator-*\ncp XXXX/curator-*4.2* . \n\n####################### \n\ncd XXX/apache-dolphinscheduler-3.1.8-bin/worker-server/libs \n\nrm -f zookeeper-3.8.0.jar\ncp XXXX/zookeeper-3.4.5.jar . \n\nrm -f curator-*\ncp XXXX/curator-*4.2* . \n\n####################### \n\ncd XXX/apache-dolphinscheduler-3.1.8-bin/tools/libs \n\nrm -f zookeeper-3.8.0.jar\ncp XXXX/zookeeper-3.4.5.jar . \n\nrm -f curator-*\ncp XXXX/curator-*4.2* . \n\n#######################\n\n以上这五点都修改完后，停掉 Apache DolphinScheduler 3.0.0\n伪代码\nsu - dolphinschedulercd XXXX/apache-dolphinscheduler-3.0.0-bin# 一键停止集群所有服务bash ./bin/stop-all.sh\n\n初始化 DolphinScheduler 3.1.8\ncd XXXX/apache-dolphinscheduler-3.1.8-bin \n\nsh ./tools/bin/upgrade-schema.sh\n\n\ncd XXXX/apache-dolphinscheduler-3.1.8-bin \n\nsh ./bin/install.sh\n\n到这里就升级完成了\n",
    "title": "Apache DolphinScheduler 3.0.0 升级到 3.1.8 教程？",
    "time": "2023-11-21"
  },
  {
    "name": "Apache_DolphinScheduler_3.1.9_version_released_improving_system_stability_and_performance",
    "content": "🚀我们很高兴宣布，Apache DolphinScheduler 的最新版本 3.1.9 已正式发布！此版本在 3.1.8 的基础上进行了关键的 bug 修复和文档更新，共计修复了 14 个 bug 和改进了 3 个文档。\n\n主要更新亮点\n本次更新重点解决了以下几个关键问题，以提升系统的稳定性和性能：\n\n修复死锁问题：解决了使用任务组抢占资源时可能导致的工作流死锁问题。\n优化工作流实例：修复了 Master 提交失败后，及时停止工作流实例，避免了无限循环的问题。\nNPE问题处理：修复了 Master 处理命令时，可能导致 processInstance 出现空指针异常（NPE）的问题。\nFlink任务命令错误：修复了 Flink 任务生成的命令错误。\n避免事件堆叠：修复了 Master 重复处理事件，避免了事件堆叠的问题。\nSwitch任务参数检查：增加了 Switch 任务的参数检查，以避免非法情况。\n任务定义日志问题：解决了任务定义日志可能重复写入的问题。\n\nChangelog 和下载\n欲了解完整的更新内容，可以查看 全部 Changelog。\n新版本现已可在 官方下载页面 获取。\nBugfix\n\n[Bug-15215][Api] non-admin should not modify tenantId and queue #15254 @zhanqian-1993\n[Bug] fix switch condition #15228 @caishunfeng\n[Fix-14805] Task definition log may be written twice #15016 @HomminLee\n[Bug][Master] fix duplicate event, avoid event stacking #14986 @caishunfeng\n[Fix-14963] Fix the error of using shell task to obtain Home variable in Ubuntu system #14964 @zhuangchong\n[Fix-14729] fix problem with the command generated by the flink task #14902 @LiuCanWu\nConstruct processInstance may NPE when master handling command #14888 @fuchanghai\n[Bug] [Mater] The process always runs when the process contains subprocess and params deliver #14856 @qingwli\n[Task] Change Seatunnel task run-mode to deploy-mode #14800 @zhangbaipeng\nfix(dolphinscheduler-alert): fix create http request error #14793 @hunter-cloud09\n[Fix-14627][dolphinscheduler-ui] Fix there are too many child node in sub_process, it cannot to filter by keywords #14770 @chenshuai1995\n[Fix-14546] seatunnel unable submit spark job to spark #14617 @Yhr-N\n**[Fix]**Solve the deadlock problem caused by queuing #13191 @dahai1996\nafter a submit failure, stop the processInstance to avoid an endless loop #13051 @fuchanghai\n\nDoc\n\ndoc: Classify docs to avoid misleading #15282 @zhongjiajie\n[Docs] fix typo #15032 @liunaijie\nFix a typo in English development-environment-setup.md #14767 @wangzheyuan\n\n致谢贡献者\n非常感谢本次版本中所有贡献者的辛勤工作，特别鸣谢以下成员：zhanqian-1993、caishunfeng、liunaijie、HomminLee、zhuangchong、LiuCanWu、qingwli、zhangbaipeng、hunter-cloud09、chenshuai1995、fuchanghai、dahai1996、Yhr-N、zhongjiajie。\n特别感谢\n特别感谢本次版本的发版人 zhuangchong (Kerwin) 对于发布工作的巨大贡献。\n结语\nApache DolphinScheduler 社区一直致力于打造一个稳定、高效、易于使用的工作流调度平台。我们期待您的持续关注和支持，共同推动 Apache DolphinScheduler 项目的发展！\nApache DolphinScheduler 社区团队\n",
    "title": "Apache DolphinScheduler 3.1.9 版本发布：提升系统的稳定性和性能！",
    "time": "2024-01-04"
  },
  {
    "name": "Apache_DolphinScheduler_community_won_the_2023_Outstanding_Open_Source_Technology_Team_award",
    "content": "在开源社区日益繁荣的今天，我们非常荣幸地宣布：Apache DolphinScheduler 社区在 OSCHINA 平台的评选中荣获了“2023 年度优秀开源技术团队”奖项。这一奖项反映了我们社区在过去一年里在内容发表的深度与广度、活动运营影响力以及对开源文化的推广方面所做的突出贡献。\n\n\n关于奖项\n&quot;年度优秀开源技术团队&quot;奖项由 OSCHINA 平台设立，今年已是第三年颁发这一荣誉。该奖项旨在表彰那些在开源领域做出杰出贡献的技术团队和社区，包括他们在技术创新、社区活动组织、内容分享等方面的优异表现。\n我们的成就\nApache DolphinScheduler 社区自成立以来，始终坚持开源文化的理念，通过定期发布高质量的技术文章、组织各种线上线下活动以及积极参与开源项目协作，成功地构建了一个知识共享、技术交流和友好合作的平台。这次获奖不仅是对社区过去一年努力的认可，也是激励我们继续前进的动力。\n未来展望\n展望未来，Apache DolphinScheduler 社区将继续致力于提供高质量的技术内容，积极开展丰富多彩的社区活动，加强与全球开源社区的交流合作。感谢 OSCHINA 平台的认可与支持，感谢社区每一位成员的贡献和努力。我们相信，只有持续地分享、合作和创新，才能使开源社区更加强大。让我们携手共创更加美好的开源未来！\n",
    "title": "Apache DolphinScheduler 社区荣获 2023 年度优秀开源技术团队 奖项",
    "time": "2023-12-18"
  },
  {
    "name": "Apache_DolphinScheduler_in_XWBank",
    "content": "金融任务实例实时、离线跑批Apache DolphinScheduler在新网银行的三大场景与五大优化\n\n\n\n在新网银行，每天都有大量的任务实例产生，其中实时任务占据多数。为了更好地处理任务实例，新网银行在综合考虑之后，选择使用 Apache DolphinScheduler 来完成这项挑战。如今，新网银行多个项目已经完成了实时与准实时的跑批，指标管理系统的离线跑批，应用于离线数据开发和任务调度、准实时数据开发和任务调度，以及其他非 ETL 用户定义数据跑批三类场景中。\n为了更好地适应业务需求，新网银行是如何基于Apache DolphinScheduler 做改造的呢？在 Apache DolphinScheduler 4 月Meetup上，来自新网银行大数据中心的高级大数据工程师 陈卫，为我们带来了《 Apache DolphinScheduler 在新网银行的实践应用》。\n本次分享分为四个环节：\n\n\n新网银行引入 Apache DolphinScheduler 的背景介绍\n\n\nApache DolphinScheduler 的应用场景\n\n\n对新网银行的优化与改造\n\n\n新网银行使用 Apache DolphinScheduler 的后续计划\n\n\n\n\n\n陈卫\n新网银行 大数据中心 高级大数据工程师\n11 年工作经验，早期从事数据仓库建设，后转向大数据基础平台、调度系统等建设，有传统金融行业、互联网数据仓库、数据集市建设经验，多年的调度系统建设经验，咪咕文化分析云调度系统设计，报表平台设计，目前主要负责新网银行 DataOps 体系相关系统建设(离线开发，指标系统，标签系统)。\n01背景介绍\n我们选择使用 Apache DolphinScheduler 主要基于三大需求：研发场景的统一、测试场景的优化，以及投产部署场景优化。\n01研发场景\n过去，我们在数据开发过程中无统一的开发工具，因此新网银行在开发工作过程中，需要在多个工具间来回切换，导致过高的开发成本；\n另一方面，我们在开发过程中的参数替换需求无法满足，无法进行即席调试，无现成工具支持开发态与生产态离线任务。\n02测试场景\n在测试场景的部署的过程中，当我们的开发人员将脚本提供给测试，返回的文档却相当不友好。尤其是需要在多个版本多个场景中部署的时候，测试人员的任务量骤增，可视化的部署也相对较弱，无法进行较友好的自动化测试。\n03投产部署\n\n\n当前调度系统配置复杂，可视化效果差；\n\n\n开发与生产环境网络物理隔离，因此开发环境代码部署至生产环境流程长，易出错。测试环境无法充分体现生产环境配置，手动配置文件易出错，易漏配；\n\n\n运维监控能力不足，可视化效果差，无法在线查看日志，故障排除进入监控机房须登录物理机器，流程复杂。\n\n\n02应用场景\n我们应用 Apache DolphinScheduler 的场景主要有以下离线数据开发以及任务调度、准实时数据开发以及任务调度以及其他非 ETL 用户定义数据跑批三类。\n01离线数据开发以及任务调度\n在离线数据开发以及任务调度中，主要应用于我们的银行业的数据仓库、数据集市等，数据包括一些离线数据，按日按月的离线加工的数据等。\n02准实时数据开发以及任务调度\n在新网银行中准实时的数据是通过 Flink 从上游的消息队列数据库的日志里面进行融合计算，补全相关维度信息后，把数据推送到 Clickhouse 内进行处理。但按分钟级进行跑批计算，但相对于日常的按日跑批的调度，会有一些特殊的需求。\n03其他非ETL用户定义数据跑批\n我们有这部分的应用是通过一些内部的低代码平台来实现功能，我们将应用系统开放给业务人员，他们可以自助分析应用数据，不需要开发人员处理。业务人员定义好后，可以自助对这部分数据进行跑批。\n1、离线数据开发以及任务调度\n其中，我们在离线数据开发和任务调度场景中应用 Apache DolphinScheduler ，主要涉及任务开发调式、历史的任务集成、工作流与任务分离、项目环境变量、数据源查找五个板块。\n1、任务开发调式（SQL,SHELL,PYTHON,XSQL等），在线开发调式（在下查看日志，在线查看 SQL 查询返回结果）。WEBIDE 可以自动对弹窗变量替换，会根据用户的设置以及默认的处理进行动态替换。\n2、历史的任务集成\n银行业大部分数仓已经建立了四五年，有很多的历史任务，因此，我们不希望我们新的系统上线的时候，用户需要自主改造代码，因为这样会导致用户的使用成本相对过高。\n3、工作流与任务分离\n开发直接开发任务并调式、测试，工作流直接引用已开发任务，这样我们的任务开发与我们的任务编排就进行了相应的切割。\n4、项目环境变量\n新增项目环境变量，项目环境变量默认适配项目内的所有作业，这样我们不需要在每一个工作流内配置，每个项目可以直接引用。\n5、数据源\n我们按数据源名称查找数据源，支持 phoenix 等数据源。后续我们希望任务可以导入导出，但在导入导出的过程中，我们任务中的参数定义，数据源等不能进行改变，这样从测试就可以直接导向直接投产，在生产方面就会较为简单。\n2、准实时的任务\n\n\n任务开发调式（SQL），在线开发调式（在线查看日志，在线查看 SQL 查询返回结果），WEBIDE 中弹窗替换脚本变量。\n\n\nClickhouse 数据源 HA 配置集成支持。但在离线跑批中会出现一个小问题，即如果当前端口不可用，可能直接报错，在这一块，需要进行额外的处理。\n\n\n准实时工作流单实例运行，如已有初始化实例，或存在正在进行的工作流实例，即使触发了下一批次，也不会触发工作流的运行。\n\n\n3、其他非ETL用户定义数据跑批\n1、我们目前有来自指标管理平台推送的模型数据计算任务，这些用户自定的简单报表，平台会动态生成 SQL ，随后直接推送到离线调度中。未来这一过程将不会有开发人员参与。\n2、在标签管理系统中，我们主要通过生成特殊的插件任务来配适。\n03优化改造\n1、新网银行现状\n在新网银行，每天都有大约 9000+ 的任务实例产生，其中实时任务占据多数。如今，我们已经使用 Apache DolphinScheduler ，在很多项目中完成实时与准实时的跑批，指标管理系统的离线跑批等，包括对集成的支持 XSQL 内部 SQL 工具进行跑批。\n\n\n\n在右侧的截图中我们可以看到，我们其实完成了任务独立，将参数进行二次替换。另外，在任务血缘方面，尤其是 SQL 类的任务，我们可以做到自动解析，也可以手动增加。这主要用于我们工作流的自动编排，如公司内部的任务地图等。\n为了满足以上的业务需求，我们对 Apache DolphinScheduler 进行了如下五大优化，同时也列出了相应的在改造过程中必须要注意的修改。\n\n\n项目通过环境进行认为在不同场景下的各类（开发、测试）；\n\n\n环境变量与项目、环境进行隔离，但不同环境环境变量名称保持一致；\n\n\n数据源通过项目、环境进行隔离，但不同环境数据源名称保持一致；\n\n\n新增非 JDBC 数据源，ES，Livy等。因为在内部透明的应用中，需要 Livy 作为数据服务框架，对接 Spark job 进行数据脱敏。\n\n\n2、独立任务\n\n\n开发独立的任务开发，调试，配置页面，能够支持项目环境变量\n\n\nJDBC，XSQL 的任务能够通过数据源名称引用数据源\n\n\n开发交互式 WEBIDE 调试开发\n\n\n完成参数优化，支持用户${参数}并引用系统内置时间函数\n\n\n完成独立 SQL、XQSL 自动血缘解析\n\n\n完成 SQL 自动参数解析\n\n\n3、工作流启动逻辑优化\n\n\n准实时工作流单实例运行，如已存在正在运行的工作流实例，则忽略本次运行\n\n\n增加环境控制策略，工作流根据不同的环境引用不同的环境变量、数据源访问连接，比如如果提前配置了灾备环境和生产环境，一旦生产环境出现问题，可以一键切换到灾备环境中。\n\n\n优化由于工作流、任务分离带来的调度问题，主要包括异常的检测\n\n\n4、导入导出优化\n\n\n新增导入导出任务、任务配置及其资源文件等\n\n\n由于银行业和金融业有许多开发测试环境网络和生产网络是不一致的，所以需要在多个环境中处理数据时，导出一个相对友好的资源脚本工作流以及资源文件信息。\n\n\n新增工作流导入导出逻辑，处理由于不同数据库实例自增ID存在的数据冲突问题\n\n\n导航式导入导出，版本管理，主要应对紧急情况时，部分代码的回退等等\n\n\n5、告警体系改进与优化\n\n\n对接新网银行内部告警系统，默认对任务创建人员订阅告警组用户进行告警\n\n\n增加策略告警（启动延迟、完成延迟），对重点任务进行启动、完成延迟告警\n\n\n6、对接内部系统\n\n\n模型类任务运行以及监控\n\n\n报表推送类任务运行以及监控\n\n\n对接内部 IAM SSO 统一登录认证系统\n\n\n按网络不同，限定特定功能（代码编辑，工作流运行，任务运行等）\n\n\n金融行业有一个特殊的现象，就是我们的投产需要在特定的机房去做，我们必须限定某些操作只能在机房中完成，但我们也需要减少修改一次的成本，我们希望开发在看到日志以后，直接在办公网络中进行修复，修复完成后再去机房进行投产。\n\n\n\n如上图所示，我们主要基于这种维度模型理论自动创建报表。配置后，我们根据配置报表逻辑，进行多个表的代码合并计算。聚合计算完成后推送到报表服务器。这样业务用户可以按照我们提供的一些基础功能。直接进行数据聚合，不需要去写 SQL ，也避免了业务端用户不安给我们提出临时的需求。\n04后续计划\n\n\n向更多的项目组推广离线数据研发平台\n\n\n逐步替换行内已有调度系统，实现所有离线任务平稳迁移\n\n\n调度系统下沉，对接行数据研发管理系统\n\n\n技术目标\n\n\n更加智能化、自动化的任务调度、编排系统，降低调度系统在用户侧的使用门槛\n\n\n运行监控、预测，面相与运维人员提供更加友好的运维监控，任务完成时间预测等功能\n\n\n全局视图功能，面向开发、运维人员提供离线任务的全局视图，提供数据血缘、影响分析功能\n\n\n进一步集成行内定制的配置模块化，降低开发人员的开发成本\n\n\n与数据质量管理平台进行整合集成\n\n\n用户定义木板任务支持\n\n\n谢谢大家，我今天的分享就到这里。\n",
    "title": "金融任务实例实时、离线跑批Apache DolphinScheduler在新网银行的三大场景与五大优化",
    "time": "2022-5-23"
  },
  {
    "name": "Apache_DolphinScheduler_releases_version_3.2.0",
    "content": "---\ntitle: Apache DolphinScheduler 重磅发布3.2.0版本！\nkeywords: Apache DolphinScheduler, 版本发布, 技术动态\ndescription: 首度公开！Apache DolphinScheduler 3.2.0 全新功能与优化汇总。\n为您带来更加完善的功能体验与系统稳定性。\n\n今天，Apache DolphinScheduler 3.2.0 版本在万众期待中终于发布了！在之前的预告中，包括《重磅预告！Apache DolphinScheduler 3.2.0 新功能“剧透”》、《3.2.0 版本预告！Apache DolphinScheduler API 增强相关功能》、《3.2.0 版本预告！远程日志解决 Worker 故障获取不到日志的问题》，以及《3.2.0 终极预告！云原生支持新增 Spark on k8S 支持》文章汇总已经大致覆盖了 3.2.0 版本的全新功能和优化。\n现在，来看看新版本的全新“样貌”吧！\nRelease Note: https://github.com/apache/dolphinscheduler/releases/tag/3.2.0\n下载地址： https://dolphinscheduler.apache.org/en-us/download/3.2.0\n主要更新包括：\n\n\n添加默认租户\n新增多种数据源\n新增任务类型\n重跑任务时指定工作流向前、向后运行\n增加远程日志功能\n参数优化\n资源中心\n增强页面易用性\n云原生支持新增 Spark on k8S 支持\n增加了部分 Restful API\n注册中心增加 ETCD、JDBC 注册中心\n架构优化\n\n\n添加默认租户\n在之前的版本中，用户部署完毕后必须手动添加租户。3.2.0 版本中添加了默认租户，方便用户更直接地使用 Apache DolphinScheduler。\n新增多种数据源\n新增了多个数据源，如 Snowflake、Databend、Kyuubi、Doris、OceanBase、Dameng、AzureSQL、StarRocks、AWS Athena、，并且更新了部分数据源，如 Redshift 增加 Access key。\n\n新增任务类型\n新增了多个任务类型，包括：\n\n\n\n\n\n通用模块中，增加 Remote-shell组件、Java Task\n\n\nCloud 模块中，新增 Amazon DMS、Azure Datafactory、AWS Database Migration，增强与各种云的互联互通\n\n\n机器学习模块中，新增 Kubeflow组件（基于云原生构建的机器学习任务工具大合集）\n\n\n其他模块中，增加 AmazonDatasync、Apache Linkis\n\n\n\n\n\n\n\n\n\n并更新了部分任务，如 DataX 支持 Presto，http任务增加output 参数传递，运行批量同时 kill 多个 Yarn 任务：\n\nDependent 支持依赖自己：\n\n支持了 Zeppelin 鉴权；\n此外，任务现在可以支持缓存；\n\nSqoop 日志支持隐藏密码；\n以及 SQL 任务支持默认切割符：\n\n新增远程日志功能\n3.2.0 版本增加了远程日志功能，并同时支持了 Google Cloud Storage、Amazon S3、阿里云 OSS 日志存储，用户可以通过编辑配置文件，把日志存储到云端，解决万一意外情况发生，Woker 日志不存在，用户无法查看日志的问题。\n\n详情参加《3.2.0 版本预告！远程日志解决 Worker 故障获取不到日志的问题》。\n参数优化\n\n\n增加了项目级别参数\n调整参数优先级，启动参数最高\n增加了内置参数计算规则\n\n\n\n\n增加了文件类型的参数\n\n\n云原生相关\n\n支持 KEDA 做 worker 自动扩缩容\n支持 Terraform 部署到 AWS\nzk 和 pg 支持多架构\n提交 Spark 任务到 Kubernetes（详情见《3.2.0 终极预告！云原生支持新增 Spark on k8S 支持》）\n获取 pod 实时日志\n自定义 k8s 任务标签\n\n资源中心\n增加了 Alibaba Cloud OSS 、Huawei Cloud OBS、Azure Blob Storage的支持，重构资源中心并设计默认使用本地作为存储介质，重新支持了 re-upload。\n\n资源中心容许覆盖上传，优化文件路径，显示文件的全部路径。另外，之前版本中资源中心已经上传的同类型文件只能删除后重新上传，新版本中对本功能进行了优化，可以点击上传按钮进行上传。\n\n支持 reupload 文件\nAPI 增强\n3.2.0 版本中，增加了部分 Restful API，包括 taskInstance、workflow state、workflowInstance、workflow and schedule、task relation，且API 触发工作流运行可以获得 instance ID，从而使得 Apache DolphinScheduler 的 API 能力得到显著增强。\n详情参见：《3.2.0 版本预告！Apache DolphinScheduler API 增强相关功能》\n增加页面易用性\n3.2.0 增加了页面易用性和便利性，如增加 workflow instance 跳转到当前工作流、复制工作流名称、调整列宽等操作。\n\n跳转到工作流实例\n\n复制工作流名称\n\n调整列表名称宽度\n默认情况下会有 default 租户和本地资源中心，安装后就能使用。\n\n默认租户\n允许在 workflow instance 中重新运行任务，任务运行日志更加明确。\n\n可以重新运行任务\njson 导出可阅读性加强。\n\n注册中心\n增加了 ETCD、JDBC 注册中心。\n架构\n\nAlert 支持 HA\n单线程更新 Kerberos\nWorker server 移除了 dao 依赖\n接管 task instance 失败的任务\n增加动态任务组配置\n重构了逻辑任务和远程命令\n资源限制（cpu 内存）从原来绝对值改成百分比\n支持了 SSO\n\n其中，支持了 SSO 后，用户可以通过 Casdoor 实现 SSO 登录。Casdoor 是基于 OAuth 2.0、OIDC、SAML 和 CAS 的面向 UI 的身份访问管理（IAM）/单点登录（SSO）平台，需要先部署 Casdoor 并获取 `Client ID` 和 `Client secret` 两个字段，再修改 dolphinscheduler-api/src/main/resources/application.yaml 文件配置 SSO。\n可以通过以下步骤通过 Casdoor 为 Apache Dolphinscheduler 添加 SSO 功能：\nsecurity:\n  authentication:\n    # Authentication types (supported types: PASSWORD,LDAP,CASDOOR_SSO)\n    type: CASDOOR_SSO\ncasdoor:\n  # Your Casdoor server url\n  endpoint:\n  client-id:\n  client-secret:\n  # The certificate may be multi-line, you can use `|-` for ease\n  certificate: \n  # Your organization name added in Casdoor\n  organization-name:\n  # Your application name added in Casdoor\n  application-name:\n  # Doplhinscheduler login url\n  redirect-url: http://localhost:5173/login \n\n贡献者列表\n感谢@zhongjiajie对此次发版的指导，以及下列贡献者的支持：\n\n106umao, Abingcbc, AliceXiaoLu, BongBongBang, CallMeKingsley97, Chris-Arith, DarkAssassinator, EricGao888, EricPyZhou, FlechazoW, Gallardot, GavinGYM, IT-Kwj, LiXuemin, LucasClt, Mukvin, NoSuchField, Orange-Summer, QuantumXiecao, Radeity, Rianico, SYSU-Coder, SbloodyS, Tianqi-Dotes, TyrantLucifer, ZhongJinHacker, Zzih, ahuljh, alei1206, alextinng, amaoisnb, arlendp, baihongbin, bmk15897, boy-xiaozhang, c3Vu, caishunfeng, calvinjiang, darrkz, davidzollo, dddyszy, devosend, ediconss, eye-gu, fengjian1129, fuchanghai, guowei-su, haibingtown, hantmac, hdygxsj, hezean, hiSandog, hoey94, hstdream, huage1994, imizao, insist777, iuhoay, jackfanwan, jbampton, jieguangzhou, kezhenxu94, kingbabingge, labbomb, lenian, ly109974, lynn-illumio, moonkop, muggleChen, pandong2011, pppppjcc, qianli2022, qindongliang, qingwli, rickchengx, ruanwenjun, sandiegoe, seedscoder, shangeyao, shenyun, simsicon, sketchmind, stalary, tracehh, whhe, xdu-chenrj, xiaomin0322, xinxingi, xuchunlai, xxjingcd, yeahhhz, youzipi, zhangfane, zhangkuantian, zhaohehuhu,zhoufanglu, zhuangchong, zhutong6688, zhuxt2015, zzzhangqi\n\n",
    "title": "Apache DolphinScheduler 重磅发布3.2.0版本！",
    "time": "2023-10-17"
  },
  {
    "name": "Apache_DolphinScheduler_s_Graduation_From_ASF_Incubator",
    "content": "Apache DolphinScheduler ASF 孵化器毕业一周年，汇报来了！\n\n\n\n不知不觉，Apache DolphinScheduler 已经从 Apache 软件基金会（以下简称 ASF）孵化器毕业一年啦！\n北京时间 ​2021 年 4 月 9 日，ASF 官方宣布 Apache DolphinScheduler 毕业成为 Apache 顶级项目，让首个由国人主导并贡献到 Apache 的大数据工作流调度领域的顶级项目进入更多人的视野。\n如今一年过去，Apache DolphinScheduler 也在众人瞩目和基金会的帮助之下，加快了奔跑的步伐，力争在 DataOps 领域更好地发光发热。\n时值 Apache DolphinScheduler 从 ASF 孵化器毕业一周年，我们在这里向大家汇报一下在这段有限的时间内，项目在 ASF 和社区帮助下取得的成绩。\n保持快速迭代，健康状态良好\n根据 ASF Project Statistics 显示，Apache DolphinScheduler 社区健康评分为 9.19，这说明社区运行状态良好。\n目前，社区共有 45 为 Committer 和 19 位 PMC，Committer-to-PMC 比例为 2:1。\n项目活动：\n软件开发：\n2021 年至今，我们发布了 11 个版本，完成了 70% 的代码重构，使得性能提升20 倍。新增 Python SDK 支持，上线了 WorkflowAsCode 功能，并实现了插件化、一键升级等社区呼声极高的功能优化。目前最新版本为 2.0.5。\nMeetup 和会议：\n\n2021 年 11 月 27 日举行的 Apache DolphinScheduler 线上 meetup，约 4000 人次观看；\n2022 年 2 月 26 日举行的 Apache DolphinScheduler 线上 meetup，约 5000 人次观看；\n2022 年 3 月 26 日和 Apache ShenYu(Incubating)联合举办线上 Meetup，约 6000 人次观看；\n2022 年 4 月之后，将定期举办一次 Meetup（包括海外联合 Meetup）......\n\n社区健康状态：\n\ndev@dolphinscheduler.apache.org 流量较上季度增加了 64%\n\n\n\n\n\n297 封电子邮件，上季度为 181 封\n上季度共 972 次 commits（增长 123%）\n上季度有 88 位代码贡献者（增长 25%）\n上季度在 GitHub 上新开 824 个 PR（增长 89%）\n上季度在 GitHub 上关闭了 818 个 PR（增长 100%）\n上季度在 GitHub 上新开 593 个 issue（增长 90%）\n上季度在 GitHub 上关闭了 608 个 issue（增长 155%）\n\n最活跃的 GitHub issues/PRs:\n\ndolphinscheduler/issues/8790[Bug] [Process Definition] Duplicate key TaskDefinition\t(31 comments)\ndolphinscheduler/issues/9068[Bug] [API server] could not get flow in exists project after upgrade from 2.0.1 to 2.0.5\t(27 comments)\ndolphinscheduler/pull/8340[Feature-8222][python] move examples into the scope of source package\t(17 comments)\ndolphinscheduler/pull/8246[Feature-8245][Alert] Add Alert Plugin Telegram\t(14 comments)\ndolphinscheduler/pull/9246[Fix-9221] [alert-server] optimization and gracefully close\t(14 comments)\ndolphinscheduler-website/pull/713[Feature-8023][Document] Add example and notice about task type Python\t(13 comments)\ndolphinscheduler/pull/8747[Fix-8744][standalone-server] start standalone server failed\t(13 comments)\ndolphinscheduler-website/pull/667[Feature-8020][Document] Add example and notice about task type SQL\t(12 comments)\ndolphinscheduler/issues/7992[Feature][Alert] Support PagerDuty Plugin &amp;&amp; Alert module judging strategy\t(11 comments)\ndolphinscheduler/pull/9336[Improvement-9338][API] show more create datasource exception message\t(11 comments)\n自成立以来，Apache DolphinScheduler 历经过次迭代，功能不断完善，性能持续提升，不断优化以符合开发者习惯的开发方式，为用户提供了经过生产实践环境检验的成熟工作流调度解决方案。\n\n目前，Apache DolphinScheduler 也开启了国际化的步伐，尝试增加了 Python，AWS，以及时区支持等，以接轨国际化的开发使用方式。\n见证中国开源奔跑的一年\n2021 年是 Apache DolphinScheduler 成长的一年，也是见证中国开源项目飞速发展的一年。\n\nCNCF 超过 20% 的开源项目来自中国，贡献度跃升至世界第二。\n2021 年，首次有华人（吴晟）当选为 Apache 软件基金会董事会董事。\n2021 年，来自中国的 5 个项目顺利进入 Apache 孵化器。截至目前，共有 14 个源自中国的 ASF 项目。\n还有 1 个孵化项目顺利毕业成为 Apache 顶级项目，那就是 Apache DolphinScheduler。\n开源大有可为，在此我们呼唤更多有志于开源的同伴，能够与 Apache DolphinScheduler 携手，在共同成长的过程中推进中国开源走向更高的世界舞台！\n\n",
    "title": "Apache DolphinScheduler ASF 孵化器毕业一周年，汇报来了！",
    "time": "2022-4-14"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.2",
    "content": "WorkflowAsCode 来了，Apache DolphinScheduler 2.0.2 惊喜发布！\n\n\n\n千呼万唤中，WorkflowAsCode 功能终于在 2.0.2 版本中如约上线，为有动态、批量创建和更新工作流需求的用户带来福音。\n此外，新版本还新增企业微信告警群聊会话消息推送，简化了元数据初始化流程，并修复了旧版本中强制终止后服务重启失败，添加 Hive 数据源失败等问题。\n01 新功能\n1 WorkflowAsCode\n首先在新功能上，2.0.2 版本重磅发布了 PythonGatewayServer， 这是一个 Workflow-as-code 的服务端，与 apiServer 等服务的启动方式相同。\n启用 PythonGatewayServer 后，所有 Python API 的请求都会发送到 PythonGatewayServer。Workflow-as-code 让用户可以通过 Python API 创建工作流，对于有动态、批量地创建和更新工作流的用户来说是一个好消息。通过 Workflow-as-code 创建的工作流与其他工作流一样，都可以在 web UI 查看。\n以下为一个 Workflow-as-code 测试用例：\n\n# 定义工作流属性，包括名称、调度周期、开始时间、使用租户等信息\nwith ProcessDefinition(\n    name=&quot;tutorial&quot;,\n    schedule=&quot;0 0 0 * * ? *&quot;,\n    start_time=&quot;2021-01-01&quot;,\n    tenant=&quot;tenant_exists&quot;,\n) as pd:\n    # 定义4个任务，4个都是 shell 任务，shell 任务的必填参数为任务名、命令信息，这里都是 echo 的 shell 命令\n    task_parent = Shell(name=&quot;task_parent&quot;, command=&quot;echo hello pydolphinscheduler&quot;)\n    task_child_one = Shell(name=&quot;task_child_one&quot;, command=&quot;echo &#x27;child one&#x27;&quot;)\n    task_child_two = Shell(name=&quot;task_child_two&quot;, command=&quot;echo &#x27;child two&#x27;&quot;)\n    task_union = Shell(name=&quot;task_union&quot;, command=&quot;echo union&quot;)\n\n    # 定义任务间依赖关系\n    # 这里将 task_child_one，task_child_two 先声明成一个任务组，通过 python 的 list 声明\n    task_group = [task_child_one, task_child_two]\n    # 使用 set_downstream 方法将任务组 task_group 声明成 task_parent 的下游，如果想要声明上游则使用 set_upstream\n    task_parent.set_downstream(task_group)\n\n    # 使用位操作符 &lt;&lt; 将任务 task_union 声明成 task_group 的下游，同时支持通过位操作符 &gt;&gt; 声明\n    task_union &lt;&lt; task_group\n\n\n上面的代码运行后，可以在 web UI 看到的工作流如下：\n                /                    \\\ntask_parent --&gt;                        --&gt;  task_union\n                \\                   /\n                  --&gt; task_child_two\n\n2 企业微信告警方式支持群聊消息推送\n在此前版本中，微信告警方式仅支持消息通知方式；在 2.0.2 版本中，用户在使用企业微信的告警时，支持进行应用内以群聊会话消息推送的方式推送给用户。\n02 优化\n1 简化元数据初始化流程\n首次安装 Apache DolphinScheduler 时，运行 create-dolphinscheduler.sh 需要从最早的版本逐步升级到当前版本。为了更方便快捷地初始化元数据流程，2.0.2 版本让用户可以直接安装当前版本的数据库脚本，提升安装速度。\n2 删除补数日期中的“+1”（天）\n删除了补数日期中的“+1”天，以避免补数时 UI 日期总显示 +1 给用户造成的困惑。\n03 Bug 修复\n[#7661] 修复 logger 在 worker 中的内存泄漏\n[#7750] 兼容历史版本数据源连接信息\n[#7705] 内存限制导致从 1.3.5 升级到 2.0.2 出现错误\n[#7786] 强制终止后服务重启失败\n[#7660] 流程定义版本创建时间错误\n[#7607] 执行 PROCEDURE 节点失败\n[#7639] 在通用配置项中添加 quartz 和 zookeeper 默认配置\n[#7654] 在依赖节点中，出现不属于当前项目的选项时报错\n[#7658] 工作流复制错误\n[#7609] worker sendResult 成功但 master 未收到错误时，工作流始终在运行\n[#7554] Standalone Server 中的 H2 会在数分钟后自动重启，导致数据异常丢失\n[#7434] 执行 MySQL 建表语句报错\n[#7537] 依赖节点重试延迟不起作用\n[#7392] 添加 Hive 数据源失败\n下载：https://dolphinscheduler.apache.org/zh-cn/download\nRelease Note：https://github.com/apache/dolphinscheduler/releases/tag/2.0.2\n04 致谢\n一如既往地，感谢所有为 2.0.2 版本建言献策并付诸行动的 Contributor（排名不分先后），是你们的智慧和付出让 Apache DolphinScheduler 更加符合用户的使用需求。\n\n\n\n05 参与贡献\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n参与 DolphinScheduler 社区有非常多的参与贡献的方式，包括：\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n非新手问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n如何参与贡献链接：https://dolphinscheduler.apache.org/zh-cn/community\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n社区官网\nhttps://dolphinscheduler.apache.org/\n代码仓地址\nhttps://github.com/apache/dolphinscheduler\n您的 Star，是 Apache DolphinScheduler 为爱发电的动力 ❤️ ～\n",
    "title": "WorkflowAsCode 来了，Apache DolphinScheduler 2.0.2 惊喜发布！",
    "time": "2022-1-13"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.3",
    "content": "Apache DolphinScheduler 2.0.3 发布，支持钉钉告警签名校验，数据源可从多个会话获取链接\n\n\n\n\n今天，Apache DolphinScheduler 宣布 2.0.3 版本正式发布。本版本支持钉钉告警签名校验，以及数据源从多个会话获取链接。此外，2.0.3 还对缓存管理、补数时间、日志中的数据源密码显示等进行优化，并修复了若干关键 Bug。\n\n新增功能\n钉钉告警支持加签名校验\n2.0.3 支持通过签名方式实现钉钉机器人报警的功能。\n\n\n\n钉钉的参数配置\n\nWebhook\n\n格式如下：https://oapi.dingtalk.com/robot/send?access_token=XXXXXX\n\nKeyword\n\n安全设置的自定义关键词\n\nSecret\n\n安全设置的加签\n自定义机器人发送消息时，可以通过手机号码指定“被@人列表”。在“被@人列表”中的人员收到该消息时，会有@消息提醒。设置为免打扰模式，会话仍然会有通知提醒，在首屏出现“有人@你”提示。\n\n@Mobiles\n\n被@人的手机号\n\n@UserIds\n\n被@人的用户 userid\n\n@All\n\n是否@所有人\n支持数据源从多个会话获取链接\n此前，使用 JdbcDataSourceProvider.createOneSessionJdbcDataSource() 方法 hive/impala 创建连接池设置了 MaximumPoolSize=1，但是调度任务中，如果 hive/impala 多任务同时运行，会出现 getConnection=null 的情况，SqlTask.prepareStatementAndBind() 方法会抛出空指针异常。\n2.0.3 优化了这一点，支持数据源从多个会话获取链接。\n优化\n缓存管理优化，减少 Master 调度过程中的 DB 查询次数\n由于主服务器调度进程，中会出现大量的数据库读操作，如 tenant、user、processDefinition 等，这一方面会给 DB 带来巨大压力，另一方面会减慢整个核心调度过程。\n考虑到这部分业务数据是多读少写的场景，2.0.3 引入了缓存模块，主要作用于 Master 节点，将业务数据如租户、工作流定义等进行缓存，降低数据库查询压力，加快核心调度进程，详情可查看官网文档：https://dolphinscheduler.apache.org/en-us/docs/3.1.2/architecture/cache\n补数时间区间从 “左闭右开” 改为 “左闭右闭”\n此前，补数时间为“左闭右开”(startDate &lt;= N &lt; endDate)，不利于用户理解。优化之后，部署时间区间改为“左闭右闭”。\n对日志中的数据源密码进行加密显示\n数据源中的密码进行加密，加强隐私保护。\nBug 修复\n\nzkRoot 配置不起作用\n修复修改管理员账号的用户信息引起的错误\n增加删除工作流定义同时删除工作流实例\nUDF 编辑文件夹对话框不能取消\n修复因为 netty 通讯没有失败重试，worker 和 master 通讯失败，导致工作流一直运行中的问题\n删除运行中的工作流，Master 会一直打印失败日志\n修复环境变量中选择 workerGroup 的问题\n修复依赖任务中告警设置不起作用的问题\n工作流历史版本查询信息出错\n解决高并发下任务日志输出影响性能的问题\nsub_process 节点的全局参数未传递给关联的工作流任务\nK8S 上 Master 任务登录时，查询日志无法显示内容\n进程定义列表中存在重复进程\n当流程实例 FailureStrategy.END 时任务失败，流程实例一直在运行\nt_ds_resources 表中的“is_directory”字段在 PostgreSQL 数据库中出现类型错误\n修复 Oracle 的 JDBC 连接\nDag 中有禁止节点时，执行流程异常\nquerySimpleList 返回错误的项目代码\n\nRelease Note: https://github.com/apache/dolphinscheduler/releases/tag/2.0.3\n下载地址： https://dolphinscheduler.apache.org/zh-cn/download\n感谢贡献者\n感谢社区 Contributor 对本版本的积极贡献，以下为 Contributor 名单，排名不分先后：\n\n\n\n",
    "title": "Apache DolphinScheduler 2.0.3 发布，支持钉钉告警签名校验，数据源可从多个会话获取链接",
    "time": "2022-1-27"
  },
  {
    "name": "Apache_dolphinScheduler_2.0.5",
    "content": "Apache DolphinScheduler 2.0.5 发布，Worker 容错流程优化\n\n\n\n今天，Apache DolphinScheduler 宣布 2.0.5 版本正式发布。此次版本进行了一些功能优化，如 Worker 的容错流程优化，在资源中心增加了重新上传文件的功能，并进行了若干 Bug 修复。\n优化\nWorker 容错流程\n2.0.5 版本优化了 worker 的容错流程，使得服务器由于压力过大导致 worker 服务中断时，可以正常将任务转移至其他 worker 上继续执行，避免任务中断。\n禁止运行任务页面标志\n优化禁止运行任务的页面显示标志，区别于正常执行的任务显示，以免用户混淆工作状态。\n\n\n\n任务框增加提示语\n2.0.5 版本在任务框上增加了提示语，可以显示出全部的长任务名字，方便用户查看。\n\n\n\n资源中心增加重新上传文件功能\n在资源中心增加了重新上传文件的功能，当用户需要修改执行脚本时，无需再重新配置任务参数，可实现自动更新执行脚本功能。\n修改工作流后跳转到列表页\n改变了此前修改工作流以后页面仍然留在 DAG 页面的现状，优化后可跳转到列表页，便于用户后续操作。\n钉钉告警插件新增 Markdown 信息类型\n在钉钉告警插件的告警内容中新增 Markdown 信息类型，丰富信息类型支持。\nBug 修复\n[#8213] 修复了当 worker 分组包含大写字母时，任务运行错误的问题；\n[#8347] 修复了当任务失败重试时，工作流不能被停止的问题；\n[#8135] 修复了 jdbc 连接参数不能输入‘@’的问题；\n[#8367] 修复了补数时可能不会正常结束的问题；\n[#8170] 修复了从页面上不能进入子工作流的问题。\n2.0.5 下载地址：\nhttps://dolphinscheduler.apache.org/zh-cn/download\nRelease Note：https://github.com/apache/dolphinscheduler/releases/tag/2.0.5\n感谢贡献者\n感谢 Apache DolphinScheduler 2.0.5 版本的贡献者，贡献者 GitHub ID 列表如下（排名不分先后）：\n\n\n\n",
    "title": "Apache DolphinScheduler 2_0_5 发布，Worker 容错流程优化",
    "time": "2022-3-7"
  },
  {
    "name": "Apache_dolphinScheduler_3.0.0_alpha",
    "content": "3.0.0 alpha 重磅发布！九大新功能、全新 UI 解锁调度系统新能力\n\n\n\n\n2022 年 4 月 22 日，Apache DolphinScheduler 正式宣布 3.0.0 alpha 版本发布！此次版本升级迎来了自发版以来的最大变化，众多全新功能和特性为用户带来新的体验和价值。\n3.0.0-alpha 的关键字，总结起来是 “更快、更现代化、更强、更易维护”。\n\n\n更快、更现代化： 重构了 UI 界面，新 UI 不仅用户响应速度提高数十倍，开发者构建速度也提高数百倍，且页面布局、图标样式都更加现代化；\n更强： 带来了许多振奋人心的新功能，如数据质量评估、自定义时区、支持 AWS，并新增多个任务插件和多个告警插件；\n更易维护： 后端服务拆分更加符合容器化和微服务化的发展趋势，还能明确各个服务的职责，让维护更加简单。\n\n新功能和新特性\n全新 UI，前端代码更健壮，速度更快\n3.0.0-alpha 最大的变化是引入了新的 UI，切换语言页面无需重新加载，并且新增了深色主题。新 UI 使用了 Vue3、TSX、Vite 相关技术栈。对比旧版 UI，新 UI 不仅更加现代化，操作也更加人性化，前端的鲁棒性也更强，使用户在编译时一旦发现代码中的问题，可以对接口参数进行校验，从而使前端代码更加健壮。\n此外，新架构和新技术栈不仅能让用户在操作 Apache DolphinScheduler 时响应速度有数十倍的提升，同时开发者本地编译和启动 UI 的速度有了数百倍的提升，这将大大缩短开发者调试和打包代码所需的时间。\n新 UI 使用体验：\n\n\n\n本地启动耗时对比\n\n\n\n首页\n\n\n\n工作流实例页面\n\n\n\nShell 任务页面\n\n\n\nMySQL  数据源页面\n支持  AWS\n随着 Apache DolphinScheduler 用户群体越来越丰富，吸引了很多海外用户。但在海外业务场景下，用户在调研过程中发现有两个影响用户便捷体验 Apache DolphinScheduler 的点，一个是时区问题，另一个则是对海外云厂商，尤其是对 AWS 的支持不足。为此，我们决定对 AWS 较为重要的组件进行支持，这也是此版本的最重大的变化之一。\n目前，Apache DolphinScheduler 对 AWS 的支持已经涵盖  Amazon EMR  和  Amazon Redshift两个 AWS 的任务类型，并实现了资源中心支持  Amazon S3 存储。\n\n针对 Amazon EMR，我们创建了一个新的任务类型，并提供了其 Run Job Flow 的功能，允许用户向 Amazon EMR 提交多个 steps 作业，并指定使用的资源数量。详情可见：https://dolphinscheduler.apache.org/zh-cn/docs/3.1.2/guide/task/emr\n\n\n\n\nAmazon EMR 任务定义\n\n对于 Amazon Redshift，我们目前在 SQL 任务类型中扩展了对 Amazon Redshift 数据源的支持，现在用户可以在 SQL 任务中选择 Redshift 数据源来运行 Amazon Redshift 任务。\n\n\n\n\nAmazon Redshift 支持\n对于 Amazon S3，我们扩展了 Apache DolphinScheduler 的资源中心，使其不仅能支持本地资源、HDFS 资源存储，同时支持 Amazon S3 作为资源中心的储存。详情可见：https://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/guide/resource/configuration.md中的\n`resource.storage.type`\n\n后续我们将用户的实际需求支持更多 AWS 任务，敬请期待。\n服务拆分\n全新的 UI 是 3.0.0-alpha 前端的最大变化，而后端最大的变化就是对服务进行拆分。考虑到容器和微服务的概念越来越火热，Apache DolphinScheduler 开发者做出了重大决定：对后端服务进行拆分。按照职能，我们将服务拆分成了以下几部分：\n\nmaster-server: master 服务\nworker-server: worker 服务\napi-server: API 服务\nalert-server: 告警服务\nstandalone-server: standalone 用于快速体验 dolphinscheduler 功能\nui: UI 资源\nbin: 快速启动脚本，主要是启动各个服务的脚本\ntools: 工具相关脚本，主要包含数据库创建，更新脚本\n所有的服务都可以通过执行下面的命令进行启动或者停止。\n\n`bin/dolphinscheduler-daemon.sh &lt;start|stop&gt; &lt;server-name&gt;`\n\n\n数据质量校验\n此版本中，用户期待已久的数据质量校验应用功能上线，解决了从源头同步的数据条数准确性，单表或多表周均、月均波动超过阈值告警等数据质量问题。Apache DolphinScheduler 此前版本解决了将任务以特定顺序和时间运行的问题，但数据运行完之后对数据的质量一直没有较为通用的衡量标准，用户需要付出额外的开发成本。\n现在，3.0.0-alpha 已经实现了数据质量原生支持，支持在工作流运行前进行数据质量校验过程，通过在数据质量功能模块中，由用户自定义数据质量的校验规则，实现了任务运行过程中对数据质量的严格控制和运行结果的监控。\n\n\n\n\n\n\n任务组\n任务组主要用于控制任务实例并发并明确组内优先级。用户在新建任务定义时，可配置当前任务对应的任务组，并配置任务在任务组内运行的优先级。当任务配置了任务组后，任务的执行除了要满足上游任务全部成功外，还需要满足当前任务组正在运行的任务小于资源池的大小。当大于或者等于资源池大小时，任务会进入等待状态等待下一次检查。当任务组中多个任务同时进到待运行队列中时，会先运行优先级高的任务。\n详见链接：https://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/guide/resource/configuration.md\n\n\n\n\n\n\n自定义时区\n在 3.0.0-alpha 之前版本，Apache DolphinScheduler 默认的时间是 UTC+8 时区，但随着用户群体扩大，海外用户和在海外开展跨时区业务的用户在使用中经常被时区所困扰。3.0.0-alpha 支持时区切换后，时区问题迎刃而解，满足了海外用户和出海业务伙伴的需求。例如，如当企业业务涉及的时区包含东八区和西五区，如果想要使用同一个 DolphinScheduler 集群，可以分别创建多个用户，每个用户使用自己当地的时区，对应 DolphinScheduler 对象显示的时间均会切换为对应时区的当地时间，更加符合当地开发者的使用习惯。\n\n\n\n详见链接：general-setting\n任务定义列表\n使用 Apache DolphinScheduler 3.0.0-alpha 此前版本，用户如果想要操作任务，需要先找到对应的工作流，并在工作流中定位到任务的位置之后才能编辑。然而，当工作流数量变多或单个工作流有较多的任务时，找到对应任务的过程将会变得非常痛苦，这不符合 Apache DolphinScheduler 所追求的 easy to use 理念。所以，我们在 3.0.0-alpha 中增加了任务定义页面，让用户可以通过任务名称快速定位到任务，并对任务进行操作，轻松实现批量任务变更。\n详见链接：https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0/guide/project/task-definition\n新增告警类型\n与此同时，3.0.0-alpha 告警类型也增加了对 Telegram、Webexteams 告警类型的支持。\nPython API 新功能\n3.0.0-alpha 中，Python API 最大的变化是将对应的 PythonGatewayServer 集成到了 API-Server 服务，使得开启对外服务更加规整，且缓解了因服务拆分导致的二进制包变大的问题。同时，Python API 还增加了 CLI 和 configuration 模块，让用户可以自定义配置文件，修改配置更加便捷。\n其他新功能\n除了上述功能外，3.0.0-alpha 版本还进行了很多细节功能增强，如重构任务插件、数据源插件模块，让扩展更简单；恢复了对 Spark SQL 的支持；E2E 测试已经完美兼容新 UI 等。\n主要优化项\n[#8584] 任务后端插件优化，新插件只需要修改插件自带的模块\n[#8874] 在工作流下提交/创建 cron 时验证结束时间和开始时间\n[#9016] Dependent 添加依赖时可以选择全局项目\n[#9221] AlertSender 优化及关闭优化，如 MasterServer\n[#9228] 实现使用 slot 扫描数据库\n[#9230] python gateway server 集成到 apiserver 来减少二进制包大小\n[#9372] [python] 将 pythonGatewayServer 迁移到 api 服务器\n[#9443] [python] 添加缺失的配置和连接远程服务器文档\n[#8719] [Master/Worker] 将任务 ack 更改为运行回调\n[#9293] [Master] 添加任务事件线程池\n主要 Bug 修复\n[#7236] 修复使用 S3a Minio 创建租户失败的问题\n[#7416] 修复文本文件 busy 的问题\n[#7896] 修复项目授权时生成一个重复授权项目的问题\n[#8089] 修复因无法连接到 PostgreSQL  而启动服务器失败的问题\n[#8183] 修复消息显示找不到数据源插件“Spark”的问题\n[#8202] 修复 MapReduce 生成的命令内置参数位置错误的问题\n[#8751] 解决更改参数用户，队列在 ProcessDefinition 中失效的问题\n[#8756] 解决使用依赖组件的进程无法在测试和生产环境之间迁移\n[#8760] 解决了资源文件删除条件的问题\n[#8791] 修复编辑复制节点的表单时影响原始节点数据的问题\n[#8951] 解决了 Worker 资源耗尽并导致停机的问题\n[#9243] 解决了某些类型的警报无法显示项目名称的问题\nRelease Note\nhttps://github.com/apache/dolphinscheduler/releases/tag/3.0.0-alpha\n感谢贡献者\n按首字母排序\nAaron Lin, Amy0104, Assert, BaoLiang, Benedict Jin, BenjaminWenqiYu, Brennan Fox, Devosend, DingPengfei, DuChaoJiaYou, EdwardYang, Eric Gao, Frank Chen, GaoTianDuo, HanayoZz, Hua Jiang, Ivan0626, Jeff Zhan, Jiajie Zhong, JieguangZhou, Jiezhi.G, JinYong Li, J·Y, Kerwin, Kevin.Shin, KingsleyY, Kirs, KyoYang, LinKai, LiuBodong, Manhua, Martin Huang, Maxwell, Molin Wang, OS, QuakeWang, ReonYu, SbloodyS, Shiwen Cheng, ShuiMuNianHuaLP, ShuoTiann, Sunny Lei, Tom, Tq, Wenjun Ruan, X&amp;Z, XiaochenNan, Yanbin Lin, Yao WANG, Zonglei Dong, aCodingAddict, aaronlinv, caishunfeng, calvin, calvinit, cheney, chouc, gaojun2048, guoshupei, hjli, huangxiaohai, janeHe13, jegger, jon-qj, kezhenxu94, labbomb, lgcareer, lhjzmn, lidongdai, lifeng, lilyzhou, lvshaokang, lyq, mans2singh, mask, mazhong, mgduoduo, myangle1120, nobolity, ououtt, ouyangyewei, pinkhello, qianli2022, ronyang1985, seagle, shuai hou, simsicon, songjianet, sparklezzz, springmonster, uh001, wangbowen, wangqiang, wangxj3, wangyang, wangyizhi, wind, worry, xiangzihao, xiaodi wang, xiaoguaiguai, xuhhui, yangyunxi, yc322, yihong, yimaixinchen, zchong, zekai-li, zhang, zhangxinruu, zhanqian, zhuangchong, zhuxt2015, zixi0825, zwZjut,\n天仇, 小张, 时光, 王强,  百岁, 弘树, 张俊杰, 罗铭涛\n",
    "title": "3.0.0 alpha 重磅发布！九大新功能、全新 UI 解锁调度系统新能力",
    "time": "2022-4-25"
  },
  {
    "name": "Application_and_practice_of_Apache_DolphinScheduler_offline_scheduling_in_Ziroom_(real_estate_agency)_multi-business_scenario",
    "content": "\n用户案例 | 自如\n随着自如业务的快速发展，不断增长的调度任务和历史逾万的存量任务对平台稳定性提出了更高的要求。同时，众多非专业开发人员也需要一种更为“亲民”的调度平台使用体验。\n如何满足这些日渐凸显的需求对自如大数据平台的开发团队来说，无疑是巨大的挑战。团队经过深入的研究和对比，发现Apache DolphinScheduler是一个能够满足自如当前所有核心需求的项目。\n至此，团队开始引入Apache DolphinScheduler，并在此基础上进行了一系列的改造和优化，其中包括自动生成SQL血缘解析调度，支持Ambari管理Apache DolphinScheduler，以及端到端调度组件的可用性监控等功能，从而更好地满足企业内需求。\n本文将详细描绘Apache DolphinScheduler在自如中的应用以及演变过程。\n作者简介\n陈卓宇，自如大数据平台运维，负责自如离线数据调度，Apache StreamPark PPMC，Apache DolphinScheduler Contributor\n业务挑战\n\n复杂的业务场景：自如的数据处理业务场景丰富多样，涵盖了To C和To B的品质居住产品、智慧生活服务、智能家装家居、智慧社区组织运营四大板块。\n大量的历史存量任务：自如历史累计的离线任务数量庞大，目前累计离线调度任务已达到1万+的规模，这对平台的稳定性提出了非常高的要求。\n离线任务增量大：目前，自如仍处于业务飞速发展的阶段，离线任务的数量持续增长，这对平台的扩展性和处理能力提出了极高的挑战。\n非专业开发人员的易用性需求：自如的数据使用人员主要包括运营人员、分析师、产品BP等非专业开发人员，他们对于配置调度的易用性要求严格，需要能支持SQL化操作，以及用户友好的配置界面，以达到&quot;平民化&quot;的使用体验。\n\n解决方案\n自如对调度技术选型的核心诉求\n自如对于调度技术的选型诉求可以从两个方面进行剖析：一是用户层面，二是技术运维层面。\n在用户层面，我们的用户期望平台能提供：\n\n简单易用的操作方式：使用户能快速上手，高效地进行需求逻辑开发。\n丰富的实践案例：供用户参考和学习，助力他们更好地理解和使用平台。\n\n在技术运维层面，我们的开发和运维团队期望平台能提供：\n\n通用的技术栈：便于进行二次开发，快速地将项目集成到自如的企业生态中。\n丰富的组件：支持多种多样的任务类型，满足各种业务需求。\n优秀的架构设计：确保项目具有高可用性、易扩展性以及支撑海量任务调度的能力。\n活跃的开源社区：遇到问题时，团队能够便捷且迅速地从社区获得必要的帮助。\n\n针对上述的核心诉求，自如团队对行业内的所有相关项目进行了深入的调研，并最终发现，Apache DolphinScheduler是唯一一个能满足自如团队所有核心诉求的项目。因此，我们选择了Apache DolphinScheduler2.0.6版本作为自如的离线调度技术解决方案。\n架构设计\n目前，自如已经成功地通过内部研发构建了一套可供全集团使用的大数据平台。为了进一步满足离线数据仓库这一垂直领域的需求，自如选择使用Apache DolphinScheduler进行集成与扩展。这一改进旨在提升整体平台的能力，从而让其能够为集团内的各个业务部门提供更强大的数据加工、数据编排的能力。\n\n如图所示，数据调度是自如数据开发流程中的第四个环节，属于整个流程中承上启下的核心。在流程的上游，如河图（数据血缘查询服务）的血缘功能以及Hue的查询能力，共同为生成SQL业务逻辑提供支持，而数据调度的角色则是编排加工这些SQL逻辑。\n在流程的下游，首先是数据质量模块，该模块的核心职责是确保调度输出的准确性，也就是我们所说的&quot;口径&quot;的确保。通过这一环节，我们保障了数据的质量，为后续步骤提供了可靠的基础。其次是数据服务化模块，这个模块的主要任务在于，对调度结果进行再次的汇总和加工，使其在指标层面上达到我们的预期，并以标准化的Restful服务的形式提供出来，以供用户使用。\n二次开发实践\n血缘解析自动生成调度\n\n如上图所示，SQL任务节点的复杂性由其涉及的众多上游依赖表关系可见一斑，近30个表关系互相交织。在这种复杂的依赖关系面前，如果仅由业务人员来完成，他们不仅需要花费大量时间去理解公司相关业务线下表的关联关系，同时还需掌握DolphinScheduler的配置和使用方式，这对他们来说是一项沉重的负担。\n因此，自如团队找到了一种新的解决方案：血缘解析自动生成调度。这个方案的核心价值在于，它完全的解放了业务人员，使其从繁琐的调度配置中脱离出来。仅需提供SQL语句，程序会读取这个SQL，解析它，得出调度需要的输入表和输出表。解析的结果将用于自动生成并配置调度。\n\n如图所示，用户只需将在Hue中调试完毕的SQL一键同步至调度平台。在此，只需点击“解析”按钮，系统就会自动根据SQL的血缘关系解析到所有相关的依赖项，并根据这些信息自动生成任务的有向无环图（DAG）。这样的设计旨在最大限度地简化用户的操作步骤，同时确保调度任务的准确性和效率。\n所以，原本复杂的配置流程被简化为用户提供SQL语句，剩下的全部由系统完成。从而实现业务人员仅需专注于业务需求，高效、简洁地完成工作，而无需为了应对这些需求困于复杂配置和技术细节。\n支持混合的数据依赖和任务依赖\n\n自如内部历史存量的调度任务有近1万个，这些任务分散在业务线自研调度、Airflow调度等多个调度平台上。团队并不打算将这些历史存量任务统一的迁移到新的调度平台上，更倾向于保持现状，同时满足新的需求。基于这样的考虑，自如在DolphinScheduler上设计实现了&quot;数据依赖&quot;功能，使DolphinScheduler支持混合的依赖模式，从而为调度平台提供更好的兼容性。\n通过这种方式，DolphinScheduler既能处理&quot;任务依赖&quot;，也能处理&quot;数据依赖&quot;。&quot;任务依赖&quot;是指通过检测任务是否执行成功来触发工作流执行，而&quot;数据依赖&quot;是通过检测数据的分区是否产生来判定工作流是否向下执行。这种混合的依赖处理方式使DolphinScheduler能够将新的任务和业务线间的历史调度任务进行有效链接，形成一个统一、高效的调度体系。\n任务依赖重构\nDolphinScheduler使用DEPENDENT插件来实现任务间的依赖关系。然而，这里会存在一个隐藏的问题，即任务间的依赖关系是基于工作流定义的code码来建立的。当构建一个复杂的基于数仓分层的任务流时，比如ods-&gt;dwd-&gt;ads-&gt;xxxx，如果用户误删了一个最上游的ods任务，并重新创建了该任务，那么任务之间的依赖关系将无法正确建立，因为尽管任务名称相同，但是code码已经改变了。\n为了解决这个问题，我们对任务依赖机制进行了重构。改为基于任务名称来建立的依赖关系，并在创建任务时进行规则验证，确保任务名称在整个集群中是唯一的。这样一来，即使用户误删除后重新创建任务，依赖关系仍然能够正确地挂载上。\n通过这个改进，我们确保了任务之间的依赖关系在整个调度系统中的准确性和稳定性。用户不再需要担心误删而导致依赖关系无法建立的问题。这样，用户可以更加自信地构建复杂的任务流，确保任务间的依赖关系正确地被建立和维护。\n支持基于配置的调度生成\n在实际工作中，我们还会遇到一些用户不懂SQL，但仍然需要进行数据的加工和使用的需求。为了满足这部分用户，我们实现了基于配置的调度生成模式。这种模式的设计思路是让用户通过简单的配置来定义数据加工和处理的流程，而无需编写复杂的SQL语句。\n\n\n通过选择配置，包括表、表关系、筛选条件、目标等信息，我们的平台可以自动根据用户所选信息生成相应的SQL语句。然后，根据生成的SQL语句，再进行上述提到的SQL解析和调度配置的步骤，完成调度任务的配置过程。\n这种基于配置的调度生成模式，使得不懂SQL语言的用户也能够轻松完成数据的加工和使用，大大降低了学习和使用的门槛。同时，它也提高了开发效率，减少了出错的可能性。这样的设计使得我们能够更好地覆盖不同技术水平的用户，并满足他们的需求。\nAmbari管理DS的支持\n\n参数配置统一管理、多版本对比\n\nDolphinScheduler作为一个分布式应用程序，对配置文件的每次修改都需要在各节点之间进行同步。这个过程不仅繁琐，而且无法追溯历史变动。特别是对线上服务的配置，一个微小的变动可能就会导致服务的瘫痪。鉴于这些问题，我们的团队急需一个能提供明确配置项展示、配置回滚以及历史版本对比等功能的工具。\nAmbari为我们提供了这样的解决方案。因此，我们通过编写插件，将DolphinScheduler与Ambari进行集成，从而实现对所有配置项的统一管理。这种集成方式不仅减轻了运维人员的工作压力，还增强了服务的稳定性，为系统的健壮性和可靠性带来了有力的保证。\n配置项统一管理：\n\n配置项修改版本对比：\n\n\n\n可视化的一键集群启停\n\n在Ambari的管理界面上，用户可以清晰地查看整个集群的状态，包括各节点的运行情况。通过简单的一键操作，用户能够启动或停止部署在多个服务器节点上的整个集群，这使得集群管理更加直观和便捷。\n\n\n支持自动服务故障“自我修复”\n\n在Ambari平台上，实现DolphinScheduler服务的“自我修复”非常简单。只需在平台上选择需要被监听的DolphinScheduler服务，就可以利用Ambari的Service Auto Start功能来轻松实现。当DolphinScheduler服务因某些异常情况宕机时，Service Auto Start会自动不断尝试重启服务，从而确保DolphinScheduler服务的高可用性，极大地提高了系统的稳定性。\n\n端到端调度组件可用性监控WatchDog\n虽然DolphinScheduler已经拥有了任务容错机制和高可用机制，但在实际执行过程中，仍需要涉及于很多外部服务，例如Hiveserver2、ClickHouse等。在这种复杂的环境中，对于运维人员来说，其主要目标并不是要求系统一定要100%无故障，而是希望在出现问题时能立即得到响应，从而使运维人员能尽快介入并解决问题。因此，自如开发了一种端到端的全链路可用性测试任务—WatchDog。\nWatchDog的目标是监控整个调度流程的可用性，一旦发现问题，立即向运维人员发出警报。这样的设计大大提高了问题的响应速度和系统的稳定性，减轻了运维人员的负担，也增强了对整个系统运行状态的控制能力。\n\n内部核心逻辑“埋点”\n在实际使用DolphinScheduler（2.0.6版本）的初期，毫不掩饰地说，确实遇到了许多问题。然而，这些问题并没有使团队气馁，因为团队也充分理解DolphinScheduler作为一个相对年轻的调度产品，出现各种问题是非常正常。因此，我们决定修复这些问题，来确保DolphinScheduler能顺利的在自如“安家”。\n在这个过程中，遇到了许多挑战。例如，调度系统在运行时，可能会有成百上千个任务在同一时间并行执行，每个工作流都对应一个线程对象和内置的存储队列。此外，还有众多的后台线程和远程Netty通讯触发流转等逻辑交织在一起，这些都给问题的定位带来了很大的困难。\n为了解决这个问题，我们开发了一个专门的异常注解。用该注解标注方法后，当发生异常时，它会记录输入参数、输出参数、线程名称、执行时间、所在服务器节点以及详细的异常堆栈信息。然后，通过企业微信和电话告警，将这些信息立即通知给运维人员，从而实现问题的及时定位和处理。因此，我们对调度流程的关键环节，如启动、停止、补数、容错和定时调度，进行了注解覆盖。这种方法极大地增强了我们对系统运行状态的控制力，使我们能更快地发现并解决问题。\nJVM参数优化监控\nJVM调优的关键在于寻求一种'平衡'，我们通过调整内存大小来平衡垃圾回收的频率与时长。\n我们的优化目标是：一，保证服务的吞吐率；二，减少服务的停顿时间。同时，我们对DolphinScheduler中可能存在内存泄露的代码进行了修正，特别是将一些static属性更改为局部变量，以此来避免内存泄漏。这是因为局部变量只在代码块中起作用，而static属性是与类对象相关的。在JVM中，类的卸载条件非常严格，我们的应用程序是由系统类加载器加载的，这样加载的类是不会被卸载的。这就意味着如果将一个对象挂在该类加载器上，该对象就不会被释放。而相比之下，局部变量在使用后会很快被释放。因此，我们的目标是避免对JVM进程中的任何集合或重量级对象进行静态标记。\n下面是我们在JVM参数优化方面所做的JVM配置，供读者参考：\n\nXX:+UseG1GC：使用G1垃圾回收器。\nXX:MaxGCPauseMillis=500：设置最大GC暂停时间为500毫秒。\nXX:+PrintGCDetails\nXX:+PrintGCTimeStamps\nXX:+PrintGCCause：打印GC的详细信息、时间戳和GC原因。\nXX:+UseGCLogFileRotation\nXX:NumberOfGCLogFiles=10\nXX:GCLogFileSize=10M：启用GC日志文件的轮转，保留10个文件，每个文件大小为10兆字节。\nXX:+HeapDumpOnOutOfMemoryError、\nXX:+HeapDumpBeforeFullGC：在OutOfMemoryError发生时生成堆转储文件，并在执行Full GC之前生成堆转储文件。\nXmx16g、-Xms16g：设置堆的最大和初始大小为16GB。\nXX:MetaspaceSize=512m\nXX:MaxMetaspaceSize=512m：设置元空间的初始大小和最大大小为512MB。\n\n此外，我们还通过使用javaAgent来收集JVM相关的指标，并将其上报到Prometheus，以建立更为全面的监控。\n权限管控\n我们对调度权限的管理主要体现在通过控制HQL任务执行队列来实现。首先，在登录过程中，调度平台会通过LDAP对用户进行验证。服务端会获取用户LDAP所在的部门，并根据部门名称在调度平台上建立相应的项目。换言之，用户在LDAP中的部门将会被映射到DolphinScheduler上的一个项目空间。\n其次，每一个项目都会关联到一个特定的Hive数据源。每个数据源的差异在于它们的队列设置。由于不同的部门任务会因为关联到的数据源而提交到相应的Yarn队列上，这一设定使得各部门的资源使用得以计价和进行有效治理。\n通过这种方式，我们确保了调度权限的严谨管控，同时也实现了部门资源的准确计算和高效治理。\n用户收益\n\n满足了业务人员、分析师、产品BP的数据加工和使用需求：通过基于配置的调度生成模式和自动生成SQL的功能，我们能够满足那些不懂SQL语言的业务人员、分析师和产品BP的数据加工和使用需求。他们可以通过简单的配置来定义数据加工流程，而无需编写复杂的SQL语句。这使得他们能够快速、轻松地完成数据的处理和使用，提高了他们的工作效率。\n满足运维人员维护庞杂数据调度的需求：对于运维人员来说，维护庞杂的数据调度任务是一项具有挑战性的任务。然而，通过我们的调度平台，运维人员能够更加方便地管理和维护这些调度任务。平台提供了可视化的操作界面和丰富的功能，如配置回滚、历史版本对比等，使得运维人员能够轻松地进行任务的管理和维护。同时，端到端的可用性监控工具WatchDog也能及时发现系统故障，提高了调度系统的稳定性和可靠性。\n\n通过满足业务人员、分析师、产品BP的数据加工和使用需求，以及运维人员维护庞杂数据调度的需求，我们基于Apache DolphinScheduler 进行升级改造后的调度平台能够为各个角色提供全面的支持，促进业务的顺利进行和高效运维。\n",
    "title": "Apache DolphinScheduler 离线调度在自如多业务场景下的应用与实践？",
    "time": "2023-9-19"
  },
  {
    "name": "Application_transformation_based_on_DolphinScheduler_in_financial_technology_data_center",
    "content": "在Apache DolphinScheduler Meetup 上，来自 成方金融科技的 大数据工程师 冯鸣夏 为大家带来 DolphinScheduler 在金融科技领域的应用实践分享。以下为演讲整理：\n-冯鸣夏 成方金融科技 大数据工程师-\n聚焦于大数据领域的实时和离线数据处理和分析，目前主要负责数据中台的研发。\n演讲概要：\n\n\n使用背景\n\n\n基于 DolphinScheduler 的二次改造\n\n\nDolphinScheduler 的插件扩充\n\n\n未来和展望\n\n\n1 使用背景\n01 数据中台建设\n目前，大数据技术在金融领域有着广泛的应用，大数据平台已经成为了金融基础设施。在大数据平台的建设中，数据中台又是最亮的那颗星，它是业务系统使用大数据的入口和接口，当纷杂的业务系统接入数据中台时，数据中台需要提供统一的管理和统一的入口，以保障服务的安全高靠高效和可靠。正如下图所示，数据中台处于各业务系统连接大数据平台的中间环节，各业务系统通过数据中台所提供的服务对大数据平台进行数据访问。\n数据中台的核心理念是实现 4 个四化，即业务数据化、数据资产化、资产服务化和服务业务化。从业务到数据，再回到业务形成的完整闭环，支持企业的数字化转型。\n数据中台逻辑架构图\n数据中台的逻辑架构如上图所示，从下往上分析，首先最底层是数据资源层，这是各个业务系统产生的原始数据；再往上一层是数据集成，数据集成的方式分为离线采集和实时采集，其中采用的技术包括 Flume、CDC 实时采集等。\n再往上一层是目前比较热的数据湖了，通过数据集成手段将数据入湖，存到 Hadoop 的分布式存储或者 MPP 架构的数据库中。\n再往上一层是数据引擎层，通过实时和离线计算引擎 Flink、Spark 等对数据湖中的数据进行处理分析，形成可供上层使用的服务数据。\n再上一层就是数据中台所需要提供的数据服务了，数据服务目前包括数据开发服务和数据共享服务，为上层的各业务系统提供数据的开发和共享能力。\n数据应用层是数据的具体应用，包括数据的异常检测、数据治理、AI 人工智能的决策以及BI分析等。\n在整个的数据中台的建设中，调度引擎在数据引擎层中是属于比较核心的位置，也是数据中台建设中比较重要的功能。\n02 数据中台面对的问题和挑战\n数据中台会面临一些问题和挑战。首先，数据任务的执行和调度是数据中台提供数据开发服务的核心和关键。\n其次，数据中台对外提供统一的数据服务管理，服务开发，服务调用和服务监控。\n第三，保障金融数据的安全是金融科技的首要任务，数据中台需要保障数据服务的安全可靠。\n在以上这些问题的和挑战下，我们对一些开源的调度引擎进行了调研。\n目前我们在生产过程中同时使用了多种调度引擎，比如 oozie，XXL-job，DolphinScheduler 是我们 2022 年通过调研分析引进的调度引擎,它在整个数据中台的建设中起到了非常重要的作用。\n首先，DolphinScheduler 部分地解决了我们统一服务管理、服务开发、服务调用和服务管理的需求。其次，它在任务容错方面有自己独到设计，支持 HA、弹性扩展、故障容错，基本保障任务的安全运行。第三，它支持任务和节点的监控。第四，它支持多租户和权限的控制。最后，它的社区非常活跃，版本更迭快速，问题修复也是非常快速。通过分析 DolphinScheduler 的架构和源码分析，我们认为它的架构符合主流的大数据框架设计，和Hbase、Kafka 等优秀的国外产品有类似的架构模式和设计。\n2 基于DolphinScheduler 的二次改造\n为了让 DolphinScheduler 更加符合我们应用场景的需要，我们基于 DolphinScheduler 进行了二次改造，共包括 6 个方面。\n\n增加异步服务调用功能\n增加元数据库Oracle适配\n增加多环境配置能力\n增加日志和历史数据清理策略\n增加对Yarn日志的获取能力\n增加服务安全策略\n\n01 增加异步服务调用功能\n首先是增加了异步服务调用功能，上图是 DolphinScheduler 2.0.5版本的架构图，大部分都是原生 DolphinScheduler 的服务组件，其中标标红的 GateWay 是我们基于 DolphinScheduler 增加的一个网关服务，通过它实现了流量控制、黑白名单，同时也是用户访问服务开发的入口。通过优化流程的启动接口，返回流程的唯一编码，我们增加了服务映射的功能。\n在经典的 DolphinScheduler 的访问模式中，用户提交的工作流执行指令会进入到原数据库中的 command 表里面，master 组件拿到 zk 锁后从 元数据库中获取command，并进行 DAG 解析，生成实际的流程实例，并将分解的任务通过 RPC 交付 work节点执行，然后同步等待执行结果。在原生的 DolphinScheduler 请求中，用户提交了指令之后，缺少执行工作流的返回码，所以我们增加了返回的唯一标识，用户可以通过这个唯一的返回标识，进行后续的流程状态的查询，日志的下载和数据的下载。\n02 增加元数据库 Oracle 适配\n我们的第二个改造是对  DolphinScheduler 与 Oracle 数据库进行了适配。目前原生 DolphinScheduler 的元数据库是 MySQL，而根据我们的生产需求需要将原数据库转换成Oracle数据库。为实现这一点，需要完成数据初始化模块和数据操作模块的适配。\n首先，对于数据的初始化模块，我们修改了 install_config.conf 配置文件，将其更改为Oracle 的配置。\n其次，需要增加Oracle的application.yml，我们在 dolphinscheduler-2.0./apache-dolphinscheduler-2.0.-bin/conf/ 目录下增加Oracle的application.yml。\n最后，我们对数据操作模块进行转换，对 mapper 文件和的文件进行相应的修改，因为 Dolphinscheduler-dao 模块是数据库操作模块，其它模块会引用该模块实现数据库的操作。它使用 Mybatis 进行数据库连接，所以需要更改 mapper文件，所有的 mapper 文件在 resources 目录下。\n03 多环境配置能力\n原生的 DolphinScheduler 版本安装无法根据环境进行配置，一般需要根据实际环境进行调整相关的参数。我们希望通过增强安装脚本对环境的选择配置，以减少人为在线修改的成本，实现自动化安装。相信小伙伴们也都遇到类似的困境，为了在开发环境、测试环境、联调环境、性能环境、准生产环境、生产环境上使用DolphinScheduler，需要进行大量环境相关配置参数的修改。\n我们通过修改 install.sh 文件，增加输入参数[dev|test|product]，选择合适install_config_${evn}.conf 进行安装，可以实现了环境的自动选择。\n另外，DolphinScheduler 的工作流跟环境是强绑定的，不同环境的工作流无法共用。下图是原生 DolphinScheduler 导出的一个工作流的JSON文件，标灰的部分表示这个流程所依赖的 resource 资源， id是一个数字，是由数据库自增所产生的。但是如果 a 环境产生的流程实例放到 b 环境中，可能就会存在ID主键冲突。换句话说，就是不同环境所产生的工作流是无法进行共用的。\n我们通过将资源的绝对路径所作为资源的唯一ID这种生成的方式来解决这个问题。\n04 日志和历史数据清理策略\nDolphinScheduler 所产生的数据非常多，数据库中会产生实例表中的实例数据，这些数据会随着实例任务不停运行不断地增长。我们采用的策略就是通过定义 DolphinScheduler的定时任务，将这些表的数据按照约定的保存周期进行清理。\n其次，DolphinScheduler 的数据主要是日志数据和任务执行目录，其中包括worker、master、API 的服务日志数据以及 worker 执行的目录，这些数据并不会随着任务的执行的结束而自动删除，也需要通过定时任务删除。通过运行日志清理脚本，我们可以实现日志的自动删除。\n05 增加对 Yarn 日志的获取能力\n原生的 DolphinScheduler 可以获取在worker 节点上执行的日志信息，但是对于 Yarn 上的任务还需要登录到 Yarn 的集群上，通过命令或者界面的方式获取。我们通过分析日志中的 YARNID 标签，获取 Yarn 任务 ID，通过 yarnclient 获取任务的日志。减少了手工查看日志的过程。\n1234567890\n06 服务安全策略\n\n增加 Monitor 组件监控\n\n上图是是 DolphinScheduler 中两大核心组件 master和 worker 与Zookeeper 的交互过程。MasterServer 服务启动时会向 Zookeeper 注册临时节点，通过监听 Zookeeper 临时节点变化来进行容错处理。WorkerServer 主要负责任务的执行。WorkerServer 服务启动时向 Zookeeper 注册临时节点，并维持心跳。目前Zookeeper起到非常重要的作用，主要进行服务的注册和心跳的检测。\n从上面这张表格可以看到  master和 worker连接 Zookeeper 时的一些相关参数，包括连接超时，session超时时间，最大连重试次数。\n由于网络抖动等因素，可能会造成 master 和 worker节点与 zk 失联的情况。失联之后，worker 和 master 因为在 zk 上注册的临时信息消失，会判定 zk 与 master 和 worker 失联，影响任务的执行。如果没有人工干预，就会导致任务迟迟得不到响应。我们增加了 monitor 组件进行服务状态的监控，通过定时任务 cron, 每 5 分钟运行一次 monitor 程序检测 worker 进程和 master 进程是否存活，如果宕机则重新调起。\n\n增加服务组件使用 zk 的 Kerberos 认证环节\n\n第二个安全策略是增加了服务组件使用 zk 的 Kerberos 认证环节。Kerberos 是一种网络认证协议，其设计目的是通过密钥系统为客户机/服务器应用程序提供强大的认证服务。Master 服务组件，API 服务组件，worker 服务组件在启动时完成  Kerberos 认证之后再使用 zk 进行相关的服务注册和心跳连接，从而保证服务的安全。\n3 基于DolphinScheduler的插件扩展\n此外，我们还基于 DolphinScheduler进行了插件扩展，我们扩展了 4 类算子，包括 Richshell、SparkSQL、Dataexport 和 GBase 算子。\n01 增加新的任务类型Richshell\n首先是新增了任务类型 Richshell，增强了原生的 Shell功能，主要是通过模板引擎实现脚本参数动态替换，用户通过服务调用实现脚本参数的替换，让用户使用参数更加灵活，是对全局参数的补充。\n02 增加新的任务类型SparkSQL\n第二个增加的算子是 SparkSQL，用户通过编写 SQL执行 Spark 任务，让任务调度在Yarn 上。DolphinScheduler 原生也支持以 JDBC 的方式执行 SparkSQL，但存在l资源争抢的情况，因为 JDBC 连接数是有限的。通过SparkSQL /Spark-beeline 等工具执行不能使用 Yarn cluster 模式。而采用该任务类型可以将 SparkSQL 程序以cluster 的模式运行在 Yarn 集群上，最大限度地利用集群资源，减少客户端的资源使用。\n03 增加新的任务类型 Dataexport\n第三个增加的是 Dataexport，也就是数据导出算子，让用户可以通过选择不同的存储组件，导出存储在组件中的数据。组件包括 ES、Hive、Hbase 等。\n在大数据平台中的数据，被导出后后续可能被用于 BI 展示、统计分析、机器学习等数据准备，而这些场景大多需要进行数据导出，利用 Spark 数据处理能力实现不同数据源的导出功能。\n04 增加新的任务类型GBase\n第四个增加的插件是 Gbase。GBase 8a MPP Cluster 是一款列式存储，Shared Nothing架构的分布式并行数据库集群，具备高性能、高可用、高扩展等特性，适用于OLAP场景（查询场景），可以为超大规模数据管理提供高性价比的通用计算平台，并广泛用于支撑各类数据仓库系统、BI  系统和决策支持系统。\nGBase 作为数据入湖的一个应用场景，我们新增了 GBase 算子，它支持 GBase 数据的导入、导出和执行。\n4 未来与展望\n未来，我们将增加云原生的支持，对目前架构进行云原生改造，DolphinScheduler 已经迭代至 3 版本，我们会持续更新支持。第二，增加AI模型算子，AI 应用场景越来越多，我们需要对 更多的AI 场景进行抽象 ；第三，增加灰度发布功能，以实现无感知发布；第四，增加基于用户优先级的调度策略。\n我相信随着 DolphinScheduler、Kylin 等中国开源社区的蓬勃发展，国产软件一定会迎来更好的未来。最后和大家分享一句话：“没有比追梦更加激荡人心的力量，没有比梦想更加铿锵有力的步伐”，与大家共勉！\n",
    "title": "金融科技数据中台基于 DolphinScheduler 的应用改造",
    "time": "2023-8-24"
  },
  {
    "name": "Awarded_most_popular_project_in_2021",
    "content": "Apache DolphinScheduler 获评 2021 年度「最受欢迎项目」！\n\n\n\n\n近日，由 OSCHINA 举办的「2021 OSC 中国开源项目」评选活动公布了评选结果。\n在广大用户和开源社区的喜爱和支持下，云原生分布式大数据调度系统 Apache DolphinScheduler 获评 2021 年度「OSCHINA 人气指数 Top 50 开源项目」和「最受欢迎项目」。\n\n获评「最受欢迎项目」\n\n\n\n\n\n\n今年，「2021 OSC 中国开源项目」活动设置了两轮投票环节，第一轮投票根据票数选出了「OSCHINA 人气指数 TOP 50 开源项目」。第二轮投票基于第一轮选出的 TOP 50 项目进行，并在此基础上通过投票选出了 30 个「最受欢迎项目」。\n在第一轮投票中，OSCHINA 根据票数选出「组织」项目 7 大分类（基础软件、云原生、大前端、DevOps、开发框架与工具、AI &amp; 大数据、IoT &amp; 5G）中每个分类的 TOP 5，Apache DolphinScheduler 在「云原生」大类中脱颖而出，凭借优秀的云原生能力入选。\n之后，经过第二轮投票的激烈角逐，Apache DolphinScheduler 再次胜出，获得「最受欢迎项目」奖项。\n\n\n\n中国开源软件生态蓬勃发展，近年来涌现出了一大批优秀的开源软件创企，他们不忘初心，深耕开源，回馈社区，为中国开源软件事业添砖加瓦，成为全球开源软件生态中不可忽视的重要力量。「OSC 中国开源项目评选」是开源中国（OSCHINA，OSC 开源社区）举办的国内最权威、最盛大的开源项目评选活动，旨在更好地展示国内开源现状，探讨国内开源趋势，激励国内开源人才，促进国内开源生态完善。\nApache DolphinScheduler：分布式工作流任务调度系统\nApache DolphinScheduler 是 Apache 基金会孵化的顶级项目。作为新一代大数据任务调度系统，致力于“解决大数据任务之间错综复杂的依赖关系，使整个数据处理流程直观可见”。DolphinScheduler 以 DAG(有向无环图)  的方式将 Task 组装起来，可实时监控任务的运行状态，同时支持重试、从指定节点恢复失败、暂停及 Kill 任务等操作，并专注于可视化 DAG、调用高可用、丰富的任务类型、以来、任务日志/告警机制和补数 6 大能力。\n迄今为止，Apache DolphinScheduler 社区已经有 280+ 经验丰富的代码贡献者，110+ 非代码贡献者，其中也不乏其他 Apache 顶级项目的 PMC 或者 Committer。自创建以来，Apache DolphinScheduler 开源社区在不断发展壮大，微信用户群已达 6000+ 人。截止 2022 年 1 月，已经有 600 + 家公司及机构在生产环境中采用 Apache DolphinScheduler。 \n最后，感谢 OSCHINA 对 Apache DolphinScheduler 以及其背后商业公司白鲸开源科技的认可，更感谢社区每一位参与者，开源之力滴水成河，Apache DolphinScheduler 社区的壮大离不开每一位贡献者的参与。社区将以此为鞭策，希望在更多小伙伴的助力之下，社区可以再上一层楼！\n参与贡献\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n参与 DolphinScheduler 社区有非常多的参与贡献的方式，包括：\n文档、翻译、答疑、测试、代码、实践文章、原理文章、会议分享等\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n\n\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n\n\n非新手问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n\n\n如何参与贡献链接：https://dolphinscheduler.apache.org/en-us/community\n\n\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n参与开源可以近距离与各路高手切磋，迅速提升自己的技能，如果您想参与贡献，我们有个贡献者种子孵化群，可以添加社区小助手微信(Leonard-ds) 手把手教会您( 贡献者不分水平高低，有问必答，关键是有一颗愿意贡献的心 )。添加小助手微信时请说明想参与贡献。\n",
    "title": "Apache DolphinScheduler 获评 2021 年度「最受欢迎项目」",
    "time": "2022-1-7"
  },
  {
    "name": "Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler",
    "content": "迁移背景\n有部分用户原来是使用 Airflow 作为调度系统的，但是由于 Airflow 只能通过代码来定义工作流，并且没有对资源、项目的粒度划分，导致在部分需要较强权限控制的场景下不能很好的贴合客户需求，所以部分用户需要将调度系统从 Airflow 迁移到 Apache Dolphinscheduler。\n\n秉承着解决用户实际需求的角度出发，Whaleops 研发了 Air2phin 迁移工具，协助用户更好的迁移到 DolphinScheduler 中。由于 Airflow 是通过 python code 来定义工作流的，并且有部分元数据信息仅仅在 python code 中而不会持久化到数据库中，所以我们需要通过解析 python code 来完成分析和迁移的步骤\n为什么要迁移到 DolphinScheduler\nAirflow 和 DolphinScheduler 都是任务调度系统，都解决了任务编排的问题。两者各有优势，这个章节中我们仅介绍 DolphinScheduler 相对 Airflow 的优势，两者的对比文章我们会在以后详细对比的文章中描述：\n\n两者都是成熟的开源工作流调度系统，都有成熟的社区，细微的区别是\n\n**Apache DolphinScheduler：**以可视化为主，API为辅，有更细粒度的权限管理，工作流层级更多，使用成本更加低，数据平民化\n**Airflow：**以代码定义工作流， 编写工作流为高级研发，灵活性较高，但是使用成本更高，基本上是面向研发人员\n\n\nDolphinScheduler 因为将工作流定义、任务定义、任务关系都存储到原数据库中，所以\n\n在增减 master worker 节点时没有额外的操作，airflow 在增加 master 和 worker 节点时，需要将 dags 文件复制到新的节点中\n同时由于存在解析文件获取工作流、任务的过程，没有新增、更改任务的延时，自然也不需要为了降低延时而牺牲 CPU 性能的说法。airflow 是使用 loop 的方式发现和调度 DAGs 的，所以在 loop 的时候 scheduler 会消耗较多的 cpu 资源\n能保留完整的历史工作流、任务运行状态。airflow 如果最新的定义中删除了部分任务，则不能找到这些任务的历史状态和日志\n原生支持版本的信息。airflow 的 DAGs 定义需要在 git log 中查找，revert 也需要通过 git\n\n\nDolphinScheduler 支持资源中心，更加方便用户管理、组织包括本地和远程的资源文件。airflow 如果有外部资源的话，一般需要和git 一起托管在版本控制中\n除开离线调度任务的工作外，DolphinScheduler 还支持实时任务、数据资料、对物理机器资源的监控等调度相关的实用功能。airflow 目前来说更加专注的是离线工作流调度\nDolphinScheduler 是一个分布式，无中心的系统，master 的服务器资源利用率更高，Airflow 由于通过 scheduler 扫描并发现可调度任务，CPU 利用率没有 DolphinScheduler 高。详见 AWS 性能测评\n\n诉求及挑战\n诉求\n作为一个迁移工具，其核心诉求就是希望能在人为介入尽可能少的情况下，实现将 Airflow DAGs 转化成 DolphinScheduler 中工作流的迁移。\n但是这需要有一个较好平衡，不能一味追求自动化，不然可能会导致程序复杂、可维护性降低、泛化能力变弱等情况，特别是我们需要去兼容不同 Airflow 版本的时候，如何取舍就是是 air2phin 必须面对的一个问题。\n挑战\n\n语法差异：Airflow 和 DolphinScheduler Python SDK 在基础的 Python 语法（for、if、else）上都是一样的，但是在具体的任务名称和参数上稍有不同，如 airflow 中的 bash operator 对应到 DolphinScheduler Python SDK 的名称是 Shell， 同时两者的参数也不一样，迁移需要兼容这部分逻辑\n任务类型差异：Airflow 和 DolphinScheduler 可能都允许用户进行一定程度的定制化扩展，如自定义插件。但是两者在在任务类型的数量和对任务的封装抽象是有差异，有部分任务类型仅在 airflow 存在，有部分任务类型仅在 DolphinScheduler 中存在，转换的时候需要处理这部分差异\n定时调度差异：Airflow 定义调度周期的时候使用 Cron 表达式（如 5 4 * * *）或者 Python 的 datetime.timedelta ，DolphinScheduler 使用的是更加精细化的 Cron 表达式，如 \n0 5 4 * * ? * 所以这部分的转换也是挑战\n内置时间参数差异：Airflow 的内置时间参数是通过 macro 来处理的，并且提供了 jinja2 模版作为时间的计算，如 ds_add('2015-01-06', -5)。DolphinScheduler 有自己的内置时间定义和计算规则，如运行时间使用 yyyy-MM-dd，需要时间增减使用 yyyy-MM-dd+1\n迁移规则的扩展：不管是 Airflow 和 DolphinScheduler Python SDK 都会随着时间而修改对应的 API，只有有不兼容的修改就会导致迁移工具失效，所以迁移工具规则的修改和新增需要尽可能简单，尽量减少维护成本\n不同版本的Airflow：Airflow 的不同版本之间可能也有差异，如在 2.0.0 之前有 airflow.operators.bash_operator 但是到2.0.0 后我们只有 airflow.operators.bash\n\n迁移工具介绍\nAir2phin 是什么\nAir2phin 是一个基于规则的 AST 转换器，提供了从 Airflow dag 文件转成 pydolphinscheudler 定义文件的功能。其使用 LibCST 解析和转换 Python 代码，并使用 Yaml 文件定义转换规则。他是一个协助用户完成转化的工具，并非是一键转化工具。\nAir2phin 的数据流转图\n\n\n从标准输入或者文件中获取原来 airflow DAGs 的定义\n将转换规则从 YAML 文件加载到 air2phin\n将 airflow DAGs 内容解析为 CST 树\n根据转换规则改变 CST 树\n将转换后的结果输出到标准输出或者文件\n\nAir2phin 如何使用\n由于 Air2phin 是 Python 的包，所以需要通过 pip 安装，安装结束后可以通过命令 air2phin migrate ~/airflow/dags 将 airflow 全部的 dags 转换成 DolphinScheduler Python SDK 的定义到了这一步 air2phin 的使命已经完成了，最后只需要使用 Python 执行这部分 SDK 的代码就能将转化后的工作流提交到 DolphinScheduler\n# Install package\npython -m pip install --upgrade air2phin\n\n# Migrate airflow‘s dags\nair2phin migrate -i ~/airflow/dags\n\n在实际生产中， ~/airflow/dags 下面可能有非常多的 DAG， 而 air2phin 默认是串行处理这部分 DAG 的，如果你想要更加高效的处理，可以使用 --multiprocess  让 air2phin 可以多进程执行转换。\n# use multiprocess to convert the airflow dags files\nair2phin migrate -i --multiprocess 12 ~/airflow/dags \n\n完成了上述的转化后，你就完成了从 Airflow dags 文件到 DolphinScheduler python sdk 定义脚本的转化，只需要将 DolphinScheduler python sdk 提交到 DolphinSchedeuler 中即可完成\n# Install apache-dolphinscheduler according to apache DolphinScheduler server you use, ref: https://dolphinscheduler.apache.org/python/main/#version\npython -m pip install apache-dolphinscheduler\n# Submit your dolphinscheduler python sdk definition\npython ~/airflow/dags/tutorial.py\n\nAir2phin 如何定义自己的转换规则\n大部分 Airflow 的用户都会自定义部分 operator，想要转化这部分的 operator 需要用户自定义规则，幸运的 Air2phin 的规则是基于 YAML 文件的，意味用户可以较为简单的新增规则。下面是一个用户客户自定义的 Redshift operator 转化成 DolphinScheduler SQL 任务类型的规则转换 YAML 文件。\n这里假设用户基于 airflow.providers.postgres.operators.postgres 自定义了一个 redshift operator，其 operator 的代码如下\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\nclass RedshiftOperator(PostgresOperator):\n    def __init__(\n        self,\n        *,\n        sql: str | Iterable[str],\n        my_custom_conn_id: str = 'postgres_default',\n        autocommit: bool = False,\n        parameters: Iterable | Mapping | None = None,\n        database: str | None = None,\n        runtime_parameters: Mapping | None = None,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            sql=sql,\n            postgres_conn_id=my_custom_conn_id,\n            autocommit=autocommit,\n            parameters=parameters,\n            database=database,\n            runtime_parameters=runtime_parameters,\n            **kwargs,\n        )\n\n由于这是用户自定义的 operator，他肯定不在 air2phin 内置的转换规则中，所以我们需要自定义一个转换规则的 YAML 文件\nname: RedshiftOperator\n\nmigration:\n  module:\n    - action: replace\n      src: utils.operator.RedshiftOperator.RedshiftOperator\n      dest: pydolphinscheduler.tasks.sql.Sql\n    - action: add\n      module: pydolphinscheduler.resources_plugin.Local\n  parameter:\n    - action: replace\n      src: task_id\n      dest: name\n    - action: add\n      arg: datasource_name\n      default:\n        type: str\n        value: &quot;redshift_read_conn&quot;\n    - action: add\n      arg: resource_plugin\n      default: \n        type: code\n        value: Local(prefix=&quot;/path/to/dir/&quot;)\n\n客户只需要将这个文件加入 air2phin 的规则路径下，就能实现该自定义 operator 的转化动作了。\nair2phin migrate --custom-rules /path/to/RedshiftOperator.yaml ~/airflow/dags\n\nAir2phin 如何解决迁移的挑战\n前面提到了 Airflow 到 Dolophinscheduler 可能面临的挑战，下面来看看 Air2phin 是如何解决的\n语法差异\n由于 Airflow 和 DolphinScheduler Python SDK 都是使用 Python 编写的。所以 Python 相关的基础语法是相似。但是由于 Airflow 和 DolphinScheduler Python SDK是两套无关联的 API， 所以两者在特定参数、类和函数等方面存在一些不同之处。air2phin 就是用来解决这个问题的，他通过定义适当的 YAML 这部分差异的转换规则，解决差异并实现从一个平台迁移到另一个平台的流程。\nYAML文件转换规则：\n\n参数映射： 对于参数的不同命名或结构，可以在 YAML 文件中定义映射规则，将 Airflow 的参数名称映射到 DolphinScheduler Python SDK 的对应参数。\n类和函数转换： 如果 Airflow 和 DolphinScheduler Python SDK 使用不同的类名或函数，可以在YAML文件中定义类和函数的转换规则，将 Airflow 的类名和函数映射到 DolphinScheduler Python SDK 的等效项。\n错误处理和告警： 鉴于两个平台可能有不同的错误处理和告警机制，可以在YAML文件中定义如何映射Airflow的错误处理到DolphinScheduler的等效机制。\n\n通过制定这些转换规则，可以确保在迁移过程中，根据 YAML 文件的定义，将 Airflow 的任务代码转换成 DolphinScheduler Python SDK 平台所需的代码，以适应平台之间的差异，并确保新增和修改任务的灵活性。任务类型差异\n定时调度差异\n在定时调度配置方面，Airflow 和 DolphinScheduler Python SDK 也存在一些区别。Airflow 使用标准的 Cron 表达式来定义任务的定时调度，而 DolphinScheduler Python SDK 采用了更加精确的 Cron 调度策略。这种差异可能会影响任务的精确调度和执行频率。\n\nAirflow的Cron表达式： Airflow使用通用的Cron表达式来定义任务的调度频率。Cron表达式由五个或六个字段组成，分别表示分钟、小时、日期、月份、星期。它允许用户定义相对宽松的调度规则，例如每小时一次、每天一次等。\nDolphinScheduler Python SDK 的精确Cron调度： DolphinScheduler则引入了更加精确的Cron调度策略。它将Cron表达式分成两部分：基本Cron和高级Cron。基本Cron用于定义任务的粗略调度规则，如分钟、小时、日期等。而高级Cron则用于定义更精确的调度规则，包括秒级精度等。这使得DolphinScheduler可以实现更细粒度的任务调度，适用于对执行时间要求更高的场景，如金融领域等。\n\n由于 DolphinScheduler Python SDK 的精度比 Airflow 的精度高，所以转化的时候不会存在精度丢失的问题，这个问题也就迎刃而解了。\n内置时间参数差异\n内置时间参数差异指的是 Airflow 和 DolphinScheduler Python SDK 在任务调度中使用内置时间参数的不同方式。Airflow使用Jinja2的宏（macros）功能来实现内置时间参数，而DolphinScheduler的Python SDK 使用自定义的方式来实现这些参数。这两种实现方法可能会导致使用和理解上的差异。\n\nAirflow的Jinja2宏： Airflow的内置时间参数是通过Jinja2宏来实现的。Jinja2宏允许在DAGs文件中使用特殊的占位符和函数，用于动态地生成调度时间。例如，可以使用{{ macros.ds_add(ds, 1) }}来在调度时间上加一天。\nDolphinScheduler Python SDK的自定义实现： DolphinScheduler的Python SDK在实现内置时间参数时，可能会使用一些自定义的函数或类，而不是直接使用Jinja2宏。这些自定义的实现方法可能需要在DolphinScheduler平台上进行特定的配置和处理。\n\n所以迁移的时候需要注意：\n\n\n语法和方式不同： Airflow 的 Jinja2 宏在语法和使用方式上与 DolphinScheduler Python SDK 的自定义实现有很大的区别，可能导致部分时间参数不能被正确迁移。Air2phin 对于部分不能自动迁移的参数会保留其原本的值\n\n\n功能相似性： 尽管实现方式不同，但两者都旨在为任务调度提供内置时间参数。确保迁移后的任务能够正确地使用新平台的内置时间参数。\n\n\n迁移规则的扩展\nAirflow允许用户根据需要定义和使用自定义Operator、Hook、Sensor等，以满足特定的任务需求。这些自定义组件可能在 DAGs 中使用，而且它们的实现和调用方式可能在迁移过程中需要特殊处理。最简单的处理方式是使用上问提到的 “Air2phin 如何定义自己的转换规则” 的方式处理。只要自定义的任务在 DolphinScheduler 中可以被定义，那就能将任务从 Airflow 迁移到 DolphinScheduler\n不同版本的 Airflow 迁移\n不同版本的 Airflow 在 operator 的语法中有所不同，在 2.0.0 之前的版本中，Airflow 对 bash 的支持仅拥有 airflow.operators.bash_operator.BashOperator 这个类，但是在 2.0.0 及之后的版本，Airflow 对 bash 更加推荐的是 airflow.operators.bash.BashOperator 这里类，同时兼容 Airflow.operators.bash_operator.BashOperator。类似的情况还有很多，所以 Air2phin 需要同时兼容上述两种类型转换成 DolphinScheduler 的 shell 任务类型。我们通过在 YAML 中支持列表的方式实现对多个类转化，详见下面的 migration.module.* 节点\nmigration:\n  module:\n    - action: replace\n      src:\n        - airflow.operators.bash.BashOperator\n        - airflow.operators.bash_operator.BashOperator\n      dest: pydolphinscheduler.tasks.shell.Shell\n  parameter:\n    - action: replace\n      src: task_id\n      dest: name\n    - action: replace\n      src: bash_command\n      dest: command\n\n用户收益\nAir2phin 迁移工具可以通过简单的配置实现用户从 Airflow 的 DAGs 代码转换为 DolphinScheduler Python SDK， 给用户带来了很多收益\n\n简化迁移过程： 迁移工具可以自动处理代码转换，避免手动逐行迁移的复杂过程，大大减轻了开发人员的工作负担。\n节省时间和成本： 手动迁移代码需要投入大量时间和人力资源。使用迁移工具可以快速、高效地完成迁移，从而节省时间和成本。\n减少错误： 手动迁移容易引入错误，而迁移工具可以基于预定义规则自动进行转换，减少了潜在的人为错误。\n规范代码风格： 迁移工具可以根据预定义的规则和模板生成代码，确保代码风格一致，降低维护成本。\n降低技术门槛： 迁移工具可以隐藏底层技术细节，使得不熟悉 DolphinScheduler 的开发人员也能够轻松迁移任务。\n灵活性和可定制性： 好的迁移工具通常会提供一些可定制的选项，以满足不同项目的需求，同时保持灵活性。\n\n总的来说，使用 Air2phin 可以显著提升迁移过程的效率和质量，降低风险，同时减轻了开发人员的工作负担，为团队带来了时间和资源的节省，以及更好的开发体验。\nAir2phin 目前还不能解决的问题\nAir2phin 是一个协助用户更简单从 Airflow 迁移到Apache DolphinScheduler 。这个的关键词是“协助”，意味着他能减少用户的迁移成本，但是并不能完全自动化。目前已知的不能解决的问题如下：\n\n不能迁移在 DolphinScheduler 不存在的任务类型：部分任务类型仅在 Airflow 中存在，但是在 DolphinScheduler 不存在，这部分任务不能被自动迁移，需要手动处理。如 Discord operator 中在 DolphinScheduler 中不存在，所以原来 Discord operator 定义会被保留，需要用户手动处理\n部分任务属性不能被迁移到 DolphinScheduler：Airflow 中部分任务属性在 DolphinScheduler 中不存在，如 successc_callback 和 retry_callback，这部分属性在迁移过程中会被直接遗弃\n\nREF\n\nair2phin 使用文档：https://air2phin.readthedocs.io/en/latest/index.html\nair2phin in PyPI: https://pypi.org/project/air2phin/\nair2phin GitHub repository: https://github.com/WhaleOps/air2phin\n\n",
    "title": "大数据调度最佳实践 |  从Airflow迁移到Apache DolphinScheduler",
    "time": "2023-10-25"
  },
  {
    "name": "Board_of_Directors_Report",
    "content": "Apache DolphinScheduler 董事会报告：社区健康运行，Commit 增长 123%\n\n\n\n\n自 2021 年 3 月 17 日从 Apache 孵化器毕业以来，Apache DolphinScheduler 不知不觉已经和社区一起经过了十个月的成长。在社区的共同参与下，Apache DolphinScheduler 在数次版本迭代后，蜕变为一个经过数百家企业生产环境检验的成熟调度系统产品。\n\n\n在将近一年的时间里，Apache DolphinScheduler 有了哪些进步？今天我们将通过这篇 Apache 报告，一起回顾这段时间发生在 Apache DolphinScheduler 及其社区中的变化。\n\n基本数据：\n成立： 2021 年 03 月 17 日(十个月前)\nChair： 代立冬\n下次报告日期: 2022 年 1 月 19 日(星期三)\n社区健康评分( Chi ): 7.55（健康）\n项目组成：\n\n目前该项目中有 39 个 committer 和 16 个 PMC 成员。\ncommitter 与 PMC 的比例大约是 5 ： 2 。\n\n与上季度相比，社区的变化：\n\n无新的 PMC 成员加入。最新加入的 PMC 成员为 Calvin Kirs，加入时间 2021 - 05 - 07 。\nShunFeng Cai 于 2021 年 12 月 18 日新晋 committer。\nZhenxu Ke 于 2021 年 12 月 12 日新晋 committer。\nWang Xingjie 于 2021 年 11 月 24 日新晋 committer。\nYi zhi Wang 于 2021 年 12 月 15 日新晋 committer。\nJiajie Zhong于 2021 年 12 月 12 日新晋 committer。\n\n社区健康指标:\n\n邮件列表趋势\ncommit 数量\nGitHub PR 数量\nGitHub issue\n最活跃的 GitHub issues/ PR\n\n邮件列表趋势:\ndev@dolphinscheder.apache.org 在过去的一个季度，流量增长 64%（ 297 封电子邮件，上季度为 181 封）：\n\n\n\ncommit 数量：\n\n上季度共 972 个 commit(增长 123 %)\n上季度共新增 88 个代码贡献者(增长 25 %)\n\n\n\n\nGitHub PR 数量:\n\nGitHub 上新开 824 个 PR ，较上季度(增长 89 %)\nGitHub 上关闭 818 个 PR ，较上季度（增长 100 %）\n\n\n\n\nGitHub issues:\nGitHub 上新开 593 个 issues， 较上季度(增长 90 %)\nGitHub 上关闭 608 个 issue，较上季度(增长 155 %)\n\n\n\n讨论最热烈的 GitHub issues/ PR :\n\ndolphinscheduler/pull/6894[Improvement][Logger]Logger server integrate into worker server(15 comments)\ndolphinscheduler/pull/6744[Bug][SnowFlakeUtils] fix snowFlake bug(15 comments)\ndolphinscheduler/pull/6674[Feature][unittest] Recover UT in AlertPluginManagerTest.java [closes: #6619](15 comments)\ndolphinscheduler/issues/7039[Bug] [Task Plugin] hive sql execute failed(14 comments)\ndolphinscheduler/pull/6782[improvement] improve install.sh if then statement(13 comments)\ndolphinscheduler/issues/7485[Bug] [dolphinscheduler-datasource-api] Failed to create hive datasource using ZooKeeper way in 2.0.1(13 comments)\ndolphinscheduler/pull/7214[DS-7016][feat] Auto create workflow while import sql script with specific hint(12 comments)\ndolphinscheduler/pull/6708[FIX-#6505][Dao] upgrade the MySQL driver package for building MySQL jdbcUrl(12 comments)\ndolphinscheduler/pull/7515[6696/1880][UI] replace node-sass with dart-sass(12 comments)\ndolphinscheduler/pull/6913Use docker.scarf.sh to track docker user info(12 comments)\n\n",
    "title": "Apache DolphinScheduler 董事会报告：社区健康运行，Commit 增长 123%",
    "time": "2022-1-13"
  },
  {
    "name": "China_Unicom_revamps_Apache_DolphinScheduler",
    "content": "中国联通改造 Apache DolphinScheduler 资源中心，实现计费环境跨集群调用与数据脚本一站式访问\n\n\n\n截止2022年，中国联通用户规模达到4.6亿，占据了全中国人口的30%，随着5G的推广普及，运营商IT系统普遍面临着海量用户、海量话单、多样化业务、组网模式等一系列变革的冲击。\n当前，联通每天处理话单量超过400亿条。在这样的体量基础上，提高服务水平，为客户提供更有针对性的服务，也成为了联通品牌追求的终极目标。而中国联通在海量数据汇集、加工、脱敏、加密等技术与应用方面已崭露头角，在行业中具有一定的先发优势，未来势必成为大数据赋能数字经济发展的重要推动者。\n在 Apache DolphinScheduler 4月 Meetup 上，我们邀请到了联通软件研究院的柏雪松，他为我们分享了《DolphinScheduler在联通计费环境中的应用》。\n本次演讲主要包括三个部分：\n\n\nDolphinScheduler在联通的总体使用情况\n\n\n联通计费业务专题分享\n\n\n下一步的规划\n\n\n\n\n\n柏雪松 联通软研院 大数据工程师\n毕业于中国农业大学，从事于大数据平台构建和 AI 平台构建，为 Apache DolphinScheduler 贡献 Apache SeaTunnel(Incubating) 插件，并为 Apache SeaTunnel(Incubating) 共享 alluxio 插件\n01  总体使用情况\n首先给大家说明一下联通在DolphinScheduler的总体使用情况：\n\n\n现在我们的业务主要运行在3地4集群\n\n\n总体任务流数量大概在300左右\n\n\n日均任务运行差不多5000左右\n\n\n我们使用到的DolphinScheduler组件包括Spark、Flink、SeaTunnel（原Waterdrop），以及存储过程中的Presto和一些Shell脚本，涵盖的业务则包含稽核，收入分摊，计费业务，还有其他一些需要自动化的业务等。\n\n\n\n02 业务专题分享\n​01 跨集群双活业务调用\n上文说过，我们的业务运行在3地4集群上，这样就免不了集群之间的互相的数据交换和业务调用。如何统一管理和调度这些跨集群的数据传输任务是一个重要的问题，我们数据在生产集群，对于集群网络带宽十分敏感，必须有组织地对数据传输进行管理。\n另一方面，我们有一些业务需要跨集群去调用，例如A集群数据到位后B集群要启动统计任务等，我们选择 Apache DolphinScheduler作为调度和控制，来解决这两个问题。\n首先说明下我们跨集群数据传输的流程在AB两个集群上进行，我们均使用HDFS进行底层的数据存储，在跨集群的HDFS数据交换上，根据数据量大小和用途，我们将使用的数据分为小批量和大批量数据，向结构表，配置表等。\n对于小批量数据，我们直接将其挂载到同一个Alluxio上进行数据共享，这样不会发生数据同步不及时导致的版本问题。\n\n\n像明细表和其他大文件，我们使用Distcp和Spark混合进行处理；\n\n\n对于结构表数据，使用SeaTunnel on Spark的方式；\n\n\n通过Yarn队列的方式进行限速设置；\n\n\n非结构数据使用Distcp传输，通过自带的参数Bandwidth进行速度限制；\n\n\n这些传输任务都是运行在DolphinScheduler平台上面，我们整体的数据流程主要是A集群的数据到位检测，A集群的数据完整性校验，AB集群之间的数据传输，B集群的数据稽核和到位通知。\n强调一点：其中我们重点用到了DolphinScheduler自带的补数重跑，对失败的任务或者不完整的数据进行修复。\n\n\n\n在完成了跨集群的数据同步和访问，我们还会使用DolphinScheduler进行跨地域和集群的任务调用。\n我们在A地有两个集群，分别是测试A1和生产A2，在B地有生产B1集群，我们会在每个集群上拿出两台具有内网IP的机器作为接口机，通过在6台接口机上搭建DolphinScheduler建立一个虚拟集群，从而可以在统一页面上操作三个集群的内容；\nQ：如何实现由测试到生产上线？\nA：在A1测试上进行任务开发，并且通过测试之后，直接将worker节点改动到A2生产上；\nQ：遇到A2生产出了问题，数据未到位等情况怎么办？\nA：我们可以直接切换到B1生产上，实现手动的双活容灾切换；\n\n\n\n最后我们还有些任务比较大，为满足任务时效性，需要利用两个集群同时计算，我们会将数据拆分两份分别放到A2和B1上面，之后同时运行任务，最后将运行结果传回同一集群进行合并，这些任务流程基本都是通过DolphinScheduler来进行调用的。\n请大家注意，在这个过程中，我们使用DolphinScheduler解决了几个问题：\n\n\n项目跨集群的任务依赖校验；\n\n\n控制节点级别的任务环境变量；\n\n\n02 AI开发同步任务运行\n1、统一数据访问方式\n我们现在已经有一个简易的AI开发平台，主要为用户提供一些Tensorflow和Spark ML的计算环境。在业务需求下，我们需要将用户训练的本地文件模型和集群文件系统打通，并且能够提供统一的访问方式和部署方法，为解决这个问题，我们使用了Alluxio-fuse和DolphinScheduler这两个工具。\n\n\nAlluxio-fuse打通本地和集群存储\n\n\nDolphinScheduler共享本地和集群存储\n\n\n由于我们搭建的AI平台集群和数据集群是两个数据集群，所以在数据集群上我们进行一个数据的存储，利用Spark SQL或者Hive进行一些数据的预加工处理，之后我们将处理完的数据挂载到Alluxio上，最后通过Alluxio fuse跨级群映射到本地文件，这样我们基于Conda的开发环境，就可以直接访问这些数据，这样就可以做到统一数据的访问方式，以访问本地数据的方法访问集群的数据。\n\n\n\n2、数据脚本一站式访问\n分离资源之后，通过预处理大数据内容通过数据集群，通过我们的AI集群去处理训练模型和预测模型，在这里，我们使用Alluxio-fuse对DolphinScheduler的资源中心进行了二次改动，我们将DolphinScheduler资源中心连接到Alluxio上，再通过Alluxio-fuse同时挂载本地文件和集群文件，这样在DolphinSchedule上面就可以同时访问在本地的训练推理脚本，又可以访问到存储在hdfs上的训练推理数据，实现数据脚本一站式访问。\n\n\n\n03 业务查询逻辑持久化\n第三个场景是我们用Presto和Hue为用户提供了一个前台的即时查询界面，因为有些用户通过前台写完SQL，并且测试完成之后，需要定时运行一些加工逻辑和存储过程，所以这就需要打通从前台SQL到后台定时运行任务的流程。\n\n\n\n另一个问题是Presto原生没有租户间的资源隔离问题。我们也是对比了几个方案之后，最后结合实际情况选择了Presto on Spark方案。\n因为我们是一个多租户平台，最开始给用户提供的方案是前端用Hue界面，后端直接使用原生的Presto跑在物理集群上，这导致了用户资源争抢占的问题。当有某些大查询或者大的加工逻辑存在时，会导致其他租户业务长时间处于等待状态。\n为此，我们对比了Presto on Yarn和Presto on Spark，综合对比性能之后发现Presto on Spark资源使用效率会更高一些，这里大家也可以根据自己的需求选择对应的方案。\n\n\n\n另一方面，我们使用了原生Presto和Presto on spark共存的方式，对于一些数据量较小，加工逻辑较为简单的SQL，我们直接将其在原生Presto上运行，而对于一些加工逻辑比较复杂，运行时间比较长的SQL，则在Presto on spark上运行，这样用户用一套SQL就可以切换到不同的底层引擎上。\n此外，我们还打通了Hue到DolphinScheduler定时任务调度流程。我们在Hue上进行SQL开发调制后，通过存储到本地Serve文件，连接到Git进行版本控制。\n我们将本地文件挂载到Alluxio fuse上，作为SQL的同步挂载，最后我们使用Hue，通过DolphinScheduler的API创建任务和定时任务，实现从SQL开发到定时运行的流程控制。\n\n\n\n04 数据湖数据统一治理\n最后一个场景是数据湖数据统一管理，在我们自研的数据集成平台上，使用分层治理的方式对数据湖数据进行统一的管理和访问，其中使用了DolphinScheduler作为入湖调度和监控引擎。\n在数据集成平台上，对于数据集成、数据入湖、数据分发这些批量的和实时的任务的，我们使用DolphinScheduler进行调度。\n底层运行在Spark和Flink上，对于数据查询和数据探索这些需要即时反馈的业务需求，我们使用嵌入Hue接入Spark和Presto的方法，对数据进行探索查询；对于数据资产登记同步和数据稽核等，直接对数据源文件信息进行查询，直接同步底层数据信息。\n最后一个场景是数据湖数据统一管理，在我们自研的数据集成平台上，使用分层治理的方式对数据湖数据进行统一的管理和访问，其中使用了DolphinScheduler作为入湖调度和监控引擎。\n在数据集成平台上，对于数据集成、数据入湖、数据分发这些批量的和实时的任务的，我们使用DolphinScheduler进行调度。\n底层运行在Spark和Flink上，对于数据查询和数据探索这些需要即时反馈的业务需求，我们使用嵌入Hue接入Spark和Presto的方法，对数据进行探索查询；对于数据资产登记同步和数据稽核等，直接对数据源文件信息进行查询，直接同步底层数据信息。\n\n\n\n目前我们集成平台基本上管理着460张数据表的质量管理，对数据准确性和准时性提供统一的管理。\n03 下一步计划与需求\n\n\n\n01 资源中心\n在资源中心层面，为了方便用户之间的文件共享，我们计划为全用户提供资源授权，同时根据它的归属租户，分配租户级别的共享文件，使得对于一个多租户的平台更为友善。\n02 用户管理\n其次与用户传权限相关，我们只提供租户级别的管理员账账户，后续的用户账户由租户管理员账户创建，同时租户组内的用户管理也是由租户管理员去控制，以方便租户内部的管理。\n03 任务节点\n最后是我们的任务节点相关的计划，现在已在进行之中：一方面是完成SQL节点的优化，让用户能够选择一个资源中心的SQL文件，而不需要手动复制SQL；另一方面是HTTP节点对返回的json自定义解析提取字段判断，对复杂返回值进行更为友好的处理。\n",
    "title": "中国联通改造 Apache DolphinScheduler 资源中心，实现计费环境跨集群调用与数据脚本一站式访问",
    "time": "2022-5-07"
  },
  {
    "name": "DS-2.0-alpha-release",
    "content": "重构、插件化、性能提升 20 倍，Apache DolphinScheduler 2.0 alpha 发布亮点太多！\n\n社区的小伙伴们，好消息！经过 100 多位社区贡献者近 10 个月的共同努力，我们很高兴地宣布 Apache DolphinScheduler 2.0 alpha 发布。这是 DolphinScheduler 自进入 Apache 以来的首个大版本，进行了多项关键更新和优化，是 DolphinScheduler 发展中的里程碑。\nDolphinScheduler 2.0 alpha 主要重构了 Master 的实现，大幅优化了元数据结构和处理流程，增加了 SPI 插件化等能力，在性能上提升 20 倍。同时，新版本设计了全新的 UI 界面，带来更好的用户体验。另外，2.0 alpha 还新添加和优化了一些社区呼声极高的功能，如参数传递、版本控制、导入导出等功能。\n注意：当前 alpha 版本还未支持自动升级，我们将在下个版本中支持这一功能。\n2.0 alpha 下载地址：https://dolphinscheduler.apache.org/en-us/download\n优化内核，性能提升 20 倍\n相较于 DolphinScheduler 1.3.8，同等硬件配置下(3 台 8 核 16 G)，2.0 alpha 吞吐性能提升 20 倍，这主要得益于 Master 的重构，Master 执行流程和优化了工作流处理流程等，包括：\n\n重构 Master 的执行流程，将之前状态轮询监控改为事件通知机制，大幅减轻了数据库的轮询压力；\n去掉全局锁，增加了 Master 的分片处理机制，将顺序读写命令改为并行处理，增强了 Master 横向扩展能力；\n优化工作流处理流程，减少了线程池的使用，大幅提升单个 Master 处理的工作流数量；\n增加缓存机制，大幅减少数据库的操作次数；\n优化数据库连接方式，极大地缩减数据库操作耗时；\n简化处理流程，减少处理过程中不必要的耗时操作。\n\n优化 UI 组件，全新的 UI 界面\n\n\n \n  UI 界面对比：1.3.9（上） VS. 2.0 alpha（下）\n\n\n2.0 UI 重要优化在以下几个方面：\n\n\n优化组件显示：界面更简洁，流程显示更清晰，一目了然；\n突出重点内容：鼠标点击任务框，显示任务详情信息；\n增强可识别性：左侧工具栏标注名称，使工具更易识别，便于操作；\n调整组件顺序：调整组件排列顺序，更符合用户习惯。\n\n除了性能与 UI 上的变化外，DolphinScheduler 也新增和优化了 20 多项功能\n及 BUG 修复。\n新功能列表\n\n优化项\n\nBug 修复\n\n感谢贡献者\nDolphinScheduler 2.0 alpha 的发布凝聚了众多社区贡献者的智慧和力量，是他们的积极参与和极大的热情开启了 DolphinScheduler 2.0 时代！\n非常感谢 100+ 位（GitHub ID）社区小伙伴的贡献，期待更多人能够加入 DolphinScheduler 社\n区共建，为打造一个更好用的大数据工作流调度平台贡献自己的力量！\n\n2.0 alpha 贡献者名单\n加入我们\n随着国内开源的迅猛崛起，Apache DolphinScheduler 社区迎来蓬勃发展，为了做更好用、易用的调度，真诚欢迎热爱开源的伙伴加入到开源社区中来，为中国开源崛起献上一份自己的力量，让本土开源走向全球。\n贡献第一个 PR(文档、代码) 我们也希望是简单的，第一个 PR 用于熟悉提交的流程和社区协作以及感受社区的友好度。\n社区汇总了以下适合新手的问题列表：https://github.com/apache/dolphinscheduler/issues/5689\n进阶问题列表：https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\n如何参与贡献链接：https://dolphinscheduler.apache.org/zh-cn/community\n来吧，DolphinScheduler 开源社区需要您的参与，为中国开源崛起添砖加瓦吧，哪怕只是小小的一块瓦，汇聚起来的力量也是巨大的。\n参与开源可以近距离与各路高手切磋，迅速提升自己的技能，如果您想参与贡献，我们有个贡献者种子孵化群，可以添加社区小助手微信(Leonard-ds) 手把手教会您( 贡献者不分水平高低，有问必答，关键是有一颗愿意贡献的心 )。添加小助手微信时请说明想参与贡献。\n来吧，开源社区非常期待您的参与。\n",
    "title": "重构、插件化、性能提升 20 倍，Apache DolphinScheduler 2.0 alpha 发布亮点太多!",
    "time": "2021-10-29"
  },
  {
    "name": "DS_architecture_evolution",
    "content": "Apache DolphinScheduler 架构演进及开源经验分享\n引言\n来自 eBay 的文俊同学在近期的上海开源大数据 Meetup 上做了十分精彩的 “Apache DolphinScheduler 的架构演进” 分享。本次分享有近 200 人参与，在线观看次数超过 2,500 次\n演讲者介绍\n阮文俊，eBay 开发工程师，DolphinScheduler 贡献者。\n视频分享参见\n\nApache DolphinScheduler介绍\nApache DolphinScheduler是一个分布式去中心化，易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。DolphinScheduler以有向无环图的方式将任务连接起来，可实时监控任务的运行状态，同时支持取消、暂停、恢复、从指定任务节点重跑等操作。\nDolphinScheduler具有以下几个优良功能特性：\n\n\nCloud Native — 支持多云/数据中心工作流管理，也支持 Kubernetes、Docker 部署和自定义任务类型，分布式调度，整体调度能力随集群规模线性增长\n\n\n高可靠与高可扩展性 — 去中心化的多 Master 多 Worker 设计架构，支持服务动态上下线，自我容错与调节能力\n\n\n支持多租户\n\n\n丰富的使用场景 — 包括流、暂停、恢复操作，以及额外的任务类型，如 Spark、Hive、MR、Shell、Python、Flink 以及 DS 独有的子工作流、任务依赖设计，扩展点采用插件化的实现方式\n\n\n简单易用 — 所有流程定义操作可视化编排，定义关键信息一目了然，一键部署\n\n\n关于DolphinSheduler更多功能介绍和开发文档请查阅官网详细信息 https://dolphinscheduler.apache.org\n架构演进过程\n1.2.x架构\nDolphinScheduler最初进入Apache孵化器的版本是1.2，在这一版本中采用的架构由以下几个重要部分组成：\n\n去中心化的master节点，负责工作流调度、DAG任务切分、任务提交监控和监听其它节点健康状态等任务\n去中心化的worker节点，负责执行任务和维护任务的生命周期等\n数据库，存储工作流元数据，运行实例数据\nZookeeper，主要负责注册中心、分布式锁、任务队列等工作任务\n\n1.2版本基本实现了高可靠的工作流调度系统，但是也存在多个问题：\n\n重量级的worker，worker节点需要负责多种任务\n异步派发任务会导致任务执行延迟\n由于masker和worker都需要依赖数据库，导致数据库压力大\n\n[]\n1.3.x架构\n针对1.2版本存在的问题，1.3架构进行了如下改进：\n\n去任务队列，保证master节点同步派发任务，降低任务执行延迟\n轻量级worker，worker节点只负责执行任务，单一化worker职责\n减小数据库压力，worker不再连接数据库\n采用多任务负载均衡策略，master根据worker节点资源使用情况分配任务，提高worker资源利用率\n\n这些改进有效改进了1.2版本的缺陷，但仍存在一些问题，例如：\n\nmaster调度工作流时需要依赖分布式锁，导致工作流吞吐量难以提升\n因为需要创建大量线程池，多数线程处于轮询数据库，导致master资源利用率低\nmaster轮询数据库，仍然导致数据库压力大\n各组件存在耦合情况\n\n\n2.0架构\n针对1.3版本的缺陷，2.0架构进一步做出改进：\n\n去分布式锁，对master进行分区编号，实现错位查询数据库，避免多个节点同时访问同一个工作流造成的冲突问题\n重构master线程模型，对所有工作流使用统一的线程池\n重构数据库中DAG元数据模型\n彻底的插件化，所有扩展点都采用插件化实现\n数据血缘关系分析\n\n1 去分布式锁\n\n2 重构 master 中的线程模型\n\nSchedulerThread 负责从数据库中查询 Command 并提交到 Command Queue\nDagExecuteThreadPool 从 Command Queue 中取 command，并构造 DAG实例添加到 DAG 队列，进行处理，当前 DAG 没有未执行的任务，则当前 DAG 执行结束\nTaskExecuteThreadPool 提交任务给 Worker\nTaskEventThread 监听任务事件队列，修改任务状态\n3 彻底的插件化\n\n所有扩展点都采用插件化实现\n告警SPI\n注册中心SPI\n资源存储SPI\n任务插件SPI\n数据源SPI\n……\nApache DolphinScheduler发展方向\n开发者阮文俊针对dolphinsheduler的未来发展方向，也分享了一些看法：\n\n系统更稳、速度更快（高吞吐、低延迟、智能化运维、高可用）\n支持更多的任务集成（深度学习任务、CI/CD等其它系统集成、存储过程和数据质量任务、容器调度任务、复杂调度场景等）\n轻量化dolphinscheduler内核，提供基础调度服务\n\n\n如何参与开源贡献\n最后，开发者阮文俊针对入门新手如何参与开源贡献的问题，提出了宝贵的指导意见：\n\n从小事做起，积累开发经验\n关注社区动态，积极参与讨论，更好融入社区\n坚持开源精神，乐于帮助他人\n持之以恒\n\n\n",
    "title": "Apache DolphinScheduler 架构演进及开源经验分享",
    "time": "2021-07-18"
  },
  {
    "name": "DS_run_in_windows",
    "content": "DolphinScheduler 在 Windows 本地搭建开发环境，源码启动\n如果您对本地开发的视频教程感兴趣的话，也可以跟着视频来一步一步操作:\n\n\n\n下载源码\n官网 ：https://dolphinscheduler.apache.org/zh-cn\n地址 ：https://github.com/apache/dolphinscheduler.git\n这里选用 1.3.6-release 分支。\n\n\nwindows 安装 zk\n\n\n下载 zk https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.6.3/apache-zookeeper-3.6.3-bin.tar.gz\n\n\n解压 apache-zookeeper-3.6.3-bin.tar.gz\n\n\n在 zk 的目录下新建 data、log 文件夹\n\n\n将 conf 目录下的 zoo_sample.cfg 文件，复制一份，重命名为 zoo.cfg，修改其中数据和日志的配置，如：\ndataDir=D:\\\\code\\\\apache-zookeeper-3.6.3-bin\\\\data\ndataLogDir=D:\\\\code\\\\apache-zookeeper-3.6.3-bin\\\\log\n\n\n\n在 bin 中运行 zkServer.cmd，然后运行 zkCli.cmd 查看 zk 运行状态，可以查看 zk 节点信息即代表安装成功。\n\n\n\n\n搭建后端环境\n\n\n新建一个自我调试的 mysql 库，库名可为 ：dolphinschedulerKou\n\n\n把代码导入 idea，修改根项目中 pom.xml，将 mysql-connector-java 依赖的 scope 修改为 compile\n\n\n修改 dolphinscheduler-dao 模块的 datasource.properties\n# mysql\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://192.168.2.227:3306/dolphinschedulerKou?useUnicode=true&amp;characterEncoding=UTF-8\nspring.datasource.username=root\nspring.datasource.password=Dm!23456\n\n\n\n刷新 dao 模块，运行 org.apache.dolphinscheduler.dao.upgrade.shell.CreateDolphinScheduler 的 main 方法，自动插入项目所需的表和数据\n\n\n修改 dolphinscheduler-service 模块的 zookeeper.properties\nzookeeper.quorum=localhost:2181\n\n\n\n在 logback-worker.xml、logback-master.xml、logback-api.xml 中添加控制台输出\n&lt;root level=&quot;INFO&quot;&gt;\n    &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;  &lt;!-- 添加控制台输出 --&gt;\n    &lt;appender-ref ref=&quot;APILOGFILE&quot;/&gt;\n    &lt;appender-ref ref=&quot;SKYWALKING-LOG&quot;/&gt;\n&lt;/root&gt;\n\n\n\n启动 MasterServer，执行 org.apache.dolphinscheduler.server.master.MasterServer 的 main 方法，需要设置 VM Options:\n-Dlogging.config=classpath:logback-master.xml -Ddruid.mysql.usePingMethod=false\n\n\n\n启动 WorkerServer，执行 org.apache.dolphinscheduler.server.worker.WorkerServer 的 main 方法，需要设置 VM Options:\n-Dlogging.config=classpath:logback-worker.xml -Ddruid.mysql.usePingMethod=false\n\n\n\n启动 ApiApplicationServer，执行 org.apache.dolphinscheduler.api.ApiApplicationServer 的 main 方法，需要设置 VM Options:\n-Dlogging.config=classpath:logback-api.xml -Dspring.profiles.active=api\n\n\n\n如果需要用到日志功能，执行 org.apache.dolphinscheduler.server.log.LoggerServer 的 main 方法。\n\n\n后端 swagger 地址 ：http://localhost:12345/dolphinscheduler/doc.html?language=zh_CN&amp;lang=cn\n\n\n\n\n搭建前端环境\n\n\n本机安装 node（不再赘述）\n\n\n进入 dolphinscheduler-ui，运行\nnpm install\nnpm run start\n\n\n\n访问 http://localhost:8888\n\n\n登录管理员账号\n\n用户：admin\n密码：dolphinscheduler123\n\n\n\n\n\n",
    "title": "DolphinScheduler在Windows环境启动源码",
    "time": "2021-07-05"
  },
  {
    "name": "DolphinScheduler_Kubernetes_Technology_in_action",
    "content": "Apache DolphinScheduler 在 Kubernetes 体系中的技术实战\n作者 | 杨滇，深圳交通中心 数据和算法平台架构师\nKubernetes 技术体系给 Apache DolphinScheduler 带来的技术新特性\nApache DolphinScheduler 是当前非常优秀的分布式易扩展的可视化工作流任务调度平台。\n基于笔者所在公司业务的特性，阐述我们使用 Kubernetes 作为 Apache DolphinScheduler 的技术底座的原因：\n\n各类独立部署项目，需要快速建立开发环境和生产环境；\n项目环境互联网访问受限，服务器只能使用离线的安装方式；\n尽可能统一的安装配置的信息，减少多个项目配置的异常；\n与对象存储技术的结合，统一非结构化数据的技术；\n便捷的监控体系，与现有监控集成；\n多种调度器的混合使用；\n全自动的资源调整能力；\n快速的自愈能力；\n\n本文的案例都是基于 Apache DolphinScheduler1.3.9 版本为基础。Hadoop\n基于 helm 工具的自动化高效部署方式\n首先，我们介绍基于官网提供的 helm 的安装方式。Helm 是查找、分享和使用软件构建 Kubernetes 的最优方式。也是云原生 CNCF 的毕业项目之一。\n\n\n\n海豚的官网和 GitHub 上有非常详细的配置文件和案例。这里我们重点介绍一些社区中经常出现的咨询和问题。\n官网文档地址 https://dolphinscheduler.apache.org/zh-cn/docs/1.3.9/kubernetes-deployment\nGitHub 文件夹地址 https://github.com/apache/dolphinscheduler/tree/1.3.9/docker/kubernetes/dolphinscheduler/\n\n\n在 value.yaml 文件中修改镜像，以实现离线安装（air-gap install）；\nhttps://about.gitlab.com/topics/gitops/\nimage:\n  repository: &quot;apache/dolphinscheduler&quot;\n  tag: &quot;1.3.9&quot;\n  pullPolicy: &quot;IfNotPresent&quot;\n\n针对公司内部安装好的 harbor，或者其他公有云的私仓，进行 pull，tag，以及 push。这里我们假定私仓地址是 harbor.abc.com，你所在构建镜像的主机已经进行了 docker login harbor.abc.com， 且已经建立和授权私仓下面新建 apache 项目。\n执行 shell 命令\ndocker pull apache/dolphinscheduler:1.3.9\ndock tag apache/dolphinscheduler:1.3.9 harbor.abc.com/apache/dolphinscheduler:1.3.9\ndocker push apache/dolphinscheduler:1.3.9\n\n再替换 value 文件中的镜像信息，这里我们推荐使用 Always 的方式拉取镜像，生产环境中尽量每次去检查是否是最新的镜像内容，保证软件制品的正确性。此外，很多同学会把 tag 写成 latest（制作镜像不写 tag 信息，这样在生产环境非常危险，任何人 push 了镜像，就相当于改变了 latest 的 tag 的镜像，而且也无法判断 latest 是什么版本，所以建议要明确每次发版的 tag，并且使用 Always。\nimage:\n  repository: &quot;harbor.abc.com/apache/dolphinscheduler&quot;\n  tag: &quot;1.3.9&quot;\n  pullPolicy: &quot;Always&quot;GitHub\n\n把 https://github.com/apache/dolphinscheduler/tree/1.3.9/docker/kubernetes/dolphinscheduler/ 整个目录 copy 到可以执行 helm 命令的主机，然后按照官网执行\nkubectl create ns ds139Git\nMySQL install dolphinscheduler . -n ds139\n\n即可实现离线安装。\n\n\n集成 DataX MySQL Oracle 客户端组件，首先下载以下组件\n\n\nhttps://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.49/mysql-connector-java-5.1.49.jar\nhttps://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/\nhttps://GitHub.com/alibaba/DataX/blob/master/userGuid.md 根据提示进行编译构建，文件包位于 {DataX_source_code_home}/target/DataX/DataX/\n基于以上 plugin 组件新建 dockerfile，基础镜像可以使用已经 push 到私仓的镜像。\nFROM harbor.abc.com/apache/dolphinscheduler:1.3.9\nCOPY *.jar /opt/dolphinscheduler/lib/\nRUN mkdir -p /opt/soft/DataX\nCOPY DataX /opt/soft/DataX\n\n保存 dockerfile，执行 shell 命令\ndocker build -t harbor.abc.com/apache/dolphinscheduler:1.3.9-MySQL-Oracle-DataX .  #不要忘记最后一个点\ndocker push harbor.abc.com/apache/dolphinscheduler:1.3.9-MySQL-Oracle-DataX\n\n修改 value 文件\nimage:\n  repository: &quot;harbor.abc.com/apache/dolphinscheduler&quot;\n  tag: &quot;1.3.9-MySQL-Oracle-DataX&quot;\n  pullPolicy: &quot;Always&quot;\n\n执行 helm install dolphinscheduler . -n ds139，或者执行 helm upgrade dolphinscheduler -n ds139，也可以先 helm uninstall dolphinscheduler -n ds139，再执行 helm install dolphinscheduler . -n ds139。\n\n\n通常生产环境建议使用独立外置 postgresql 作为管理数据库，并且使用独立安装的 zookeeper 环境（本案例使用了 zookeeper operator https://GitHub.com/pravega/zookeeper-operator ，与 Apache DolphinScheduler 在同一个 Kubernetes 集群中）。\n## If not exists external database, by default, Dolphinscheduler&#x27;s database will use it.\npostgresql:\n  enabled: false\n  postgresqlUsername: &quot;root&quot;\n  postgresqlPassword: &quot;root&quot;\n  postgresqlDatabase: &quot;dolphinscheduler&quot;\n  persistence:\n    enabled: false\n    size: &quot;20Gi&quot;\n    storageClass: &quot;-&quot;\n\n## If exists external database, and set postgresql.enable value to false.\n## external database will be used, otherwise Dolphinscheduler&#x27;s database will be used.\nexternalDatabase:\n  type: &quot;postgresql&quot;\n  driver: &quot;org.postgresql.Driver&quot;\n  host: &quot;192.168.1.100&quot;\n  port: &quot;5432&quot;\n  username: &quot;admin&quot;\n  password: &quot;password&quot;\n  database: &quot;dolphinscheduler&quot;\n  params: &quot;characterEncoding=utf8&quot;\n\n## If not exists external zookeeper, by default, Dolphinscheduler&#x27;s zookeeper will use it.\nzookeeper:\n  enabled: false\n  fourlwCommandsWhitelist: &quot;srvr,ruok,wchs,cons&quot;\n  persistence:\n    enabled: false\n    size: &quot;20Gi&quot;\n    storageClass: &quot;storage-nfs&quot;\n  zookeeperRoot: &quot;/dolphinscheduler&quot;\n\n## If exists external zookeeper, and set zookeeper.enable value to false.\n## If zookeeper.enable is false, Dolphinscheduler&#x27;s zookeeper will use it.\nexternalZookeeper:\n  zookeeperQuorum: &quot;zookeeper-0.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.zookeeper.svc.cluster.local:2181&quot;\n  zookeeperRoot: &quot;/dolphinscheduler&quot;\n\n\n\n基于 argo-cd 的 Gitops 部署方式\nargo-cd 是基于 Kubernetes 的声明式 Gitops 持续交付工具。argo-cd 是 CNCF 的孵化项目，Gitops 的最佳实践工具。关于 Gitops 的解释可以参考https://about.gitlab.com/topics/gitops/\n\n\n\nGitops 可以为 Apache DolphinScheduler 的实施带来以下优点。\n\n图形化安装集群化的软件，一键安装；\nGit 记录全发版流程，一键回滚；\n便捷的海豚工具日志查看；\n\n使用 argo-cd 的实施安装步骤：\n\n\n从 GitHub 上下载 Apache DolphinScheduler 源码，修改 value 文件，参考上个章节 helm 安装需要修改的内容；\n\n\n把修改后的源码目录新建 Git 项目，并且 push 到公司内部的 Gitlab 中，GitHub 源码的目录名为 docker/Kubernetes/dolphinscheduler；\n\n\n在 argo-cd 中配置 Gitlab 信息，我们使用 https 的模式；\n\n\n\n\n\n\nargo-cd 新建部署工程，填写相关信息\n\n\n\n\n\n\n\n\n对 Git 中的部署信息进行刷新和拉取，实现最后的部署工作；可以看到 pod，configmap，secret，service 等等资源全自动拉起。\n\n\n\n\n\n\n\n\n\n通过 kubectl 命令可以看到相关资源信息；\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME                                     READY   STATUS    RESTARTS   AGE\ndolphinscheduler-alert-96c74dc84-72cc9   1/1     Running   0          22m\ndolphinscheduler-api-78db664b7b-gsltq    1/1     Running   0          22m\ndolphinscheduler-master-0                1/1     Running   0          22m\ndolphinscheduler-master-1                1/1     Running   0          22m\ndolphinscheduler-master-2                1/1     Running   0          22m\ndolphinscheduler-worker-0                1/1     Running   0          22m\ndolphinscheduler-worker-1                1/1     Running   0          22m\ndolphinscheduler-worker-2                1/1     Running   0          22m\n\n[root@tpk8s-master01 ~]# kubectl get statefulset -n ds139\nNAME                      READY   AGE\ndolphinscheduler-master   3/3     22m\ndolphinscheduler-worker   3/3     22m\n\n[root@tpk8s-master01 ~]# kubectl get cm -n ds139\nNAME                      DATA   AGE\ndolphinscheduler-alert    15     23m\ndolphinscheduler-api      1      23m\ndolphinscheduler-common   29     23m\ndolphinscheduler-master   10     23m\ndolphinscheduler-worker   7      23m\n\n[root@tpk8s-master01 ~]# kubectl get service -n ds139\nNAME                               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)              AGE\ndolphinscheduler-api               ClusterIP   10.43.238.5   &lt;none&gt;        12345/TCP            23m\ndolphinscheduler-master-headless   ClusterIP   None          &lt;none&gt;        5678/TCP             23m\ndolphinscheduler-worker-headless   ClusterIP   None          &lt;none&gt;        1234/TCP,50051/TCP   23m\n\n[root@tpk8s-master01 ~]# kubectl get ingress -n ds139\nNAME               CLASS    HOSTS           ADDRESS\ndolphinscheduler   &lt;none&gt;   ds139.abc.com\n\n\n\n\n可以看到所有的 pod 都分撒在 Kubernetes 集群中不同的 host 上，例如 worker1 和 2 都在不同的节点上。\n\n\n\n\n\n\n\n\n\n我们配置了 ingress，公司内部配置了泛域名就可以方便的使用域名进行访问；\n\n\n\n\n可以登录域名进行访问。\n\n\n\n具体配置可以修改 value 文件中的内容：\ningress:\n  enabled: true\n  host: &quot;ds139.abc.com&quot;\n  path: &quot;/dolphinscheduler&quot;\n  tls:\n    enabled: false\n    secretName: &quot;dolphinscheduler-tls&quot;\n\n\n方便查看 Apache DolphinScheduler 各个组件的内部日志：\n\n\n\n\n\n对部署好的系统进行检查，3 个 master，3 个 worker，zookeeper 都配置正常；\n\n\n\n\n\n\n\n\n\n\n\n\n使用 argo-cd 可以非常方便的进行修改 master，worker，api，alert 等组件的副本数量，海豚的 helm 配置也预留了 cpu 和内存的设置信息。这里我们修改 value 中的副本值。修改后，提交公司内部 Gitlab。\nmaster:\n  ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\n  podManagementPolicy: &quot;Parallel&quot;\n  ## Replicas is the desired number of replicas of the given Template.\n  replicas: &quot;5&quot;\n\nworker:\n  ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\n  podManagementPolicy: &quot;Parallel&quot;\n  ## Replicas is the desired number of replicas of the given Template.\n  replicas: &quot;5&quot;\n\nalert:\n  ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\n  replicas: &quot;3&quot;\n\napi:\n  ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\n  replicas: &quot;3&quot;\n\n\n\n\n只需要在 argo-cd 点击 sync 同步，对应的 pods 都按照需求进行了增加\n\n\n\n\n\n\n\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME                                     READY   STATUS    RESTARTS   AGE\ndolphinscheduler-alert-96c74dc84-72cc9   1/1     Running   0          43m\ndolphinscheduler-alert-96c74dc84-j6zdh   1/1     Running   0          2m27s\ndolphinscheduler-alert-96c74dc84-rn9wb   1/1     Running   0          2m27s\ndolphinscheduler-api-78db664b7b-6j8rj    1/1     Running   0          2m27s\ndolphinscheduler-api-78db664b7b-bsdgv    1/1     Running   0          2m27s\ndolphinscheduler-api-78db664b7b-gsltq    1/1     Running   0          43m\ndolphinscheduler-master-0                1/1     Running   0          43m\ndolphinscheduler-master-1                1/1     Running   0          43m\ndolphinscheduler-master-2                1/1     Running   0          43m\ndolphinscheduler-master-3                1/1     Running   0          2m27s\ndolphinscheduler-master-4                1/1     Running   0          2m27s\ndolphinscheduler-worker-0                1/1     Running   0          43m\ndolphinscheduler-worker-1                1/1     Running   0          43m\ndolphinscheduler-worker-2                1/1     Running   0          43m\ndolphinscheduler-worker-3                1/1     Running   0          2m27s\ndolphinscheduler-worker-4                1/1     Running   0          2m27s\n\nApache DolphinScheduler 与 s3 对象存储技术集成\n许多同学在海豚的社区中提问，如何配置 s3 minio 的集成。这里给出基于 Kubernetes 的 helm 配置。\n\n\n修改 value 中 s3 的部分，建议使用 ip+端口指向 minio 服务器。\ncommon:\n  ## Configmap\n  configmap:\n    DOLPHINSCHEDULER_OPTS: &quot;&quot;\n    DATA_BASEDIR_PATH: &quot;/tmp/dolphinscheduler&quot;\n    RESOURCE_STORAGE_TYPE: &quot;S3&quot;\n    RESOURCE_UPLOAD_PATH: &quot;/dolphinscheduler&quot;\n    FS_DEFAULT_FS: &quot;s3a://dfs&quot;\n    FS_S3A_ENDPOINT: &quot;http://192.168.1.100:9000&quot;\n    FS_S3A_ACCESS_KEY: &quot;admin&quot;\n    FS_S3A_SECRET_KEY: &quot;password&quot;\n\n\n\nminio 中存放海豚文件的 bucket 名字是 dolphinscheduler，这里新建文件夹和文件进行测试。\n\n\n\n\n\n\n\n\nApache DolphinScheduler 与 Kube-prometheus 的技术集成\n\n\n我们在 Kubernetes 使用 kube-prometheus operator 技术，实现了在部署海豚后，自动实现了对海豚各个组件的资源监控。\n\n\n请注意 kube-prometheus 的版本，需要对应 Kubernetes 主版本。https://GitHub.com/prometheus-operator/kube-prometheus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache DolphinScheduler 与 Service Mesh 的技术集成\n\n\n通过 Service Mesh 技术可以实现对海豚内部的服务调用，以及海豚 api 外部调用的可观测性分析，以实现 Apache DolphinScheduler 产品的自身服务优化。\n\n\n我们使用 linkerd 作为 Service Mesh 的产品进行集成，linkerd 也是 CNCF 优秀的毕业项目。\n\n\n \n\n\n\n\n只需要在海豚 helm 的 value 文件中修改 annotations，重新部署，就可以快速实现 mesh proxy sidecar 的注入。可以对 master，worker，api，alert 等组件都注入。\nannotations: #{}\n  linkerd.io/inject: enabled\n\n\n\n可以观察组件之间的服务通信质量，每秒请求的次数等等。\n\n\n\n\n\n\n未来 Apache DolphinScheduler 基于云原生技术的展望\nApache DolphinScheduler 作为面向新一代云原生大数据工具，未来可以在 Kubernetes 生态集成更多的优秀工具和特性，满足更多的用户群体和场景。\n\n\n和 argo-workflow 的集成，可以通过 api，cli 等方式在 Apache DolphinScheduler 中调用 argo-workflow 单个作业，dag 作业，以及周期性作业；\n\n\n使用 hpa 的方式，自动扩缩容 worker，实现无人干预的水平扩展方式；\n\n\n集成 Kubernetes 的 spark operator 和 Hadoopoperator 工具，全面的云原生化；\n\n\n实现多云和多集群的分布式作业调度；\n\n\n采用 sidecar 实现定期删除 worker 作业日志；\n\n\n\n\n\n",
    "title": "Apache DolphinScheduler 在 Kubernetes 体系中的技术实战",
    "time": "2022-2-18"
  },
  {
    "name": "DolphinScheduler漏洞情况说明",
    "content": "【安全通报】【影响程度：低】DolphinScheduler 漏洞情况说明\nApache DolphinScheduler 社区邮件列表最近通告了 1 个漏洞，考虑到有很多用户并未订阅此邮 件列表，我们特地在此进行情况说明：\nCVE-2021-27644\n重要程度： 低\n影响范围： 暴露服务在外网中、且内部账号泄露。如果无上述情况，用户可根据实际情况决定是否需要升级。\n影响版本： &lt;1.3.6\n漏洞说明：\n此问题是由于 mysql connectorj 漏洞引起的，DolphinScheduler 登陆用户（未登录用户无法执行此操作，建议企业做好账号安全规范）可在数据源管理页面-Mysql 数据源填写恶意参数，导致安全隐患。（未使用 Mysql 数据源的不影响）\n修复建议： 升级到&gt;=1.3.6 版本\n特别感谢\n特别感谢漏洞报告者：来自蚂蚁安全非攻实验室的锦辰同学，他提供了漏洞的还原过程以及对应的解决方案。整个过程呈现了专业安全人员的技能和高素质，感谢他们为开源项目的安全守护所作出的贡献。\n建议\n十分感谢广大用户选择 Apache DolphinScheduler 作为企业的大数据任务调度系统，但必须要提醒的是调度系统属于大数据建设中核心基础设施，请不要将其暴露在外网中。此外应该对企业内部人员账号做好安全措施，降低账号泄露的风险。\n贡献\n迄今为止，Apache DolphinScheduler 社区已经有近 200+ 位代码贡献者，70+位非代码贡献者。其中也不乏其他 Apache 顶级项目的 PMC 或者 Committer，非常欢迎更多伙伴也能参与到开源社区建设中来，为建造一个更加稳定安全可靠的大数据任务调度系统而努力，同时也为中国开源崛起献上自己的一份力量！\nWebSite ：https://dolphinscheduler.apache.org/zh-cn\nMailList ：dev@dolphinscheduler@apache.org\nTwitter ：@DolphinSchedule\nYouTube ：https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\nSlack ：https://s.apache.org/dolphinscheduler-slack\nContributor Guide：https://dolphinscheduler.apache.org/zh-cn/community\n如果对漏洞有任何疑问，欢迎参与讨论，竭诚解决大家的疑虑：\n",
    "title": "DolphinScheduler漏洞情况说明",
    "time": "2021-10-26"
  },
  {
    "name": "Eavy_Info",
    "content": "亿云基于 DolphinScheduler 构建资产数据管理平台服务，助力政务信息化生态建设 | 最佳实践\n\n\n\n作者| 孙浩\n\n编 者 按：基于 Apache Dolphinscheduler 调度平台，云计算和大数据提供商亿云信息已经服务公司多个项目部的地市现场平稳运行一年之久。\n\n\n结合政务信息化生态建设业务，亿云信息基于 DolphinScheduler 构建了资产数据管控平台的数据服务模块。他们是如何进行探索和优化的？亿云信息研发工程师 孙浩 进行了详细的用户实践交流分享。\n\n01 研发背景\n亿云主要的业务主要是 ToG 的业务，而业务前置的主要工作，在于数据的采集和共享，传统 ETL 工具，例如 kettle 等工具对于一线的实施人员的来说上手难度还是有的，再就是类似 kettle 的工具本身做为独立的部分，本身就增加了现场项目运维的使用难度。因此，如何实现一套数据采集（同步）—数据处理—数据管理的平台，就显得尤为重要。\n出于这样的考虑，我们开发了数据资产管理平台，而管理平台的核心就是我们基于DolphinSchduler（简称 DS）实现的数据服务模块。\nDolphinScheduler 是一个分布式去中心化，易扩展的可视化 DAG 调度系统，支持包括 Shell、Python、Spark、Flink 等多种类型的 Task 任务，并具有很好的扩展性。其整体架构如下图所示：\n\n\n\n典型的 master-slave 架构，横向扩展能力强，调度引擎是 Quartz，本身作为 Spring Boot 的 java 开源项目，对于熟悉 Spring Boot 开发的人，集成使用更加的简单上手。\nDS 作为调度系统支持以下功能：\n调度方式：系统支持基于 cron 表达式的定时调度和手动调度。命令类型支持：启动工作流、从当前节点开始执行、恢复被容错的工作流、恢复暂停流程、从失败节点开始执行、补数、定时、重跑、暂停、停止、恢复等待线程。其中恢复被容错的工作流和恢复等待线程两种命令类型是由调度内部控制使用，外部无法调用。\n定时调度：系统采用 quartz 分布式调度器，并同时支持 cron 表达式可视化的生成。\n依赖：系统不单单支持 DAG 简单的前驱和后继节点之间的依赖，同时还提供任务依赖节点，支持流程间的自定义任务依赖。\n优先级 ：支持流程实例和任务实例的优先级，如果流程实例和任务实例的优先级不设置，则默认是先进先出。\n邮件告警：支持 SQL任务 查询结果邮件发送，流程实例运行结果邮件告警及容错告警通知。\n失败策略：对于并行运行的任务，如果有任务失败，提供两种失败策略处理方式，继续是指不管并行运行任务的状态，直到流程失败结束。结束是指一旦发现失败任务，则同时Kill掉正在运行的并行任务，流程失败结束。\n补数：补历史数据，支持区间并行和串行两种补数方式。\n我们基于 Dolphinscheduler 与小伙伴一起进行如下的实践。\n02 基于DS构建数据同步工具\n回归业务本身，我们的业务场景，数据同步的业务需要是类型多，但数据量基本不会特别大，对实时要求并不高。所以在架构选型之初，我们就选择了 datax+ds 的组合，并进行对应业务的改造实现，现在作为服务产品融合在各个项目中，提供离线同步服务。\n\n\n\n同步任务分为了周期任务和一次性任务，在配置完成输入输出源的配置任务之后，周期任务的话，需要配置 corn 表达式，然后调用保存接口，将同步任务发送给DS 的调度平台。\n\n\n\n我们这里综合考虑放弃了之前 DS 的 UI 前端（第二部分在自助开发模块会给大家解释），复用 DS 后端的上线、启停、删除、日志查看等接口。\n\n\n\n\n\n\n整个同步模块的设计思路，就是重复利用 datax 组件的输入输出 plugin 多样性，配合 DS 的优化，来实现一个离线的同步任务，这个是当前我们的同步的一个组件图，实时同步这块不再赘述。\n\n\n\n03 基于DS的自助开发实践\n熟悉 datax的人都知道它本质上是一个 ETL 工具，而其 Transform 的属性体现在，它提供了一个支持 grovy 语法的 transformer 模块，同时可以在 datax 源码中进一步丰富 transformer 中用到工具类，例如替换、正则匹配、筛选、脱敏、统计等功能。而 Dolphinscheduler 的任务，是可以用 DAG 图来实现，那么我们想到，是否存在一种可能，针对一张表或者几张表，把每个 datax 或者 SQL 抽象成一个数据治理的小模块，每个模块按照 DAG 图去设计，并且在上下游之间可以实现数据的传递，最好还是和 DS 一样的可以拖拽式的实现。于是，我们基于前期对 datax 与 ds 的使用，实现了一个自助开发的模块。\n\n\n\n每个组件可能是一个模块，每个模块功能之间的依赖关系，我们利用 ds 的depend 来处理，而对应组件与组件传递数据，我们利用前端去存储，也就是我们在引入 input（输入组件）之后，让前端来进行大部分组件间的传递和逻辑判断，因为每个组件都可以看作一个 datax 的(输出/输出)，所有参数在输入时，最终输出的全集基本就确定了，这也是我们放弃 DS 的 UI 前端的原因。之后，我们将这个 DAG 图组装成 DS 的定义的类型，同样交付给 ds 任务中心。\nPS：因为我们的业务场景可能存在跨数据库查询的情况（不同实例的 mysql 组合查询），我们的 SQL 组件底层使用 Presto 来实现一个统一 SQL 层，这样即使是不同 IP 实例下的数据源（业务上有关联意义），也可以通过 Presto 来支持组合检索。\n04 其他的一些简单尝试\n熟悉治理流程的人都知道，如果能够做到简单的治理流程化，那么必然可以产出一份质量报告。我们在自助开发的基础上进行优化，将部分治理的记录写入 ES 中，再利用 ES 的聚合能力来实现了一个质量报告。\n\n\n\n\n\n\n\n\n\n以上便是我们使用 DS 结合 datax 等中间件，并结合业务背景所做的一些符合自身需求的实践。\n05 感谢\n从 EasyScheduler 到现在的 DolphinScheduler 2.0，我们更多时候还是作为旁观者，或者是追随者，而这次更多地是从这一年来使用 Dolphinscheduler 构建我们数据资产管控平台数据服务模块的实践来进行交流分享。当前基于Dolphinscheduler 调度平台，我们已经服务了公司多个项目部的地市现场运行。随着 DolphinScheduler 2.0 发版，我们也和 DolphinScheduler 一起在不断进步的社区环境中共同成长。\n06 欢迎更多实践分享\n如果你也是 Apache DolphinScheduler 的用户或实践者，欢迎投稿或联系社区分享你们的实践经验，开源共享，互帮互助！\n",
    "title": "亿云基于 DolphinScheduler 构建资产数据管理平台服务，助力政务信息化生态建设",
    "time": "2021-12-30"
  },
  {
    "name": "Exploration_and_practice_of_Tujia_Big_Data_Platform_Based",
    "content": "途家大数据平台基于 Apache DolphinScheduler 的探索与实践\n\n\n\n\n途家在 2019 年引入 Apache DolphinScheduler，在不久前的 Apache DolphinScheduler 2 月份的 Meetup上，途家大数据工程师 昝绪超 详细介绍了途家接入 Apache DolphinScheduler 的历程，以及进行的功能改进。\n\n\n\n\n途家大数据工程师数据开发工程师，主要负责大数据平台的开发，维护和调优。\n本次演讲主要包括4个部分，第一部分是途家的平台的现状，介绍途家的数据的流转过程，如何提供数据服务，以及Apache DolphinScheduler在平台中扮演的角色。第二部分，调度选型，主要介绍调度的一些特性，以及接入的过程。第三部分主要介绍我们对系统的的一些改进和功能扩展，包括功能表依赖的支持，邮件任务扩展，以及数据同步的功能，第四部分是根据业务需求新增的一些功能，如Spark jar包支持发布系统，调度与数据质量打通，以及表血缘展示。\n途家数据平台现状\n01 数据架构\n首先介绍一下途家数据平台的架构以及Apache DolphinScheduler在数据平台扮演的角色。\n\n\n\n途家数据平台架构\n上图为我司数据平台的架构，主要包括数据源，数据采集，数据存储，数据管理，最后提供服务。\n数据源主要来源包括三个部分：业务库MySQL API数据同步，涉及到Dubbo接口、http 接口，以及web页面的埋点数据。\n数据采集采用实时和离线同步，业务数据是基于Canal的增量同步，日志是Flume，Kafka实时收集，落到HDFS上。\n数据存储过程，主要涉及到一些数据同步服务，数据落到HDFS 后经过清洗加工，推送到线上提供服务。\n数据管理层面，数据字典记录业务的元数据信息，模型的定义，各层级之间的映射关系，方便用户找到自己关心的数据；日志记录任务的运行日志，告警配置故障信息等。调度系统，作为大数据的一个指挥中枢，合理分配调度资源，可以更好地服务于业务。指标库记录了维度和属性，业务过程指标的规范定义，用于更好的管理和使用数据。Abtest记录不同指标和策略对产品功能的影响；数据质量是数据分析有效性和准确性的基础。\n最后是数据服务部分，主要包括数据的即席查询，报表预览，数据下载上传分析，线上业务数据支持发布等。\n02 Apache DolphinScheduler在平台的作用\n下面着重介绍调度系统在平台扮演的角色。数据隧道同步，每天凌晨定时拉去增量数据。数据清洗加工后推送到线上提供服务。数据的模型的加工，界面化的配置大大提高了开发的效率。定时报表的服务，推送邮件，支持附件，正文table 以及折线图的展示。报表推送功能，数据加工后，分析师会配置一些数据看板，每天DataX把计算好的数据推送到MySQL，做报表展示。\n接入DS\n第二部分介绍我们接入Apache DolphinScheduler做的一些工作。\nApache DolphinScheduler具有很多优势，作为大数据的一个指挥中枢，系统的可靠性毋庸置疑，Apache DolphinScheduler 去中心化的设计避免了单点故障问题，以及节点出现问题，任务会自动在其他节点重启，大大提高了系统的可靠性。\n此外，调度系统简单实用，减少了学习成本，提高工作效率，现在公司很多人都在用我们的调度系统，包括分析师、产品运营，开发。\n调度的扩展性也很重要，随着任务量的增加，集群能及时增添资源，提供服务。应用广泛也是我们选择它的一个重要原因，它支持丰富的任务类型：Shell、MR、Spark、SQL(MySQL、PostgreSQL、Hive、SparkSQL)，Python，Sub_Process，Procedure等，支持工作流定时调度、依赖调度、手动调度、手动暂停/停止/恢复，同时支持失败重试/告警、从指定节点恢复失败、Kill任务等操作等。它的优势很多，一时说不完，大家都用起来才知道。\n接下就是我们定时调度的升级。\n在采用 Apache DolphinScheduler之前，我们的调度比较混乱，有自己部署本地的Crontab，也有人用Oozie做调度，还有部分是在系统做定时调度。管理起来比较混乱，没有统一的管理调度平台，时效性，和准确性得不到保障，管理任务比较麻烦，找不到任务的情况时有发生。此外，自建调度稳定性不足，没有配置依赖，数据产出没有保障，而且产品功能单一，支持的任务调度有限。\n\n\n\n2019年，我们引入Apache DolphinScheduler ，到现在已经接近三年时间，使用起来非常顺手。\n下面是我们迁移系统的一些数据。\n我们搭建了DS集群 ，共4台实体机 ，目前单机并发支持100个任务调度。\n算法也有专门的机器，并做了资源隔离。\nOozie 任务居多 ，主要是一些Spark和Hive 任务 。还有Crontab 上的一些脚本，一些邮件任务以及报表系统的定时任务。\n基于DS的调度系统构建\n在此之前，我们也对系统做过优化，如支持表级别的依赖，邮件功能的扩展等。接入Apache DolphinScheduler后，我们在其基础之上进行了调度系统构建，以能更好地提供服务。\n**第一． 支持表依赖的同步，**当时考虑到任务迁移，会存在并行的情况，任务一时没法全部同步，需要表任务运行成功的标记，于是我们开发了一版功能，解决了任务迁移中的依赖问题。然而，每个人的命名风格不太一样，导致配置依赖的时候很难定位到表的任务，我们也无法识别任务里面包含哪些表，无法判断表所在的任务，这给我们使用造成不小的麻烦。\n\n\n\n\n\n\n第二．邮件任务支持多table。 调度里面自带邮件推送功能，但仅支持单个table ,随着业务要求越来越多，我们需要配置多个 table和多个sheet，要求正文和附件的展示的的条数不一样，需要配置，另外还需要支持折线图的功能，丰富正文页面。此外，用户还希望能在正文或者每个表格下面加注释，进行指标的说明等。我们使用Spark jar包实现了邮件推送功能，支持异常预警、表依赖缺失等。\n\n\n\n\n\n\n**第三．支持丰富的数据源同步。**由于在数据传输方面存在一些问题，在以前迁移的过程中，我们需要修改大量的配置代码，编译打包上传，过程繁琐，经常出现漏改，错该，导致线上故障，数据源不统一，测试数据和线上数据无法分开；在开发效率方面，代码有大量重复的地方，缺少统一的配置工具，参数配置不合理，导致MySQL压力大，存在宕机的风险；数据传输后，没有重复值校验，数据量较大的时候，全量更新，导致MySQL压力比较大。MySQL 传输存在单点故障问题，任务延迟影响线上服务。\n\n\n\n\n\n\n我们在此过程中简化了数据开发的流程，使得MySQL支持 pxc/mha的高可用，提升了数据同步的效率。\n我们支持输入的数据源支持关系型数据库，支持FTP同步，Spark作为计算引擎，输出的数据源支持各种关系型数据库，以及消息中间件Kafka、MQ和Redis。\n接下来讲一下我们实现的过程。\n我们对Apache DolphinScheduler 的数据源做了扩展，支持kafka mq和namespace 的扩展，MySQL 同步之前首先在本地计算一个增量，把增量数据同步到MySQL，Spark 也支持了MySQL pxc/qmha 的高可用。另外，在推送MQ和Redis时会有qps 的限制，我们根据数据量控制Spark的分区数和并发量。\n改进\n第四部分主要是对系统新增的一些功能，来完善系统。主要包含以下三点：\n\nSpark支持发布系统\n数据质量打通\n数据血缘的展示\n\n01Spark 任务支持发布系统\n由于我们平时的调度80%以上都是Spark jar 包任务，但任务的发布流程缺少规范，代码修改随意，没有完整的流程规范，各自维护一套代码。这就导致代码不一致的情况时有发生，严重时还会造成线上问题。\n这要求我们完善任务的发布流程。我们主要使用发布系统，Jenkens 打包功能，编译打包后生成btag，在测试完成后再发布生成rtag ，代码合并到master 。这就避免了代码不一致的问题，也减少了jar包上传的步骤。在编译生成jar 包后，系统会自动把jar包推送到Apache DolphinScheduler的资源中心，用户只需配置参数，选择jar包做测试发布即可。在运行Spark任务时，不再需要把文件拉到本地，而是直接读取HDFS上的jar包。\n02 数据质量打通\n数据质量是保障分析结论的有效性和准确性的基础。我们需要要完整的数据监控产出流程才能让数据更有说服力。质量平台从四个方面来保证数据准确性，完整性一致性和及时性，并支持电话、企业微信和邮件等多种报警方式来告知用户。\n接下来将介绍如何将数据质量和调度系统打通。调度任务运行完成后，发送消息记录，数据质量平台消费消息，触发数据质量的规则监控 根据监控规则来阻断下游运行或者是发送告警消息等。\n03 数据血缘关系展示\n数据血缘是元数据管理、数据治理、数据质量的重要一环，其可以追踪数据的来源、处理、出处，为数据价值评估提供依据，描述源数据流程、表、报表、即席查询之间的流向关系，表与表的依赖关系、表与离线ETL任务，调度平台、计算引擎之间的依赖关系。数据仓库是构建在Hive之上，而Hive的原始数据往往来自生产DB，也会把计算结果导出到外部存储，异构数据源的表之间是有血缘关系的。\n\n追踪数据溯源：当数据发生异常，帮助追踪到异常发生的原因；影响面分析，追踪数据的来源，追踪数据处理过程。\n评估数据价值：从数据受众、更新量级、更新频次等几个方面给数据价值的评估提供依据。\n生命周期：直观地得到数据整个生命周期，为数据治理提供依据。\n血缘的收集过程主要是 ：Spark 通过监控Spark API来监听SQL和插入的表，获取Spark的执行计划 ，并解析Spark 执行计划。\n\n",
    "title": "途家大数据平台基于 Apache DolphinScheduler 的探索与实践",
    "time": "2022-3-10"
  },
  {
    "name": "Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial",
    "content": "极速开发扩充 Apache DolphinScheduler Task 类型 | 实用教程\n\n\n\n背景简介\n目前在大数据生态中，调度系统是不可或缺的一个重要组件。Apache DolphinScheduler 作为一个顶级的 Apache 项目，其稳定性和易用性也可以说是名列前茅的。而对于一个调度系统来说，能够支持的可调度的任务类型同样是一个非常重要的因素，在调度、分布式、高可用、易用性解决了的情况下，随着业务的发展或者各种需求使用到的组件增多，用户自然而然会希望能够快速、方便、简洁地对 Apache Dolphinscheduler 可调度的任务类型进行扩充。本文便带大家了解如何方便、极速扩充一个 Apache DolphinScheduler Task。\n作者简介\n\n\n\n张柏强，大数据开发工程师，主要研究方向为实时计算、元数据治理、大数据基础组件。\n1 什么是 SPI 服务发现(What is SPI)？\nSPI 全称为 (Service Provider Interface) ，是 JDK 内置的一种服务提供发现机制。大多数人可能会很少用到它，因为它的定位主要是面向开发厂商的，在 java.util.ServiceLoader 的文档里有比较详细的介绍，其抽象的概念是指动态加载某个服务实现。\n2 为什么要引入 SPI(Why did we introduce SPI)?\n不同的企业可能会有自己的组件需要通过 task 去执行，大数据生态中最为常用数仓工具 Apache Hive 来举例，不同的企业使用 Hive 方法各有不同。有的企业通过 HiveServer2 执行任务，有的企业使用 HiveClient 执行任务，而 Apache DolphinScheduler 提供的开箱即用的 Task 中并没有支持 HiveClient 的 Task，所以大部分使用者都会通过 Shell 去执行。然而，Shell 哪有天然的TaskTemplate 好用呢？所以，Apache DolphinScheduler 为了使用户能够更好地根据企业需求定制不同的 Task，便支持了 TaskSPI 化。\n我们首先要了解一下 Apache DolphinScheduler 的 Task 改版历程，在 DS 1.3.x 时，扩充一个 Task 需要重新编译整个 Apache DolphinScheduler，耦合严重，所以在 Apache DolphinScheduler 2.0.x 引入了 SPI。前面我们提到了 SPI 的抽象概念是动态加载某个服务的实现，这里我们具象一点，将 Apache DolphinScheduler 的 Task 看成一个执行服务，而我们需要根据使用者的选择去执行不同的服务，如果没有的服务，则需要我们自己扩充，相比于 1.3.x 我们只需要完成我们的 Task 具体实现逻辑，然后遵守 SPI 的规则，编译成 Jar 并上传到指定目录，即可使用我们自己编写的 Task。\n3 谁在使用它(Who is using it)?\n1、Apache DolphinScheduler\n\n\ntask\n\n\ndatasource\n2、Apache Flink\n\n\nflink sql connector，用户实现了一个flink-connector后，Flink也是通过SPI来动态加载的\n3、Spring boot\n\n\nspring boot spi\n4、Jdbc\n\n\njdbc4。0以前， 开发人员还需要基于Class。forName(&quot;xxx&quot;)的方式来装载驱动，jdbc4也基于spi的机制来发现驱动提供商了，可以通过META-INF/services/java。sql。Driver文件里指定实现类的方式来暴露驱动提供者\n5、更多\n\n\ndubbo\n\n\ncommon-logging\n\n\n4 Apache DolphinScheduler SPI Process?\n\n\n\n剖析一下上面这张图，我给 Apache DolphinScheduler 分为逻辑 Task 以及物理 Task，逻辑 Task 指 DependTask，SwitchTask 这种逻辑上的 Task；物理 Task 是指 ShellTask，SQLTask 这种执行任务的 Task。而在 Apache DolphinScheduler中，我们一般扩充的都是物理 Task，而物理 Task 都是交由 Worker 去执行，所以我们要明白的是，当我们在有多台 Worker 的情况下，要将自定义的 Task 分发到每一台有 Worker 的机器上，当我们启动 Worker 服务时，worker 会去启动一个 ClassLoader 来加载相应的实现了规则的 Task lib，可以看到 HiveClient 和 SeatunnelTask 都是用户自定义的，但是只有 HiveTask 被 Apache DolphinScheduler TaskPluginManage 加载了，原因是 SeatunnelTask 并没有去遵守 SPI 的规则。SPI 的规则图上也有赘述，也可以参考 java.util.ServiceLoader 这个类，下面有一个简单的参考(摘出的一部分代码，具体可以自己去看看）\npublic final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt; {\n    //scanning dir prefix\n    private static final String PREFIX = &quot;META-INF/services/&quot;;\n\n    //The class or interface representing the service being loaded\n    private final Class&lt;S&gt; service;\n\n    //The class loader used to locate, load, and instantiate providers\n    private final ClassLoader loader;\n\n    //Private inner class implementing fully-lazy provider lookup\n    private class LazyIterator implements Iterator&lt;S&gt; {\n        Class&lt;S&gt; service;\n        ClassLoader loader;\n        Enumeration&lt;URL&gt; configs = null;\n        String nextName = null;\n\n        //......\n        private boolean hasNextService() {\n            if (configs == null) {\n                try {\n                    //get dir all class\n                    String fullName = PREFIX + service.getName();\n                    if (loader == null)\n                        configs = ClassLoader.getSystemResources(fullName);\n                    else\n                        configs = loader.getResources(fullName);\n                } catch (IOException x) {\n                    //......\n                }\n                //......\n            }\n        }\n    }\n}\n\n\n5 如何扩展一个 data sourceTask or DataSource (How to extend a task or datasource)?\n5.1 创建 Maven 项目\n\nmvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.dolphinscheduler \\\n    -DarchetypeArtifactId=dolphinscheduler-hive-client-task \\\n    -DarchetypeVersion=1.10.0 \\\n    -DgroupId=org.apache.dolphinscheduler \\\n    -DartifactId=dolphinscheduler-hive-client-task \\\n    -Dversion=0.1 \\\n    -Dpackage=org.apache.dolphinscheduler \\\n    -DinteractiveMode=false \n    \n    \n\n5.2 Maven 依赖\n\n &lt;!--dolphinscheduler spi basic core denpendence--&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-spi&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version&gt;\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-task-api&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version&gt;\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency&gt;\n    \n    \n\n5.3 创建 Task 通道工厂(TaskChannelFactory)\n首先我们需要创建任务服务的工厂，其主要作用是帮助构建 TaskChannel 以及 TaskPlugin 参数，同时给出该任务的唯一标识，ChannelFactory 在 Apache DolphinScheduler 的 Task 服务组中，其作用属于是在任务组中的承上启下，交互前后端以及帮助 Worker 构建 TaskChannel。\npackage org.apache.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.spi.params.base.PluginParams;\nimport org.apache.dolphinscheduler.spi.task.TaskChannel;\nimport org.apache.dolphinscheduler.spi.task.TaskChannelFactory;\n\nimport java.util.List;\n\npublic class HiveClientTaskChannelFactory implements TaskChannelFactory {\n    /**\n     *  创建任务通道,基于该通道执行任务\n     * @return 任务通道\n     */\n    @Override\n    public TaskChannel create() {\n        return new HiveClientTaskChannel();\n    }\n\n    /**\n     *  返回当前任务的全局唯一标识\n     * @return 任务类型名称\n     */\n    @Override\n    public String getName() {\n        return &quot;HIVE CLIENT&quot;;\n    }\n\n    /**\n     * 前端页面需要用到的渲染,主要分为\n     \n     * @return\n     */\n    @Override\n    public List&lt;PluginParams&gt; getParams() {\n        List&lt;PluginParams&gt; pluginParams = new ArrayList&lt;&gt;();\n        InputParam nodeName = InputParam.newBuilder(&quot;name&quot;, &quot;$t('Node name')&quot;)\n                .addValidate(Validate.newBuilder()\n                        .setRequired(true)\n                        .build())\n                .build();\n        PluginParams runFlag = RadioParam.newBuilder(&quot;runFlag&quot;, &quot;RUN_FLAG&quot;)\n                .addParamsOptions(new ParamsOptions(&quot;NORMAL&quot;, &quot;NORMAL&quot;, false))\n                .addParamsOptions(new ParamsOptions(&quot;FORBIDDEN&quot;, &quot;FORBIDDEN&quot;, false))\n                .build();\n\n        PluginParams build = CheckboxParam.newBuilder(&quot;Hive SQL&quot;, &quot;Test HiveSQL&quot;)\n                .setDisplay(true)\n                .setValue(&quot;-- author: \\n --desc:&quot;)\n                .build();\n\n        pluginParams.add(nodeName);\n        pluginParams.add(runFlag);\n        pluginParams.add(build);\n\n        return pluginParams;\n    }\n}\n\n5.4 创建 TaskChannel\n有了工厂之后，我们会根据工厂创建出 TaskChannel，TaskChannel 包含如下两个方法，一个是取消，一个是创建，目前不需要关注取消，主要关注创建任务。\n    void cancelApplication(boolean status);\n\n    /**\n     * 构建可执行任务\n     */\n    AbstractTask createTask(TaskRequest taskRequest);\n    public class HiveClientTaskChannel implements TaskChannel {\n    @Override\n    public void cancelApplication(boolean b) {\n        //do nothing\n    }\n\n    @Override\n    public AbstractTask createTask(TaskRequest taskRequest) {\n        return new HiveClientTask(taskRequest);\n    }\n}\n\n\n5.5 构建 Task 实现\n通过 TaskChannel 我们得到了可执行的物理 Task，但是我们需要给当前 Task 添加相应的实现，才能够让Apache DolphinScheduler 去执行你的任务，首先在编写 Task 之前我们需要先了解一下 Task 之间的关系：\n\n\n\n通过上图我们可以看到，基于 Yarn 执行任务的 Task 都会去继承 AbstractYarnTask，不需要经过 Yarn 执行的都会去直接继承 AbstractTaskExecutor，主要是包含一个 AppID，以及 CanalApplication setMainJar 之类的方法，想知道的小伙伴可以自己去深入研究一下，如上可知我们实现的 HiveClient 就需要继承 AbstractYarnTask，在构建 Task 之前，我们需要构建一下适配 HiveClient 的 Parameters 对象用来反序列化JsonParam。\n  package com.jegger.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.ResourceInfo;\n\nimport java.util.List;\n\npublic class HiveClientParameters extends AbstractParameters {\n    /**\n     * 用HiveClient执行,最简单的方式就是将所有SQL全部贴进去即可,所以我们只需要一个SQL参数\n     */\n    private String sql;\n\n    public String getSql() {\n        return sql;\n    }\n\n    public void setSql(String sql) {\n        this.sql = sql;\n    }\n\n    @Override\n    public boolean checkParameters() {\n        return sql != null;\n    }\n\n    @Override\n    public List&lt;ResourceInfo&gt; getResourceFilesList() {\n        return null;\n    }\n}\n\n\n实现了 Parameters 对象之后，我们具体实现 Task，例子中的实现比较简单，就是将用户的参数写入到文件中，通过 Hive -f 去执行任务。\n package org.apache.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.request.TaskRequest;\nimport org.apache.dolphinscheduler.spi.utils.JSONUtils;\n\nimport java.io.BufferedWriter;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\n\n\npublic class HiveClientTask extends AbstractYarnTask {\n\n    /**\n     * hive client parameters\n     */\n    private HiveClientParameters hiveClientParameters;\n\n    /**\n     * taskExecutionContext\n     */\n    private final TaskRequest taskExecutionContext;\n\n\n\n    public HiveClientTask(TaskRequest taskRequest) {\n        super(taskRequest);\n        this.taskExecutionContext = taskRequest;\n    }\n\n    /**\n     * task init method\n     */\n    @Override\n    public void init() {\n        logger.info(&quot;hive client task param is {}&quot;, JSONUtils.toJsonString(taskExecutionContext));\n        this.hiveClientParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), HiveClientParameters.class);\n\n        if (this.hiveClientParameters != null &amp;&amp; !hiveClientParameters.checkParameters()) {\n            throw new RuntimeException(&quot;hive client task params is not valid&quot;);\n        }\n    }\n\n    /**\n     * build task execution command\n     *\n     * @return task execution command or null\n     */\n    @Override\n    protected String buildCommand() {\n        String filePath = getFilePath();\n        if (writeExecutionContentToFile(filePath)) {\n            return &quot;hive -f &quot; + filePath;\n        }\n        return null;\n    }\n\n    /**\n     * get hive sql write path\n     *\n     * @return file write path\n     */\n    private String getFilePath() {\n        return String.format(&quot;%s/hive-%s-%s.sql&quot;, this.taskExecutionContext.getExecutePath(), this.taskExecutionContext.getTaskName(), this.taskExecutionContext.getTaskInstanceId());\n    }\n\n    @Override\n    protected void setMainJarName() {\n        //do nothing\n    }\n\n    /**\n     * write hive sql to filepath\n     *\n     * @param filePath file path\n     * @return write success?\n     */\n    private boolean writeExecutionContentToFile(String filePath) {\n        Path path = Paths.get(filePath);\n        try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {\n            writer.write(this.hiveClientParameters.getSql());\n            logger.info(&quot;file:&quot; + filePath + &quot;write success.&quot;);\n            return true;\n        } catch (IOException e) {\n            logger.error(&quot;file:&quot; + filePath + &quot;write failed.please path auth.&quot;);\n            e.printStackTrace();\n            return false;\n        }\n\n    }\n\n    @Override\n    public AbstractParameters getParameters() {\n        return this.hiveClientParameters;\n    }\n}\n\n\n5.6 遵守 SPI 规则\n # 1,Resource下创建META-INF/services文件夹,创建接口全类名相同的文件\nzhang@xiaozhang resources % tree ./\n./\n└── META-INF\n    └── services\n        └── org.apache.dolphinscheduler.spi.task.TaskChannelFactory\n# 2,在文件中写入实现类的全限定类名\nzhang@xiaozhang resources % more META-INF/services/org.apache.dolphinscheduler.spi.task.TaskChannelFactory \norg.apache.dolphinscheduler.plugin.task.hive.HiveClientTaskChannelFactory\n\n\n5.7 打包和部署\n## 1,打包\nmvn clean install\n## 2,部署\ncp ./target/dolphinscheduler-task-hiveclient-1.0.jar $DOLPHINSCHEDULER_HOME/lib/\n## 3,restart dolphinscheduler server\n\n\n以上操作完成后，我们查看 worker 日志 tail -200f $Apache DolphinScheduler_HOME/log/Apache DolphinScheduler-worker.log\n\n\n\nApache DolphinScheduler 的插件开发就到此完成~涉及到前端的修改可以参考：\nApache DolphinScheduler-ui/src/js/conf/home/pages/dag/_source/formModel/\n",
    "title": "极速开发扩充 Apache DolphinScheduler Task 类型 | 实用教程",
    "time": "2022-4-14"
  },
  {
    "name": "Hangzhou_cisco",
    "content": "杭州思科对 Apache DolphinScheduler Alert 模块的改造\n\n\n\n杭州思科已经将 Apache DolphinScheduler 引入公司自建的大数据平台。目前，杭州思科大数据工程师   李庆旺  负责 Alert 模块的改造已基本完成，以更完善的 Alert 模块适应实际业务中对复杂告警的需求。\n\n\n\n李庆旺\n杭州思科 大数据工程师，主要负责 Spark、调度系统等大数据方向开发。\n我们在使用原有的调度平台处理大数据任务时，在操作上多有不便。比如一个对数据进行处理聚合分析的任务，首先由多个前置 Spark 任务对不同数据源数据进行处理、分析。最后的 Spark 任务对这期间处理的结果进行再次聚合、分析，得到我们想要的最终数据。但遗憾的是当时的调度平台无法串行执行多个任务，需要估算任务处理时间来设置多个任务的开始执行时间。同时其中一个任务执行失败，需要手动停止后续任务。这种方式既不方便，也不优雅。\n而 Apache DolphinScheduler 的核心功能——工作流定义可以将任务串联起来，完美契合我们的需求。于是，我们将 Apache DolphinScheduler 引入自己的大数据平台，而我主要负责 Alert 模块改造。目前我们其他同事也在推进集成 K8s，希望未来任务在 K8s 中执行。\n今天分享的是 Alert 模块的改造。\n01 Alert 模块的设计\n\n\n\nDolphinScheduler Alert 模块的设计\nApache DolphinScheduler 1.0 版本的 Alert 模式使用配置 alert.properties 的方式，通过配置邮箱、短信等实现告警，但这样的方式已经不适用于当前的场景了。官方也进行过告警模块重构，详情设计思路参考官方文档：\nhttps://github.com/apache/dolphinscheduler/issues/3049\nhttps://github.com/apache/dolphinscheduler/blob/dev/docs/docs/zh/contribute/backend/spi/alert.md\nApache DolphinScheduler 告警模块是一个独立启动的服务，核心之一是 AlertPluginManager 类。告警模块集成了很多插件，如钉钉、微信、飞书、邮件等，以独立的形式写在源码中，启动服务时会解析插件并将配置的参数格式化成 JSON 形式，前端通过 JSON 自动渲染出页面。AlertPluginManager 在启动时会缓存插件到内存中。AlertServer 类会启动线程池，定时扫描 DB。\n当工作流配置了通知策略，同时 Worker 执行工作流结束，执行结果匹配通知策略成功后，DB 插入告警数据后，线程池扫描 DB，调用 AlertSender 类的 send 方法传入告警数据。告警数据绑定的是告警组，一个告警组对应了多个告警实例。AlertSender 类遍历告警实例，通过 AlertPluginManager 类获取插件实例，调用实例的发送方法，最后更新结果。这是 Apache DolphinScheduler 的整个告警流程。\n需要注意的是，Alert server 启动的同时也启动了 RPC 服务，这是一种针对特殊类型任务，如 SQL 查询报表而设计的告警方式，可以让 Worker 通过 RPC 直接访问 Alert  Server，利用 Alert 模块完成告警，这个数据不写入 DB。但从整体上来说，Apache DolphinScheduler 的告警模式还是以写 DB，异步交互的方式为主。\n\n\n\n定义工作流之后，可以在启动前设置通知策略，绑定告警组。\n\n\n\n在任务维度，可以配置超时告警，当任务超时可以触发报警。这里没有告警组配置，任务和工作流共用一个告警组，当任务超时，会推送到工作流设置的告警组。\n\n\n\n上图为系统告警配置的流程图。可以看到，一个工作流可以配置多个任务实例，任务可以配置超时触发告警，工作流成功或者失败可以触发告警。一个告警组可以绑定多个告警实例。这样的配置不太合理，我们希望告警实例也可以匹配工作流/任务实例的状态，也就是工作流成功和失败调用同一个告警组，但是触发不同的告警实例。这样使用起来更符合真实场景。\n\n\n\n创建告警组，一个告警组可以绑定多个告警实例。\n02 大数据任务告警场景\n\n\n\n以下是我们日常工作中的一些 常见的大数据任务告警场景。\n对于定时任务，在开始执行前、任务上线、下线或修改参数，以及任务执行成功或失败时都发送通知。区别是，对于同一任务不同结果，我们希望触发不同的通知，比如成功发短信通知或者钉钉微信群通知即可，而任务失败了需要在第一时间通知对应的研发人员，以得到更快的响应，这时候钉钉微信群中@对应研发人员或者电话通知会更及时。目前，公司的任务调度平台是任务中调用 API 进行通知，这种与代码强耦合的方式极其不方便，实际上可以抽象成一个更为通用的模块来实现。\nApache DolphinScheduler 的架构虽然符合实际场景需求，但问题在于告警模块页面配置只能选择成功触发通知，或失败触发通知，绑定的是同一个告警组，即无论成功还是失败，告警的途径是相同的，这一点并不满足我们在实际生产环境中需要不同结果以不同方式通知的需求。因此，我们对 Alert 模块进行了一些改造。\n03 Alert 模块的改造\n\n\n\n改造的第一步是告警实例。此前，新增一个告警实例，触发告警就会触发该实例的 send 方法，我们希望在定义告警实例时可以绑定一个告警策略，有三个选项，成功发、失败发，以及成功和失败都发。\n在任务定义维度，有一个超时告警的功能，实际上对应失败的策略。\n\n\n\n上图为改造完成的配置页面，在创建告警实例页面，我们添加了一个告警类型字段，选择是在成功、失败，或者无论成功或失败时调用插件。\n\n\n\n上图为改造后 Apache DolphinScheduler 告警模块的架构，我们对其中进行了两点改造。\n其一，在执行完工作流或任务时，如果触发告警，在写入 DB 时，会保存本次工作流或者任务的执行结果，具体是成功还是失败。\n第二，调用告警实例发送方法添加了一个逻辑判断，将告警实例与任务状态进行匹配，匹配则执行该告警实例发送逻辑，不匹配则过滤。\n改造后告警模块支持场景如下：\n\n\n\n详细设计请参考 issue：https://github.com/apache/dolphinscheduler/issues/7992\n代码详见：https://github.com/apache/dolphinscheduler/pull/8636\n此外，我们还针对 Apache DolphinScheduler 的告警模块向社区提出几点优化的建议，感兴趣的小伙伴可以跟进 issue，一起来做后续的工作：\n\n工作流启动或上下线或参数修改时，可以触发通知；\n告警场景针对 worker 的监控，如果 worker 挂掉或和 ZK 断开失去心跳，会认为 worker 宕机，会触发告警，但会默认匹配 ID 为 1 的告警组。这样的设置是在源码中写明的，但不看源码不知道其中的逻辑，不会专门设置 ID 为 1 的告警组，无法第一时间得到 worker 宕机的通知；\n告警模块目前支持飞书、钉钉、微信、邮件等多种插件，这些插件适用于国内用户，但国外用户可能使用不同的插件，如思科使用的 Webex Teams，国外常用告警插件 PagerDuty，我们也都进行开发并贡献给了社区。同时还有一些比较常用的比如 Microsoft Teams 等，感兴趣的小伙伴也可以提个 PR，贡献到社区。\n最后一点，可能大数据领域的小伙伴对于前端不太熟悉，想要开发并贡献告警插件，但是想到需要开发前端就不想进行下去了。开发 Apache DolphinScheduler 告警插件是不需要写前端代码的，只需要在新建告警实例插件时，在 Java 代码中配置好页面中需要输入的参数或者需要选择的按钮（源码详见 org.apache.dolphinscheduler.spi.params），系统会自动格式化成 JSON 格式，前端通过 form-create 可以通过 JSON 自动渲染成页面。因此，完全不用担心写前端的问题。\n\n",
    "title": "杭州思科对 Apache DolphinScheduler Alert 模块的改造",
    "time": "2022-3-16"
  },
  {
    "name": "Help_the_data_pipeline_platform_in_the_automotive_industry_to_connect_data_islands_and_strengthen_data_unification",
    "content": "讲师简介\n长城汽车-IDC-数据中台部-刘永飞 高级工程师\n我是长城汽车 IDC-数据中台部的刘永飞，给大家分享一下我们自研的一个数据同步工具平台，以及在使用这个工具过程中遇到的问题。今天的分享主要有四个部分：\n\n我们自研的数据管道工具平台的定位和功能；\nDolphinScheduler 在这个数据管道平台中的应用；\n总结了我们在使用 DolphinScheduler 时遇到的一些问题；\n对于数据管道平台的总结。\n\n数据管道\n本章节我将介绍一下我们自研的数据管道平台，包括技术架构、支持多种数据源、支持多种管道、主要界面、引擎设置、数据类型映射、人工告警和推广几个方面。\n平台简介\n数据管道是一个基于分布式技术构建的数据传输平台，支持多种数据源海量数据的实时、离线的方式传输。\n数据管道可通过简单的页面配置即可完成数据传输，操作过程简单且高效，降低用户使用门槛；内设告警机制，传输任务出现异常可第一时间通过钉钉将信息发送具体责任人。\n我们从立项之初，其实是为了解决长城汽车在数据方面的一些问题，主要目标就是连接数据孤岛，加速数据的一元化。大家知道凡是涉及到数据，数据孤岛问题就是一个绕不开的问题，我们就希望能够通过数据管道连接好各个业务线、各个领域、各个系统，真正的打破数据孤岛。\n另一个目标就是加速数据一元化了，数据一元化是长城汽车在数智化转型过程中一个关键目标，做数据一元化的第一步就是数据的快速汇集，我们也能够承担好这个快速汇集数据的角色。\n技术架构\n给大家介绍一下我们的这个管道平台的技术架构。\n整个架构中，最左边是一个数据源的源端，也就是整个数据的起点。最右边就是数据源的目的端，是数据的目的地。通过中间的这个数据管道，可以实现数据的传输，中间最下边就是数据管道资源池。\n在数据管道中有一个资源池的概念，我们把它分为公共资源和私有化资源。公共资源是我们平台提供的，公共资源也做到了资源队列隔离，相互之间不会有影响。如果用户对于资源有特殊要求，我们也支持用户提供机器，提供私有化的资源。\n在资源之上就是管道引擎层，引擎层中是我们自研的数据传输引擎，细节就不在这里体现了。\n最上面的 web 层，我们提供了项目级隔离，任务管理、资源管理、日志查看、告警等能力，更加友好的让用户使用我们平台。\n支持多种数据源\n截止到当前的V2.1.4版本，数据管道平台可以支持 23 种数据源，基本上涵盖了主流的关系型数据库常见的大数据组件。\n支持多种管道\n在现有支持的 23 种数据源基础上，细分到离线任务、实时任务的全量同步、增量同步维度后，数据管道平台可支持将近 900 种管道。\n以常见的关系型数据库 MySQL 做为数据源为例，一共可以支持 38 个管道。\n操作简单、容易上手\n这是数据管道的 UI 界面，我们自研的初衷就是要简单，通过简单的交互，用户录入源端数据源、目的端数据源，连通性测试通过后，就可以进行任务的创建了。通过简单的页面配置，用户很快就可以创建出一个能够支持大数据量同步的任务。\n主要界面\n这是数据源的管理用户界面，你可以根据你想要的类型进行对应的数据源连接参数创建，下面这张图以一个离线任务创建任务为例，来展示新建任务设置的界面。\n引擎设置\n数据管道平台可以根据任务使用的计算引擎（Spark/Flink）来设置任务运行过程中所需的资源参数。\n数据管道平台可以根据任务使用的计算引擎（Spark/Flink）来设置任务运行过程中所需的资源参数。\n数据类型映射\n目标库设置时可以方便的进行源端字段和目标端字段的映射。我们收集了Spark/Flink的数据类型映射字典，用于进行源端数据类型到目标数据类型的转换。\n任务告警\n用户在创建任务的时候开启告警设置并选择通知用户后，如果任务执行失败，会在第一时间将告警信息发给通知用户的钉钉账户。\n如果用户已经在数据管道平台处于登录状态，则点击”查看错误日志”可以直接跳转到任务实例的提交日志界面，查看日志详情。\n方便、丰富的日志查看\n在任务创建成功，设置任务”上线”后，点击”手动运行”便可以运行任务了。数据管道平台提供了丰富的日志管理功能，供用户查看任务执行信息。用户可以通过平台生成的日志链接很方便的查看任务向集群提交时的提交日志、任务在集群运行时的运行日志，如果是实时任务，还可以直接跳转到 Flink的web UI 进行任务信息的查看。\n推广成果\n目前该产品已经在我们内部的一些部门及子公司进行了使用，创建任务 300+ 个，每日近 2000 个任务实例运行 。\nDolphinScheduler 在数据管道平台中的应用\n主要流程\n我们的数据管道依赖了 DolphinScheduler（V3.0.0）的能力，用户在数据管道上创建任务、运行任务，会经海豚调度器进行调度，提交工作流后，最终任务将在集群中执行。\n对大家可以看到，最左侧就是数据管道平台创建数据源，创建任务，数据管道根据不同的数据源获取模板，更新模板，绑定配置文件，最终在数据管道上点执行任务，就会依赖 DolphinScheduler 的能力去执行工作流，提交任务，并在 Yarn 集群中执行。同时在这个过程中，DolphinScheduler 会收集到提交任务的日志，我们利用这个能力，在我们的平台上可以查看任务的实时日志。\n数据管道使用了哪些DolphinScheduler的API服务\n数据管道前台使用了我们自定义的 UI 界面，后台的许多功能使用了DolphinScheduler 的 API 服务，包括项目相关的操作，任务状态相关、数据源相关等，具体如下图所示：\n数据管道创建任务会生成工作流定义数据\n用户在数据管道上创建任务之后会生成一个 Resource Name，还有一些配置文件。配置文件会上传到资源中心，上传成功之后会有一个Resource ID，之后我们会组装数据格式，把它合成任务所需要的参数，然后再组装出来一个任务节点的定义，形成一个任务节点定义列表。任务节点关系就形成任务节点关系列表，任务节点位置就形成任务节点位置列表。任务的执行类型、全局参数等数据组装起来之后，到 DolphinScheduler 创建功能的定义接口，这样创建工作流的流程就做完了。\n然后我再讲几个特色的功能给大家分享一下。\n参数设置\n用户在数据管道创建任务的时候可以进行参数的设置。这里我们使用了 DolphinScheduler 内置的时间参数进行参数复制，在过滤条件里边使用定义好的参数进行数据过滤。\n离线任务比较常见的是补数，这一块，我们通过参数式的功能支持用户在界面上进行参数的自定义，如上图所示。\n提交日志\n从数据管道平台运行任务后，我们会调用DolphinScheduler 的运行工作流接口，我们DolphinScheduler 的提交任务日志详情接口拿到提交任务日志数据，用户可以刷新、下载日志。\n扩充实例列表支持实时任务断点续传\n数据管道平台在创建任务时支持创建多个子任务。每个子任务均可查看实例列表。这里我们调用了 DolphinScheduler 的实例列表接口来展示运行信息，并在该接口的基础上，添加了实例的运行状态、运行开始时间、运行结束时间、实例运行时长等。同时，我们提供了实时任务的停止、运行按钮，可支持实时任务的断点续传功能。\n在使用DolphinScheduler时遇到的问题\n现在我来说一下我们团队小组的成员在使用 DolphinScheduler 时遇到的一些问题。当然遇到的问题很多，我摘出了三个比较有代表性的问题。并给出了我们对应的解决方案。\n问题1：获取到的任务状态不对\n最初，我们在数据管道平台调用工作流实例列表接口获取实例的信息时，发现接口返回的 state字段值是 SUCCESS，但其实任务是执行失败的。于是就去仔细研究了一下这个 state 字段，发现其实这只是海豚调度器提交任务时获取到的一个状态，并不能真实反映任务的运行状态，于是我们在改接口的基础上又添加了实例的运行状态的逻辑封装。\n问题2：DolphinScheduler 集群扩容，workgroup 分组遇到的问题\n这个问题是这样，我们在扩容时新增的节点的也加入到了默认的 default 组，由于新扩容的节点和现在的 work 节点属于不同的 Hadoop 集群，这样的话，提交任务到 default 组，会存在这个组的节点不是属于同一个集群而报错。所以需要把这些新增的节点根据hadoop集群而进行分组。\n最初我们修改了 install_env.sh 配置文件里面的 works 设置，分发文件，重启集群，但是通过DolphinScheduler web 界面发现 work分组设置没有生效，新节点还是属于 default 组。为什么没生效呢？找了好长的时间，最后发现新节点的 worker-server 的application.yaml 配置里面看到 groups 是 default，于是修改 default 为新的workgroup 名称，再次重启DolphinScheduler 集群，分组就显示正常了。\n问题3：资源中心配置\n这个说起来也是因为我们的 DolphinScheduler 上面有两个 Hadoop 集群，配置一个 hdfs，提交到另一个集群的任务可能会存在找不到文件的情况。我们知道对于 standalone 环境，可以选择本地文件目录作为上传文件夹，我们想了两个方案，一个是 NFS 文件共享，另一个是 OSS，我们选择了后者，OSS 通过服务器挂载就像普通磁盘一样使用方便，还有就是 OSS 底层是多副本存储，数据存储上和NFS相比更安全。\n总结\n说完了我们所遇到的问题，最后总结一下我在使用 DolphinScheduler 过程中的一些心得体会吧！\n首先，得益于 DolphinScheduler 强大的能力、丰富的文档、火热的社区等多方面综合因素，我们在技术选项的时候首选了DolphinScheduler。\n也得益于这个选择，截止到当前数据管道最新版为止，DolphinScheduler 对数据管道平台提供了强有力的支撑，使我们的开发工作重心可全面投入到数据管道本身产品功能上去，跟工作流调度有关的实现直接调用 DolphinScheduler 的 API 服务即可，我们会在此基础上添加了针对数据管道平台场景的逻辑补充去完善数据管道的产品功能。\n最后，在后续的数据管道版本迭代中，我们会根据功能需求，继续深入研究我们尚未体验和使用的 DolphinScheduler 功能，也希望 DolphinScheduler 社区能够一直活跃下去， 让 DolphinScheduler 能够越来越好。\n",
    "title": "助力长城汽车数据管道平台连接 “数据孤岛”，加强数据一元化，Apache DolphinScheduler 的角色定位",
    "time": "2023-8-31"
  },
  {
    "name": "How_Does_360_DIGITECH_process_10_000+_workflow_instances_per_day",
    "content": "日均处理 10000+ 工作流实例，Apache DolphinScheduler 在 360 数科的实践\n\n\n\n\n从 2020 年起，360 数科全面将调度系统从 Azkaban 迁移到 Apache DolphinScheduler。作为 DolphinScheduler 的资深用户，360 数科如今每天使用 DolphinScheduler 日均处理 10000+ 工作流实例。为了满足大数据平台和算法模型业务的实际需求，360 数科在 DolphinScheduler 上进行了告警监控扩展、worker增加维护模式、多机房等改造，以更加方便运维。他们具体是如何进行二次开发的呢？360 数科大数据工程师 刘建敏 在不久前的 Apache DolphinScheduler 2 月份的 Meetup 上进行了详细的分享。\n\n\n\n\n刘建敏\n360 数科 大数据工程师，主要从事 ETL 任务与调度框架的研究，大数据平台以及实时计算平台的开发。\n从 Azkaban 迁移到 DolphinScheduler\n2019 年之前，360 数科采用 Azkaban 调度进行大数据处理。\nAzkaban 是由 Linkedin 开源的批量工作流任务调度器。其安装简单，只需安装 web 与 executor server，就可以创建任务，并通过上传 zip 包实现工作流调度。\nAzkaban 的 web-executor 架构如下图所示：\n\n\n\nAzkaban 的缺点\nAzkaban 适用于场景简单的调度，经过三年时间的使用，我们在发现它存在三个重要缺陷：\n\n\n体验性差\n没有可视化创建任务功能，创建与修改任务都需要通过上传 zip 包来实现，这并不方便；另外，Azkaban 没有管理资源文件功能。\n\n\n功能不够强大\nAzkaban 缺乏一些生产环境中不可或缺的功能，比如补数、跨任务依赖功能；用户与权限管理太弱，调度配置不支持按月，在生产上我们需要用很多其他方式进行弥补。\n\n\n稳定性不够好\n最重要的一点是 Azkaban 稳定性不够，当 executor 负载过高时，任务经常会出现积压；小时级别或分钟级别的任务容易出现漏调度；没有超时告警，虽然我们自己开发了有限的短信告警，但还是容易出现生产事故。\n\n\n\n\n\n针对这些缺陷，我们在 2018 曾进行过一次改造，但由于 Azkaban 源码复杂，改造的过程很是痛苦，因此我们决定重新选型。当时，我们测试了 Airflow、DolphinScheduler 和 XXL-job，但 Airflow Python 的技术栈与我们不符，而 XXL-job 功能又过于简单，显然，DolphinScheduler 是更好的选择。\n2019 年，我们 Folk 了 EasyScheduler 1.0 的代码，在 2020 年进行改造与调度任务的部分迁移，并上线运行至现在。\nDolphinScheduler 选型调研\n为什么我们选择 DolphinScheduler？因为其有四点优势：\n\n去中心化结构，多 Master 多 Worker；\n调度框架功能强大，支持多种任务类型，具备跨项目依赖、补数功能；\n用户体验性好，可视化编辑 DAG 工作流，操作方便；\nJava 技术栈，扩展性好。\n\n\n改造过程非常流畅，我们顺利地将调度系统迁移到了 DolphinScheduler。\nDolphinScheduler 的使用\n在 360 数科，DolphinScheduler 不仅用于大数据部门，算法部门也在使用其部分功能。为了让算法模型部们更方便地使用 DolphinScheduler 的功能，我们将其整合进了我们自己的毓数大数据平台。\n毓数大数据平台\n\n\n\n毓数是一个由基础组件、毓数平台、监控运维和业务支撑层组成的大数据平台，可实现查询、数据实时计算、消息队列、实时数仓、数据同步、数据治理等功能。其中，离线调度部分便是通过 DolphinScheduler 调度数据源到 Hive 数仓的 ETL 任务，以及支持 TiDB 实时监控，以实时数据报表等功能。\nDolphinScheduler 嵌套到毓数\n为了支持公司算法建模的需求，我们抽取了常用的一些节点，并嵌套了一层 UI，并调用 API。\n\n\n\n算法部门多用 Python 脚本和 SQL 节点，用框表逻辑进行定时，再配置机器学习算法进行训练，组装数据后用 Python 调用模型生成模型分。我们封装了一些 Kafka 节点，通过 Spark 读取 Hive 数据并推送到 Kafka。\n任务类型\n\n\n\nDolphinScheduler 支持的任务类型有 Shell、SQL、Python 和 Jar。其中，Shell 支持 Sqoop DataX mr 同步任务和 Hive-SQL、Spark-SQL；SQL 节点主要是支持 TiDB SQL（处理上游分库分表的监控） 和 Hive SQL；Python 任务类型支持离线调用模型脚本等；Jar 包主要支持 Spark Jar 离线管理。\n任务场景\n\n\n\nDolphinScheduler 的任务场景主要是将各种数据源，如 MySQL、Hbase 等数据源调度同步至 Hive，再通过 ETL 工作流直接生成 DW。通过 Python 脚本组装或调用，生成模型和规则结果，再推送到 Kafka。Kafka 会给出风控系统调额、审批和分析，并将结果反馈给业务系统。这就是 DolphinScheduler 调度流程的一个完整工作流示例。\nDolphinScheduler 的运维\n目前 ，DolphinScheduler 日均处理工作流已达到 10000+ 的规模，很多离线报表依赖于 DolphinScheduler，因此运维非常重要。\n\n在 360 数科，对 DolphinScheduler 的运维主要分为三部分：\n\nDS 依赖组件运维\n\n\n\n\nDolphinScheduler 依赖组件的运维主要是针对 MySQL 监控和 Zookeeper 监控。\n因为工作流定义元信息、工作流实例和任务实例、Quartz 调度、Command 都依赖 MySQL，因此 MySQL 监控至关重要。曾经有一次机房网络中断，导致很多工作流实例出现漏调度，之后通过 MySQL 监控才得以排查。\nZookeeper 监控的重要性也不言而喻，Master worker 状态和 task 队列都依赖 Zookeeper，但运行至今，Zookeeper 都比较稳定，还未有问题发生。\n\nMaster 与 Worker 状态监控\n我们都知道，Master 负责的是任务切分，实际上对业务的影响并不是很大，因此我们采用邮件监控即可；但 Worker 挂掉会导致任务延迟，增加集群压力。另外，由于之前 Yarn 运行任务不一定被成功地 kill，任务容错还可能导致 Yarn 任务重复运行，因此我们对  Worker 状态采用电话告警。\n\nGrafana 大盘监控工作流实例状态\n\n\n\n此外，我们还为运维创建了 Grafana 监控看板，可以实时监控工作流实例的状态，包括工作流实例的数量，各项目工作流实例运行状态，以及超时预警设置等。\nDolphinScheduler 改造\n体验性改造\n\n增加个人可以授权资源文件与项目，资源文件区分编辑与可读权限，方便授权；\n扩展全局变量(流程定义id, 任务id等放到全局变量中),提交到yarn上任务可追踪到调度的任务，方便集群管理，统计工作流实例锁好资源，利于维护\n工作流复制、任务实例查询接口优化，提高查询速度，以及 UI 优化。\n\n增加短信告警改造\n因为原有的邮箱告警不足以保证重要任务的监控，为此我们在 UI 上增加了 SMS 告警方式，保存工作流定义。另外，我们还把告警 receivers 改造成用户名，通过 AlertType 扩展成短信、邮箱等告警方式关联到用户信息，如手机号等，保证重要性任务失败后可以及时收到告警。\nWorker 增加维护模式改造\n此外，当 worker 机器需要维护时，需要这台 worker 上不会有新的运行任务提交过来。为此，我们对 worker 增加了维护模式改造，主要包括 4 点：\n\nUI 设置 worker 进行维护模式，调用 API 写入worker 指定的 zk 路径；\nWorkerServer 进行定时调度轮询是否维护模式；\nWorkerServer 进行维护模式，FetchTaskThread 不获取新增任务；\nworker 任务运行结束，可进行重启。\n经过以上改造，我们运维的压力大大减轻。\n\n多机房改造\n最后，我们还进行了多机房改造。因为我们之前有 2 套集群，分布在不同的机房，我们的改造目标是在调度中设置多机房，这样当某一个机房出现故障后，可一键切换到其他机房使用，实现多机房使用同一套调度框架。\n\n\n\n改造的切入点就是在调度中设置多机房，以保证某一任务可以在多机房启动。改造流程如下：\n\n在各机房分别部署 ZK，Master 与 Worker 注册到对应的机房，Master 负责任务切分，Worker 负责任务处理，各自处理各机房的任务；\nschedule 与 command 带上 datacenter 信息\n为保证双机房任务切换，资源文件进行上机房任务上传，同时改动任务接口，任务依赖、Master容错都需要改造过滤对应机房。\n\n",
    "title": "日均处理 10000+ 工作流实例，Apache DolphinScheduler 在 360 数科的实践",
    "time": "2022-3-15"
  },
  {
    "name": "How_Does_Live-broadcasting_Platform_Adapt_to_Apache_DolphinScheduler",
    "content": "论语音社交视频直播平台与 Apache DolphinScheduler 的适配度有多高\n\n\n\n在 Apache DolphinScheduler Meetup 上，YY 直播 软件工程师 袁丙泽为我们分享了《YY直播基于Apache DolphinScheduler的适配与探索》。\n本次演讲主要包括四个部分：\n\nYY直播引入Apache DolphinScheduler的背景\nApache DolphinScheduler的引入过程\nApache DolphinScheduler应用的适配\nYY直播未来的规划\n\n\n\n\n袁丙泽YY直播 软件工程师，10 余年工作经验，主要从事风控大数据平台开发工作，对常用大数据组件深感兴趣，研发经验丰富。\n背景\nYY直播是中国领先的语音社交视频直播企业，目前我们团队的主要职责是保障公司的业务安全。\n01技术现状\n目前我们采用分层的技术架构，最底层是数据源层，其次从下往上依次是采集层、存储层和管理层和计算层与应用层。\n\n在数据源层，我们目前会去拉取各个业务方的一个关系型数据库数据，以及通过API向我们传输的数据，还有一些数据是通过Kafka这种流的方式来传输给我们。\n\n采集层采用了我们自己研发的一套数据采集系统。\n\n存储层中，我们目前将数据主要放在了关系型数据库中，如Clickhouse，还有一小部分会放在一些非关系型数据库中，如Redis和图库。当然大部分数据都存储在大数据系统中。\n\n管理层我们主要有大数据管理系统，结合自己研发的一个计算调度以及任务管理系统和服务治理平台。\n02调度Apache DolphinScheduler之前的问题\n1、调度平台复杂：团队除了有基于Xxl-job的任务调度外，部分老项目中有使用Crontab、Springboot、Scheduler、Quartz等管理任务的启动。\n2、任务依赖需求强烈：目前我们所使用的的调度，仅能设置单个任务的执行，无法通过任务依赖形成工作流，任务依赖设置严重依赖于个人经验设定定时时间。实际上很多任务都需要有依赖关系。\n3、任务复杂多样：目前任务有基于大数据系统的Spark、Flink任务，服务治理平台中各种Java服务任务、Shell、Java application、Python等。\n引入过程\n在需求调研中，我们实际上需要一款调度平台，需要满足如下条件：\n1、统一管理任务及依赖关系\n随着业务计算的需求越来越多，特别是各种各样的画像计算和任务，这些任务分散在各个系统当中，管理起来非常困难，部分任务之间有一定的依赖关系，但配置其时间依靠的是个人经验。急需一款能够统一配置管理依赖的产品。\n2、兼容公司内部各平台系统\n我们需要调度任务平台管理我们的任务，同时为了快速投入使用，调度平台需要兼容我们公司其他的平台系统，如内部的Datax和Crontab服务。\n3、高可用、高性能、高并发，容易使用\n最后为了保证业务的稳定性，我们也需要这种调度平台能够高可用、高性能、高并发，并且容易使用。\n\n通过调研我们发现，Apache DolphinScheduler几乎就是为我们设计的，适配过程中无需太多修改，就能满足我们需求。\n应用适配\nApache DolphinScheduler 是一个分布式去中心化，易扩展的可视化DAG工作流任务调度系统，致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用，这非常符合我们的需求。\n首先了解下Apache DolphinScheduler的架构，便于理解接下来的适配案例。\n\n\n\nApache DolphinScheduler主要有API、master、 worker、 log以及 alert这5个模块。\nAPI接口层，主要负责处理前端UI层的请求。该服务统一提供RESTful api向外部提供请求服务。接口包括工作流的创建、定义、查询、修改、发布、下线、手工启动、停止、暂停、恢复、从该节点开始执行等等。\n\nMasterServer采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交监控，并同时监听其它MasterServer和WorkerServer的健康状态。MasterServer服务启动时向Zookeeper注册临时节点，通过监听Zookeeper临时节点变化来进行容错处理。\nWorkerServer也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务。WorkerServer服务启动时向Zookeeper注册临时节点，并维持心跳。workServer还提供有logger服务。\n\nAlert提供告警相关接口，接口主要包括两种类型的告警数据的存储、查询和通知功能。其中通知功能又有邮件通知和**SNMP(暂未实现)**两种。\n\n目前我们部署的是2.0版本，主要使用了4台物理机，在这4台物理机上部署了2个master实例，2个API实例和3个worker与logger实例，一个alert实例。\n\n接下来分享3个具体的适配案例。\n\n首先是与我们服务治理平台的适配，该适配主要目的是用于任务监控；尽管Apache DolphinScheduler本身提供有任务监控模块，我们同事早已经习惯利用服务治理平台统一管理监控。所以我们需要把Apache DolphinScheduler任务运行状态及时上报至服务治理平台。\n01服务治理适配—MasterServer服务说明\n在适配之前，再次详细了解下MasterServer服务，MasterServer提供有：\n\nDistributed Quartz分布式调度组件，主要负责定时任务的启停操作，当quartz挑起任务后，Master内部会有线程池具体负责处理任务的后续操作；\nMasterSchedulerThread是一个扫描线程，定时扫描数据库中的command表，根据不同的命令类型进行不同的业务操作；\nMasterExecThread（WorkflowExecutThread.java）主要负责DAG任务切分、任务提交监控、各种不同命令类型的逻辑处理；\nMasterTaskExecThread主要负责任务的持久化。\n\n02服务治理适配-code\n我们的需求是监控任务，通过代码分析，我们发现任务提交与监听主要在WorkflowExecuteThread类中的方法中实现，该类会启动多个实例线程。分别负责任务执行与监听。其流程图如下：\n\n\n\n任务提交及监控流程图\n我们的需求是监控任务，通过分析代码后发现，WorkflowExecuteThread主要有startprocess和handle events两个方法分别实现了任务执行与监听。其实我们主要在handleEvents方法中注入我们的服务治理平台数据收集代码，这样就能把任务监听情况及时上报到我们服务治理平台了。\n\n其修改部分如下：\n\n\n\n在服务治理平台中具体的效果图如下：\n\n\n\n除了监控我们的具体任务状况外，我们还会分 project去做一些监控，最后都通过服务治理平台来做监控操作，比如像一些任务如果比较重要，我们就会配置一些电话报警，即一旦这个任务失败或者未按时执行完毕，便会进行电话通知。\n03Datax服务适配过程\n第2个案例是关于Datax服务的适配过程。我们在研究Apache DolphinScheduler的时候，发现其已经集成了Datax类型的任务，这个对我们非常友好。因为我们也有数量相当多的任务是通过Datax来实现的，并且我们也开发了一部分Datax的插件，来去适配内部各个系统与存储的数据读写。\nDatax适配的时候主要分为两部分，一部分是通过这种自定义模板来去实现，这部分其实就是我们将之前的一些Datax的服务拷贝过来，稍加修改，就能够实现了，主要涉及到的是一些非关型数据库之间的一些数据交互。\n\n而纯粹的关型数据库之间的交互，我们还是需要通过配置方式实现。\n\n首先我们在配置Clickhouse读写任务时，就遇见了一个小bug。\n04Datax服务适配—Clickhouse兼容#8092\n我们在使用Datax来读取Clickhouse数据源的数据时，发现在sql当中，只要引用参数，无论时间参数还是其他参数，在提交的时都会失败，我们就怀疑其中可能有一些bug，阅读错误日志的时候，也发现在Apache DolphinScheduler提交 SQL时，是参数并未被替换就直接提交给了Clickhouse去执行，由于clickhouse并不能识别我们的Apache DolphinScheduler参数,所以就直接抛出异常了。我们梳理了一下Apache DolphinScheduler在执行datax任务时读取clickhouse的流程。其中在将我们在Apache DolphinScheduler配置转为datax配置流程如下：\n\n\n \n系统首先要做的就是先去解析sql的所有语法，然后通过语法拿到一些列的信息，这时它要去调用sql解析器。在这个过程当中，如果Apache DolphinScheduler没有对我们的这个参数去做替换，在执行这个 circle的时候就会发生错误，最后导致整个任务失败。\n\n因此在解决的过程中，既然可能获取不到Clickhouse的解析器，最好的方法就是直接加入一个解析器。首先构建一个Json文件，然后格式化解析出来的所有的链，最后对语法去做一次解析，层层调用，最后能够调用到目标解析器。\n05Time参数适配Apache DolphinScheduler现状\n最后的案例是关于时间参数适配。\n\nApache DolphinScheduler虽然提供有时间参数，但是我们自己的数据大部分都需要精确到毫秒级别的unixtime时间。通过阅读Apache DolphinScheduler的文档，我们遗憾地发现其并未提供该类型时间参数的实现。翻阅源码过程中，我们发现Apache DolphinScheduler提供有timestamp函数，实际上能够提供unixtime时间值。\n\n在使用timestamp的时候，我们发现有两个小问题，首先timestamp直接表达unixtime有一些歧义，其次timestamp仅支持到秒级别，而我们大部分数据需要毫秒级别。为了方便使用，我们对此部分做了一些修改进行适配。\n\n\n\n适配过程\n首先我们做的第一件事情就是消除歧义，在Apache DolphinScheduler中，Timestamp是表达时间的方式，从Wiki百科获得的关于Timestamp和Unix time时间表达的解释能看出，Timestamp通常是通过日期加时间来表示的，但是Unix time时间采用的是格林威治时间，从1970年1月1日零时零分零秒至今，并且不考虑微秒的时间表达，采用的是整数。\n明确了需求，接下来就需要了解如何实现了。我们通过分析代码发现，时间参数函数的实现是通过api方式层层调用，最终主要函数均通过在TimePlaceHolderUtils类中calculateTime的方法实现。该方法实现过程中，也会调用TaskConstants类中的表达时间函数名称的常量。于是我们对其中 TaskConstants类的一些常量进行了修改。又因为我们需要毫秒级别的函数，加入了一个 milli_unixtime函数，最后为了满足设备用户的需求，我们加入了一些更精度更高的函数，如微秒和纳秒的函数。\n\n\n\n\n\n\n在补数功能上，在使用Apache DolphinScheduler之后，我们只需要在手动执行任务的时候选中补数的功能，再填充上我们要调度的日期，就可以直接进行补充了，同时我们还可以填写并行度。这个功能对我们这来说非常实用的，在Apache DolphinScheduler 2.0版本以后，时间的配置和执行的时间有日绩差的问题也被解决，在使用上带来了很大的便利。\n未来规划\n在使用的过程中，我们发现通过Apache DolphinScheduler配置的任务在使用数据源方面，目前还不支持高可用的方案，这个需求在我们这里是比较强烈的，因此目前我们也正在做高可用的适配。\n其次，我们目前使用的是Apache DolphinScheduler的2.0版本，因为社区比较活跃，版本升级也比较快，即使是一个小版本的升级，也会带来一些很大的功能和设计上的一些变化。比如在新版本当中，告警功能已经插件化，也解决了一些补数日期换算的问题。这也驱动着我们团队升级到新的版本去体验一些新的功能。虽然目前Apache DolphinScheduler只是在我们自己的小团队内部使用，但我们也正在思考让整个公司普遍使用的可行性方案。\n\n尽管Apache DolphinScheduler非常完美地解决我们的大部分问题，并且大幅度提高我们的工作效率。但在各种复杂的情况下，我们还是会遇见一些小的Bug，我们未来也会在修复后提交给官方，当然我们自己在使用过程中也尝试了一些小Future，未来也会提交给官方共同讨论。\n",
    "title": "论语音社交视频直播平台与 Apache DolphinScheduler 的适配度有多高",
    "time": "2022-4-16"
  },
  {
    "name": "How_Does_Ziru_Build_A_Job_Scheduling_System_Popular_Among_Data_Analysts",
    "content": "数据分析师干了专业数仓工程师的活，自如是怎么做到的？\n\n\n\n数据分析师作为企业数据资产的缔造者之一，具有一定的维度与指标体系管理、血缘分析、ETL 调度平台等技能。能够灵活使用调度平台会为数据分析师带来很大的便利，然而对于编程技能水平参差不齐的数据分析师来说，一个操作简单，使用成本低的调度平台才能让他们如虎添翼，而不是增加额外的学习成本。\n与大多企业相比，自如大数据平台的独特之处在于，大量的数仓加工并非由专业的数仓工程师完成，而是由数据分析师所做。而自如的数据分析师之所以能够做到专业团队才能完成的复杂的数据处理、分析工作，与其调度系统迁移到 Apache DolphinScheduler 分不开。\n在不久前的 Apache DolphinScheduler&amp; Apache ShenYu(Incubating) Meetup 上，自如大数据研发经理 刘涛，为我们分享了受数据分析师们欢迎的调度系统是什么样的。\n\n\n\n刘涛自如大数据研发经理，负责自如大数据基础平台构建，建设一站式大数据开发平台。\n01 自如大数据平台现状\n\n\n\n自如大数据平台\n上图是自如大数据离线平台的简单图示，数据源包括 MySQL、Oracle 等业务库数据，以及各种日志数据，通过 Hive 离线 T 加 1 采集、另外使用Hive acid加上Flink实现了一个10分钟级别的业务库数据更新。\n数据加工是分析师关心的部分，这个过程可以配置调度、配置依赖和 SQL 开发。而在数据落地上，我们采用了 ClickHouse 的 OLAP 引擎，数据应用层使用网易有数提供报表平台。\n自如的大数据平台与业界大多数平台相差不大，但独特之处在于除了支持专业数仓开发工程师外，大量的数据分析师参与到了数仓加工之中。这就要求大数据平台要足够简化。\n02 分析师的期望\n\n\n\n由于数据分析师的编码水平参差不齐，有些分析师会写 SQL，而有些分析师根本不会写 SQL。即使是对于会写 SQL 的分析师，在面对任务依赖概念的理解上，也会觉得难度很大。\n因此，分析师群体对于调度的期望是要简单，上手成本低。\n03 Airflow的实现方式\n\n\n\n\n\n\n一开始，自如选用的是 Airflow，使用Airflow 可视化插件Airflow DAG createmanager plug-in来供分析师用，底层使用hivepartitionsensor，用数据依赖的方式配置调度，便于分析师理解和使用，这套解决方案，对于分析师来说体验尚可，但是面临几个较大的问题：\n\n数据依赖的底层实现导致的任务重跑非常复杂；\n任务量比较多后，调度性能较差，有些任务调起延迟较大；\n与一站式大数据开发平台集成二开成本比较高；\n原生不支持多租户。\n\n04 Apache DolphinScheduler改造与 Airflow任务迁移\n以上几个比较重要的挑战，促使我们重新进行调度选型。经过对比分析后，我们选择了 Apache DolphinScheduler。\n对于分析师来说，数据依赖是一个好理解的概念，但任务依赖就比较让人费解。\n比较理想的方案是对分析师展示的是数据依赖，底层实现是任务依赖，并且这数据依赖是自动生产的，不需要分析师手动输入依赖表。\n做到这一点，首先需要解决一个问题，如何根据一段 SQL，判断出这段 SQL 的输入输出表？\n\n\n\n\n\n\n由于是在 Hive 的环境中，所以需要看下 Hive  sql 的解析过程。\n如上图所示hive利用antlr 进行语法和语义解析，生成抽象语法树。举例，如下一段 sql 语句：\n\n\n\n解析成的语法树：\n\n\n\n遍历这棵抽象语法树就可以准确获得输入输出，我们发现并不需要从头来做，Hive 147 中就实现了这个功能。\nhttps://issues.apache.org/jira/browse/HIVE-147\n\n\n\n我们解析了输入输出之后，就可以把输入输出表和对应的 Apache DolphinScheduler调度任务关联起来，这样就完成了对分析师看到的是数据依赖，底层实现是任务依赖。当然这种实现就会让每个任务都很小，大部分任务都是只最终产出一张表，调度数量会比较多，但目前来看，没有带来性能问题。\n\n\n\n这之后就是面临的如何把Airflow中的任务平滑的迁移到 Apache DolphinScheduler 中，Airflow的任务都是一个个Python文件，Airflow 的调度器不停地扫描Pyhton文件所在文件目录，生成调度任务。核心实现类就是上图中的 DagFileProcessorManager，我们就可以参考这个类的实现来解析Python任务，生成Apache DolphinScheduler 任务定义需要的 Json 串，从而完成调度任务的迁移。\n最后是做个广告，我们是自如大数据基础平台，负责大数据的部署、 运维、 监控、优化、二开，并且在此之上构建一站式的大数据开发平台，欢迎加入我们。\n我的分享就到这里，感谢大家！\n05 特别感谢\n联合主办方\nApache ShenYu(Incubating)\n合作方\n示说网、开源中国、CSDN、稀土掘金、开源社、SeaTunnel 社区、思否 和 ALC 北京\n礼品赞助\nYY 直播\nApache ShenYu(Incubating)\n感谢主持人，低代码无代码平台 Treelab  张德通，以及活动志愿者 曹海洋 对本场活动的大力支持！\n",
    "title": "数据分析师干了专业数仓工程师的活，自如是怎么做到的？",
    "time": "2022-4-16"
  },
  {
    "name": "How_can_real_estate_companies_let_data_analysts_do_the_work_of_professional_data_warehouse_engineers",
    "content": "\n数据分析师作为企业数据资产的缔造者之一，具有一定的维度与指标体系管理、血缘分析、ETL 调度平台等技能。能够灵活使用调度平台会为数据分析师带来很大的便利，然而对于编程技能水平参差不齐的数据分析师来说，一个操作简单，使用成本低的调度平台才能让他们如虎添翼，而不是增加额外的学习成本。\n与大多企业相比，自如大数据平台的独特之处在于，大量的数仓加工并非由专业的数仓工程师完成，而是由数据分析师所做。而自如的数据分析师之所以能够做到专业团队才能完成的复杂的数据处理、分析工作，与其调度系统迁移到 Apache DolphinScheduler 分不开。\n在不久前的 Apache DolphinScheduler&amp; Apache ShenYu(Incubating) Meetup 上，自如大数据研发经理 刘涛，为我们分享了受数据分析师们欢迎的调度系统是什么样的。\n刘涛\n自如大数据研发经理，负责自如大数据基础平台构建，建设一站式大数据开发平台。\n\n01 自如大数据平台现状\n\n上图是自如大数据离线平台的简单图示，数据源包括 MySQL、Oracle 等业务库数据，以及各种日志数据，通过 Hive 离线 T 加 1 采集、另外使用Hive acid加上Flink实现了一个10分钟级别的业务库数据更新。\n数据加工是分析师关心的部分，这个过程可以配置调度、配置依赖和 SQL 开发。而在数据落地上，我们采用了 ClickHouse 的 OLAP 引擎，数据应用层使用网易有数提供报表平台。\n自如的大数据平台与业界大多数平台相差不大，但独特之处在于除了支持专业数仓开发工程师外，大量的数据分析师参与到了数仓加工之中。这就要求大数据平台要足够简化。\n02 分析师的期望\n\n由于数据分析师的编码水平参差不齐，有些分析师会写 SQL，而有些分析师根本不会写 SQL。即使是对于会写 SQL 的分析师，在面对任务依赖概念的理解上，也会觉得难度很大。\n因此，分析师群体对于调度的期望是要简单，上手成本低。\n03 Airflow的实现方式\n\n\n一开始，自如选用的是 Airflow，使用Airflow 可视化插件Airflow DAG createmanager plug-in来供分析师用，底层使用hivepartitionsensor，用数据依赖的方式配置调度，便于分析师理解和使用，这套解决方案，对于分析师来说体验尚可，但是面临几个较大的问题：\n\n\n数据依赖的底层实现导致的任务重跑非常复杂；\n\n\n任务量比较多后，调度性能较差，有些任务调起延迟较大；\n\n\n与一站式大数据开发平台集成二开成本比较高；\n\n\n原生不支持多租户。\n\n\n04 Apache DolphinScheduler改造与 Airflow任务迁移\n以上几个比较重要的挑战，促使我们重新进行调度选型。经过对比分析后，我们选择了 Apache DolphinScheduler。\n对于分析师来说，数据依赖是一个好理解的概念，但任务依赖就比较让人费解。\n比较理想的方案是对分析师展示的是数据依赖，底层实现是任务依赖，并且这数据依赖是自动生产的，不需要分析师手动输入依赖表。\n做到这一点，首先需要解决一个问题，如何根据一段 SQL，判断出这段 SQL 的输入输出表？\n\n\n由于是在 Hive 的环境中，所以需要看下 Hive sql 的解析过程。\n如上图所示hive利用antlr 进行语法和语义解析，生成抽象语法树。举例，如下一段 sql 语句：\n\n解析成的语法树：\n\n遍历这棵抽象语法树就可以准确获得输入输出，我们发现并不需要从头来做，Hive 147 中就实现了这个功能。\nhttps://issues.apache.org/jira/browse/HIVE-147\n\n我们解析了输入输出之后，就可以把输入输出表和对应的 Apache DolphinScheduler调度任务关联起来，这样就完成了对分析师看到的是数据依赖，底层实现是任务依赖。当然这种实现就会让每个任务都很小，大部分任务都是只最终产出一张表，调度数量会比较多，但目前来看，没有带来性能问题。\n\n这之后就是面临的如何把Airflow中的任务平滑的迁移到 Apache DolphinScheduler 中，Airflow的任务都是一个个Python文件，Airflow 的调度器不停地扫描Pyhton文件所在文件目录，生成调度任务。核心实现类就是上图中的 DagFileProcessorManager，我们就可以参考这个类的实现来解析Python任务，生成Apache DolphinScheduler 任务定义需要的 Json 串，从而完成调度任务的迁移。\n最后是做个广告，我们是自如大数据基础平台，负责大数据的部署、 运维、 监控、优化、二开，并且在此之上构建一站式的大数据开发平台，欢迎加入我们。\n我的分享就到这里，感谢大家！\n",
    "title": "数据分析师干了专业数仓工程师的活，自如是怎么做到的？",
    "time": "2023-9-11"
  },
  {
    "name": "How_does_Apache_DolphinScheduler_completely_set_up_GMT_+8_zone",
    "content": "\n\ntitle: Apache DolphinScheduler如何完全设置东八区？\nkeywords: Apache DolphinScheduler, 时区, 东八区\ndescription: 为了兼容全世界不同时区，Apache DolphinScheduler 使用的是 UTC 0 时区，包括保存到数据库表中的数据时区，以及展示到页面上的时区。\n默认情况\n为了兼容全世界不同时区，Apache DolphinScheduler 使用的是 UTC 0 时区，包括保存到数据库表中的数据时区，以及展示到页面上的时区。\n如果我们想在页面上看到东八区时间，则需要在页面上手动选择上海时区，如下图所示：\n\n这样选择之后，虽然页面上显示的时间是对的，但是具体单个任务中的日志时间依然是 0 时区，而且底层表中所有数据的时间也是 0 时区。\n如果想要页面上显示的时间是东八区，而且任务日志中的时区也是东八区，并且表中保存的数据时间也是东八区，则需要修改如下几个地方的设置。（修改有风险。请备份好您的相关文件。）\n配置修改\n首先切换到你解压的安装包根目录，然后修改下面说明的文件。\n1.环境变量文件\n文件位置：bin/env/dolphinscheduler_env.sh\n文件修改内容：\nexport SPRING_DATASOURCE_URL=&quot;jdbc:mysql://node01:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai&quot;\nexport SPRING_JACKSON_TIME_ZONE=${SPRING_JACKSON_TIME_ZONE:-GMT+8}\n\n第一个环境配置的连接 mysql 的 url，最后面添加时区设置，同时也要注意，自己的 mysql 数据库使用的也是东八区。\n第二个环境配置的是启动 spring 容器时使用的时区设置，也设置为东八区。该环境变量会被海豚所有角色在启动 JVM 时设置到 JAVA OPTS 中。\n2.各角色 spring 配置文件\n文件位置：${角色}/conf/application.yaml\n角色包括：alert-server、api-server、master-server、worker-server。\n文件修改内容：\nspring:\n  banner:\n    charset: UTF-8\n  jackson:\n    time-zone: GMT+8\n    date-format: &quot;yyyy-MM-dd HH:mm:ss&quot;\n\n\n1. 要修改的是time-zone的内容，将其改为GMT+8。\n\n修改之后，直接执行bin/install.sh文件，安装海豚调度器即可。\n最后要记得，Apache DolphinScheduler 页面右上角依然要选择上海时区。\n\n",
    "title": "Apache DolphinScheduler如何完全设置GMT+8区？",
    "time": "2023-11-9"
  },
  {
    "name": "How_does_Apache_DolphinScheduler_solve_the_pain_points_of_Chuanzhi_Education_data_warehouse_scheduling",
    "content": "精彩回顾\n在  Apache DolphinScheduler&amp;Hudi 联合 Meetup 上，传智教育的资深研究员 孔帅，为我们带来题为《Apache DolphinScheduler在传智教育的实践》的分享。\n演讲主要分为 3 部分：\n1. 传智教育数仓架构的演变\n2. DolphinScheduler在传智的实践与思考\n3. 展望未来\n1、传智教育数仓架构的演变\n首先我们来看一下传智教育数仓架构的演变。\n01 传智教育数仓 1.0 架构\n在传智教育数仓 1.0 的架构体系中，我们首先聊一下我们的主体业务流程。\n\n上图为传智教育的核心分析流程。首先客户会访问官方网站，客户在访问网站期间可以咨询在线客服，了解我们的课程。如果客户感觉这些课程有帮助，并产生学习意向，会给我们留下一些线索，比如电话、微信、QQ 号等。\n我们拿到这些之后，线下客服人员会根据这些线索和客户进行进一步沟通联系。如果这些线索联系不到，则把这些线索设置为无效线索；如果能够联系上，客服会和客户进行电话沟通或微信交流。\n当客户报名后，就会开始进行学习对应课程。\n在这一系列操作过程当中，所有的数据记录都会通过后台存储在服务器上，而我们的大数据系统会在每天凌晨用 T+1 的方式进行数据采集和分析计算。分析计算的主要有访问数据、咨询数据、意向数据、线索数据，还有一些报名数据、考勤数据等。将这些数据分析计算结果以后，我们会把这些结果同步到应用系统中，比如决策系统、报表系统等，并在应用系统当中的 BI 进行展示。\n基于这套业务流程，我们设计了教育数仓 1.0 架构。在 1.0 版本的架构中，我们使用的原始数据主要有内部的一些电咨系统、线下的面授系统，以及在线教育系统、客户管理系统等，主要使用的是这些系统里的一些结构化数据。\n对于这些结构化数据来说，Sqoop 能够很好地进行数据的采集，所以早期我们主要使用 Sqoop 进行 ETL，数据存储主要使用 HDFS，并基于 MapReduce 引擎的 Hive 来进行分析。\n我们在数仓中采取了一些分层，主要有 6 层，第一层是我们的 ODS 原始数据层，保留最原始的数据，不会做任何的变更，这一层也会保留我们最完整的数据。为了保证数据的安全性，我们 ODS 层采用的都是 外部表。\n接下来是 DWD 层，主要进行清洗转换操作，清洗主要是把无效垃圾过滤掉，转换则是进行枚举值、字段等转换，以及数据脱敏工作。\n经过 DWD 层的信息转换之后，我们能够提供一个比较干净的应用数据。再接下来就是我们的 DWM 层，这一层主要是用来生成我们中间可以复用的一些组件，主要的操作有两种，一种是生成明细宽表，用来为后续的查询提高性能，减少服务器的消耗；另一种是做一些轻度汇总的工作。基于这些天数据和小时数据，我们在后面的  DWS 层进行进一步的数据汇总，这是一个数据集市，会基于中间层的数据进行数据汇总，把时间维度计算出来，比如说年维度、月度、季度、周等，并把总累计值数据计算出来。\n数据集市会把最终的数据导出到 RPT 应用层，主要保存的是个性化业务数据。\n另外我们还有一个 DIM 维度层，主要是存放一些标准维度数据，最后将这些数据导出给应用系统。应用系统一般使用都MySQL 数据库，导出的数据通过 Spring boot 来对外提供数据接口，用 BI 展现。\n因为我们之前采用的是离线数仓，采用的也是 T+1 的计算方案。所以初期的数据能够满足我们计算需求。\n初期的任务脚本，我们选择的也是当时比较流行的 Shell 脚本，而且 1.0 版本时我们的调度任务并不是特别的多，所以当时 Oozie 的功能和性能都能够满足我们的调度需求。\n以下为离线数仓的若干核心主题设计思路：\n\n\n\n\n基本流程是将不同主题表格抽取到 ODS 层，形成明细宽表，既有宽表的相关指标和维度，将宽表合并成一张大的数据集市，在 DWM 层进行汇总和计算。\n02 2.0 架构\n随着非结构化数据和业务场景越来越多，原来 的 ETL 工具 Sqoop 已经无法满足我们的 ETL 需求了，于是我们对 1.0 架构进行了一些调整，主要包括：\n\n\n除 Sqoop 之外，增加 Flume 进行日志文件采集，使用 Python 自研了 ETL 工具；\n\n\n随着数据量逐步增加，Hive 已经不能满足计算性能的需求。通过对各种分析引擎的对比，选择了 Presto；\n\n\n任务脚本也在逐步地体系化，Shell 脚本的可维护性和拓展性不如 Python，所以 把脚本替换成了 Python。\n\n\n\n另外，系统分层也进行了一些变更，我们在 2.0 版本中去掉了维度层，把维度表都直接放在了 DWD 层中；原来的 DWS 层，数据集市层被替换为了 DM 层，也是数据集市层，作用是一样的。另外，我们新加了一个轻度汇总层和明细宽表层，也就是 DWB 层和 DWS 层。DWB 层的主要作用是把 DWD 里的表，在 DWB 层进行宽表化，将雪花模型转换为星型模型。生成了明细宽表以后，在后续计算中能够提升性能，降低服务器的损耗。DWS 层主要进行轻度汇总，比如按小时、按天先进行轻度的汇总，再在 DM 层中按照年、季度等进行计算。\n剩下的 RPT 和 Presto 是整个公司提供所有分层数据的一个入口。\n最后，我们还变更了调度工具。因为之前的调度工具 Oozie 配置比较复杂，而且功能相对比较少，可能还会出现服务阻塞的情况，对于升级兼容问题权限问题也都不是特别好。所以，我们把 Oozie 替换DolphinScheduler。\n03 核心技术点\n首先是 Sqoop。在数仓 1.0 版本中，我们的原始数据主要是结构化数据，Sqoop 正好就是一款专业的结构化数据传送的 ETL 工具，但随着数据和业务场景增多，ETL 场景也越来越复杂，Sqoop 已经满足不了我们的需求了。所以在 2.0 版本中，我们把 Sqoop 替换成了DolphinScheduler 体系。在 DolphinScheduler 和 Flume 基础之上，我们通过自研的 Python ETL 工具，就可以做到更加灵活地支持各种复杂的业务场景。\nHive 成本低、稳定性好，生态兼容性好，但其缺点也很明显：慢。随着数据量大幅增加，Hive 已经不能满足我们的计算性能需求了。经过各种对比，我们决定在 2.0 版本中选择 Presto 作为新的引擎。\n之前我们还参考了 SparkSql、Impala、HAWQ 和 ClickHouse 等，结果如下：\n\n\nSparkSql 虽然比 Hive 快，但和 Presto 比单表&amp;多表的查询性能都不突出。\n\n\nImpala 多表查询性能优异，但单表查询不如 Presto，不支持 OrcFile、不支持 update、delete 和 grouping sets 等语法，不支持 Date 数据类型。\n\n\nHAWQ、Greenplum 比较中庸，用起来比较复杂，性能也不突出。\n\n\nClickHouse 的单表性能较好，但是多表性能不突出，兼容性也不如Presto。\n\n\nPresto 的优势：\n\n\nPresto 和 Hive 都是 Facebook 公司开源的，两者兼容好，Trino 是从 Presto 分支出来的，两者非常相似。\n\n\nPresto/Trino 单表&amp;多表的查询性能优异，数据量支持 EB 级的数仓和数据湖。\n\n\nPresto/Trino 兼容 Hive，还支持 mysql、oracle 等关系型数据库，kafka、redis、MongoDB 等非关系型数据库。\n\n\nPresto/Trino 能够进行异构数据源的跨库读写操作。\n\n\n接下来就是 shell 脚本。1,0 版本汇总我们主要使用 Shell，因为当时任务较少，也并不是特别的复杂，但是随着脚本的体系化，我们需要脚本的可维护性和可扩展性，这些缺点就越来越突出，Shell是用来进行系统管理的脚本，功能有限，性能低开销大。Python 无论是性能方面，还是一致性、扩展性，都比需要脚本要好，现在已经成为了全球编程语言排行榜的第一名。\n所以在 2.0 的数仓架构体系中，我们把 shell 脚本变更为了 Python 脚本。\n最后就是调度工具，传智教育最初使用的调度软件是Oozie，由于Oozie功能的限制，在不断迭代的需求中，我们重新调研了各种调度工具，最终选择了国产开源的Apache DolphinScheduler。\n\n\n上面两张图是我们使用 Apache DolphinScheduler 的一些功能，比如文件管理功能和数据源。文件管理功能非常方便，可以让我们很方便地引用文件，这些文件之间也可以互相引用。\n数据源创建之后也可以不断复用，对我们提升效率来说很有帮助。\n由于 Oozie 历史的原因，目前 Sqoop、HiveSQL、Presto 等任务在DolphinScheduler 中使用的主要还是 Shell 组件，但受益于 DolphinScheduler 的可视化界面、资源管理、多租户管理、权限管理、告警组管理、多项目管理等功能，调度效率已有大幅提高。\n\n这个就是我们这个数仓架构的技术演变，接下来介绍一下传智教育在技术演变的过程中调度工具使用的痛点，以及我们如何解决这些问题。\n2、DolphinScheduler 在传智教育的实践\n01 调度的痛点\n\nXML 配置复杂\n\n传智教育以前采用的工作流调度软件是 Oozie。Oozie 是一个工作流引擎，它的一个特点是默认采用 HPDL语言（XML）定义流程，可视化支持依赖于第三方工具软件（比如HUE），自带的可视化界面功能较弱，安装也标胶复杂。\nOozie  工作流的核心组成部分有两个：job.properites 和 workflow.xml。前者主要保存一些常用的参数变量，后者是核心文件，具体的工作流都是在 workflow 里面去定义的。\n\nworkflow.xml\n\njob.properties\n上图展示了一个非常简单的工作流，打印 Oozie，再返回一个错误信息进行输出。但就是这么简单的一个工作流，我们可以看到这个过程需要配置大量的 XML 标签，非常麻烦，工作效率非常低。随着我们的调度工作增加，业务场景复杂化，这个调度软件对我们的生产效率已经产生了非常大的影响。\n\n功能组件少\n\n另外就是功能组建比较少，Oozie 对流行技术的反应有些迟钝，比如PySpark等当下流行的分析计算引擎。虽然官方已经宣称支持 PySpark 任务，但在实际应用中 Oozie 的 PySpark 任务却支持较差、问题不断。\n此外，多租户管理、告警组管理、多环境管理、Worker 分组管理、项目管理、资源管理等功能，Oozie 也都不具备。\n\n阻塞死锁\n\n\nOozie 在执行过程中，每个任务都会启动一个 oozie-launcher 加载器，oozie-launche 会占用很多内存；\nOozie launcher 的生命周期是数据任务开始之前到结束，期间资源不会释放，如果此时数据任务得不到充足的资源就会一直等待，有充足资源时才会执行数据任务；\n如果同时提交了多个 Oozie 任务，或是 Oozie 有多个并行子任务，会导致内存不够，而 Oozie launcher 在得不到充足的资源时就会一直等待资源，导致资源和任务互相等待从而造成死锁现象。\n\n权限控制 &amp; 升级兼容\n\n**权限：**Oozie 基本没有权限控制，也没有多租户功能；\n**兼容：**Oozie 依赖于 Hadoop 集群版本，如果更新最新版，容易出现与现有集群不兼容的问题。\n3、DolphinScheduler 解决痛点\n以上是 1.0 版本中我们的调度所遇到的一些痛点。那么我们是如何解决这些痛点的呢？\nApache DolphinScheduler 是一个分布式易扩展的可视化工作流任务调度平台。\n相对于 Oozie 的复杂 xml 配置流程，DolphinScheduler 所有的流、定时操作都是可视化的，通过拖拽任务来绘制 DAG，并可进行实时监控。\n同时 DolphinScheduler 支持一键部署，无需复杂的安装过程，提升工作效率。\n\n功能丰富\n\nDolphinScheduler 的一个好处是功能比较丰富，和 Oozie 相比，相比， DolphinScheduler 紧跟流行技术。\n对 PySpark 等当下流程的分析计算引擎，DolphinScheduler 做到了快速升级进行兼容。并且新组件使用起来便捷高效，对于工作效率提升很大。\n同时，DolphinScheduler 还在不断升级完善各种功能，比如多租户管理、告警组管理、多环境管理、Worker 分组管理、项目管理、资源管理等功能，功能丰富，升级也快。\n\n高可靠性\n\n\n另一个特点是高可靠性，与 Oozie 的阻塞死锁现象对比，DolphinScheduler 采用任务缓冲队列机制来避免过载；单个机器上可调度的任务数量可以灵活配置，当任务过多时会缓存在任务队列中，不会导致机器卡死，就像红绿灯一样。\n同时，DolphinScheduler 支持去中心化的多 Master 和多 Worker 服务对等架构，可以避免单 Master 压力过大。\n\n权限控制 &amp; 升级兼容\n\n权限：Oozie 基本没有权限控制；DolphinScheduler 可以通过对用户进行资源、项目、数据源的访问授权，不同用户间互不影响。\n兼容：Oozie 容易出现与现有集群不兼容的问题；DolphinScheduler 升级不会影响之前集群的设置，升级方式操作简单。\n\n\n以上为 DolphinScheduler 工作界面截图，帮助我们解决了很多调度痛点。\n4、展望未来\n最后是我们对未来的展望。\n我们目前虽然还未使用 Apache DolphinScheduler 的所有组件，但它已经帮助我们解决了以前大多数的痛点，大幅提高了工作效率，后续我们准备向更多的项目组推广使用。\n在目前的使用中，DolphinScheduler 对 PySpark 任务支持良好，但Presto 插件使用时还有些适配问题，我们也在做 Presto 功能的适配，后续计划将数据源类型优化为支持动态热插拔的功能，让任意类型的数据源都可以被随时使用。\n如上所述，由于 DolphinScheduler 社区活跃、版本更新速度较快，在使用过程中会遇到一些适配问题，同时我们也有很多新的 idea，我们会积极地参与到社区中，不断优化完善，助力 Apache DolphinScheduler 更上一层楼！\n",
    "title": "Apache DolphinScheduler 如何解决传智教育数仓调度痛点？",
    "time": "2023-9-5"
  },
  {
    "name": "How_does_Apache_Dolphinscheduler_solve_the_infinite_loop_of_Master_service_without_restarting",
    "content": "个人建议\nApache Dolphinscheduler作为一个开源的调度平台，目前已经更新到了3.X版本，4.0版本也已经呼之欲出。3.0版本作为尝鲜版本，新添加了许多的功能，同时也存在非常多的隐患，本人使用3.0版本作为生产调度也踩了很多坑，到现在依然存在很多难以解决的问题，所以建议小伙伴们尽量使用2.x版本，相对稳定一些。\n\n近期在跟社区的沟通中，**最新3.2.0版本，该问题已经得到解决！**感兴趣可以了解最新版本。\n\n\n下面主要记录的是一个3.0比较难搞的问题，相信不少使用过3.0的用户都遇到过Master服务中存在一些工作流一直不停的死循环的问题，本人到现在也没找到触发的原因，但是通过与同事的摸索，暂时找到了一个可以借助Arthas解决死循环的方法。\n死循环的影响\nCPU飙高：每个工作流的运行在Master中都是一个线程，当这个线程一直没有结束时，是会占用CPU资源的，当服务中存在大量的线程死循环时，可想而知，服务器的资源压力有多大。\n磁盘打满：循环的线程内存在日志打印，当大量的线程无时无刻在打印日志时，日志文件会迅速堆积，磁盘的大小是固定的，当磁盘使用率超过一定的阀值时，其他的程序也会因为磁盘可用空间不足而受影响。有些人设置了Logback等日志框架配置，限定了日志文件的总大小，但是这样也会引发日志快速覆盖问题，无法找到可用的日志。\n数据库压力：每个循环里面都有相关的数据库查询操作，大量的查询会造成数据库压力短期内迅速增大，如果数据库性能不能很好的话，可能数据库就会先挂了。\n解决思路\n首先我们需要判断循环类型，是内存性死循环还是数据库性死循环，因为内存性死循环，我们大部分可以通过修改数据库来解决，但是内存性死循环，我们就必须借助某些工具，去内存中修改。\n通过日志查看，循环代码，并找到循环数据的来源。\n\n通过查看日志，发现每次出现循环时都会出现“Start workflow error”、&quot;Failed to submit the workflow instance&quot;报错，当工作流出现问题时，程序会将工作流事件重新放回到执行队列中，等待下次执行，这样就变成了无限循环报错。\n通过“Failed to submit the workflow instance”，我们在项目里全局搜索，查看报错的逻辑是什么，是如何将报错的工作流处理事件重新添加到处理队列中的。\n\n从上面被红框圈出来的关键处代理，我们可以梳理出一个基本的master服务处理工作流的一个事件流程，工作流的线程类（WorkflowExecuteRunnable）被放到缓存中，缓存的key是工作流实例的ID,同时每一个工作流都有对应的事件，事件中存储工作流实例的ID，每次执行事件时都会从缓存中获取线程类，当线程类执行失败时便重新创建一个事件加入事件队列中执行，依次往复，除非缓存中的数据被清除了，才会结束循环。具体流程如下图：\n\n通过清理内存中的工作流线程即可解决循环问题。\n三、实际操作\n进入Master服务的日志目录\n通过日志查找所有在循环中的工作流实例的id\ngrep WorkflowInstance  dolphinscheduler-master.log|grep &quot;Start workflow error&quot; |awk -F 'WorkflowInstance-' '{print $2}'| awk -F']' '{print $1}' |sort |uniq\n\n安装Arthas，启动Arthas,选择API-Server服务,先使用API服务物理删除循环的工作流实例相关的数据库数据，防止下次重启后依然循环。\n在Arthas中调用下面的方法\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(&quot;processServiceImpl&quot;).deleteWorkProcessInstanceById(&quot;工作流实例id&quot;)'\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(&quot;processServiceImpl&quot;).deleteAllSubWorkProcessByParentId(&quot;工作流实例id&quot;)'\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(&quot;processServiceImpl&quot;).deleteWorkProcessMapByParentId(&quot;工作流实例id&quot;)'\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicati\nonContext@applicationContext.getBean(&quot;processServiceImpl&quot;).deleteWorkTaskInstanceByProcessInstanceId(&quot;工作流实例id&quot;)'\n\n\nognl表达式参考这个链接 https://arthas.aliyun.com/doc/ognl.html\n通过Arthas进入master-server 调用ognl清除工作流缓存\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(&quot;processInstanceExecCacheManagerImpl&quot;).removeByProcessInstanceId(&quot;工作流实例id&quot;)'\n\n",
    "title": "Apache Dolphinscheduler如何不重启解决Master服务死循环",
    "time": "2023-10-31"
  },
  {
    "name": "How_does_the_national_dairy_giant_Yili_open_up_a_path_for_enterprise_digital_transformation_based_on_Apache_DolphinScheduler",
    "content": "\n精彩回顾\n近期，国民乳业巨头伊利集团在社区线上 Meetup 上带来了《伊利集团基于 DolphinScheduler 的探索与实践》。\n伊利集团数字化中心统筹推进全集团的数字化转型工作，姜涛及其团队在数字化中心负责大数据技术能力的建设以及相关的业务支持工作。在数据驱动转型的过程中，会存在海量的数据集成任务，整体集成架构的合理性、调度的稳定性会严重影响数据流转的效率，也会制约数据的价值挖掘，伊利基于 Apache DolphinScheduler 构建了统一的调度服务与数据集成体系，加速数据流转，助力企业数字化转型。\n作者简介\n姜涛\n伊利集团 数字化中心 数据技术经理\n演讲大纲：\n背景与应用情况介绍\n实践&amp;探索\n未来的规划\n关于开源的思考\n背景与应用情况介绍\n伊利介绍\n大家对伊利并不陌生，每年有13亿的国民消费者品尝过伊利的产品，平均每天有1亿包商品被消费者喝掉。伊利的业务起点可以追溯到一棵草的培育、一头牛的养殖、一杯奶的生产，最后经过复杂的供应链体系送到消费者手里，业务横跨一、二、三产业。\n应用背景介绍\n庞大的业务体量背后是复杂的应用矩阵，这对我们技术架构提出了很大的挑战。\n应用的多云分布现状，使跨云的数据搬迁与多云统一调度成为刚需\n\n如今，伊利在同云服务商的合作过程中，不仅会考虑云服务商所提供的 Iaas、Paas 能力，还会考虑其背后的生态，如私域生态、电商、物流等等，所以我们的应用天然分散在多云之上，应用产生的数据也分散在多云之上，当下集中式的数据架构需要大量的数据物理搬迁。我们每日需要迁移 8000 张表，涉及 80 多个系统，在这种情况下，稳定、可扩展的多云数据集成能力就十分重要。\n统一技术架构来对抗熵增，降本增效\n第一个挑战是海量的搬迁需求对数据集成和调度能力易用性、稳定性、扩展性的挑战。而第二个挑战来源于大量的应用落地所带来的熵增效应，在技术资源有限的情况下，我们很难去有效控制我们的成本，并提高我们的效率，尤其我们在面对数据集成与调度的这个场景，有以下几个核心问题：\n**同类工具产品重复建设：**因为缺少统一规划，导致同类功能产品存在重复建设。\n**技术选型杂：**AirFlow、Azkaban、Oozie、 自研调度等。\n**建设及衍生成本高：**多技术栈的人员资源储备、运维、使用培训等成本。\n**扩展性问题：**本地化个性需求的扩展性支持。}\n在引入 Apache DolphinScheduler 之前，我们在调度和数据集成的场景中，整体的技术架构是不统一的。为了满足我们内部的刚性需求，同时从扩展性、稳定性等维度考量，我们计划建设统一的调度&amp;数据集成服务体系。在调研了大量调度产品后，我们决定基于 Apache DolphinScheduler 作为我们的核心引擎，并在此之上进行本地化的改造，去满足我们的需求。\n伊利大数据调度服务平台定位&amp;应用现状\n在技术资源有限的情况下，去发起一个工具产品的研发并不容易，我们要思考清楚：谁是我们的核心用户？这个工具产品的核心定位是什么？支撑的场景有哪些？\n经过充分的调研，我们明确了这个工具产品的核心定位，他不是一个简单的工作流调度平台，是满足伊利多云业务场景的一体化数据开发平台，同时，以一个轻量化的工具，面向数据、应用开发人员，支持内外部的数据集成、应用侧分布式调度需求、以及数据任务的可视化编排和调度。\n从伊利目前的应用情况来看，日调度任务数达到了 1.3 万个，集群规模 15 个节点，每日搬迁表8000+。\n\n从上面的架构图可以看到，我们在多云大数据基础设施之上，构建了一个统一的数据集成、开发、调度和运维的数据平台，通过这个平台，去屏蔽多云大数据平台的差异性，为用户提供统一的开发体验。\n伊利大数据调度服务平台系统总览\n\n我们对 2.0.2 版本的 DolphinScheduler 的整个模型做了重新抽象，项目作为最顶层的模型，同时抽象了资源、角色、工具三个概念，并绑定到项目里。其中工具集合是最重要的，所有数据开发过程中会用到的工具都会在这里去扩展，用户在平台内通过授权项目，即可完成数据集成、开发、运维、资产管理等工作。同时，我们将 Apache DolphinScheduler 的租户与大数据平台租户做了融合打通，通过大数据平台侧 Ranger+Ldap+Kerberos 的租户管理体系完成资源配额、权限的细粒度管理。\n实践&amp;探索\n我们基于 Apache DolphinScheduler 2.0.2 版本做了三次迭代，迭代了很多的功能，但是有一个主线逻辑贯穿我们的产品设计和研发过程，那就是以满足用户的核心需求去建设新功能，以提升用户体验去优化旧功能，让产品上手容易、低门槛、平民化、动线合理。\n\n面向场景的数据集成\n何为面向场景的数据集成？我们的产品完全开放给用户使用，开发人员在做数据集成时，有非常明确的上下文，比如把 Mysql 的数据同步到 Hive，把 ES 的数据同步到 StarRocks 等。所以，我们提供了大量的集成组件，可以基于配置化的方式，完成集成的工作。全过程 0 代码，提升了数据集成开发的效率。\n在未规划数据集成的核心功能前，我们面临着以下几个痛点：\n\n\n多种数据集成组件或技术让用户难以选择\n\n\n技术架构不统一导致维护成本增加\n\n\n数据集成任务配置繁琐\n\n\n二次开发扩展性较差\n\n\n针对这四个痛点，我们分别给出了对应的策略：\n\n\n只给用户一个最好的选择：多种选择往往无法选择，那干脆只提供一个最好的\n\n\n基于 DataX 统一数据集成架构，优化资源问题，降低运维成本\n\n\n面向业务应用时，通过模板生成器生成任务模板 ，简化操作\n\n\n合理的代码设计\n\n\n轻量化的资产管理\n\n这个功能所对应的核心诉求是这样的，应用基于原生大数据能力去构建轻量化数仓时，需要去清晰的洞察技术元数据信息，包括库、表、字段、基础属性信息，同时，需要对表打一些业务标签信息，包括主题域、主题等，方便分析人员使用数据，基于这样的诉求我们在平台上扩展了这个轻量化的资产管理功能。\n正逆向建表是核心功能，正向建表是指支持用户在平台上创建表模型，并将表模型物化到大数据平台上。而逆向建表用的更多，很多时候用户还是以传统的开发方法， 通过ETL服务器上写脚本，提交到大数据平台上做计算，这个时候表已经提前存在了，而不是通过可视化的方式创建的，那逆向建表就是自动扫描元数据，并注册到资产平台上。\n资产检索是面向开发和业务分析提供了资产检索的功能，可以按照主题域、标签、字段名等进行轻量化检索，展示资产卡片和上下游血缘信息。\n多云任务调度\n多云任务调度是为了解决跨云平台的一体化调度问题，如前边所讲，伊利的应用多云分布，而这个功能可以基于一张画布完成跨云任务编排，我们基于 Apache DolphinScheduler 做了一些扩展改造，灵活运用环境与 worker 组的绑定关系，同时大数据基础设施侧解决 Kerberos 跨域、多集群 Keytab 互信的问题。\n\n如示例所示，我们可以在不同的云上分别触发数据集成的任务，包括数据处理任务。处理完成之后，在另外一朵云上触发数据计算和面向业务应用的计划编排，最后提供数据服务，通过这种方式实现多云调度。\n监控看板优化\n我们在原有 Apache DolphinScheduler 看板的基础上，做了优化和升级，面向运维侧，构建了运维工作所关注的核心指标，并将指标做了可视化展示。\n整个监控看板分为任务监控和集群服务监控。\n对于任务监控，我们设计的关键指标包括项目数、工作流（上线/未上线）、应调度任务数、已成功调度任务数、分小时运行情况概览、运行失败统计 TOP5、运行时长统计 TOP5 等。\n\n对于服务监控，关键指标包括 CPU、内存、平均负载、各云环境 worker 负载，分小时 worker 运行情况概览等。\n\n体验升级优化\n接下来是体验上的升级优化，比如用户定时配置步骤优化，整体布局优化等。核心的出发点还是优化用户的使用体验，降低使用门槛。\n\n未来规划\n最后是我们团队关于 DolphinScheduler 的未来规划。\n长期规划\n从长远来看，不管是社区，还是我们公司内部，都需要对于 DolphinScheduler 有一个清晰的定位，即它不是一个简单的工作流调度平台，而是一个一站式的智能数据开发平台，这个数据开发平台至少具备三个重要的特点：\n\n\n\n多云统一：可以与去中心化的新型数据架构完美契合，这是一个非常重要的方向；\n\n\n低代码：从用户角度来说，可以支持全链路一站式可视化建模、开发、分析体验；\n\n\n智能化的管理：现在的数据架构基于降低数据管理复杂度为出发点，基于元数据驱动治理，实现数据质量管理智能化，也会是一个很大的命题。\n\n\n短中期规划\n短期来看，我们即将着手三点：\n\n\n云原生的结合：DolphinScheduler 社区已提供基于 k8S 部署服务，所以我们的 master、worker 等服将务基于容器化进行改造；\n\n\n引入测试、上线流程：在项目内添加开发者、审核者等角色，同时对上线动作进行管控，上线后元数据自动同步；\n\n\n质量模块：DolphinScheduler 3.0 已经加入了数据质量模块，我们将整合这一部分，并进行本地化拓展，包括对数据质量预定义、稽核，引入数据血缘等\n\n\n开源的思考\n\n伊利引入 DolphinScheduler，与开源产品共同成长。在这个过程中，我们有了一些关于协同的思考，企业内部与开源之间要构建良好的协同方式非常重要。\n我们内部会指派专人跟进开源社区的动态，研发团队会主动发现 Bug 并提交给社区，并修复后提交 PR。\n同时，我们也会积极地与社区互动，通过周、月会查看社区动向。对于技术研发资源有限的企业，应该去找一个契合企业内部实际使用情况的开源产品，要真正理解并把开源精神落到实处。开源的力量非常强大，可以解决很多内部根本无法解决的问题，也会发现内部不会发现的问题。所以，企业内部需要把参与开源的流程落到实处，与社区共同成长。\n我的分享就到这里，感谢！\n",
    "title": "国民乳业巨头伊利如何基于 DolphinScheduler 开辟企业数字化转型“蹊径”？",
    "time": "2023-10-11"
  },
  {
    "name": "How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications",
    "content": "在Apache DolphinScheduler的2.0.1版本 加入了插件化架构改进，将任务、告警组件、数据源、资源存储、注册中心等都将被设计为扩展点，以此来提高 Apache DolphinScheduler 本身的灵活性和友好性。在企业级应用中根据不同公司的告警需求可能各有不同，针对性的告警插件开发可以很好的解决这一痛点。\n当前版本：3.1.2\n告警插件开发\n先来看下alert目录的结构\n\n\ndolphinscheduler-alert-api\n该模块是 ALERT SPI 的核心模块，该模块定义了告警插件扩展的接口以及一些基础代码，其中 AlertChannel 和 AlertChannelFactory 是告警插件开发需要实现的接口类\ndolphinscheduler-alert-plugins\n该模块包含了官方提供的告警插件，目前我们已经支持数十种插件，如 Email、DingTalk、Script等\ndolphinscheduler-alert-server\n告警服务模块,主要功能包括注册告警插件,Netty告警消息发送等\n\n本文以官方的http告警插件为例讲解如何进行插件开发\n\n首先明确需求,http告警插件需要通过http发送请求,发送请求首先需要确定哪些参数.在 HttpAlertConstants 可以看到有定义一些相关参数\n\npackage org.apache.dolphinscheduler.plugin.alert.http;\npublic final class HttpAlertConstants {\n    public static final String URL = &quot;$t('url')&quot;;\n\n    public static final String NAME_URL = &quot;url&quot;;\n\n    public static final String HEADER_PARAMS = &quot;$t('headerParams')&quot;;\n\n    public static final String NAME_HEADER_PARAMS = &quot;headerParams&quot;;\n\n...........................省略多余代码\n\n    private HttpAlertConstants() {\n        throw new UnsupportedOperationException(&quot;This is a utility class and cannot be instantiated&quot;);\n    }\n}\n\n\n对应此处告警实例需要填写的参数\n\n\n其中 $t('url') 样式的参数可以通过编辑\n\ndolphinscheduler-ui/src/locales/zh_CN/security.ts\n\n添加对应的参数,前端收到后会自动替换,同样的英文字典也需要替换,不然切换英文时会报错\n\n在HttpAlertChannelFactory需要实现AlertChannelFactory并实现它的方法name,params和create。其中InputParam.newBuilder的第一个参数是显示的值,第二个参数是参数名，这里用我们前面在MailParamsConstants写好的常量。所有参数写好后添加到paramsList后返回\n\n@AutoService(AlertChannelFactory.class)\npublic final class HttpAlertChannelFactory implements AlertChannelFactory {\n    @Override\n    public String name() {\n        return &quot;Http&quot;;\n    }\n    @Override\n    public List&lt;PluginParams&gt; params() {\n        InputParam url = InputParam.newBuilder(HttpAlertConstants.NAME_URL, HttpAlertConstants.URL)\n                                   .setPlaceholder(&quot;input request URL&quot;)\n                                   .addValidate(Validate.newBuilder()\n                                                        .setRequired(true)\n                                                        .build())\n                                   .build();\n        InputParam headerParams = InputParam.newBuilder(HttpAlertConstants.NAME_HEADER_PARAMS, HttpAlertConstants.HEADER_PARAMS)\n                                            .setPlaceholder(&quot;input request headers as JSON format &quot;)\n                                            .addValidate(Validate.newBuilder()\n                                                                 .setRequired(true)\n                                                                 .build())\n                                            .build();\n        InputParam bodyParams = InputParam.newBuilder(HttpAlertConstants.NAME_BODY_PARAMS, HttpAlertConstants.BODY_PARAMS)\n                                          .setPlaceholder(&quot;input request body as JSON format &quot;)\n                                          .addValidate(Validate.newBuilder()\n                                                               .setRequired(false)\n                                                               .build())\n                                          .build();\n...........................省略多余代码\n        return Arrays.asList(url, requestType, headerParams, bodyParams, contentField);\n    }\n    @Override\n    public AlertChannel create() {\n        return new HttpAlertChannel();\n    }\n}\n\n\n在HttpAlertChannel需要实现AlertChannel并实现process方法,其中alertInfo.getAlertData().getAlertParams()可以拿到在创建告警实例时填写的参数,在此处编写相关代码发送请求后,需要返回AlertResult对象用来标记请求发送or失败\n\npublic final class HttpAlertChannel implements AlertChannel {\n    @Override\n    public AlertResult process(AlertInfo alertInfo) {\n        AlertData alertData = alertInfo.getAlertData();\n        Map&lt;String, String&gt; paramsMap = alertInfo.getAlertParams();\n        if (null == paramsMap) {\n            return new AlertResult(&quot;false&quot;, &quot;http params is null&quot;);\n        }\n        return new HttpSender(paramsMap).send(alertData.getContent());\n    }\n}\n\n至此插件开发就完成的,是不是很简单：）设计优秀架构合理的代码就应该是这样优雅高效解耦合.\n完成以上开发后,启动告警服务,就可以在添加告警实例时选择对应的插件了\n\n源码解读\n在启动告警服务时,可以在日志看到有注册告警插件的信息\n\n以此为切入口来探索插件实现的相关代码\n\n在dolphinscheduler-alert-server的AlertPluginManager的 installPlugin 方法可以看到注册告警插件的内容,这里先获取所有实现了AlertChannelFactory.class的类,遍历后获取AlertChannel的实例,添加到数据库和channelKeyedByIdMap\n\n    private final Map&lt;Integer, AlertChannel&gt; channelKeyedById = new HashMap&lt;&gt;();\n    \n    @EventListener\n    public void installPlugin(ApplicationReadyEvent readyEvent) {\n        PrioritySPIFactory&lt;AlertChannelFactory&gt; prioritySPIFactory = new PrioritySPIFactory&lt;&gt;(AlertChannelFactory.class);\n        for (Map.Entry&lt;String, AlertChannelFactory&gt; entry : prioritySPIFactory.getSPIMap().entrySet()) {\n            String name = entry.getKey();\n            AlertChannelFactory factory = entry.getValue();\n            logger.info(&quot;Registering alert plugin: {} - {}&quot;, name, factory.getClass());\n            final AlertChannel alertChannel = factory.create();\n            logger.info(&quot;Registered alert plugin: {} - {}&quot;, name, factory.getClass());\n            final List&lt;PluginParams&gt; params = new ArrayList&lt;&gt;(factory.params());\n            params.add(0, warningTypeParams);\n            final String paramsJson = PluginParamsTransfer.transferParamsToJson(params);\n            final PluginDefine pluginDefine = new PluginDefine(name, PluginType.ALERT.getDesc(), paramsJson);\n            final int id = pluginDao.addOrUpdatePluginDefine(pluginDefine);\n            channelKeyedById.put(id, alertChannel);\n        }\n    }\n\n\n完成插件的开发和注册后,需要有个轮询线程来遍历查询需要发送的消息和完成发送的动作,在AlertSenderService的run方法完成了这些\n\n@Override\npublic void run() {\n    logger.info(&quot;alert sender started&quot;);\n    while (!ServerLifeCycleManager.isStopped()) {\n        try {\n            List&lt;Alert&gt; alerts = alertDao.listPendingAlerts();\n            AlertServerMetrics.registerPendingAlertGauge(alerts::size);\n            this.send(alerts);\n            ThreadUtils.sleep(Constants.SLEEP_TIME_MILLIS * 5L);\n        } catch (Exception e) {\n            logger.error(&quot;alert sender thread error&quot;, e);\n        }\n    }\n}\n\n\n关键方法是this.send(alerts),这里遍历Alert后获取告警插件的实例集合,在 this.alertResultHandler(instance, alertData)传入插件实例对象和告警参数,最后更新这条告警消息的状态\n\npublic void send(List&lt;Alert&gt; alerts) {\n    for (Alert alert : alerts) {\n        // get alert group from alert\n        int alertId = Optional.ofNullable(alert.getId()).orElse(0);\n        int alertGroupId = Optional.ofNullable(alert.getAlertGroupId()).orElse(0);\n        List&lt;AlertPluginInstance&gt; alertInstanceList = alertDao.listInstanceByAlertGroupId(alertGroupId);\n        if (CollectionUtils.isEmpty(alertInstanceList)) {\n            logger.error(&quot;send alert msg fail,no bind plugin instance.&quot;);\n            List&lt;AlertResult&gt; alertResults = Lists.newArrayList(new AlertResult(&quot;false&quot;,\n                    &quot;no bind plugin instance&quot;));\n            alertDao.updateAlert(AlertStatus.EXECUTION_FAILURE, JSONUtils.toJsonString(alertResults), alertId);\n            continue;\n        }\n        AlertData alertData = AlertData.builder()\n                .id(alertId)\n                .content(alert.getContent())\n                .log(alert.getLog())\n                .title(alert.getTitle())\n                .warnType(alert.getWarningType().getCode())\n                .alertType(alert.getAlertType().getCode())\n                .build();\n\n        int sendSuccessCount = 0;\n        List&lt;AlertResult&gt; alertResults = new ArrayList&lt;&gt;();\n        for (AlertPluginInstance instance : alertInstanceList) {\n            AlertResult alertResult = this.alertResultHandler(instance, alertData);\n            if (alertResult != null) {\n                AlertStatus sendStatus = Boolean.parseBoolean(String.valueOf(alertResult.getStatus()))\n                        ? AlertStatus.EXECUTION_SUCCESS\n                        : AlertStatus.EXECUTION_FAILURE;\n                alertDao.addAlertSendStatus(sendStatus, JSONUtils.toJsonString(alertResult), alertId,\n                        instance.getId());\n                if (sendStatus.equals(AlertStatus.EXECUTION_SUCCESS)) {\n                    sendSuccessCount++;\n                    AlertServerMetrics.incAlertSuccessCount();\n                } else {\n                    AlertServerMetrics.incAlertFailCount();\n                }\n                alertResults.add(alertResult);\n            }\n        }\n        AlertStatus alertStatus = AlertStatus.EXECUTION_SUCCESS;\n        if (sendSuccessCount == 0) {\n            alertStatus = AlertStatus.EXECUTION_FAILURE;\n        } else if (sendSuccessCount &lt; alertInstanceList.size()) {\n            alertStatus = AlertStatus.EXECUTION_PARTIAL_SUCCESS;\n        }\n        alertDao.updateAlert(alertStatus, JSONUtils.toJsonString(alertResults), alertId);\n    }\n}\n\n\n在alertResultHandler用alertPluginManager.getAlertChannel(instance.getPluginDefineId())获取AlertChannel实例.还记得前面注册告警插件时往channelKeyedById里put的AlertChannel实例的动作吗?\n\npublic Optional&lt;AlertChannel&gt; getAlertChannel(int id) {\n    return Optional.ofNullable(channelKeyedById.get(id));\n}\n\n\n然后构建AlertInfo对象,通过CompletableFuture.supplyAsync()来异步回调执行alertChannel.process(alertInfo),用future.get()获得回调执行返回的AlertResult再return\n\nprivate @Nullable AlertResult alertResultHandler(AlertPluginInstance instance, AlertData alertData) {\n    String pluginInstanceName = instance.getInstanceName();\n    int pluginDefineId = instance.getPluginDefineId();\n    Optional&lt;AlertChannel&gt; alertChannelOptional = alertPluginManager.getAlertChannel(instance.getPluginDefineId());\n    if (!alertChannelOptional.isPresent()) {\n        String message = String.format(&quot;Alert Plugin %s send error: the channel doesn't exist, pluginDefineId: %s&quot;,\n                pluginInstanceName,\n                pluginDefineId);\n        logger.error(&quot;Alert Plugin {} send error : not found plugin {}&quot;, pluginInstanceName, pluginDefineId);\n        return new AlertResult(&quot;false&quot;, message);\n    }\n    AlertChannel alertChannel = alertChannelOptional.get();\n\n    Map&lt;String, String&gt; paramsMap = JSONUtils.toMap(instance.getPluginInstanceParams());\n    String instanceWarnType = WarningType.ALL.getDescp();\n\n    if (paramsMap != null) {\n        instanceWarnType = paramsMap.getOrDefault(AlertConstants.NAME_WARNING_TYPE, WarningType.ALL.getDescp());\n    }\n\n    WarningType warningType = WarningType.of(instanceWarnType);\n\n    if (warningType == null) {\n        String message = String.format(&quot;Alert Plugin %s send error : plugin warnType is null&quot;, pluginInstanceName);\n        logger.error(&quot;Alert Plugin {} send error : plugin warnType is null&quot;, pluginInstanceName);\n        return new AlertResult(&quot;false&quot;, message);\n    }\n\n    boolean sendWarning = false;\n    switch (warningType) {\n        case ALL:\n            sendWarning = true;\n            break;\n        case SUCCESS:\n            if (alertData.getWarnType() == WarningType.SUCCESS.getCode()) {\n                sendWarning = true;\n            }\n            break;\n        case FAILURE:\n            if (alertData.getWarnType() == WarningType.FAILURE.getCode()) {\n                sendWarning = true;\n            }\n            break;\n        default:\n    }\n\n    if (!sendWarning) {\n        logger.info(\n                &quot;Alert Plugin {} send ignore warning type not match: plugin warning type is {}, alert data warning type is {}&quot;,\n                pluginInstanceName, warningType.getCode(), alertData.getWarnType());\n        return null;\n    }\n\n    AlertInfo alertInfo = AlertInfo.builder()\n            .alertData(alertData)\n            .alertParams(paramsMap)\n            .alertPluginInstanceId(instance.getId())\n            .build();\n    int waitTimeout = alertConfig.getWaitTimeout();\n    try {\n        AlertResult alertResult;\n        if (waitTimeout &lt;= 0) {\n            if (alertData.getAlertType() == AlertType.CLOSE_ALERT.getCode()) {\n                alertResult = alertChannel.closeAlert(alertInfo);\n            } else {\n                alertResult = alertChannel.process(alertInfo);\n            }\n        } else {\n            CompletableFuture&lt;AlertResult&gt; future;\n            if (alertData.getAlertType() == AlertType.CLOSE_ALERT.getCode()) {\n                future = CompletableFuture.supplyAsync(() -&gt; alertChannel.closeAlert(alertInfo));\n            } else {\n                future = CompletableFuture.supplyAsync(() -&gt; alertChannel.process(alertInfo));\n            }\n            alertResult = future.get(waitTimeout, TimeUnit.MILLISECONDS);\n        }\n        if (alertResult == null) {\n            throw new RuntimeException(&quot;Alert result cannot be null&quot;);\n        }\n        return alertResult;\n    } catch (InterruptedException e) {\n        logger.error(&quot;send alert error alert data id :{},&quot;, alertData.getId(), e);\n        Thread.currentThread().interrupt();\n        return new AlertResult(&quot;false&quot;, e.getMessage());\n    } catch (Exception e) {\n        logger.error(&quot;send alert error alert data id :{},&quot;, alertData.getId(), e);\n        return new AlertResult(&quot;false&quot;, e.getMessage());\n    }\n}\n\n综上描述，可以画出注册插件和发送消息的时序图\n\n以上就是告警插件的主要实现代码,是不是发现源码看下来也没有发现多高深和复杂：）所以多看看源码吧,以后你也可以写出这样优秀的开源软件来贡献开源\n参考连接\n\n[Feature] Alert Plugin Design · Issue #3049 · apache/dolphinscheduler (github.com)\n\n\nalert (apache.org)\n\n",
    "title": "企业级应用如何用 Apache DolphinScheduler 有针对性地进行告警插件开发？",
    "time": "2024-01-19"
  },
  {
    "name": "Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2",
    "content": "Apache DolphinScheduler\n查看官方的升级文档，可知有提供升级脚本，如果只是跨小版本的更新那么只用执行脚本就好了，但跨多个大版本升级时依然容易出现各种问题，特此总结。\n旧版本：1.3.4\n新版本：3.1.2\n1. 升级完成后使用资源中心报错 IllegalArgumentException: Failed to specify server's Kerberos principal name\n资源中心使用的HDFS，开启了kerberos认证\n解决方法：\n编辑 dolphinscheduler/api-server/conf/hdfs-site.xml 添加以下内容\n&lt;property&gt;\n    &lt;name&gt;dfs.namenode.kerberos.principal.pattern&lt;/name&gt;\n    &lt;value&gt;*&lt;/value&gt;\n&lt;/property&gt;\n\n2. 升级完成后查看任务实例的日志，报错未找到日志\n查看报错信息，检查新版本的目录结构和表里的日志路径，发现原因是新版本的日志路径有变更\n升级前的日志路径在 /logs/ 下\n升级后的日志路径在 /worker-server/logs/ 下\n因此需要修改这里的目录\n解决方法：\n执行sql修改日志路径\nupdate t_ds_task_instance set log_path=replace(log_path,&#x27;/logs/&#x27;,&#x27;/worker-server/logs/&#x27;);\n\n然后将原日志文件copy到新的日志路径\ncp -r {旧版本dolphinscheduler目录}/logs/[1-9]* {新版本dolphinscheduler目录}/worker-server/logs/*\n\n3.升级完成后创建工作流报错\n查看报错信息，原因是 t_ds_process_definition_log 和 t_ds_process_definition 主键的初始值不一致\n那么修改成一致的就好\n解决方法：\n执行sql\n# 查出主键自增值\nselect AUTO_INCREMENT FROM information_schema.TABLES WHERE TABLE_SCHEMA = &#x27;dolphinscheduler&#x27; AND TABLE_NAME = &#x27;t_ds_process_definition&#x27; limit 1\n# 将上面sql的执行结果填写到下方参数处执行\nalter table dolphinscheduler_bak1.t_ds_process_definition_log auto_increment = {max_id};\n\n4.升级后任务实例列表为空\n检查查询的sql\n在dolphinscheduler-dao/src/main/resources/org/apache/dolphinscheduler/dao/mapper/TaskInstanceMapper.xml文件里，select id=&quot;queryTaskInstanceListPaging&quot;的sql\n\t\tselect\n        &lt;include refid=&quot;baseSqlV2&quot;&gt;\n            &lt;property name=&quot;alias&quot; value=&quot;instance&quot;/&gt;\n        &lt;/include&gt;\n        ,\n        process.name as process_instance_name\n        from t_ds_task_instance instance\n        left join t_ds_task_definition_log define on define.code=instance.task_code and define.version=instance.task_definition_version\n        left join t_ds_process_instance process on process.id=instance.process_instance_id\n        where define.project_code = #{projectCode}\n        &lt;if test=&quot;startTime != null&quot;&gt;\n            and instance.start_time &lt;![CDATA[ &gt;=]]&gt; #{startTime}\n        &lt;/if&gt;\n\t\t......省略多余部分\n\n查询任务实例列表的sql会关联 t_ds_task_definition_log 表，经检查发现是 define.code=instance.task_code 这一句关联不上。\n结合下面的查询条件 define.project_code = #{projectCode} 可知，关联t_ds_task_definition_log 主要是为了过滤 projectCode，那么来修改下这个sql\n解决方法：\n    \tselect\n        &lt;include refid=&quot;baseSqlV2&quot;&gt;\n            &lt;property name=&quot;alias&quot; value=&quot;instance&quot;/&gt;\n        &lt;/include&gt;\n        ,\n        process.name as process_instance_name\n        from t_ds_task_instance instance\n--         left join t_ds_task_definition_log define \n--\t\t\t\ton define.code=instance.task_code and \n--\t\t\t\t\tdefine.version=instance.task_definition_version\n        join t_ds_process_instance process\n        \ton process.id=instance.process_instance_id\n        join t_ds_process_definition define\n        \ton define.code=process.process_definition_code\n        where define.project_code = #{projectCode}\n        &lt;if test=&quot;startTime != null&quot;&gt;\n            and instance.start_time &lt;![CDATA[ &gt;=]]&gt; #{startTime}\n        &lt;/if&gt;\n\t\t......省略多余部分\n\n直接用 t_ds_process_definition 关联，也有project_code字段可以用来关联过滤\n这里修改后就能查出数据了\n5. 执行升级脚本的过程中报错空指针\n5.1分析日志，定位到 UpgradeDao.java 517行\n查看代码\n513 if (TASK_TYPE_SUB_PROCESS.equals(taskType)) {\n514                       JsonNode jsonNodeDefinitionId = param.get(&quot;processDefinitionId&quot;);\n515                       if (jsonNodeDefinitionId != null) {\n516                           param.put(&quot;processDefinitionCode&quot;,\n517                                  processDefinitionMap.get(jsonNodeDefinitionId.asInt()).getCode());\n518                            param.remove(&quot;processDefinitionId&quot;);\n519                        }\n520                    }\n\n很明显是 processDefinitionMap.get(jsonNodeDefinitionId.asInt())** 返回了null,加个null判断，如果返回null直接跳过，并将相关信息打印出来，升级结束后可以根据日志核对。**\n解决方法：\n修改后\nif (jsonNodeDefinitionId != null) {\n    if (processDefinitionMap.get(jsonNodeDefinitionId.asInt()) != null) {\n        param.put(&quot;processDefinitionCode&quot;,processDefinitionMap.get(jsonNodeDefinitionId.asInt()).getCode());\n        param.remove(&quot;processDefinitionId&quot;);\n    } else {\n        logger.error(&quot;*******************error&quot;);\n        logger.error(&quot;*******************param:&quot; + param);\n        logger.error(&quot;*******************jsonNodeDefinitionId:&quot; + jsonNodeDefinitionId);\n    }\n}\n\n5.2分析日志，定位到 UpgradeDao.java 675行\n查看代码\n669 if (mapEntry.isPresent()) {\n670                            Map.Entry&lt;Long, Map&lt;String, Long&gt;&gt; processCodeTaskNameCodeEntry = mapEntry.get();\n671                            dependItem.put(&quot;definitionCode&quot;, processCodeTaskNameCodeEntry.getKey());\n672                            String depTasks = dependItem.get(&quot;depTasks&quot;).asText();\n673                            long taskCode =\n674                                    &quot;ALL&quot;.equals(depTasks) || processCodeTaskNameCodeEntry.getValue() == null ? 0L\n675                                            : processCodeTaskNameCodeEntry.getValue().get(depTasks);\n676                            dependItem.put(&quot;depTaskCode&quot;, taskCode);\n677                        }\n\n很明显是processCodeTaskNameCodeEntry.getValue().get(depTasks) 返回了null.\n修改下逻辑，不为null才赋值并打印相关日志\n解决方法：\n修改后\nlong taskCode =0;\n                            if (processCodeTaskNameCodeEntry.getValue() != null\n                                    &amp;&amp;processCodeTaskNameCodeEntry.getValue().get(depTasks)!=null){\n                                taskCode =processCodeTaskNameCodeEntry.getValue().get(depTasks);\n                            }else{\n                                logger.error(&quot;******************** depTasks:&quot;+depTasks);\n                                logger.error(&quot;******************** taskCode not in &quot;+JSONUtils.toJsonString(processCodeTaskNameCodeEntry));\n                            }\n                            dependItem.put(&quot;depTaskCode&quot;, taskCode);\n\n6.接入LDAP后登陆失败，不知道email字段名\n可在 api-server/conf/application.yaml 配置接入LDAP\nsecurity:\n  authentication:\n    # Authentication types (supported types: PASSWORD,LDAP)\n    type: LDAP\n    # IF you set type `LDAP`, below config will be effective\n    ldap:\n      # ldap server config\n      urls: xxx\n      base-dn: xxx\n      username: xxx\n      password: xxx\n      user:\n        # admin userId when you use LDAP login\n        admin: xxx\n        identity-attribute: xxx\n        email-attribute: xxx\n        # action when ldap user is not exist (supported types: CREATE,DENY)\n        not-exist-action: CREATE\n\n要成功接入LDAP至少需要urls,base-dn,username,password,identity和email 正确填写，不知道email字段名可以按下面的方式处理，email先空着\n启动服务后用LDAP用户登录\n解决办法：\n** LDAP 认证的代码在 dolphinscheduler-api/src/main/java/org/apache/dolphinscheduler/api/security/impl/ldap/LdapService.java 的 ldapLogin()**\nctx = new InitialLdapContext(searchEnv, null);\nSearchControls sc = new SearchControls();\nsc.setReturningAttributes(new String[]{ldapEmailAttribute});\nsc.setSearchScope(SearchControls.SUBTREE_SCOPE);\nEqualsFilter filter = new EqualsFilter(ldapUserIdentifyingAttribute, userId);\nNamingEnumeration&lt;SearchResult&gt; results = ctx.search(ldapBaseDn, filter.toString(), sc);\nif (results.hasMore()) {\n    // get the users DN (distinguishedName) from the result\n    SearchResult result = results.next();\n    NamingEnumeration&lt;? extends Attribute&gt; attrs = result.getAttributes().getAll();\n    while (attrs.hasMore()) {\n        // Open another connection to the LDAP server with the found DN and the password\n        searchEnv.put(Context.SECURITY_PRINCIPAL, result.getNameInNamespace());\n        searchEnv.put(Context.SECURITY_CREDENTIALS, userPwd);\n        try {\n            new InitialDirContext(searchEnv);\n        } catch (Exception e) {\n            logger.warn(&quot;invalid ldap credentials or ldap search error&quot;, e);\n            return null;\n        }\n        Attribute attr = attrs.next();\n        if (attr.getID().equals(ldapEmailAttribute)) {\n            return (String) attr.get();\n        }\n    }\n}\n\n第三行会根据填的字段过滤，先注释第三行\n// sc.setReturningAttributes(new String[]{ldapEmailAttribute});\n\n重新执行后第10行会返回全部字段\nNamingEnumeration&lt;? extends Attribute&gt; attrs = result.getAttributes().getAll();\n\n通过打印或调试在里面找到email字段填到配置文件里，再还原上面注释的代码，重启服务后即可正常接入LDAP登录。\n7.管理员给普通用户授权资源文件不生效\n经多次测试,发现普通用户只能看到所属用户为自己的资源文件，管理员授权后依然无法查看资源文件\n解决办法:\n文件 dolphinscheduler-api/src/main/java/org/apache/dolphinscheduler/api/permission/ResourcePermissionCheckServiceImpl.java的listAuthorizedResource()方法,将 return 的集合修改为 relationResources\n@Override\n        public Set&lt;Integer&gt; listAuthorizedResource(int userId, Logger logger) {\n            List&lt;Resource&gt; relationResources;\n            if (userId == 0) {\n                relationResources = new ArrayList&lt;&gt;();\n            } else {\n                // query resource relation\n                List&lt;Integer&gt; resIds = resourceUserMapper.queryResourcesIdListByUserIdAndPerm(userId, 0);\n                relationResources = CollectionUtils.isEmpty(resIds) ? new ArrayList&lt;&gt;() : resourceMapper.queryResourceListById(resIds);\n            }\n            List&lt;Resource&gt; ownResourceList = resourceMapper.queryResourceListAuthored(userId, -1);\n            relationResources.addAll(ownResourceList);\n            return relationResources.stream().map(Resource::getId).collect(toSet()); // 解决资源文件授权无效的问题\n//            return ownResourceList.stream().map(Resource::getId).collect(toSet());\n        }\n\n检查新版本的change log ，发现在3.1.3版本修复了这个bug\nhttps://github.com/apache/dolphinscheduler/pull/13318\n8.kerberos过期的问题\n因为kerberos配置了票据过期时间，一段时间后资源中心的hdfs资源将无法访问，最好的解决办法是添加定时更新凭证的相关逻辑\n解决办法:\n在文件 dolphinscheduler-service/src/main/java/org/apache/dolphinscheduler/service/utils/CommonUtils.java 添加方法\n /**\n     * * 定时更新凭证\n     */\n    private static void startCheckKeytabTgtAndReloginJob() {\n        // 每天循环，定时更新凭证\n        Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(() -&gt; {\n            try {\n                UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n                logger.warn(&quot;Check Kerberos Tgt And Relogin From Keytab Finish.&quot;);\n            } catch (IOException e) {\n                logger.error(&quot;Check Kerberos Tgt And Relogin From Keytab Error&quot;, e);\n            }\n        }, 0, 1, TimeUnit.DAYS);\n        logger.info(&quot;Start Check Keytab TGT And Relogin Job Success.&quot;);\n    }\n\n然后在该文件的loadKerberosConf 方法返回 true 前调用\npublic static boolean loadKerberosConf(String javaSecurityKrb5Conf, String loginUserKeytabUsername,\n                                           String loginUserKeytabPath, Configuration configuration) throws IOException {\n        if (CommonUtils.getKerberosStartupState()) {\n            System.setProperty(Constants.JAVA_SECURITY_KRB5_CONF, StringUtils.defaultIfBlank(javaSecurityKrb5Conf,\n                    PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH)));\n            configuration.set(Constants.HADOOP_SECURITY_AUTHENTICATION, Constants.KERBEROS);\n            UserGroupInformation.setConfiguration(configuration);\n            UserGroupInformation.loginUserFromKeytab(\n                    StringUtils.defaultIfBlank(loginUserKeytabUsername,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME)),\n                    StringUtils.defaultIfBlank(loginUserKeytabPath,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH)));\n            startCheckKeytabTgtAndReloginJob();  // 此处调用\n            return true;\n        }\n        return false;\n    }\n\n",
    "title": "Apache DolphinScheduler 从 1.3.4 升级至3.1.2 过程中的问题记录及解决方案",
    "time": "2024-01-19"
  },
  {
    "name": "K8s_Cisco_Hangzhou",
    "content": "全面拥抱 K8s，ApacheDolphinScheduler 应用与支持 K8s 任务的探索\n\n\n\n\nK8s 打通了主流公私云之间的壁垒，成为唯一连通公私云的基础架构平台。K8s 是未来云端的趋势，全面拥抱 K8s 成为更多企业推动 IT 现代化的选择。\n\n杭州思科基于 Apache DolphinScheduler，也在进行支持 K8s 的相关探索，且部分功能已经成功上线运行。今天，来自杭州思科的大数据工程师 李千，将为我们分享他们的开发成果。\n\n\n\n\n\n李千，杭州思科 大数据工程师，多年大数据解决方案经验，有 Spark，Flink，以及调度系统，ETL 等方面的项目经验。\n正文：\n本次我的分享主要分为这几部分，Namespace 管理，持续运行的 K8s 任务，K8s 任务的工作流调度，以及未来的规划。\nNamespace 管理\n资源管理\n第一部分中，我首先介绍一下资源管理。我们引入资源管理目的，是为了利用 K8s 集群运行不属于 Apache DolphinScheduler 所属的调度概念上的任务，比如 Namespace，更类似于一个数据解决方案，如果 CPU 的 memory 有限，就可以限制队列中的资源，实现一定的资源隔离。\n以后我们可能会把一部分资源管理功能合并到 Apache DolphinScheduler 上。\n增删维护管理\n我们可以加一些 Type，即标记的类型，比如某些 Namespace 只允许跑一些特定类型的 job。我们可以统计Namespace 下面的任务数量、pod 数量、请求资源量、请求等，查看队列的资源使用情况，界面默认只有管理员才可以操作。\n\n\n\n多 K8s 集群\nK8s 支持多个集群，我们通过 Apache DolphinScheduler 客户端连接到多个 K8s 集群，batch、PROD 等可以搭建多套这K8s 集群，并通过 Namespace 支持多套 K8s 集群。\n我们可以编辑所开发的集群，修改所有的属性，如内存等。\n在新版中，用户权限的管理位于 user master 中，可以给某个用户授权，允许用户可以向某个 Namespace 上提交任务，并编辑资源。\n02 持续运行的 K8s 任务\n第二部分是关于我们目前已经支持的任务类型。\n启动不退出的普通镜像，如 ETL 等\n比如 ETL 这种提交完之后必须要手动操作才会退出的任务。这种任务一旦提交，就会把数据 sink，这种任务理论上只要不做升级，它永远不会停。\n\n\n\n这种任务其实调度可能用不到，因为它只有启停这两种状态。所以，我们把它放在一个实时列表中，并做了一套监控。POD是实时运行的状态，主要是通过一个 Fabris operator 进行交互，可以进行动态进行扩展，以提高资源利用率。\nFlink 任务\n我们对于 CPU 的管理可以精确到 0.01%，充分利用了 K8s 虚拟 CPU。\n\n\n\n\n\n\n\n\n\n另外，我们也常用 Flink 任务，这是一种基于 ETL 的扩展。Flink 任务界面中包含编辑、查看状态、上线、下线、删除、执行历史，以及一些监控的设计。我们用代理的模式来设计 Flink UI，并开发了权限管控，不允许外部的人随意修改。\nFlink 默认了基于 checkpoint 启动，也可以指定一个时间创建，或基于上一次 checkpoint 来提交和启动。\nFlink 任务支持多种模式镜像版本，因为 K8s 本身就是运行镜像的，可以直接指定一些镜像来选择使用包，或通过文件上传的方式提交任务。\n另外，Batch 类型的任务可能一次运行即结束，或是按照周期来调度，自动执行完后退出，这和 Flink 不太一样，所以对于这种类型的任务，我们还是基于 Apache DolphinScheduler 做。\n03 K8s 任务的运行\nK8s 任务的工作流调度\n我们在最底层增加了一些 Flink 的 batch 和 Spark 的 batch 任务，添加了一些配置，如使用的资源，所运行的 namespace 等。镜像信息可以支持一些自定义参数启动，封装起来后就相当于插件的模式，Apache DolphinScheduler 完美地扩展了它的功能。\n\n\n\nSpark 任务\nSpark 任务下可以查看 CPU 等信息，上传文件支持 Spark Jar 包，也可以单独上传配置文件。\n\n\n\n这种多线程的上层，可以大幅提高处理速度。\n04 其他和规划\nWatch 状态\n\n\n\n除了上述改动，我们还对任务运行状态进行了优化。\n当提交任务后，实际情况下运行过程中可能会出现失败，甚至任务的并行度也会基于某些策略发生改变。这时，我们就需要一种 watch 的方式来动态实时地来获取任务状态，并同步给 Apache DolphinScheduler 系统，以保证界面上看到的状态一定是最准确的。\nBatch 做不做 watch 都可以，因为这不是一个需要全量监听的独立任务而且 namespace 的资源使用率也是基于 watch 模式，这样就可以保证状态都是准确的。\n多环境\n多环境是指，同一个任务可以推送到不同的 K8s 集群上，比如同一个Flink 任务。\n从代码上来说，watch 有两种方式，一种是单独放一些 pod，比如当使用了 K8s 模块时，定义多个 K8s 集群的信息，在每个集群上创建一些watch pod 来监听集群中的任务状态，并做一些代理的功能。另一种是跟随api或单独服务，启动一个监听服务监听所有k8s集群。但这样无法而外做一些k8s内外网络的代理。\nBatch 有多种方案，一种是可以基于 Apache DolphinScheduler 自带功能，通过同步的方式进行 watch，这和 Apache DolphinScheduler 比较兼容。关于这方面的工作我们未来可能很快会提交 PR。Spark 使用相同的模式，提供一些 pod 来进行交互，而内部代码我们使用的是 Fabric K8s 的 client。\n今后，我们将与 Apache DolphinScheduler 一起共建，陆续支持这里讨论的功能，并和大家分享更多关于我们的工作进展。谢谢大家！\n",
    "title": "全面拥抱 K8s，ApacheDolphinScheduler 应用与支持 K8s 任务的探索",
    "time": "2022-3-21"
  },
  {
    "name": "K8s_Integration_with_Apache_DolphinScheduler_by_Hangzhou_Cisco",
    "content": "K8s 打通了主流公私云之间的壁垒，成为唯一连通公私云的基础架构平台。K8s 是未来云端的趋势，全面拥抱 K8s 成为更多企业推动 IT 现代化的选择。\n杭州思科基于 Apache DolphinScheduler，也在进行支持 K8s 的相关探索，且部分功能已经成功上线运行。今天，来自杭州思科的大数据工程师 李千，将为我们分享他们的开发成果。\n李千\n杭州思科 大数据工程师，多年大数据解决方案经验，有 Spark，Flink，以及调度系统，ETL 等方面的项目经验。\n本次我的分享主要分为这几部分，Namespace 管理，持续运行的 K8s 任务，K8s 任务的工作流调度，以及未来的规划。\n01 Namespace 管理\n资源管理\n第一部分中，我首先介绍一下资源管理。我们引入资源管理目的，是为了利用 K8s 集群运行不属于 Apache DolphinScheduler 所属的调度概念上的任务，比如 Namespace，更类似于一个数据解决方案，如果 CPU 的 memory 有限，就可以限制队列中的资源，实现一定的资源隔离。\n以后我们可能会把一部分资源管理功能合并到 Apache DolphinScheduler 上。\n增删维护管理\n我们可以加一些 Type，即标记的类型，比如某些 Namespace 只允许跑一些特定类型的 job。我们可以统计Namespace 下面的任务数量、pod 数量、请求资源量、请求等，查看队列的资源使用情况，界面默认只有管理员才可以操作。\n多 K8s 集群\nK8s 支持多个集群，我们通过 Apache DolphinScheduler 客户端连接到多个 K8s 集群，batch、PROD 等可以搭建多套这K8s 集群，并通过 Namespace 支持多套 K8s 集群。\n我们可以编辑所开发的集群，修改所有的属性，如内存等。\n在新版中，用户权限的管理位于 user master 中，可以给某个用户授权，允许用户可以向某个 Namespace 上提交任务，并编辑资源。\n02 持续运行的 K8s 任务\n第二部分是关于我们目前已经支持的任务类型。\n启动不退出的普通镜像，如 ETL 等\n比如 ETL 这种提交完之后必须要手动操作才会退出的任务。这种任务一旦提交，就会把数据 sink，这种任务理论上只要不做升级，它永远不会停。\n这种任务其实调度可能用不到，因为它只有启停这两种状态。所以，我们把它放在一个实时列表中，并做了一套监控。POD是实时运行的状态，主要是通过一个 Fabris operator 进行交互，可以进行动态进行扩展，以提高资源利用率。\nFlink 任务\n我们对于 CPU 的管理可以精确到 0.01%，充分利用了 K8s 虚拟 CPU。\n另外，我们也常用 Flink 任务，这是一种基于 ETL 的扩展。Flink 任务界面中包含编辑、查看状态、上线、下线、删除、执行历史，以及一些监控的设计。我们用代理的模式来设计 Flink UI，并开发了权限管控，不允许外部的人随意修改。\nFlink 默认了基于 checkpoint 启动，也可以指定一个时间创建，或基于上一次 checkpoint 来提交和启动。\nFlink 任务支持多种模式镜像版本，因为 K8s 本身就是运行镜像的，可以直接指定一些镜像来选择使用包，或通过文件上传的方式提交任务。\n另外，Batch 类型的任务可能一次运行即结束，或是按照周期来调度，自动执行完后退出，这和 Flink 不太一样，所以对于这种类型的任务，我们还是基于 Apache DolphinScheduler 做。\n**03 **K8s 任务的运行\nK8s 任务的工作流调度\n我们在最底层增加了一些 Flink 的 batch 和 Spark 的 batch 任务，添加了一些配置，如使用的资源，所运行的 namespace 等。镜像信息可以支持一些自定义参数启动，封装起来后就相当于插件的模式，Apache DolphinScheduler 完美地扩展了它的功能。\nSpark 任务\nSpark 任务下可以查看 CPU 等信息，上传文件支持 Spark Jar 包，也可以单独上传配置文件。\n这种多线程的上层，可以大幅提高处理速度。\n04 其他和划化\nWatch 状态\n除了上述改动，我们还对任务运行状态进行了优化。\n当提交任务后，实际情况下运行过程中可能会出现失败，甚至任务的并行度也会基于某些策略发生改变。这时，我们就需要一种 watch 的方式来动态实时地来获取任务状态，并同步给 Apache DolphinScheduler 系统，以保证界面上看到的状态一定是最准确的。\nBatch 做不做 watch 都可以，因为这不是一个需要全量监听的独立任务而且 namespace 的资源使用率也是基于 watch 模式，这样就可以保证状态都是准确的。\n多环境\n多环境是指，同一个任务可以推送到不同的 K8s 集群上，比如同一个Flink 任务。\n从代码上来说，watch 有两种方式，一种是单独放一些 pod，比如当使用了 K8s 模块时，定义多个 K8s 集群的信息，在每个集群上创建一些watch pod 来监听集群中的任务状态，并做一些代理的功能。另一种是跟随 API 或单独服务，启动一个监听服务监听所有K8s集群。但这样无法在外做一些K8s内外网络的代理。\n而针对 Batch 有多种方案，一种是可以基于 Apache DolphinScheduler 自带功能，通过同步的方式进行 watch，这和 Apache DolphinScheduler 比较兼容。关于这方面的工作我们未来可能很快会提交 PR。Spark 使用相同的模式，提供一些 pod 来进行交互，而内部代码我们使用的是 Fabric K8s 的 client。\n今后，我们将与 Apache DolphinScheduler 一起共建，陆续支持这里讨论的功能，并和大家分享更多关于我们的工作进展。谢谢大家！\n",
    "title": "全面拥抱 K8s，ApacheDolphinScheduler 应用与支持 K8s 任务的探索",
    "time": "2023-8-8"
  },
  {
    "name": "Lizhi-case-study",
    "content": "荔枝机器学习平台与大数据调度系统“双剑合璧”，打造未来数据处理新模式！\n\n\n\n\n编 者 按：在线音频行业在中国仍是蓝海一片。根据 CIC 数据显示，中国在线音频行业市场规模由 2016 年的 16 亿元增长至 2020 年的 131 亿元，复合年增长率为 69.4%。随着物联网场景的普及，音频场景更是从移动端扩展至车载端、智能硬件端、家居端等各类场景，从而最大限度发挥音频载体的伴随性优势。\n\n\n近年来，国内音频社区成功上市的案例接踵而至。其中，于 2020 年在纳斯达克上市的“在线音频”第一股荔枝，用户数早已超过 2 亿。进入信息化时代，音频行业和所有行业一样，面对平台海量 UGC 数据的产生，音频用户和消费者对于信息传递的效率也有着高要求，互联网音频平台也希望信息能够精准、快速地推送给用户，同时保证平台 UGC 内容的安全性。 为了最高效地达到这一目标，利用日益成熟的机器学习技术与大数据调度系统相结合，以更好地海量数据处理任务，成为各大音频平台探索的方向。荔枝的机器学习智能推荐平台就走在探索的最前沿，尝试将机器学习平台与大数据调度系统 DolphinScheduler 相结合。\n\n01 背景\n\n\n\n荔枝是一个快速发展的UGC音频社区公司，非常注重 AI 和数据分析两个领域的技术。因为 AI 可以在海量碎片化的声音中找到每个用户喜欢的声音，并将其打造成一个可持续发展的生态闭环。而数据分析可以为公司快速发展的业务提供指引。两者都需要对海量数据做处理，所以需要使用大数据调度系统。荔枝机器学习平台团队尝试利用大数据调度平台的调度能力与机器学习平台的能力相结合，处理每天涌进荔枝平台的海量 UGC 音频内容。\n在 2020 年初之前，荔枝机器学习平台使用的是 Azkaban 调度系统，但是对于 AI 来说，虽然大数据调度的 sql/shell/python 脚本和其他大数据相关组件也能完成整个AI 流程，但不够易用，也很难复用。机器学习平台是专门为 AI 构建的一套系统，它将 AI 开发范式，即获取数据、数据预处理、模型训练、模型预测、模型评估和模型发布过程抽象成组件，每个组件提供多个实现，用 DAG 串联，使用拖拽和配置的方式，就可以实现低代码开发。\n目前，荔枝每天有 1600+ 流程和 12000+ 任务（IDC）在 DolphinScheduler 上顺利运行。\n02 机器学习平台开发中的挑战\n荔枝在进行机器学习平台开发时，对于调度系统有一些明确的需求：\n1、需要对海量数据存算，比如筛选样本、生成画像、特征预处理、分布式模型训练等；\n2、需要 DAG 执行引擎，将获取数据-&gt;数据预处理-&gt;模型训练-&gt;模型预测-&gt;模型评估-&gt;模型发布等流程用 DAG 串联起来执行。\n而使用之前的 Azkaban 调度系统进行开发时，他们遇到了一些挑战：\n挑战1：开发模式繁琐，需要自己打包脚本和构建 DAG 依赖，没有 DAG 拖拽的实现；\n挑战2：组件不够丰富，脚本编写的 AI 组件不够通用，需要重复开发，且易出错；\n挑战3：单机部署，系统不稳定，容易出故障，任务卡顿容易导致下游任务都无法进行。\n03 切换到DolphinScheduler\n在旧系统中踩坑无数后，荔枝机器学习团队，包括推荐系统开发工程师喻海斌、林燕斌、谢焕杰和郭逸飞，决定采用 DolphinScheduler。喻海斌表示，他们团队调度系统的主要使用人员为推荐算法 &gt; 数据分析 &gt; 风控算法 &gt; 业务开发（依次减弱），他们对调度系统了解不多，所以对于一个简单易用，可拖拉拽实现的调度系统的需求比较大。相比之下，DolphinScheduler 完美切合了他们的需求：\n1、分布式去中心化架构及容错机制，保障系统高可用；\n2、可视化 DAG 拖拽 UI，使用简洁方便，迭代效率高；\n3、拥有丰富的组件，且可以轻松开发集成自己的组件；\n4、社区活跃，无后顾之忧；\n5、与机器学习平台运行模式非常接近，使用 DAG 拖拽 UI 编程。\n04 应用案例\n选型 DolphinScheduler 之后，荔枝机器学习平台在其之上进行了二次开发，并将成果应用于实际业务场景汇总，目前主要是在推荐和风控场景。其中，推荐场景包括声音推荐、主播推荐、直播推荐、播客推荐、好友推荐等，风控场景包括支付、广告、评论等场景中的风险管控。\n在平台底层，荔枝针对机器学习的五大范式，获取训练样本、数据预处理、模型训练、模型评估、模型发布过程，进行了扩展组件优化。\n一个简单的xgboost案例：\n\n\n\n1、获取训练样本\n目前还没有实现像阿里 PAI 那样直接从 Hive 选取数据，再对样本进行 join，union，拆分等操作，而是直接使用 shell 节点把样本处理好。\n2. 数据预处理\nTransformer&amp;自定义预处理配置文件，训练和线上采用同一份配置，获取特征后进行特征预处理。里面包含了要预测的 itemType 及其特征集，用户 userType 及其特征集，关联和交叉的 itemType 及其特征集。定义每个特征预处理的 transformer 函数，支持自定义transformer 和热更新，xgboost 和 tf 模型的特征预处理。经过此节点后，才是模型训练真正要的数据格式。这个配置文件在模型发布时也会带上，以便保持训练和线上预测是一致的。这个文件维护在 DolphinScheduler 的资源中心。\n\n\n\n3. Xgboost 训练\n支持 w2v、xgboost、tf模型训练组件，训练组件先使用 TensorFlow或 PyTorch封装，再封装成 DolphinScheduler 组件。\n例如，xgboost 训练过程中，使用 Python封装好 xgboost训练脚本，包装成 DolphinScheduler 的 xgboost 训练节点，在界面上暴露训练所需参数。经过“训练集数据预处理”输出的文件，经过 hdfs 输入到训练节点。\n\n\n\n4. 模型发布\n发布模型会把模型和预处理配置文件发到 HDFS，同时向模型发布表插入记录，模型服务会自动识别新模型，进而更新模型，对外提供在线预测服务。\n\n\n\n喻海斌表示，基于历史和技术原因，目前荔枝还没有做到像阿里 PAI 那样真正的机器学习平台，但其实践经验已经证明基于 DolphinScheduler 可以做到类似的平台效果。\n此外，荔枝还基于 DolphinScheduler 进行了很多二次开发，让调度系统更加符合实际业务需求，如：\n\n上线工作流定义的时候弹窗是否上线定时\n增加所有工作流定义的显示页面，以方便查找\na) 增加工作流定义筛选并跳转到工作流实例页面，并用折线图表示其运行时长的变化\nb) 工作流实例继续下潜到任务实例\n运行时输入参数，还可以进行配置禁用任务节点\n\n05 基于调度的机器学习平台实现或将成未来趋势\n深度学习是未来的大趋势，荔枝也针对深度学习模型开发了新的组件，目前已完成 tf 整个流程，后续在开发的还有 LR 和 GBDT 模型相关组件。后两者相对简单，上手更快，一般的推荐场景都可以使用，迭代更快，实现之后可以让荔枝机器学习平台更加完善。\n荔枝认为，如果调度系统可以在内核稳定性，支持拖拽 UI，方便扩展的组件化，任务插件化，以及和任务参数传递方向上更加出色，基于调度系统实现机器学习平台，未来有可能会成为业界的普遍做法之一。\n06 期待就是我们前进的动力\n用户的期待就是 DolphinScheduler 前进的动力，荔枝也对 DolphinScheduler 的未来发展提出了很多具有指导性意义的建议，如期待 DolphinScheduler 未来可以优化插件机制，精减成一个专注于调度的“微内核”，并集成组件插件化，同时支持 UI 插件化，后台开发可以定制插件的 UI 界面。此外，荔枝还希望 DolphinScheduler 能够优化MySQL 表维护的“字典管理”，包含 key，value，desc 等主要字段，方便用户把一些易变量维护在字典表中，而不是维护在 .properties 文件里，这使得配置可以运行时更新，比如某个阈值，避免新增组件带来的配置参数放在 .properties 文件里导致的臃肿和不易维护等问题。\n最后，荔枝也表达了对 DolphinScheduler 寄语的期待，希望社区发展越来越好，探索更加广阔的领域，在海外也能取得更好的成绩，实现可持续的商业化开源！\n",
    "title": "荔枝机器学习平台与大数据调度系统“双剑合璧”，打造未来数据处理新模式!",
    "time": "2021-11-23"
  },
  {
    "name": "Master_Quartz_Time_Management_and_Task_Scheduling_Expert_in_Java",
    "content": "Quartz，一个在Java应用中无可替代的时间管理与任务调度库，以其独特的功能和灵活性广受开发者青睐。得名于“石英”，Quartz不仅象征着时间的精确性，也体现了其在任务调度领域的核心地位。本文将深入剖析Quartz的内部工作原理和最佳实践，帮助开发者更好地理解和运用这一强大的库。\n\nQuartz的核心特性\n\n作为库的身份\n不同于独立部署的应用程序，Quartz以库（library）的形式存在，方便地嵌入到任何Java应用中。\n专为Java设计\n作为一个纯Java编写的库，Quartz专门服务于Java生态系统，与Java应用的集成无缝且高效。\n灵活的部署能力\n从单机应用到大型分布式系统，Quartz均能提供稳定的任务调度支持。\n低依赖性\nQuartz的设计注重独立性，几乎不依赖外部的框架或库，确保了其高度的自给自足。\nQuartz的实现机制\n关键组件\n\nJob: 表示任务本身，包括任务名称、组别及其具体执行逻辑。\nTrigger: 定义任务触发规则，例如执行的频率和条件。\nScheduler: 调度器，负责安排和执行Job。\n\n使用流程\n\n\n创建Scheduler: 通过工厂方法初始化Scheduler实例。\n配置任务: 将Job与相应的Trigger配置到Scheduler中。\n启动调度: 激活Scheduler以开始任务的执行。\n\n特别提示\n尽管未启动的Scheduler不执行任务，但其内部线程仍会持续轮询，这是其设计上的一种谨慎考虑。\n与SpringBoot的集成\n在SpringBoot环境下，Quartz展现了更高的灵活性和便利性：\n\n参数传递方式: 在Spring环境中，Quartz的Job不仅可以通过Map接收参数，还可以直接注入Spring容器管理的对象。\n任务调度灵活性: 支持多种Trigger配置，可实现从简单到复杂的各类调度需求。\n\n总体而言，Quartz为Java应用带来了强大而灵活的任务调度能力，无论是简单的单次任务还是复杂的分布式定时任务，都能轻松应对。\n高级特性\n\nCorn Trigger\nQuartz广泛应用的Corn Trigger允许通过Cron表达式定义复杂的调度规则，实现精细化的任务调度。这种灵活性使Quartz成为处理定时任务的首选工具。\n日历触发器\n除了Corn Trigger，Quartz还支持基于日历的触发器，提供更多样化的调度选项，满足特定时间安排的需求。\n触发器与作业依赖关系\n在Quartz中，触发器（Trigger）与作业（Job）的关系是核心设计。一种常见的设计模式是一对一关系，其中一个Trigger对应一个Job。这种设计简化了调度逻辑，并提高了容错能力。\n集群支持\nQuartz通过集群支持提升执行效率，尤其在大规模的应用场景中表现突出。集群模式允许多个应用实例共同参与任务调度，优化资源利用。\nQuartz架构\n\n部署模型\nQuartz设计为嵌入式库，可以轻松集成到Java应用中。其核心依赖于作业存储（Job Store），并支持多实例部署。\n核心模块\nQuartz的代码结构简洁，主要由核心模块和少量扩展模块组成。核心模块不仅代码量适中，而且由专人维护，保证了代码质量和一致性。\n监听器\nQuartz提供两种类型的监听器：作业监听器和调度器监听器。这些监听器可以回调状态变化，如作业执行情况和调度器状态，为外部监控和日志记录提供便利。\n架构组件\n\nQuartz的架构可分为以下几个主要部分：\n\n监听器（Listeners）: 用于监控作业和调度器的状态变化。\n调度器（Scheduler）: 调度器是Quartz的心脏，负责任务调度和执行。\n触发器（Triggers）: 定义任务的触发规则。\n作业（Jobs）: 表示具体的任务实现。\n作业存储（Job Stores）: 存储作业和触发器的信息。\n\n通过这些组件，Quartz能够提供灵活而强大的任务调度解决方案，适应从简单到复杂的各种业务场景。\n核心类\nQuartzSchedulerThread\nQuartzSchedulerThread是Quartz的心脏，负责轮询数据库，查找当前可执行的Trigger。由于数据库本身不会主动推送数据，Quartz采用拉取方法，即定期查询数据库以确定任务执行的时机。\nSimpleThreadPool\nSimpleThreadPool是Quartz的线程池实现，用于执行Job。与常见的线程池不同，它没有等待队列。线程池的大小直接决定了可以并行执行的任务数量。若所有线程都在忙，新的Trigger必须等待直到有空闲线程。\n工作机制\n触发器与作业关系\nQuartz中，Trigger与Job的关系是一对一的。一个Trigger只能对应一个Job，而一个Job可以有多个Trigger。这种设计简化了调度过程，确保了系统的容错性。\n集群模式\nQuartz的集群模式旨在提高任务执行效率。通过数据库锁，保证同一时刻只有一个实例可以访问特定Trigger。这种机制在系统规模庞大时显示其优势，通过集群部署，不同实例可以并行处理任务。\n触发器状态处理\nTrigger状态的处理是Quartz调度过程中的关键环节。一旦Trigger被触发，它会被标记为“已获取（Acquired）”，并生成一个Trigger实例表示任务正在执行。任务完成后，该实例会被删除，从而无法追踪历史执行记录。\n调度流程\n步骤1：触发器检索\n\n首先，Quartz会查询数据库，确定当前可执行的Trigger列表。这一步涉及到复杂的SQL查询，考虑到执行时间和状态。\n步骤2：触发器处理\n\n处理步骤包括设置Trigger状态为“已获取”，并在数据库中创建相应的Trigger实例。\n步骤3：任务执行\n\n将Trigger关联的Job提交到线程池进行执行。\n步骤4：触发器实例清理\n\n任务执行完成后，对应的Trigger实例将被删除，这意味着无法从数据库中追溯任务的历史执行。\n总结\nQuartz的核心类和工作机制共同构成了一个高效且灵活的任务调度系统。尽管Quartz在设计上注重简洁和性能，但它的确切实现细节和架构选择显示出其在处理复杂任务调度场景中的强大能力。\n总的来说，Quartz作为Java中的时间管理与任务调度专家，提供了一套全面且高效的解决方案，以应对各种复杂的调度需求。其灵活的配置选项、与SpringBoot的无缝集成以及强大的集群支持，使其在Java应用中的任务调度领域独树一帜。对于希望提高应用效率、优化资源分配和扩展应用功能的Java开发者而言，掌握Quartz的使用无疑是提升开发能力的关键步骤。\n",
    "title": "精通Quartz：Java中的时间管理与任务调度专家",
    "time": "2023-12-25"
  },
  {
    "name": "Practical_development_and_application_of_Apache_DolphinScheduler_in_DDS",
    "content": "\n​\n这次在 7月 Meetup 为大家带来的是基于DolphinScheduler的智能调度引擎在DDS的应用，这场演讲主要会跟大家介绍宇动源-DDS（自研的图形化数据开发工作室）、大数据架构、DDS产品和使用中遇到的问题，包括在迁移过程中的调研情况、遇到的困难、解决方案以及针对需求的优化，还有一些心得体会，希望你有所收获。\n​\n王子健\n宇动源大数据平台开发工程师\n原搜狐畅游数据仓库开发工程师\n本次演讲主要包含四个部分：\n\n\n关于宇动源-DDS\n\n\n全新调度引擎的迁移-DS\n\n\n引擎功能优化\n\n\n总结与计划\n\n\n关于宇动源-DDS\n\n😀​宇动源DDS架构图\n01 什么是BDP？\nBDP是宇动源自研的大数据基础平台，类似的商业的应用主要有fusioninsget 和 EMR，都是在工业互联网领域比较领先的大数据平台，他们也都对现有开源大数据底层组件的封装和统一管理，使其更适用于工业领域的实时数据、时序数据、生产监控数据等，为DDS上层应用提供一个使用更方便、更容易使用的基础平台。\n主要提供的功能：统一计算调度及管理、计算节点管理、存储节点管理、数据统一访问接口、统一权限控制、全局智能运维\n有了BDP之后，我们不需要手动的安装/维护这些组件，包括我们DDS所有的组件也都是通过BDP进行安装和维护。\n02 关于DDS\nDDS实际上是对底层的调度引擎的优化，主要是使用的workflow code 和Stream code 的两种开发方式，利用拖拉拽快速完成大数据开发。\n除此之外还支持Notebook 交互式开发方式，当我们使用Shell SQL节点的时候可以在线编辑。\n最终在执行过程，其实和我们平时使用DolphinScheduler一样，在运行后，也会生成一个调度任务，并且可以在调度管理中查看/管理。\n03 缺点与不足\n案例一：某能源集团火力发电远程告警系统\n问题：基于上一代调度引擎在性能、权限控制等方面存在缺陷，导致出现告警延迟、无法精准控制权限等问题。\n案例二：某石油加工集团云边端设备采集项目\n问题：DDS与Hadoop架构紧密耦合，不仅前期需要额外部署Hadoop集群（即使客户不使用），后期Hadoop集群维护也增加了运维成本。\n案例三：某军部数字化建设项目\n问题：无法对整个集群的运行状态进行统计，比如1000个任务，在运行过程中，我们不知道哪些任务是正常运行的，哪些任务挂掉了；\nDDS中老调度引擎无法获取日志（日志的粒度依赖oozie 的日志）从而增加了调试维护成本，在开发过程中，命令行的方式也不是很友好，XML配置文件容易出错，开发效率低，无法更全面的统计与维护。\n综上所诉的这些缺点和问题，导致了我们需要迭代一个新的版本，在DDS的后续版本准备对调度引擎进行替换，来解决这些使用当中的痛点。\n全新调度引擎的迁移\n01调研阶段\n接下来跟大家分享的就是在迁移过程当中，我们的一些调研工作、遇到的问题以及解决方案。\n我们调研了oozie，azkaban，Airflow， 还有DolphinScherduler，当然还有一些其他的调度，这个表格里就没整理，在整个调研的阶段，我们特别重视的指标主要是资源的监控和分布式和日志，还有权限等方面的控制。\n\n​\n为什么选择DolphinScheduler ？\n资源方面，DolphinsScheduler 可以有自己的数据中心，可以对master worker运行状态和指标实时监控；\n分布式方面，支持HA，是去中心化的，支持多Master多worker；\n**日志方面，**通过DolphinsScheduler我们能准确的定位到每一个task任务执行的日志。也可以监控在执行过程当中启动时间，停止时间，然后运行的机器所在的运行阶段；\n容错机制也很有特色；\n支持多租户。DolphinsScheduler的租户能和Hadoop的用户能实现映射关系，实现对资源的精确管理；\n\n​\n02 引擎接入方案(dds-adaptor)\n接入方案-典型问题\n在迁移的过程当中，我们遇到了一个典型的问题：DolphinScheduler中的描述文件（json）无法与DDS的工作流描述文件（xml）兼容。就是DDS中的工作流是用xml的格式来绘制描述的。而DolphinScheduler中是使用json格式来描述的，这样就会导致于现有的前端生成的XML，无法兼容DolphinScheduler。\n问题解决方案\n我们在DDS和DolphinScheduler中间，加入了DDS-adaptor服务，它有一个自己的API，DDS前端绘制完工作流，会将XML格式的请求体，通过API接口的形式发送到parser-engine，parser-engine收到了带有XML参数的请求后，会将它解析成JSON格式 ，当然这个JSON就是我们DS引擎能解析和兼容的。这样就解决了保留DDS前端框架不变的情况下，将原有的XML描述方式，适配到DolphinScheduler的json方式。\n这样也就实现了我们在DDS前端创建task 任务，在DS的后端会生成DolphinScheduler task 规则的任务。\n也就是说可能有个客户DDS使用了几年。上面配了几千个任务，用这个架构接入方案它就不需要重新的做任务的迁移，可以无缝的实现底层调度引擎的迁移。\n03 实现简介\nDDS_adaptor架构\nDDS_adaptor项目里边包含两个子项目，一个dsengine-core和external-model，那么前面说的API是在controller层里，当我们接收到xml的请求，会通过engine-core去对它进行解析。\n具体的实现\n\n​\n实现流程：\n1.前端先发送XML文件到HTTP接口\n2.HTTP接到之后会创建一个解析工厂也就是SchedulerParserFactory\n3.解析工厂生成一个IParser接口\n4.然后实现Dds2DolphinModelParser，也就是模型解析实现类\n5.其中parser方法实际上是XML格式的字符串，里面调用的是核心方法、核心类\n6.DdsDolphinModelAdaptor，即模型适配器。在接收parser传进来的参数后会有两个方法，分别是from和.to，from是将DDS的Graph转换成中间对象的Graph。.to是将中间对象的Graph转换成Dolphin的Graph。\n7.创建SystemConvertDepartmentFactory，它同时会创建DDS转换成Dolphin特有的转换方法，分别有3个：\n方法一：\nINodeConvertercreateNodeConverter（nodeType）：创建节点转换器\n方法二：\nAdaptorNodeTypeconvertNodeType2Adaptor(nodeType) ：目标节点转为中间节点\n方法三：\nStringconvertAaptor2NodeType(AdaptorNodeType)：中间节点转为目标节点\n在第一步中需要将DDS Graph 转为 Adaptor Graph ，这个实际上执行的时是AdaptorNodeType convertNodeType2Adaptor(nodeType) 方法，将Graph做转换，\n但是每个Graph 中包含很多组件，因此每个组件都需要进行转换，需要调用到的方法是\nconvertAdpator2Node(AdaptorNode , OutputNode)\nconvertNode2Adaptor(AdaptorNodeBuilder,InputNode)，针对每个组件进行转换，实现了将DDS Graph 转为 Adaptor Graph。\n第二步是中间对象转换成目标对象，我们想要的是Dolphin的Graph，这个时候实际上就是将中间组件转换成目标组件，这样就实现了中间的Graph转换成目标Graph，中间的Graph就是Adaptor-Garph，目标组件就是Dolphin-Graph，这样就实现了前端以XML的方式发送，引擎解析出来是Dolphin支持的json格式\n引擎功能优化（更智能）\n主要介绍扩展的两个功能，第一个是节点推荐机制，第二个是重跑控制策略。\n节点推荐机制\n\n​\n在DDS调度启动之前，可以进行一个调度策略的设置，分别有一个分配策略和推荐节点；\n在分配策略的时候，我们可以根据实际情况来选择，比如你是采集任务还是计算任务？是让它网络优先，还是IO优先，在这里你可以根据不同的情况自定义选择。\n时间架构\n\n​\n这里没有展示时间关系代码，主要依赖是基于DDS-monitor。在每一个子节点上部署 agent，那么agent它会采集系统的各种信息，比如说我们的系统版本，CPU，内存，磁盘等。\nagent会将这些信息上报给server端，sever端将这些指标全部存在时序数据库里，数据库主要有两个用途，一个是我们监控数据，我们可以监控每一个agent上面的信息，另一个就是分析计算，指标分析计算后，根据我们的策略规则，会将符合功能策略的节点推荐返回给server，从而实现了节点的推荐机制。\n分析指标\n我们指标分析计算里面涉及到的一些指标，有系统版本、CPU、内存、磁盘、进程、端口、网络io等等\n\n​\n那么配置规则里有IO相关的配置，有符合我们特定业务类型的配置。\n\n​\n重跑控制策略\n我们对调度的是否重跑可以进行控制。比如说当任务运行到一半的时候，节点重启，所有的任务都失败了，有一些任务可以直接进行重跑覆盖，但是有一些任务重跑会影响它的结果，所以对这些重跑的限制也是针对需求来进行优化的点。\n总结与计划\n一方面，不管是基于DolphinScheduler引擎去开发自己的调度系统，还是直接使用DolphinScheduler进行二次开发，都是需要根据自己的开发环境来实际操作，比如说公司的环境要求、规范、前后端技术栈、开发环境等等，不要拿过来就开发，需要多方面的考虑。\n另一方面，我们在devops过程当中需要收集和整理产生的这些缺陷、缺点和一些用户的痛点, 我们需要在这个版本的迭代当中，有针对性的去处理这些问题。\n我们后续也有很多功能在实现当中，给大家列举一下，比如说后续的Task组件节点之间结果集的传递、SQL 运行后传递给后续节点等，然后还需要集成一些热门的时序数据库，比如说influxdb、open plant等。\n等实现之后希望还有机会和大家一起交流分享，谢谢大家。\n",
    "title": "小海豚“变身”全新智能调度引擎，深入浅出在DDS的实践开发应用？",
    "time": "2023-9-6"
  },
  {
    "name": "Qifu_Technology_The_practical_path_of_big_data_tasks_from_diagnosis_to_self-healing",
    "content": "一、为什么要做诊断引擎\n毓数平台是奇富科技公司自主研发的一站式大数据管理、开发、分析平台，覆盖大数据资产管理、数据开发及任务调度、自助分析及可视化、统一指标管理等多个数据生命周期流程，让用户使用数据的同时，挖掘数据最大的价值。而毓数平台的大数据任务调度底层是基于Apache DolphinScheduler实现的。\n整个大数据平台有1000+机器、70P数据量，每日新增200T数据。每天在毓数工作流上运行的任务实例有13万+，周活跃用户400+；每天在毓数自助查询中运行的sql有16万+，周活跃用户500+。运行的任务类型有Spark任务、Sqoop任务、DataX任务等10多种任务类型。\n而我们的几百位业务同学对大数据框架底层原理几乎都不太了解，因此日常会需要数据平台部门同学协助业务去分析和排查大数据任务运行问题。数据平台部门同学每天都会花费很多时间和精力在任务人工诊断上，而相应业务同学面对异常任务也会很苦恼。每个月数据平台部门协助用户诊断任务问题平均耗费4人/天工时。\n对于，异常任务，让我印象很深刻的一件事情是，23年有一位业务同学在群里询问他的sql为啥报错。将业务的Sql简写后异常如下图所示。\n\n然后过了几分钟另外一个业务同学在群里回复他说：“你多写了一个库名”。这个现象让我发现，虽然我们数据平台的同学认为这个问题很简单，很容易排查。但是当业务的sql比较复杂并且业务对写sql不太熟悉时，就会经常被异常困扰几分钟甚至更长时间，不能快速解决sql的异常。\n而且sql语法异常可能大家排查和解决比较简单，而spark sql的数据倾斜、数据膨胀、OOM等异常需要业务登录spark ui去排查。这样对业务简直就太困难了，因此我们决定做一款面向自助查询和工作流的大数据任务实时诊断引擎。\n二、诊断引擎需求分析\n（一)、诊断所有大数据任务\n我们理想中诊断引擎必须支持毓数平台中运行的所有大数据任务。前面我们说过，我们工作流有Spark任务、Sqoop任务、DataX任务等10多种任务类型；而自助查询支持mysql、tidb、doris、tidb、spark等数据源查询。因此诊断引擎必须能够对所有大数据任务进行诊断。\n(二)、实时看见诊断结果\n诊断引擎必须实时产出诊断报告，我们不太能接受用户看见了异常点击诊断按钮，然后旋转几圈才能看见诊断报告（也可能看不见报告）。也不能接受用户需要从A平台跳转到B平台去看诊断报告。\n用武侠小说里的一句话，“毒蛇出没之处，七步之内必有解药”。必须让用户在看见异常1秒钟内看见所对应的诊断报告。让用户使用毓数平台的诊断体验达到最好。\n其次，通过自助查询提交的Spark任务虽然还在运行中，没有失败，但是从Spark指标或者日志中已经发现了数据倾斜、数据膨胀、OOM等。也应该在任务提交处，及时弹出诊断报告。达到任务还没有失败时，诊断报告已经产出。最终实现异常与诊断报告实时产出。\n(三)、诊断规则易扩展\n诊断引擎的诊断规则新增流程必须快速，不能通过发版的方式，例如每周发布一次。如果诊断规则，每周发布一次，对于我们快速帮助业务解决问题而言，太慢了。\n因此，我们对于诊断规则发布和生效要求在分钟级。开发、测试好后的诊断规则在1分钟内就能发布到生产环境进行诊断。\n(四)、诊断规则灰度发布\n诊断引擎的诊断规则必须支持灰度发布，在生产环境运行一段时间后，效果评估没有问题后，用户才能在任务提交入口处看见诊断报告。\n(五)、支持阈值实时调整\n比如我们的诊断规则有Spark大任务、Spark 小文件。那么多少Spark的task达到多少阈值才算命中Spark大任务？平均写出HDFS多少字节算是小文件？这些阈值可以脱离诊断规则，配置在常量表中。同样修改这些阈值配置，支持分钟级生效。\n三、诊断引擎架构剖析\n（一）、大数据任务提交入口\n诊断元数据采集的入口有工作流、自助查询，但是未来也不排除需要支持Linux客户端、Jupyter等提交任务入口。\n\n（二）、大数据诊断元数据类型\n需要支持的大数据诊断元数据有log、metrics，未来也不排除支持Trace等数据。\n\n（三）、诊断引擎架构\n以下是我们诊断引擎架构图，我们会实时收集Apache DolphinScheduler worker上所有任务产生的日志到Kafka中。\n我们也会和我司自研的自助查询系统合作，自助查询系统也会实时发送用户提交任务的日志到Kafka中。其次，我们重写了Spark的metric sink，实现将运行的Spark任务metric实时发送到kafka中。\n我们自研了一个基于Flink和Janino的规则引擎，通过Flink消费Kafka的数据，并对数据进行实时诊断。而Flink引擎每隔1分钟会将存在mysql中新增的java代码交给Janino编译成字节码，从而实现了规则实时加载和实时生效。\n规则引擎实时加载规则，并对kafka中的数据进行实时诊断，将诊断结果写入mysql中。而工作流或者自助查询系统调用接口查询mysql中是否有诊断报告，并对用户展示诊断报告。\n\n（四）、诊断引擎与开源对比\n以下是我司自研的大数据诊断引擎与OPPO开源的罗盘对比。我觉得我们的诊断引擎在易用性、实时性，规则灰度、任务自愈、大数据任务类型等方面是优于罗盘的。不过我们在诊断规则深度方面还不如罗盘，比如Spark CPU浪费、内存浪费等方面，我们还没有开发规则。\n不过后续，我们也可以实现这些资源浪费规则，目前精力主要做的是异常覆盖率提升、还有调度自愈等。\n\n\n\n功能\n毓数诊断引擎\nOppo罗盘\n\n\n\n\n实时诊断\n支持\n不支持\n\n\n易用性\n容易\n困难\n\n\n参数调优\n支持\n支持\n\n\n自助查询\n支持\n不支持\n\n\n诊断规则实时加载\n支持\n不支持\n\n\n诊断规则灰度\n支持\n不支持\n\n\n一键调优\n规划中\n不支持\n\n\n任务自愈\n支持\n不支持\n\n\n\nOPPO罗盘：https://github.com/cubefs/compass\n四、诊断引擎实现\n一）任务元数据采集\n\nPush：有侵入性、运维简单、容易丢数据、适应性好。一般适用于任务类采集。\nPull：没有侵入性、运维复杂、不容易丢数据、适应性差。一般适用于服务类采集。\n在对工作流任务运行的log采集上，我们采用了push方式主动上报任务运行日志到kafka中。\n不采用pull方式的原因是，worker上运行的任务log写到了磁盘很多个文件中，而通过filebeat等方式采集磁盘log文件很难实现。\n\n在这里，可以将taskAppId理解为分布式跟踪中的TraceId，每条log都需要携带一条TraceId，通过TraceId和工作流或者自助查询等关联起来。\n对自助查询也是，自助查询提交的sql运行的日志也会实时push到kafka中。通过解析自助查询发送到kafka中log携带的TaskAppId，就能关联到自助查询每个Query的唯一的ID。\n\n而采集Spark运行的指标，因为我们要做到实时诊断，因此没有采集Spark history的数据，而是实现一个spark metric kafka sink。这样随着启动Spark任务，便会将Spark任务运行的metric实时上报到kafka中。\n{\n    &quot;app_name&quot;: &quot;SPARK&quot;,\n    &quot;sparkMetricReportVersion&quot;: &quot;v1&quot;,\n    &quot;applicationUser&quot;: &quot;hive&quot;,\n    &quot;dataCenter&quot;: &quot;xx&quot;,\n    &quot;ip_address&quot;: &quot;xx&quot;,\n    &quot;metrics&quot;: [\n        {\n            &quot;metric&quot;: {\n                &quot;metricLabel&quot;: {},\n                &quot;metricName&quot;: &quot;driver_ExecutorAllocationManager_executors_numberExecutorsDecommissionUnfinished&quot;,\n                &quot;metricValue&quot;: 0\n            }\n        },\n        {\n            &quot;metric&quot;: {\n                &quot;metricLabel&quot;: {},\n                &quot;metricName&quot;: &quot;driver_ExecutorAllocationManager_executors_numberExecutorsExitedUnexpectedly&quot;,\n                &quot;metricValue&quot;: 0\n            }\n        },\n        {\n            &quot;metric&quot;: {\n                &quot;metricLabel&quot;: {},\n                &quot;metricName&quot;: &quot;driver_ExecutorAllocationManager_executors_numberExecutorsGracefullyDecommissioned&quot;,\n                &quot;metricValue&quot;: 0\n            }\n        }\n    ],\n    &quot;application_id&quot;: &quot;application_1691417751496_779447&quot;,\n    &quot;event_time&quot;: 1692272861862,\n    &quot;applicationName&quot;: &quot;liukunyuan-test&quot;,\n    &quot;sparkVersion&quot;: &quot;3.3.2&quot;\n}\n\n以上是Spark的metric上报的部分指标，发现数据无法满足我们诊断需求。而我们需要的是Spark UI中展示的metric数据。\n\n通过分析知道，既然Spark UI可以展示这些指标，那么这些指标必然存在Spark Driver中，只要我们从Driver中拿到这些指标就能发送到Kafka中了。后来我们从Spark源码中知道，Spark UI展示的这些metric存在于AppStatusStore数据结构中。因此我们从AppStatusStore中获取指标，并发送到kafka中。\n{\n    &quot;app_name&quot;:&quot;SPARK_METRIC&quot;,\n    &quot;sparkMetricReportVersion&quot;:&quot;v2&quot;,\n    &quot;applicationUser&quot;:&quot;xx&quot;,\n    &quot;dataCenter&quot;:&quot;xx&quot;,\n    &quot;ip_address&quot;:&quot;xx&quot;,\n    &quot;metrics&quot;:[\n        {\n            &quot;metric&quot;:{\n                &quot;shufflerecordsWritten&quot;:10353530,\n                &quot;taskKey&quot;:100,\n                &quot;localBytesRead&quot;:0,\n                &quot;attempt&quot;:0,\n                &quot;duration&quot;:3900003,\n                &quot;executorDeserializeCpuTime&quot;:5917691,\n                &quot;shufflewriteTime&quot;:0,\n                &quot;resultSize&quot;:4480,\n                &quot;host&quot;:&quot;xx&quot;,\n                &quot;peakExecutionMemory&quot;:2228224,\n                &quot;recordsWritten&quot;:0,\n                &quot;remoteBlocksFetched&quot;:0,\n                &quot;stageId&quot;:1,\n                &quot;bytesWritten&quot;:0,\n                &quot;jvmGcTime&quot;:0,\n                &quot;remoteBytesRead&quot;:3221225479,\n                &quot;executorCpuTime&quot;:16868539,\n                &quot;executorDeserializeTime&quot;:5,\n                &quot;memoryBytesSpilled&quot;:0,\n                &quot;executorRunTime&quot;:16,\n                &quot;fetchWaitTime&quot;:0,\n                &quot;errorMessage&quot;:&quot;&quot;,\n                &quot;recordsRead&quot;:0,\n                &quot;shuffleRecordsRead&quot;:1035353,\n                &quot;bytesRead&quot;:0,\n                &quot;remoteBytesReadToDisk&quot;:0,\n                &quot;diskBytesSpilled&quot;:0,\n                &quot;shufflebytesWritten&quot;:32212254,\n                &quot;speculative&quot;:false,\n                &quot;jobId&quot;:0,\n                &quot;launchTime&quot;:1692783512714,\n                &quot;localBlocksFetched&quot;:0,\n                &quot;resultSerializationTime&quot;:0,\n                &quot;name&quot;:&quot;task&quot;,\n                &quot;taskId&quot;:100,\n                &quot;status&quot;:&quot;SUCCESS&quot;\n            }\n        }\n    ],\n    &quot;application_id&quot;:&quot;application_1692703487691_0205&quot;,\n    &quot;event_time&quot;:1692783655749,\n    &quot;applicationName&quot;:&quot;xx&quot;,\n    &quot;sparkVersion&quot;:&quot;3.3.2&quot;\n}\n\n（二）为何选择Janino\nJanino是一个超小、超快的开源Java 编译器。Janino不仅可以像javac一样将一组Java源文件编译成一组字节码class文件，还可以在内存中编译Java表达式、代码块、类和.java文件，加载字节码并直接在JVM中执行。而我们可以通过Janino动态编译java代码，从而实现一个轻量级规则引擎。\n\n\n\n测试项\nJanino\nJava\n\n\n\n\n一千万次耗时\n7753毫秒\n5077毫秒\n\n\n\n参考《揭秘字节跳动埋点数据实时动态处理引擎https://www.sohu.com/a/483087518_121124379》中：“Janino编译出的原生class性能接近原生class，是Groovy的4倍左右”。并且Flink大量使用Janino动态生成java代码。而我编写了一个诊断规则测试执行1千万次Janino和java耗时。发现的确如文章所说，Janino和Java性能差不多，因此选用Janino作为诊断规则的规则引擎。\n\n这样，规则代码就可以存放到mysql中，实现诊断规则动态加载。不过，需要注意的是，诊断规则编译这些java字符串代码时，需要md5存map中，通过md5值判断是否已经编译过，防止重复编译，否则Flink运行一段时间，就会OOM。\n（三）诊断数据模型设计\n对于诊断引擎数据模型应该怎样设计呢？我首先想到的是大数据任务诊断类似于医院体检。\n我们想象一下现实中，我们是如何体检的。首先，医生会采集体检对象的血液、身高、体重、X光检测等。这些采集到的数据，对应诊断系统中的metric指标，例如shuffle条数、gc耗时等。医生还会询问我们是否熬夜，最近是不是有哪里不舒服。我们会回答，没有不舒服的地方或者我胃不太舒服。而“胃不太舒服”就是对应诊断系统中的log。医生拿到指标和log后，会根据metric和log以及医生的诊断经验（诊断规则）生成诊断报告。而诊断报告有几个要素：疾病名字、疾病描述、命中的metric或者log、结论、药方。\n\n病类型\n\n病类型表：存储疾病类型，例如胃病；疾病描述，例如此病会影响食欲甚至危急生命。对应诊断引擎就是“语法错误”、“数据倾斜”等。\n\n药方\n\n优化建议表：存储了这个疾病通用的优化规则，例如应该吃什么药。对应诊断引擎就是“如何调整spark参数”、“在写库名时，不小心写成了 库名.库名.表名，这种情况下需要改写为 库名.表名”等。\n\n诊断规则\n\n诊断规则表：存储了诊断规则，例如如何看X光机拍摄的照片，或者分析抽血得到血液指标等。对应诊断引擎中java代码部分，运行诊断规则会生成诊断结论。\n\n诊断结论\n\n诊断结论表：存储了运行诊断规则得到的结论。我得了胃病，严不严重，并能关联到药方。\n对应诊断引擎就是运行java代码时分析metric或者log得到疾病类型、生成结论，并关联诊断建议（药方）。\n\n常量表\n\n常量表：常量表存储了诊断规则需要用到的一些阈值。例如高血压值是多少，低血压值是多少？对于诊断引擎就是数据倾斜多少倍数算倾斜？数据膨胀多少倍算是膨胀？平均字节数多少算是小文件？为什么不把阈值放诊断规则里呢？是因为考虑到诊断规则是多变的，因此把常用阈值放入常量表中，这样就不用修改诊断规则java代码就可以调整诊断逻辑了。\n五、大数据任务诊断应用\n（一）诊断效果概览\n1、诊断引擎生产应用情况\n\n\n\n指标\n值\n\n\n\n\n诊断规则数\n55条\n\n\n每日诊断log条数\n7500万\n\n\n每日诊断log次数\n36亿\n\n\n每日诊断metric条数\n440万\n\n\n每日诊断metric次数\n2640万\n\n\n每日诊断报告数\n5.5万\n\n\n\n我们共开发了55条基于log或者metric的诊断规则，每天诊断的log条数约为7500万条，每天对log诊断次数约为36亿次；每天诊断的metric条数约为440万条，每天对metric诊断次数约为2640万次；每天生成的5.5万份诊断报告。\n自从我们2023年10月份在工作流和自助查询上线了诊断引擎之后，平均减少了数据平台协助用户排查问题0.5人/天工时，工作流异常诊断覆盖率达到了85%，自助查询异常诊断覆盖率达到了88%，用户在服务群中询问问题的次数明显减少。\n2、工作流失败任务诊断效果\n以某日工作流诊断效果来看，工作流排除了依赖失败，共有2400多个失败任务。其中354个失败任务没有诊断报告，2062个失败任务生成了诊断报告，失败任务诊断报告命中率85.35%。\n\n\n\n规则名称\n失败任务数\n\n\n\n\n未命中诊断规则\n354\n\n\n依赖失败\n526\n\n\n处理文件失败\n464\n\n\n强规则不通过\n398\n\n\n语法错误\n188\n\n\n同步异常\n96\n\n\n内存溢出\n81\n\n\n表不存在\n81\n\n\nHDFS缺失\n73\n\n\n同步丢数据\n35\n\n\nSQOOP失败\n24\n\n\nBINLOG同步失败\n21\n\n\nDISTCP失败\n16\n\n\n数据膨胀\n13\n\n\n数据倾斜\n13\n\n\n发送邮件失败\n11\n\n\n表重复创建\n10\n\n\n权限不足\n6\n\n\n大任务\n5\n\n\nHDFS异常\n1\n\n\n\n3、工作流成功任务诊断效果\n诊断引擎是根据log和metric进行诊断，因此是没有区分成功任务还是失败任务。因此可以根据诊断命中情况对成功任务在未来运行情况进行预测。虽然任务运行成功了，如果任务发生了内存溢出、数据倾斜、数据膨胀等，可能也需要用户去关注。\n还有一种情况是，数据质量的弱校验规则，数据质量比对没有通过。因为是弱数据质量规则，并不会导致任务失败，不过也需要提示出来。还有“同步0记录”诊断规则，比如DataX将Hive表数据同步到Mysql时，如果同步了0条数据，虽然同步任务成功了，我们也会在工作流页面提示出来。\n\n\n\n规则名称\n命中成功任务数\n\n\n\n\n内存溢出\n512\n\n\nHDFS缺失\n445\n\n\n数据倾斜\n241\n\n\n同步异常\n201\n\n\n同步丢数据\n198\n\n\n数据膨胀\n114\n\n\n弱规则警告\n113\n\n\n同步0记录\n56\n\n\n大任务\n25\n\n\n尝试自愈\n2\n\n\n\n4、自助查询失败任务诊断效果\n自助查询失败的任务当日有2694个，而生成了诊断报告的失败任务有2378个，诊断命中率88.27%。业务在自助查询平台中提交的sql是在做探索性分析，大部分失败是语法错误，也是合理的。\n\n\n\n规则名称\n失败任务数\n\n\n\n\n未命中诊断规则\n316\n\n\n语法错误\n1713\n\n\n表不存在\n366\n\n\n分区不能为空\n115\n\n\n权限不足\n109\n\n\n表重复创建\n48\n\n\n内存溢出\n17\n\n\nHDFS缺失\n8\n\n\n数据膨胀\n2\n\n\n\n（二）同步异常规则\n诊断规则通过字符串匹配DataX抛出的异常日志判断是否命中。工作流在展示工作流实例时查询诊断命中表，并展示诊断结果。用户点击诊断结果会跳转到诊断报告。\n\n\n在诊断报告中会展示病症大类、ID（application_id）、病症描述、病症发生时间、命中的关键日志或者指标、病症结论以及诊断建议（药方）。比如在这个诊断建议中，就说明“数据同步，目前只支持orc格式，请创建orc格式的hive表”，用户就明白可以将自己的Hive表格式转为orc格式解决这个异常。\n通过这种方式，用户在查询工作流实例时，在看见实例失败的1秒钟内，就能找到诊断报告去解决这个异常问题，实现了“七步之内必有解药”。\n\n（三）语法错误规则\n语法错误在自助查询平台和工作流中都是很常见的异常，特别是自助查询平台提交的任务语法错误情况会非常多。因此语法错误这个异常大类会对应很多诊断规则，属于一对多的情况。\n比如以Spark中查询的字段列数量和和插入表的列数量不匹配这个情况举例。我们在诊断规则中匹配“requires that the data to be inserted have the same number of columns as the target table”，如果log中出现了这个字符串，将会命中我们的诊断规则。\n\n\n\n语法错误更多是发生在自助查询中，以让我印象很深刻的sql语句中多写了一个库名举例，我们现在也实现了在发生这种情况时自动弹出“毓智AI专家”按钮。业务点击按钮就可以看见这个异常对应的诊断报告和结论：“sql中库名写多了一个，库名叫:dp_data_db。错误的写法为dp_data_db.dp_data_db.。正确的库名写法为:dp_data_db.”。\n\n同样原理，我们在诊断规则中，通过多关键字匹配log，并从log中正则匹配出多写的库名。这样我们诊断结论能精确给出用户多写了哪一个库名。\n\n同样，我们再以group by和select 字段不匹配这个语法异常说明。当group by后面缺少biz_type时，spark会抛出异常。\n\n\n而用户点击诊断按钮，会看见我们诊断引擎给出的诊断报告：“group by语法有错误,group by中缺少该字段'spark_catalog.dp_data_db.lky_test.biz_type'”。在sql语句非常复杂时，该诊断能让用户快速定位语法错误问题。\n我们暂时只实现了14种“语法错误”诊断规则，后续还可以继续覆盖语法异常情况。\n\n（四）内存溢出规则\nSpark运行经常会遇到内存溢出，而我们也通过诊断规则判断log或者metric中是否有内存溢出相关关键字判断是否发生了内存溢出。\n\n\n\n我们对“内存溢出”这种情况，我们会给一下spark参数调优建议，例如“降低spark.sql.adaptive.shuffle.targetPostShuffleInputSize参数值.例如在sql前面加:set spark.sql.adaptive.advisoryPartitionSizeInBytes=67108864;”，或者“增加spark.sql.shuffle.partitions参数值.例如在sql前面加:set spark.sql.shuffle.partitions=1000;”等。\n用户根据诊断建议，也多次解决了spark的内存溢出问题。内存溢出诊断规则对于调度任务自愈也是必备的前置条件。后面我们会讲如何根据内存溢出结论对调度任务做自愈的。\n（五）数据膨胀规则\n在开发spark任务中，用户也经常因为数据产生膨胀导致运行失败问题。因此我们根据spark的metric输入和输出指标，判断数据膨胀倍数，并给用户解决数据膨胀情况的诊断报告。\n\n\n通过spark metric计算数据膨胀比根据log正则匹配复杂很多，以下是我们数据膨胀诊断规则流程图。\n\n可以发现我们诊断数据膨胀是非常复杂的，而通过Janino实现的诊断规则兼容java语法，因此可以非常方便的实现一些非常复杂的诊断规则。\n、\n（六）数据倾斜规则\n同样在开发spark任务过程中，我们也会经常遇到数据倾斜问题。我们也通过诊断规则实现了数据倾斜异常诊断并给出参数缓解该问题。\n\n\n六、大数据任务自愈应用\n（一）任务自愈目标\n我们公司的核心大数据任务必须保证按时产出，而大数据任务高峰期又是在凌晨。因此很多用户会在凌晨值班时被告警电话叫起来处理问题。因此，我们想针对失败任务做自愈功能，比如数据质量校验不通过自愈、spark oom任务凌晨自愈。举例，在用户感知不到情况下，DolphinScheduler的worker通过对spark oom失败任务添加内存从而自愈，最终减少用户凌晨起夜率。而白天的oom任务，我们暂时没有开启自愈功能。\n（二）任务自愈实现流程\n以spark oom自愈举例，以下是我们调度根据诊断引擎生成的诊断报告对oom任务进行自愈的流程图。\n\n调度系统对oom自愈会限制时间段、是否SLA（核心）任务、上一次是否因为oom导致失败、集群资源是否空闲、资源资源是否充足等进行判断。从而实现了在资源可控状态下对oom任务进行内存资源扩容，从而让任务尝试自愈。\n调度系统默默做了自愈，虽然让用户凌晨不用处理。但是，我们还是希望提示给用户，让用户白天处理，避免后续任务自愈失败。因此我们写了一个“尝试自愈”的诊断规则在用户查看工作流实例时提示给用户，后续也会在白天直接通过告警方式提示给用户。\n\n\n从诊断报告中我们可以看出该任务在凌晨3点前命中了“内存溢出”诊断规则，而调度系统自动对该任务的executor内存扩容到6144M，从而在05：31分运行成功。\n（三）任务自愈落地效果\n自从我们上线了oom任务自愈功能后，在生产环境取得了比较好的效果。对SLA（核心）任务和普通任务都开启自愈功能后，调度系统每天凌晨会进行10次左右“尝试自愈”操作。而只统计SLA(核心)任务，调度系统每周平均进行6次“尝试自愈”操作，减少了数仓同学67%的凌晨值班告警电话。\n七、诊断引擎展望\n（一）提升诊断覆盖率\n目前工作流异常诊断覆盖率能达到85%，自助查询异常覆盖率能达到88%。后续可以继续新增诊断规则提升异常覆盖率。理论上工作流异常诊断覆盖率可以很容易达到95%，自助查询异常覆盖率达到98%。\n（二）诊断分级\n目前异常诊断没有分级功能，用户在查看诊断时不知道哪些诊断是“严重”，需要立即解决，哪些诊断属于“警告”，可以晚一点解决。我们可以添加诊断分级功能，甚至评分功能，让诊断有优先级能力。\n（三）一键参数优化\n目前对于数据膨胀、数据倾斜、内存异常，我们有spark参数推荐。不过还需要用户手动修改工作流进行设置，后面我们想用户点击“一键优化”能够自动将优化参数应用到任务中。\n",
    "title": "奇富科技：大数据任务从诊断到自愈的实践之路",
    "time": "2024-01-29"
  },
  {
    "name": "Solution_to_the_incompatibility_problem_between_ZooKeeper_and_CDH_in_Apache_DolphinScheduler",
    "content": "背景\n看到Apache DolphinScheduler社区群有很多用户反馈和讨论这块问题，针对不兼容的问题，不仅需要自己重新编译各一个新包，而且因为默认是使用zk-3.8的配置，所以会出现不兼容问题。使用zk-3.4配置即可适配3.4.x\n解决办法（一）\n#切换到项目源码的根路径中执行\nmvn clean package -T 1C -Prelease '-Dmaven.test.skip=true' '-Dcheckstyle.skip=true' '-Dmaven.javadoc.skip=true' '-Dzk-3.4'\n\n上述命令解释\nmvn clean package  依次执行了clean、resources、compile、testResources、testCompile、test、jar(打包)等７个阶段。\n\n指定多线程编译，可以增加~\n拓展\n-Dmaven.compile.fork=true 表示开启多线程\nmvn -T 4 install -- will use 4 threads\nmvn -T 1C install -- will use 1 thread per available CPU core\nmvn clean package -T 1C -Dmaven.compile.fork=true\n-Prelease 是 Maven Release Plugin 的配置\n\nMaven中-DskipTests和-Dmaven.test.skip=true的区别\n在使用mvn package进行编译、打包时，Maven会执行src/test/java中的JUnit测试用例，有时为了编译过程中跳过测试步骤，会使用参数-DskipTests和-Dmaven.test.skip=true，这两个参数的主要区别是：\n-DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。\n-Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类。\n-D参数\n如果参数不存在于 pom.xml 文件中，它将被设置。如果参数已经存在 pom.xml 文件中，其值将被作为参数传递的值覆盖。\n\n解决办法(二）\n修改源码中的pom.xml配置文件\n1、从github下载源码\n直接访问https://github.com/，登陆之后搜索Apache DolphinScheduler！\n\n在百度直接搜：\n\n\n\n官网网址：\nhttps://github.com/apache/dolphinscheduler\n选择 release版本\n\n\n2、将下载好的zip包解压出来，并导入IDEA工具中\n\n\n3、修改maven和jdk配置\n\n\n4、MVN命令操作\n根目录执行\nmvn clean package -T 1C -Prelease '-Dmaven.test.skip=true' '-Dcheckstyle.skip=true' '-Dmaven.javadoc.skip=true' '-Dzk-3.4'\n报错如下\nFailed to execute goal com.diffplug.spotless:spotless-maven-plugin:2.27.2:check (default) on project dolphinscheduler: The following files had format violations:\n\n这个是问题没有因为没有格式化代码，所以在校验的时候不通过\n根据上面的提示只需要执行下命令：mvn spotless:apply 就可以了\n\n随后会刷屏，过一会就会出现如下图，完事了\n\n再次根目录执行\nmvn clean package -T 1C -Prelease '-Dmaven.test.skip=true' '-Dcheckstyle.skip=true' '-Dmaven.javadoc.skip=true' '-Dzk-3.4'\n报错如下\n\n从提示中可以看出来，com.github.eirslett：frontend-maven-plugin这个插件有问题\n首先看看本地的maven仓库中，有没有把这个插件通过依赖下载出来\n\n在d:\\IdeaProjects\\dolphinscheduler-3.2.0-release\\dolphinscheduler-ui\\pom.xml文件中\n把标签全都删除掉，防止构建时用npm的方式下载插件。\n\n再次根目录执行\nmvn clean package -T 1C -Prelease '-Dmaven.test.skip=true' '-Dcheckstyle.skip=true' '-Dmaven.javadoc.skip=true' '-Dzk-3.4'\n\n可以看到，已经编译成功了。找到打好包生成的目标文件，位置是：项目源码目录\\dolphinscheduler-dist\\target\n我的位置是 D:\\IdeaProjects\\dolphinscheduler-3.2.0-release\\dolphinscheduler-dist\\target\n\n5、修改源码中的依赖配置\n我不知道配置文件的指定的位置，在IDEA开发工具中按ctrl+shift+f组合键， 在项目所有文件中搜索：zookeeper.version\n\n在d:\\\\IdeaProjects\\\\dolphinscheduler-3.2.0-release\\\\dolphinscheduler-bom\\\\pom.xml文件中找到\n\n修改一下源码，修改后如下图所示\n\n根目录执行\nmvn clean package -T 1C -Prelease '-Dmaven.test.skip=true' '-Dcheckstyle.skip=true' '-Dmaven.javadoc.skip=true'\n\n\n可以看到，已经编译成功了。希望本文能帮助更多用户解决问题，如果您对这个话题感兴趣，欢迎来社区交流！\n",
    "title": "Apache DolphinScheduler 中 ZooKeeper与CDH 不兼容问题的解决方案",
    "time": "2024-02-18"
  },
  {
    "name": "The_implementation_practice_of_Apache_DolphinScheduler_as_a_unified_scheduling_center_in_Lenovo",
    "content": "点亮 ⭐️ Star · 照亮开源之路\nGitHub:https://github.com/apache/dolphinscheduler\n\n在 ApacheCon Asia 2022 Meetup上，来自联想的数据架构师、Apache DolphinScheduler PMC &amp; Committer 、Apache Local Community 北京成员 李岗 老师分享了 Apache DolphinScheduler 在联想作为统一调度中心的落地实践。\n感谢本文整理者 关博 对社区的贡献，您的贡献是社区不断前进的动力！\n本次演讲主要包含以下四个部分：\n\n统一调度中心的需求背景介绍\n为什么会选择 Apache DolphinScheduler 作为统一调度中心\nApache DolphinScheduler 在联想的落地实践中涉及的改造点\n后期规划(Roadmap)\n\n统一调度中心的需求背景介绍\n定时任务对每个开发人员来讲是一个常见的业务场景，我们通常都会遇到需要对数据进行备份、同步。一般的解决方式最开始会选用 Linux Crontab、基于 Spring 开发定时的任务，随着定时任务的需求越来越多，如何更好更方便的管理定时任务的生命周期，这时候统一调度系统的需求就呼之欲出了。\n需求收集\n在联想内部也是基于以上类似的背景需求，收集到公司其他业务部门一些迫切的需求，如下所示：\n支持丰富的任务类型的需求：\n\n\n支持定时通知任务。\n\n\n支持 ETL 任务。\n\n\n支持 HTTP 任务执行链。\n\n\n方便对任务进行监控、统一管理类的需求：\n\n\n对任务实时监控。\n\n\n管理多种任务类型。\n\n\n业务系统开发的定时任务方便统一管理。\n\n\n任务脚本便于管理和维护。\n\n\n保证多任务正确、准时触发类需求：\n\n\n任务调度需要保证可靠性。\n\n\n上下游依赖的任务需要事件通知触发机制。\n\n\n需求抽象汇总\n\n高可用：\n\n\nMaster 节点支持 HA 模式。\n\n\nWorker 节点支持分布式。\n\n\n支持丰富的任务类型和用户友好性能：\n\n\n支持多种任务类型。\n\n\n轻松定义复杂的任务依赖关系。\n\n\n轻量化：\n\n易操作，通过 UI 方便手动调度、手动暂停/停止/恢复，同时支持失败重试/告警、从指定节点恢复失败、Kill 任务等操作。\n\n支持业务隔离性：\n\n支持多租户。\n\n资源线性可扩展：\n\n随着业务任务接入越来越多，方便线性扩展集群，保证任务的性能。\n\n业务可扩展：\n\n便于业务系统二次开发。\n\n为什么会选择Apache DolphinScheduler 作为统一调度中心？\n通过以下点四点来介绍，我们最终选择 Apache DolphinScheduler 作为统一调度中心的原因：\n\n\n调度系统调研对比\n\n\nDolphinScheduler 核心功能介绍\n\n\nDolphinScheduler 架构设计\n\n\nDolphinScheduler 社区发展\n\n\n调度系统调研对比\n\n\n当时，我们通过调研 Apache DolphinScheduler、XXL-JOB、AirFlow，并基于以下几点选用了 Apache DolphinScheduler：\n\n\n高可靠，支持集群HA，集群去中心化。\n\n\n简单易用，DAG 监控界面，所有流程定义可视化，通过托拉拽任务定制 DAG，通过 API 与第三方系统对接，支持一键部署。\n\n\n丰富的使用场景，支持多租户、多种任务类型、支持暂定恢复等操作。\n\n\n高扩展，Master 和 Worker 支持动态上下线、支持自定义任务类型。\n\n\n社区活跃度高。\n\n\nDolphinScheduler 核心功能介绍\nDAG 编排\nDolphinScheduler 通过拖拽将 Task 以 DAG 的方式组装起来。通过 UI 可以实时监控任务的运行状态，同时支持重试、从指定的节点恢复失败、暂定以及 Kill 任务等操作。支持丰富的使用场景，提供近20多种任务类型。\n任务类型主要分为两个类：逻辑任务和物理任务，逻辑任务包含子流程、依赖节点、条件任务等。物理任务包含 Flink、Spark, Hive, M/R, Python、Shell 任务等。\n\n上图是工作流编排页面，保存之后会生成工作流定义。\n工作流定义\n工作流定义列表提供了编辑、上线、下线、定时、运行、删除等操作等操作功能。\n\n对某个工作流定义点击按钮，操作之后会生成工作流实例。\n工作流实例\n工作流实例中可以看到任务的运行类型、调度时间、运行时常、运行状态等。我们可以对正在运行工作流进行停止、暂停等操作，也可以对已经终止的流程进行重跑、删除等操作。\n\n每个工作流实例包含1个或者多个任务实例。\n任务实例\n点击工作流实例名称，可跳转到工作流实例 DAG 图查看任务状态，或者点击操作列中的“查看日志”按钮，可以查看任务执行的日志情况。\n\n2.X 版本新特性\n\n\n任务结果参数的传递。\n\n\n新增工作流血缘关系 UI，可以清晰的看到工作流的状态（在线、下线）。\n\n\n新增任务类型：Switch（分支节点）、SeaTunnel(原名WaterDrop）（ETL节点）。\n\n\n拆分工作流定义和任务之间的关系。\n\n\n新增工作流版本管理。\n\n\n\n\nApache DolphinScheduler 架构设计\n社区贡献\nApache DolphinScheduler1.2 架构\n\n在 1.2 版本中，Master、Worker 采用无中心设计，Master 主要负责 DAG 切分，任务的提交以及监控，监听与其他 Master 和 Worker 的健康状态以及容错处理。Worker 主要负责维护任务的生命周期管理，Worker 启动的时候会向 ZooKeeper 注册临时节点并维持心跳。\n1.2 架构的不足之处，在于任务队列基于 ZooKeeper 实现。Master 将任务数据存放到任务队列，Worker 通过分布式锁的方式去消费任务队列来执行任务，造成了延迟任务开始执行的时间。另外为保证任务队列的性能，ZooKeeper 节点中并未存储执行任务所需的全部数据。许多任务的元数据如租户，队列和任务实例信息等都需要由 Worker 操作数据库进行获取，对于复杂的工作流，时效性、任务的吞吐量、数据库压力都会成为调度系统的瓶颈。所以在 1.3 的架构设计中，我们着重考虑到减少 Worker 的压力，设计了如下新架构。\nDolphinScheduler 1.3 架构\n\n在 1.3版本中，Master 职能更加丰富，Worker 则更加专注于执行。\n任务队列基于 Netty 实现。Worker 去除数据库操作，只负责任务的运行，职责更单一。Master 和 Worker 直接通信进行任务分发，降低调度延迟。Master 多种策略分发任务，Worker 节点负载均衡策略：随机、轮询以及 CPU 和内存的线性加权多种负载均衡。\nDolphinScheduler 2.X 架构\n\n\n新增可扩展设计能力\n\n\n所有扩展点都采用 SPI 插件化实现，如：注册中心 SPI、数据源 SPI、任务插件 SPI、资源存储 SPI 、告警 SPI，SPI 设计保持简洁，不过多依赖第三方 JAR 包，各个插件保持独立。\n\n高性能：Master 重构\n\n2.0版本对 Master 进行了重构，主要优化点是去分布式锁和减少线程的利用。\n\n\n\nMaster 重构-去分布式锁方案\n\n\n\n\n在 1.0 的架构中多个 Master 共同工作，和 DB 交互需要借助分布式锁，导致并发会降低。2.0架构中，去掉了分布式锁，当 Master 上下线的时候会根据自己的分片编号用合适的算发计算出自己的 command 的槽位，Master 根据槽位查询数据库获得 command，并生成工作流实例，然后来构建 DAG，生成任务实例，提交执行任务。\n\n\n\nMaster 重构-线程池工作关系\n\n\n\n\n1.0 架构中的线程池使用比较多，每个工作流都会创建一个任务线程池，造成资源的大量浪费。2.0架构减少了线程池的使用。\nMasterSchedulerService 负责从 Command 表中分片获取 command，构建工作流实例，启动 WorkflowExecuteThread 处理，WorkflowExecuteThread 负责构建 DAG、DAG 切分、生成任务实例、提交到任务队列。同时负责处理任务状态和工作流状态变化。StateEventProcessor 负责接收 Master 和 Worker 发过来的任务状态和工作流状态变化事件，并提交 WorkflowExecuteThread 处理状态，StateWheelExecuteThread 负责任务或者工作流的超时监控。\nApache DolphinScheduler 社区发展\n\n通过以上关键性的指标可以看到从进入孵化器、从孵化器毕业，一直到现在，DolphinScheduler 在持续不断地健康发展。\n为什么选择DolphinScheduler？\n基于以上的几点，我们最终选择了 Apache DolphinScheduler，以下特性非常符合我们的业务需求：\n\n\n支持复杂的依赖调度。\n\n\n支持任务失败重试。\n\n\n任务告警机制。\n\n\n支持资源文件在线上传和管理。\n\n\n支持更多的丰富任务类型，如：Spark、shell、MR、Hive、python 等。\n\n\n支持集群高可用。\n\n\n可视化 DAG 界面。\n\n\n支持多租户、权限管理。\n\n\n简单易用、高扩展。\n\n\nApache DolphinScheduler 在联想的\n落地实践中涉及的改造点\n为了支持业务部门的需求，联想内部对 Apache DolphinScheduler 做了如下几点的改造：\n\n\nHTTP 任务参数传递\n\n\n引入 Java Client\n\n\n接入安全认证\n\n\n项目全局参数改造\n\n\nHTTP 任务参数传递\n在 2.X 的版本中 DolphinScheduler 已经支持了 shell、sql 任务参数的传递，但是 http 任务的参数传递还需要自研。\n\n上面的图中可以看到，在上一个 Http 任务请求参数定义时，通过 OUT 来指定输出结果的变量。\n\n而在下一个 Http 任务请求参数中来引用上一个 Http 输出结果的变量，这样就实现上下游依赖的 Http 任务参数的传递。\n引入 Java Client\nJava Client（执行器）是基于 Spring 开发。\n执行器任务不会纳入到 DolphinScheduler 的集群里来管理。Worker 可以调用 Java Client 进程的多个业务任务。另外，执行器的日志可以通过 SDK提供的日志类 TaskLogger.log 来打印出来，通过 UI 去查看。\n\nWorker 如何调用 Java Client 进程的多个业务任务呢？\n示例：我们在开发执行器的时候，定义业务任务方法 lenovoDemoJobHandler，在方法名上面加上类似的注@DolphinJavaMethodTask(&quot;lenovoDemoJobHandler&quot;)，通过以下 UI 指定业务任务方法以及执行器的地址，Worker 就会调用执行器对应的业务任务方法了。\n\n接入安全认证\n接入联想内部的认证，这块就不过多介绍了。\n项目全局参数改造\n我们首先看下在 DolphinScheduler 里面支持那些参数。\n首先是任务级别的参数，支持在每个任务里面自定义任务的局部参数，设置任务级别的参数后，参数优先级别是：任务参数&gt; 工作流定义全局参数 。\n下图是设置任务参数：\n\n如果在引入项目全局参数后，参数优先级别是：任务参数&gt; 工作流定义全局参数 &gt; 项目全局参数，下图是设置：\n\n下图是任务参数、工作流定义全局参数 、项目全局参数优先级顺序。\n\n我们也可以修改工作流定义的启动参数，下图是设置工作流定义的启动参数：\n\n加入工作流定义的启动参数后，参数的优先级别谁发生稍微的变化，任务参数 &gt; 工作流定义启动参数 &gt;工作流定义参数 &gt; 项目全局参数。下图是优先级顺序：\n\n后期规划(Roadmap)\n目前社区已经支持了 3.0 版本，有以下几个改进点：\n\n\nUI 重构\n\n\n支持数据质量\n\n\n支持任务组\n\n\n多种任务类型\n\n\nUI 重构\n全新 UI，前端代码更健壮。\n\n数据质量\n通过拖拉拽生成数据质量任务，内嵌十种数据集校验规则。\n支持在工作流运行前进行数据质量校验过程，通过在数据质量功能模块中，由用户自定义数据质量的校验规则，实现了任务运行过程中对数据质量的严格控制和运行结果的监控。\n\n任务组\n任务组主要用于控制任务实例并发并明确组内优先级。用户在新建任务定义时，可配置当前任务对应的任务组，并配置任务在任务组内运行的优先级。当任务配置了任务组后，任务的执行除了要满足上游任务全部成功外，还需要满足当前任务组正在运行的任务小于资源池的大小。当大于或者等于资源池大小时，任务会进入等待状态等待下一次检查。当任务组中多个任务同时进到待运行队列中时，会先运行优先级高的任务。\n\n\nRoadmap\n\n最后非常欢迎大家加入 DolphinScheduler 大家庭，融入开源世界！\n我们鼓励任何形式的参与社区，最终成为 Committer 或 PPMC，如：\n\n\n将遇到的问题通过 GitHub 上 issue 的形式反馈出来。\n\n\n回答别人遇到的 issue 问题。\n\n\n帮助完善文档。\n\n\n帮助项目增加测试用例。\n\n\n为代码添加注释。\n\n\n提交修复 Bug 或者 Feature 的 PR。\n\n\n发表应用案例实践、调度流程分析或者与调度相关的技术文章。\n\n\n帮助推广 DolphinScheduler，参与技术大会或者 meetup 的分享等。\n\n\n欢迎加入贡献的队伍，加入开源从提交第一个 PR 开始。\n\n比如添加代码注释或找到带有 ”easy to fix” 标记或一些非常简单的 issue(拼写错误等) 等等，先通过第一个简单的 PR 熟悉提交流程。\n\n注：贡献不仅仅限于 PR 哈，对促进项目发展的都是贡献。\n相信参与 DolphinScheduler，一定会让您从开源中受益！\n",
    "title": "Apache DolphinScheduler 在联想作为统一调度中心的落地实践",
    "time": "2023-10-16"
  },
  {
    "name": "The_most_comprehensive_introductory_tutorial_written_in_a_month",
    "content": "达人专栏 | 还不会用 Apache Dolphinscheduler？大佬用时一个月写出的最全入门教程\n\n\n\n作者 | 欧阳涛 招联金融大数据开发工程师\n海豚调度(Apache DolphinScheduler，下文简称 DS)是分布式易扩展的可视化 DAG 工作流任务调度系统，致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。Apache DolphinScheduler 作为 Apache 的顶级开源项目，与其他开源项目相似的地方在于，其运行以及安装都是从脚本开始的。\n脚本的位置都是根目录的 script 文件夹下的，脚本执行顺序如下:\n1、查看启动的脚本 start-all.sh，可以发现启动 4 个最重要的启动服务，分别是 dolphinscheduler-daemon.sh start master-server/worker-server/alert-server/api-server\n2、在 dolphinscheduler-daemon.sh 脚本中会首先执行 dolphinscheduler-env.sh 脚本，这个脚本作用是引入环境，包括 Hadoop、Spark、Flink、Hive 环境等。由于 DS 需要调度这些任务，如果不引入这些环境，即使调度成功，执行也无法成功。\n3、紧接着在 dolphinscheduler-daemon.sh 脚本中循环执行上述 4 个模块下的 bin/start.sh.如下图所示：\n\n\n\n如下图所示：执行 dolphinscheduler-daemon.sh start master-server 时会去 master 模块的 src/main/bin 执行 start.sh，打开 start.sh 后，可以发现启动了一个 MasterServer，其他 Worker，Alert 以及 API 模块等同理。\n\n\n\n至此，从脚本如何运行代码这块就已经结束了，接下来我们将详细介绍一下这 4 个模块的主要用途。Master 主要负责 DAG 任务切分、任务提交监控，并同时监听其它 Master 和 Worker 的健康状态等；Worker 主要负责任务的执行；Alert 是负责警告服务；API 负责 DS 的增删改查业务逻辑，即网页端看到的项目管理、资源管理、安全管理等等。\n其实，如果大家接触过其他大数据项目，例如 Flink、Hdfs、Hbase 等，就会发现这些架构都是类似的，像 hdfs 是 NameNode 和 WorkNode 的架构；Hbase 是 HMasterServer 和 HRegionServer 的架构；Flink 是 JobManager 和 TaskManager 的架构等，如果你能够熟练掌握这些框架，想必对于 DS 的掌握也会更容易的了。\nMaster，Worker 这些都是通过 SpringBoot 的启动，创建的对象也都是由 Spring 托管，如果大家平常接触 Spring 较多的话，那么笔者认为您理解 DS 一定会比其他的开源项目更容易。\n备注:\n1、运行脚本中还有一个 python-gateway-server 模块，这个模块是用 python 代码编写工作流的，并不在本文考虑范围之内，所以就暂时忽略，如果详细了解此模块的话，在社区请教其他同学的了。\n2、启动 Alert 脚本是执行 Alert 模块下的 alert-server 的脚本，因为 Alert 也是个父模块的，笔者不打算讲 alert-server。相信在看完 Master 和 Worker 的执行流程之后，Alert 模块应该不难理解。\n3、另外，初次接触 DS 的同学会发现 Alert 模块有个 alert-api 模块，笔者想说的是这 alert-api 和前面所说的 api-server 没有一丁点关系，api-server 是启动 api 模块的 ApiApplicationServer 脚本，负责整个 DS 的业务逻辑的，而 alert-api 则是负责告警的 spi 的插件接口，打开 alert-api 模块可以发这里面的代码全是接口和定义，没有处理任何逻辑的，所以还是很好区分的了。同理 task 模块下的 task-api 与 alert-api 只是职责相同，处理的是不同功能而已。\n4、DS 的全都是 SpringBoot 管理的，如果有同学没搞过 SpringBoot 或者 Spring 的话，可以参考下列网址以及网上的其他相关资料等。\nhttps://spring.io/quickstart\n如果想详细了解警告模块，请参考下方链接以及咨询其他同学。\nhttps://dolphinscheduler.apache.org/zh-cn/blog/Hangzhou_cisco\nApache DolphinScheduler 项目官网的地址为:https://github.com/apache/dolphinscheduler\n下一章，笔者将介绍 DS 最重要的两个模块 Master 和 Worker，以及它们如何进行通信的，敬请期待。\n",
    "title": "还不会用 Apache Dolphinscheduler？大佬用时一个月写出的最全入门教程（1）",
    "time": "2022-5-23"
  },
  {
    "name": "The_operation_practice_of_smooth_transition_from_Azkaban_to_Apache_DolphinScheduler",
    "content": "Fordeal的数据平台调度系统之前是基于Azkaban进行二次开发的，但是在用户层面、技术层面都存在一些痛点问题难以被解决。比如在用户层面缺少任务可视化编辑界面、补数等必要功能，导致用户上手难体验差。在技术层面，架构过时，持续迭代难度大。基于这些情况，经过竞品对比和调研后，Fordeal数据平台新版系统决定基于Apache DolphinScheduler进行升级改造。那整个迁移过程中开发人员是如何让使用方平滑过渡到新系统，又做出了哪些努力呢？\n5月 Apache Dolphinscheduler  线上 Meetup， 来自 Fordeal 的大数据开发工程师卢栋给大家分享了平台迁移的实践经验\nSTART\n讲师介绍\n卢栋\nFordeal 大数据开发工程师。5年的数据开发相关经验，目前就职于Fordeal，主要关注的数据技术方向包括：湖仓一体、MPP数据库、数据可视化等。\n本次演讲主要包含四个部分：\n\n\nFordeal数据平台调度系统的需求分析\n\n\n迁移到Apache Dolphin Scheduler过程中如何适配\n\n\n适配完成后如何完成特新增强\n\n\n未来规划\n\n\n一、需求分析\n01 Fordeal 应用背景\nFordeal 数据平台调度系统最早是基于Azkaban进行二次开发的。支持机器分组，SHELL动态参数、依赖检测后勉强可以满足使用，但在日常使用中依然存在以下三个问题，分别是在用户、技术和运维的层面。\n首先在用户层面，缺乏可视化的编辑、补数等必要的功能。只有技术的同学才能使用该调度平台，而其他没有基础的同学如果使用就非常容易出错，并且Azkaban 的报错模式导致开发人员对其进行针对性地进行修改。\n第二在技术层面，Fordeal 数据平台调度系统的技术架构非常陈旧，前后端并不分离，想要增加一个功能，二开的难度非常高。\n第三在运维层面，也是最大的问题。系统不定时会出来 flow 执行卡死的问题。要处理这个问题，需要登录到数据库，删除 execution flow里面的ID，再重启 Worker 和 API服务，过程十分繁琐。\n02 Fordeal 所做的调研\n因此，在2019年Apache DolphinScheduler开源时，我们就及时地关注到，并开始了解是否可以进行迁移。当时一同调研了三款软件，Apache Dolphin Scheduler、Azkaban和Airflow。我们基于五大需求。\n\n\n**首选JVM系语言。**因为JVM系语言在线程、开发文档等方面较为成熟。\nAirflow基于Python其实和我们现在的体系并无二异，非技术同学无法使用\n\n\n**分布式架构，支持HA。**Azkaban的work并不是分布式web和master服务是耦合在一起，因此属于单节点。\n\n\n**工作流必须支持DSL和可视化编辑。**这样可以保证技术同学可以用DSL进行书写，可视化则面向用户，用以扩大用户面。\n\n\n**前后端分离，主流架构。**前后端可以分开进行开发，剥离开来后耦合度也会降低。\n\n\n**社区活跃度。**最后关注的的社区活跃度对于开发也十分重要，如果经常存在一些“陈年”老bug都需要自己进行修改，那会大大降低开发效率。\n\n\nFordeal 现在的架构\n如今我们的数据架构如上图。Apache Dolphin Scheduler承接了整个生命周期从HDFS、S3采集到K8S计算再到基于Spark、Flink的开发。两边的olphinScheduler和Zookeeper都是作为基础性的架构。我们的调度信息如下：Master x2、Worker x6、API x1（承载接口等），目前日均工作流实例：3.5k，日均任务实例15k+。（下图为1.2.0版本架构图）\n二、适配迁移\n01 内部系统对接\nFordeal内部系统需要上线对用户提供访问，这时候必须对接几个内部服务，以降低用户上手成本和减少运维工作。主要包括以下三个系统。\n\n\n单点登录系统：\n基于JWT实现的SSO系统，一次登录，认证所有。\n\n\n** 工单系统：**\nDS对项目的授权接入工单，避免人肉运维。\n（接入所有授权动作，实现自动化）\n\n\n** 告警平台：**\n扩展DS告警模式，将告警信息全部发送到内部告警平台，用户可配置电话、企业微信等模式告警。\n\n\n下方三张图就是对应分别是登录系统、工单权限和企业微信的告警。\n02 Azkaban 的兼容\nAzkaban的Flow管理是基于自定义的DSL配置，每个Flow配置包含的Node数量多则800+少则1个，其更新的方式主要有三类。\n1、用户本地保存，每次修改后zip压缩上传，用户自行维护Flow的信息。\n2、所有的flow配置和资源都托管git，在Azkaban项目设置中绑定git地址，git是由我们自行开发的，git提交后在页面点击刷新按钮。\n3、所有的Flow托管到配置中心，对接Azkaban的上传接口去覆盖掉之前的调度信息。\n上图为一部分数仓项目的flow配置文件。想要把Azkaban迁移到Apache DolphinScheduler中，我们一共列出了十点需求。\n\n\nDS上传接口支持Flow配置文件的解析并生成工作流。（支持嵌套flow）Flow的配置文件就相当于 Azkaban 的DAG文件，如果不配适我们就要自己写代码解析配置文件，将Flow转成Json。\n\n\nDS资源中心支持文件夹（托管Azkaban项目下的所有资源）当时我们的1.2.0版本当时没有文件夹功能，而我们的数仓有许多文件夹，因此我们必须要支持。\n\n\nDS提供client包，提供基础的数据结构类和工具类，方便调用API，生成工作流的配置。\n\n\nDS支持工作流并发控制（并行或跳过）\n\n\nDS时间参数需支持配置时区（例如：dt=$[ZID_CTT yyyy-MM=dd=1]）。虽然我们配置的时区大多在海外，但对于用户而言，他们更希望看到北京时区。\n\n\nDS跑数和部署界面支持全局变量覆写。因为我们的版本较低，一些类似补数的功能都没有，工作流用什么变量跑，希望用户可以自己设置。\n\n\nDS DAG图支持task多选操作。\n\n\nDS task日志输出最终执行内容，方便用户检查调试。\n\n\nDS 支持运行中失败任务手动重试。通常一次跑数仓需要数个小时，其中有几个task可能因为代码问题报错，我们希望可以在不中断任务流的情况下，手动重试，把错误的节点逐一修改完后重试。这样最终的状态是成功的。\n\n\n数仓项目需支持一键迁移，保持用户的工作习惯（jenkins 对接DS）。\n\n\n在我们与五六个组进行不断的沟通和改造后，这十点需求最终满足。\n03 功能优化汇总\n从 Azkaban 完全迁移到 Apache DolphinScheduler 完成大概用时一年，因为涉及到API用户，涉及到 git 用户，还有支持各种各样功能用户，每个项目组都会提出自己的需求，在协助其他团队迁移的整个过程中，根据用户使用反馈，共提交了140+个优化 commit，以下是 commit 分类词云。\n三、特性增强\n01 前端重构\n对于为什么我们要重构，我们的痛点到底是什么？我们列出了一下几点。\n首先，Azkaban的操作步骤过于繁琐。用户想要找一个工作流定义时，首先要打开项目，找到项目首页中的工作流列表，再找到定义，用户无法一眼找到我想要的定义。\n第二，我无法通过名字、分组等条件检索到工作流定义和实例。\n第三，无法通过URL分享工作流定义和实例详情。\n第四，数据库表和API设计不合理，查询卡顿，经常会出现长事务告警。第五，界面很多地方写死布局，如设置了宽度，导致添加列不能很好适配电脑和手机。第六，工作流定义和实例缺少批量操作。凡是程序肯定有错误，如何批量重试，成为用户非常头疼的问题。\n执行方案\n\n\n基于 AntDesign 库开发新的一套前端界面。\n\n\n弱化项目概念，不想让用户过多去关注项目这个概念，项目只作为工作流或实例的标签。\n目前电脑版只有四个入口，首页、工作流列表、执行列表和资源中心列表，手机版只有两个入口，分别是工作流列表和执行列表。\n\n\n简化操作步骤，将工作流列表和执行列表放在第一入口。\n\n\n优化查询条件和索引，增加批量操作接口等。\n增加联合索引。\n\n\n完全适配电脑和手机（除了编辑 dag ，其他功能都一致）\n\n\n02 依赖调度\n**什么是依赖调度？**即工作流实例或 Task 实例成功后主动出发下游工作流或 Task 跑数（执行状态为依赖执行）。设想以下几个场景，下游工作流需要根据上游工作流的调度时间去设置自己的定时时间；上游跑数失败后，下游定时跑数是也会出现错误；上游补数，只能通知所有下游业务方补数。数仓上下游定时间隔调整难，计算集群资源利用率没有最大化（K8S）。因为用户并不是持续提交的。\n构思图（按层触发工作流）\n依赖调度规则\n\n\n工作流支持时间，依赖，两者组合调度（且与或）\n\n\n工作流内的Task支持依赖调度（不受定时限制）。\n\n\n依赖调度需要设置一个依赖周期，只有当所有的依赖在这个周期内满足才会触发。\n\n\n依赖调度最小的设置单位是 Task ，支持依赖多个工作流或 Task （只支持且关系）。\n\n\n工作流仅仅只是一个执行树中的组概念，就是说不会限制Task。\n\n\n手机工作流依赖详情\n03 任务拓展\n拓展更多的 Task 类型，将常用的功能抽象并提供编辑界面，降低使用成本，我们主要扩展了以下几个。\n\n\n数据开放平台（DOP）：\n主要是提供数据导入导出功能（支持Hive、Hbase，Mysql、ES、Postgre、Redis、S3）\n\n\n**数据质量：**基于Deequ开发的数据校验。\n对数据进行抽象供用户使用。\n\n\n**SQL-Prest数据源：**SQL模块支持Presto数据源\n\n\n**血缘数据采集：**内置到所有Task中，Task暴露血缘需要的所有数据\n\n\n04 监控告警\n架构为Java+Spring 下的服务监控，平台是有一套通用的Grafana监控看板，监控数据存储在Prometheus，我们的原则是服务内部不做监控，只需要把数据暴露出来即可，不重复造轮子，改造列表为：\n\n\nAPI、Master和Worker服务接入micrometer-registry-prometheus，采集通用数据并暴露Prometheus采集接口。\n\n\n采集Master和Worker执行线程池状态数据，如Master和Worker正在运行的工作流实例、数据库等，用于后续的监控优化和告警（下右图）。\n\n\nPrometheus侧配置服务状态异常告警，比如一段时间内工作流实例运行数小于n（阻塞）、服务内存&amp;CPU告警等等。\n\n\n四、未来规划\n01 跟进社区特性\n目前Fordeal线上运行的版本是基于社区第一个**Apache版本（1.2.0）**进行二开的，通过监控我们也发现了几个问题。\n\n\n数据库压力大，网络IO费用高\n\n\nZookeeper 充当了队列角色，时不时对导致磁盘IOPS飙升，存在隐患\n\n\nCommand 消费和Task分发模型比较简单，导致机器负载不均匀\n\n\n这个调度模型中使用了非常多的轮询逻辑（Thread.sleep），调度消费、分发、检测等效率不高\n\n\n社区发展迅速，当下的架构也更加的合理易用，很多问题得到了解决，我们近期比较关注的问题是Master直接对Worker的分发任务，减轻Zookeeper 的压力，**Task 类型插件化，易于后续扩展。**Master配置或自定义分发逻辑，机器复杂更加合理。更完美的容错机制和运维工具（优雅上下线），现在Worker没有优雅上下线功能，现在更新Worker的做法是切掉流量，让线程池归零后再上下线，比较安全。\n02 完善数据同步\n目前只提供了工作流实例的执行统计，粒度比较粗，后续需要支持更细化的统计数据，如按照 Task 筛选进行统计分析，按照执行树进行统计分析，按照最耗时的执行路径分析等（对其进行优化）。\n再次，增加更多的数据同步功能，如执行统计添加同步、环比阈值告警等功能，这些都是基于工作流的告警。\n03  连接其他系统\n当调度迭代稳定后，会逐步充当基础组件使用，提供更加便利的接口和可嵌入的窗口（iframe），让更多的上层数据应用（如BI系统，预警系统）等对接进来，提供基础的调度功能。\n我的分享就到这里，谢谢大家认真阅读！\n",
    "title": "数据平台调度升级改造 | 从Azkaban 平滑过度到Apache DolphinScheduler 的操作实践",
    "time": "2023-8-16"
  },
  {
    "name": "The_practice_of_Apache_Dolphinscheduler_in_fresh_food_industry",
    "content": "点亮 ⭐️ Star · 照亮开源之路\nGitHub:https://github.com/apache/dolphinscheduler\n\n精彩回顾\n近期，食行生鲜的数据平台工程师单葛尧在社区线上 Meetup 上给大家分享了主题为《Apache Dolphinscheduler在食行生鲜的落地实践》的演讲。\n随着大数据的进一步发展，不管是离线任务量还是实时任务量都变得越来越多，对调度系统的要求也越来越高，不仅要求系统稳定还要求操作简单，上手方便。\n而 Apache Dolphinscheduler 就是当下非常流行且好用的一款调度系统。首先它是分布式运行且是去中心化的，其次有一个非常好的页面，使得调度的任务变得非常容易上手。\n讲师介绍\n\n单葛尧\n食行生鲜 数据平台工程师\n文章整理：硕磐科技-刘步龙\n今天的演讲会围绕下面三点展开：\n\n\n背景介绍\n\n\n实施落地\n\n\n元数据系统 Datahub 与 Dolphinscheduler 集成\n\n\n1 背景介绍\n我司食行生鲜是一家采用“预订制”模式，通过全程冷链配送和社区智能冷柜自提方式，为用户提供优质生鲜服务的新零售企业。\n随着业务发展，大量的离线同步及计算任务开始对我们的数据架构的易用性与稳定性带来了挑战。\n01 数据架构\n\n上图是我们目前的基础架构体系，主要是批处理和流处理。批处理主要是以 Hive 和 Spark 为主的的全量数仓的分级计算。\n流处理以 Flink 为主，主要用于用户轨迹实时 ETL 和实时业务监控，目前采用美柚开源的巨鲸平台，后续会陆续迁移 Apache 新晋项目 StreamPark 中，它支持多个版本的 Flink，提供一系列开箱即用的连接器，大大减轻了开发部署实时任务的复杂度。\n我们的数据来源有 MySQL、PostgreSQL、物流供应链端的 SQLServer 数据、同行的数据及风控类的数据。\n相对应的日志类数据非常多且复杂，故数据类型也多种多样。\n我们的业务主体有两种：业务产生的数据，比如说用户去下单，用户的各种余额，积分优惠券；埋点系统的轨迹数据，比如说用户的点击、下单、进入商品详情等行为轨迹类操作；\n一般来说，T+1的数据采用离线计算，轨迹数据用的是实时计算。\n抽数工具是以 Sqoop 为主，其次是 binlog 消费，对于部分不支持的数据源，就用了 Apache SeaTunnel。\n经过数仓的复杂计算之后，我们的下游数据的 OLAP 场景主要以 TiDB 和GreenPlum 为主。\nTiDB 运用于业务的查询，比如查询近7日某商品的购买量；\nGreenPlum 主要以内部的看板为主。比如集团核心的财务指标，运营部门的运营成果及绩效指标；\n另外会用 HBase 存储一些维度数， ElasticSearch 存储一些算法模型训练出的画像结果。\nKylin 用于指标体系。它服务于我们内部的指标计算。比如站点状态的监控，展现业务成果的各维度。比如今天的实时订单情况，是否需要向供应链增派人力，最近下单的数据流向是否有猛增等现象，以此来调整销售策略。\n02 DMP的能力与组成\n任务数量随着业务发展日益增长，数据资产的管理、数据质量的监控等问题愈发严峻，DMP（Data Management Platform）的需求应运而生。\n\n一般而言，DMP 衍生出数据应用，数据应用包括以下能力：\n决策支持类：主题报表（月度/季度/年度/专题）、舆情监控、热点发现、大屏数据可视化展示等；\n数据分析类：交互式商业智能、OLAP分析、数据挖掘、数据驱动的机器学习等；\n数据检索类：全文检索、日志分析、数据血缘分析、数据地图等；\n**用户相关：**用户画像服务、用户成长/流失分析及预测、点击率预测、智能推荐等；\n**市场相关：**数据服务于搜索引擎、数据服务于推荐引擎、热点发现、舆情监控等；\n制造生产相关：预测性维护、生产过程实时数据监控、数字孪生等；\n2 实施落地\n日益增长的业务系统数据催生了对调度系统的高可用要求，原有自研的单节点调度系统不再适合我们当前的业务体量。\n我们开始在市面上调研新的调度工具，然而我们不仅需要调度系统是分布式高可用，还能简单易用，对无编程经验的分析师们提供友好的交互体验，对开发人员也可以支持高扩展性，便于后期可以随着业务增长良好的扩展其可支持的任务类型及集群规模。\n01 选择Apache DolphinScheduler\n\n最终我们选择了海豚调度，然而对于我司调度系统的发展经历了几个工具的迁移。\n最开始用的是 Azkaban ，因为一些历史原因，后续弃用了 Azkaban ；\n随后自研了一套调度系统，而随着业务数据的激增，自研系统存在的一个致命问题：该系统为单点式，没有办法扩展资源，只能单机运行；\n去年六月份，我们对 AirFlow 和 Dolphinscheduler 做了一个调研。\n面对业务场景，我们希望以 SQL 的形式去定义 flow ；\n希望系统以分布式的形式运行，而不是单机，以此来解决单机的瓶颈问题；\nAirFlow 的技术栈是 Python，而公司主要是以 Java 为主；\n经过比较，我们最终选择了 Dolphinscheduler 。\n02 实施落地\n去年6月，首次在生产环境接入了 DolphinScheduler 的1.3.6版本，经过业务的锤炼与社区的共建，现已成功更新至3.0.0，至今服务于我司一年有余，平均每日稳定运行6000+任务。\n03 任务执行\n\n我们在使用 DolphinScheduler 时，主要使用其 Shell 组件，内部封装了 Hadoop 相关 Tools ，用来通过 Shell 提交相关 SQL ，并指定任务提交的 Yarn 资源队列。\n我们根据 DolphinScheduler 内部的五个优先级 **HIGHEST、HIGH、MEDIUM、LOW、LOWEST **也分别创建了五个对应的 Yarn 资源队列，便于根据流程的优先级提交到指定的优先级队列，更好的去利用并分配资源。\n在原有的 Worker 线程池的等待队列中，把从原有的 LinkedBlockingQueue 转换 PriorityBlockingQueue ，以实现超 Worker 其 exec-threads 时可以依照其设定的优先级重新排序，实现高优先级任务在出现异常时，可以在资源较满的情况下实现“插队”效果。\n04 告警策略\nDolphinScheduler 提供了开箱即用的多种告警组件。\n\n\nEmail 电子邮件告警通知\n\n\nDingTalk 钉钉群聊机器人告警，相关参数配置可以参考钉钉机器人文档。\n\n\nEnterpriseWeChat 企业微信告警通知相关参数配置可以参考企业微信机器人文档。\n\n\nScript 我们实现了 Shell 脚本告警，会将相关告警参数透传给脚本，在 Shell 中实现相关告警逻辑，如果需要对接内部告警应用，这是一种不错的方法。\n\n\nFeiShu 飞书告警通知\n\n\nSlack Slack告警通知\n\n\nPagerDuty PagerDuty告警通知\n\n\nWebexTeams WebexTeams告警通知 相关参数配置可以参考WebexTeams文档。\n\n\nTelegram Telegram告警通知 相关参数配置可以参考Telegram文档。\n\n\nHTTP Http告警，调用大部分的告警插件最终都是Http请求。\n根据 Alert SPI 的设计，为其扩展了两个插件：内部OA通知+阿里云电话告警，以保证服务的可用性及数据产出的及时性。\nDolphinScheduler 的 Alert SPI 设计的相当优秀，我们在新增插件时，只需关注扩展 org.apache.dolphinscheduler.alert.api.AlertChannelFactory 即可。\n另外，DolphinScheduler 的告警覆盖场景也相当广泛，可以根据工作流及任务的平时的完成时间来设置超时时间，与新出的数据质量模块相结合，可以较好的保证数据的及时性与准确性。\n\n\n3 元数据系统 Datahub 与 Dolphinscheduler 集成\nDatahub由 LinkedIn 开源，原来叫做 WhereHows 。经过一段时间的发展 Datahub 于2020年2月在 Github 开源，首先简单介绍一下 Datahub 这个系统。\n01 总体架构\nDataHub 是一个现代数据目录，旨在实现端到端的数据发现、数据可观察性和数据治理。\n这个可扩展的元数据平台是为开发人员构建的，以应对其快速发展的数据生态系统的复杂性，并让数据从业者在其组织内充分利用数据的价值。\n\n02 搜索元数据\nDataHub 的统—搜索支持跨数据库、数据湖、BI平台、ML功能存储、编排工具等显示结果。\n支持的 Source 相当丰富，目前截止v0.8.45已有\nAirflow、Spark、Great Expectations、Protobuf Schemas、Athena、Azure AD、BigQuery、Business Glossary.ClickHouse.csv、dbt、Delta Lake、Druid、ElasticSearch.Feast、FileBased Lineage、File、Glue.SAP HANA、Hive、lceberg.Kafka Connect、Kafka、LDAP、Looker、MariaDB、Metabase、Mode、MongoDB、MicrosoftsQLServer、MySQL、Nifi、Okta、OpenAPI、Oracle,Postgres、PowerBl、Presto onHive、Pulsar、Redash.Redshift、S3 Data Lake.SageMaker、Salesforce、Snowflake、Other SQLAlchemydatabases、Superset.Tableau、Trino、Vertica等。\n03 血缘支持\n可通过跨平台、数据集、ETL/ELT管道、图表、仪表板等跟踪血缘,快速了解数据的端到端的流向。\n与市面上其他元数据系统不—样的是，Datahub 一直支持从数据集到B看板的整个流向的追踪，已经为我们提供了如 Redash、SuperSet 之类开源看板的元数据接入。\n\n04 元数据的抽取步骤\n**第一步：**开启元数据采集和创建密钥的权限；\n**第二步：**选择所摄取血缘的数据源（除了当前所支持的外，也支持自定义）；\n\n第三步：配置采集血缘的表以及下游走向；\n**第四步：**设置时区与定时，元数据采集就会像我们的调度系统一样，定时调取完成采集。\n05 Metadata Ingestion 架构 Pull-based lntegration\nDataHub 附带一个基于 Python 的元数据摄取系统，该系统可以连接到不同的源以从中提取元数据。然后，此元数据通过 Kafka 或 HTTP 推送到 DataHub 存储层。元数据摄取管道可以与 Airflow 集成，以设置计划摄取或捕获血缘。\nPush-based Integration\n只要您可以向 Kafka 发出元数据更改建议(MCP)事件或通过 HTTP 进行 REST 调用，您就可以将任何系统与 DataHub 集成。\n为方便起见，DataHub 还提供简单的 Python 发射器供您集成到系统中，以在源点发出元数据更改(MCP-s)。\n\n06 Datahub与Dolphinscheduler集成\n方案一\n通过 Kafka 作为 MetadataChangeEvent 发出简单的 dataset 到 dataset 的血缘\n\nimport datahub.emitter.mce_builder as builder\nfrom datahub.emitter.kafka_emitter import DatahubKafkaEmitter, KafkaEmitterConfig\n\n# Construct a lineage object.\nlineage_mce = builder.make_lineage_mce(\n    [\n        builder.make_dataset_urn(&quot;bigquery&quot;, &quot;upstream1&quot;),\n        builder.make_dataset_urn(&quot;bigquery&quot;, &quot;upstream2&quot;),\n    ],\n    builder.make_dataset_urn(&quot;bigquery&quot;, &quot;downstream&quot;),\n)\n\n# Create an emitter to DataHub's Kafka broker.\nemitter = DatahubKafkaEmitter(\n    KafkaEmitterConfig.parse_obj(\n        # This is the same config format as the standard Kafka sink's YAML.\n        {\n            &quot;connection&quot;: {\n                &quot;bootstrap&quot;: &quot;broker:9092&quot;,\n                &quot;producer_config&quot;: {},\n                &quot;schema_registry_url&quot;: &quot;http://schema-registry:8081&quot;,\n            }\n        }\n    )\n)\n\n# Emit metadata!\ndef callback(err, msg):\n    if err:\n        # Handle the metadata emission error.\n        print(&quot;error:&quot;, err)\n\nemitter.emit_mce_async(lineage_mce, callback)\nemitter.flush()\n\n方案二：通过Rest去emit血缘关系。\n\nimport datahub.emitter.mce_builder as builder\nfrom datahub.emitter.rest_emitter import DatahubRestEmitter\n\n# Construct a lineage object.\nlineage_mce = builder.make_lineage_mce(\n    [\n        builder.make_dataset_urn(&quot;bigquery&quot;, &quot;upstream1&quot;),\n        builder.make_dataset_urn(&quot;bigquery&quot;, &quot;upstream2&quot;),\n    ],\n    builder.make_dataset_urn(&quot;bigquery&quot;, &quot;downstream&quot;),\n) \n\n# Create an emitter to the GMS REST API.\nemitter = DatahubRestEmitter(&quot;http://localhost:8080&quot;)\n\n# Emit metadata!\nemitter.emit_mce(lineage_mce)\n\n上述形式适用于所有 dataset 到 dataset 的血缘关系构建，可以在任何数据集处理下使用。\n后续在社区的贡献计划\n01 对流处理的支持（flink stream与debezium）\n在社区PMC蔡顺峰的帮助下，现在已经完成了对流任务的初步集成，可以通过 Flink sdk 去提交任务到 Yarn ，可视化的启动、停止、Savepoint，直观的在列表里看到任务的 Yarn Application ID 和 Job ID 等信息。\n接下来的TODO LIST顺峰已经写在 related items 里\n\n\nflink 集群管理\n\n\n支持 flink sql\n\n\n增加 flink 的metric\n\n\n支持其他流任务（如 kafka connector）\n\n\n事件驱动调度（最终目标）\n\n\n\n02 与版本管理工具的集成（GIT与SVN）\n\n社区确实是能人辈出，我们准备的这个 RoadMap ，我不仅在 DSIP 里找到了提案，而且提案还提到了以下几个资源插件：\n\n\nGitHub\n\n\nGitLab\n\n\nAmazon S3\n\n\nAliCloud OSS\n\n\n当然，基于底层 Decorator implementation 的存在，该 Resource Plugin 会非常的易于扩展。\n当时在准备 Data Quality 相关开发时，就惊喜的发现社区提供了相关的提案，我们仅是在3.0.0上稍作改动，就投入了生产环境的使用，提供了我们数据准确性、及时性等多重保障。\n我们后期准备在该基础上扩展社区的 HiveCli 插件，并把我们目前的工程逐步从 SVN 迁移到 Git 上，以摆脱目前纯 Shell 使用，让分析师们更关注于业务。\n03 更好的与yarn集群及队列的管理与使用\n我司目前的所有资源调度都是基于 Yarn 的，包括所有的 MapReduce、Spark及Flink 任务，统一都由 Yarn 来管理。\n由于历史遗留原因及测试生产环境的隔离等因素，目前集群存在多套 Yarn 环境，每个 Yarn 的资源总量及策略配置各不相同，导致管理困难。\n再者，基于 DolphinScheduler 设计来看，Yarn 队列与执行的用户绑定，用户来定义默认的租户及提交队列。这个设计不太符合生产环境的要求，租户来定义数据的权限，队列来定义任务的资源，后面我们会把队列单独作为一个配置或是直接把提交队列和任务的优先级绑定。\nYarn 环境的多套集群管理，可以后期远程提交任务到指定集群，来替换掉目前的方案，后期可以在调度里可以直接监控调度系统里的任务在 Yarn 的一些运行状态。\n04 更好的与DataHub的集成\n\n给大家提供一个好用的Python插件，SqlLineage,可解析SQL语句中的信息。\n给定一个 sql 语句，sqllineage 将告诉您源表和目标表。如果您想要血缘结果的图形可视化，可以切换它的切换图形可视化选项，此时就会启动一个 web ，在浏览器中显示血缘结果的 DAG 图，目前我司基于此组件解析了我们版本管理工具下的所有 sql ，在此基础上构建了我们的上下游血缘。\n后期我们将会依照 Datahub 的 Airflow 组件功能，扩展开发 Datahub 的 Dolphinscheduler 元数据组件。\n[lineage]\nbackend = datahub_provider.lineage.datahub.DatahubLineageBackend\ndatahub_kwargs = {\n    &quot;datahub_conn_id&quot;: &quot;datahub_rest_default&quot;,\n    &quot;cluster&quot;: &quot;prod&quot;,\n    &quot;capture_ownership_info&quot;: true,\n    &quot;capture_tags_info&quot;: true,\n&quot;graceful_exceptions&quot;: true }\n\nDatahub 的 Airflow 血缘配置如上所示，可以发现 Datahub 为 Airflow 提供了开箱即用的 acryl-datahub[airflow] 插件，提供以下功能：\n\n\nAirflow Pipeline (DAG) metadata\n\n\nDAG and Task run information\n\n\nLineage information when present\n\n\n我们会扩展 Dolphinscheduler 的 Python Gateway 能力，后续将会回馈到社区，希望可以为大家提供更好的元数据系统集成体验。\n",
    "title": "突破单点瓶颈、挑战海量离线任务，Apache Dolphinscheduler在生鲜电商领域的落地实践",
    "time": "2023-8-1"
  },
  {
    "name": "The_practice_of_building_a_data_platform_based_on_Apache_DolphinScheduler",
    "content": "很荣幸收到社区的邀请来给大家分享一下公司基于构建数据平台的实践。我将首先介绍DataLink大数据平台，然后讲解调度系统的升级和选型，最后分享我们是如何基于Apache DolphinScheduler构建数据平台的，以及平台的现状和未来的规划。\n公司介绍\n三合一信息科技有限公司是专业从事工业互联网行业大数据以及人工智能方向领域研发的一个公司，目前公司有六款产品，分别是物联网平台、数字孪生、专域数驱、大数据平台、工业算法平台及低代码平台等等。\n数据平台介绍\n\n数据接入层\n我们的数据接入层主要包括离线采集平台和实时采集平台。离线采集方面，我们主要基于SeaTunnel的Spark引擎，而实时采集则依赖于Flink CDC或者接通Kafka数据源，以及直接从物联网平台获取数据。\n数据平台层\n数据平台层包含数据加工平台（即数据开发）、实时计算平台、AI算法平台和数据资产平台。\n目前，我们正在调研一款开源的数据资产平台产品。后面如果有朋友有兴趣的话， 也可以一起交流一下。\n数据服务层\n数据服务层由标签管理服务和数据服务组成。至于应用层，虽然在此未全部展示，但包括了报表和数据挖掘等多个方面。\n\n基础组件包括MySQL、Oracle、PostgreSQL等关系型数据库，以及像Doris、ClickHouse等OLAP数据库。我们的数据底座是基于CDH集群构建的，其中包含Flink、Spark、Hive、Kafka、Yarn等组件。\n调度层\n我们之前的架构使用的是Airflow，但由于遇到的一些痛点，我们后来替换为了Apache DolphinScheduler。任务组件方面，我们使用的是DataX离线任务来进行数据同步，以及HiveSQL、FlinkSQL、Python脚本和Spark等。\n服务层\n服务层主要负责任务管理、运维、脚本构建、资源管理、数据服务、数据入湖和即席查询等功能。\n监控\n监控分为任务监控、日志监控和基础监控。我们之前使用Airflow的邮件进行任务监控，实时任务提交到Yarn上面，现在的话主要还是通过Yarn Restful来监控实时任务的运行状况。\n日志监控方面，我们实时收集数据到Kafka，并利用Flink进行实时分析。我们也有离线的日志监控方案，比如使用ELK，数据在ES上，我们可以用离线同步把ES的数据，同步到数仓， 直接再进行离线的分析。\n基础监控方面，我们使用Prometheus和Grafana。\n调度架构介绍\n之前的调度框架主要是由K8s+Airflow+Celery+Redis+Mysql+NFS组成，整个架构的运行流程，比如说NFS是其实是做DAG的目录共享，通过就是把脚本上传NFS，然后它会共享到Airflow的每个节点上面去。\n\nAirflow Scheduler会定时的去DAG的目录上面去扫一下，把脚本解析成任务和DAG。然后把它入到数据库里面，这个时候在前端其实就可以在页面上查询到任务。Airflow Flower主要就是Celery任务里面 work运行的状况。\n可以看一下上图的右边部分。右边是里面的话是Celery Excutor由队列管理的，然后我讲一下 当时是怎么实现HA一个方案，因为刚开始部署的话也是在物理机上面部署的，但当时的话发现就是这个节点其实非常容易挂，你一旦任务的多了之后，晚上运行的话，Scheduler就会可能因为它虽然有储存架构，但可能有的时候会发现这两个都挂掉了，那这个就很尴尬。\n为了避免这种情况再发生的话，当时还是用K8s来运维Airflow整个集群的。通过横向扩展Scheduler的来实现它的高可用。\nK8s针对Scheduler也会进行自动运维。自动运维比如说Scheduler如果挂了的话，K8s其实会自动把Scheduler进行拉起。\n调度系统的升级和选型\n我们曾面临多个挑战，例如Airflow的二次开发成本较高、性能问题以及镜像过大等。\n镜像大的原因，刚刚讲了我们是用K8s开部署的，然后打包Airflow镜像的时候 就需要把一个Hadoop、Hive及其他的一些客户端把它打包到镜像里面去，不然的话在镜像内部其实是没有办法跟集群进行交互的。所以前前后后打了很多的包进去。而且我们还要做集成，所以要把集成的一些工具也要把它打到镜像里面去，所以导致镜像可能有3-4个G都不止。\n\n每次发版的时候，有你就会发现可能有几个节点，在有几个Pod在K8s上面是会加载失败或起不来，会报一些错。这个也很头疼，然随我们的业务往后的话，插件越来越多，我觉得可能后面这种情况肯定会更常见。\n第二个的话是性能问题。因为随着DAG目录增多的话， 在创建任务的时候，Scheduler是定时的去扫DAG目录，然后解析任务，把它写到数据库里面，那这样的话， 比如说我现在新建了一个任务，因为任务越来越多，它扫描和解析的时间会越来越高。就导致我上来任务之后可能要过了一会，我的任务才会在页面上面展示出来。\n\n基于上面一些问题，导致我们决定要去更换我们的调度框架，然后在正好在那个时候，我当时在荣耀的时候接触了个HUE项目组，他们当时在使用DolphinScheduler的1.x版本，因为当时我是做集成组件做数据集成的。\n后期有调度的需求，然后他就给推荐了Apache DolphinScheduler，期间我们两个项目组保持相互沟通。我自己体验了一下之后感觉还是比较舒适的，你可以在页面上面直接通过拖拽的方式肯定比你自己写代码，这种方式要舒适很多。\n调研选型\n回到合肥之后，我对DolphinScheduler进行了深入的调研。首先，要了解任何工具，必须对其架构有基本的认识。\nDolphinScheduler架构\n从官方获取的DolphinScheduler架构图显示，其核心特点是多Worker多Master的高可用架构。这一架构明显地体现在其Web UI界面上，通过Restful API调用APIService。\n\n例如，工作流的创建、删除、编辑等操作都会被记录到数据库中。执行任务后，会产生任务实例和相应的日志。\nAPIService可以与WorkerService进行交互，后者包含LoggerService。\nMaster主要负责任务的DAG切分及定时任务的启动，并将任务下发给Worker执行。\nWorker执行任务（如Flink、SQL、Shell等）后，将结果反馈给Master。如果任务出错，AlertService会触发相应的告警。整个集群通过Zookeeper实现高可用。\n功能调研\n架构的优越性并非唯一考量点，功能同样重要。DolphinScheduler支持DAG，提供丰富的任务类型支持（目前约30种），并支持自定义参数，非常适用于离线场景的增量处理或数据补录。\n\n开发方式简单，基于界面拖拉拽即可生成工作流，无需编写代码。此外，它还支持多租户功能和告警模块，与国内流行的企业通讯平台（如企业微信、钉钉、飞书）集成良好，实现告警功能。\n决定更换调度框架\n综合考虑后，我们决定更换调度框架，选择了Apache DolphinScheduler。其多Worker多Master的高可用架构，简易的开发方式，多租户支持等特性是我们采用它的主要原因。\n\n任务类型与自定义功能\nDolphinScheduler支持30多种任务类型，并且支持自定义任务类型。对于正在执行的工作流，需要先终止当前任务才能执行补数操作。支持多租户功能，这对于我们主要使用Java的团队而言是一个重要的考量点。如果需要基于Python进行开发，可能会增加一些复杂性。\n应用性与扩展性\n在应用性方面，DolphinScheduler提供了代码编写和界面拖拉拽两种方式。在扩展性方面，由于基于Zookeeper注册，对集群的扩展变得相对简单，只需配置相应的工作节点并在Zookeeper中注册即可。\n总的来说，Apache DolphinScheduler以其强大的功能和灵活的架构，为我们构建数据平台提供了坚实的基础。\n数据集成和数据开发平台构建\n我们对DolphinScheduler的源码进行了深入的研究和理解，并自主开发前端，集成了多种插件。\n\n平台工作流程\n我们的平台主要基于DS（DolphinScheduler）构建，主要包括离线数据集成和离线数据开发。平台的工作流程如下：\n\n\n任务创建与管理：在平台页面上操作，发起创建请求。创建的任务默认为下线状态，可进行编辑、删除和查看等操作。\n任务上线与运行：任务上线后，可以运行或创建定时任务。运行任务将产生工作流实例，允许进行停止、查看、删除日志等操作。\n定时任务管理：创建的定时任务可以直接上线。在定时任务状态下，任务可自由上下线。需要注意的是，任务从上线状态改为下线时，相应的调度任务也会随之下线。\n\nDS插件集成与应用\n\n插件使用：初始版本使用的是DS 3.0 Alpha版，该版本不稳定，主要是因为ST插件的集成。但经过bug修复，我们成功用它替换了DataX。目前，我们在DS插件集成方面积累了丰富的经验，欢迎有相关需求的公司与我们交流。\n数据集成与开发：数据集成任务本质上是单节点的DS任务，可以进行各种调度操作。数据开发则涉及拖动数据集成任务，并与其他组件相连，最终生成工作流。工作流任务根据项目需求分配到不同的project中。\n\n用户体验优化\n\n任务创建流程简化：为提升用户友好度，我们优化了任务创建流程。用户可以在页面上直接选择数据源（支持多种数据源），选择源端和目标端，勾选所需字段。平台底层基于Spark实现离线集成，支持自动建表等功能。\n技术支持：平台支持多表同步和整库同步。除了基础的数据集成任务外，我们还在页面上引入了扩展转换组件，比如过滤器等，来构建更复杂的数据开发任务。这些功能已由社区支持，并部分开源。\n\n平台现状与未来规划\n当前状况\n\n目前，我们的平台已经支持了包括实时离线OLAP数据源、服务数据、数据资产等多种场景。我们正积极探索数据标准化和指标平台的建设。对于这些领域，我们欢迎有相关经验的朋友与我们交流和共同探讨。\n技术应用\n\n离线处理：目前，我们主要使用ST（可能是指某特定技术或工具）来替代DATAX，主要作业类型包括Spark、Python、Shell、ST等。\n实时处理：在实时数据处理方面，我们主要以Flink为核心。应用场景包括实时数据同步和风控报警，特别是在工业环境中对设备如温度、湿度等实时监控至关重要。\n\n未来规划\n\n数据挖掘平台：我们计划开发数据挖掘平台，目前有两个初步想法：\n\n基于DS，封装Spark节点，将Spark算法集成进平台。用户通过拖拽界面操作实现数据挖掘任务。\n使用ST，其底层也基于Spark。我们计划将机器学习算法作为转换组件集成，用户可以通过页面上的拖拽表单构建工作流来实现数据挖掘功能。\n\n\n实时风控平台：考虑到实时风控场景的普遍性，我们打算基于Flink CEP构建一个实时风控平台。由于Flink SQL的开发门槛较高，我们计划提供一个更为友好的用户界面，让客户能够通过表单和拖拽方式轻松生成任务。\n\n我们期待未来平台的发展能够更好地服务于客户，同时欢迎业界朋友加入我们的探索和创新之旅。非常感谢大家的聆听，希望我的分享能给大家带来启发和帮助。如果大家对我们的平台或者相关技术有兴趣，欢迎与我们联系交流。\n",
    "title": " 基于Apache DolphinScheduler构建数据平台的实践分享",
    "time": "2023-12-13"
  },
  {
    "name": "Twos",
    "content": "恭喜 Apache DolphinScheduler 入选可信开源社区共同体（TWOS）预备成员！\n\n\n\n近日，可信开源社区共同体正式宣布批准 6 位正式成员和 3 位预备成员加入。其中，云原生分布式大数据调度系统 Apache DolphinScheduler 入选，成为可信开源社区共同体预备成员。\n\n\n\nApache DolphinScheduler 是一个分布式易扩展的新一代工作流调度平台，致力于“解决大数据任务之间错综复杂的依赖关系，使整个数据处理过程直观可见”，其强大的可视化 DAG 界面极大地提升了用户体验，配置工作流程无需复杂代码。\n自 2019 年 4 月正式对外开源以来，Apache DolphinScheduler 经过数代架构演进，迄今相关开源相关代码累积已获得 7100+ 个 Star，280+ 经验丰富的代码贡献者，110+ 非代码贡献者参与其中，其中也不乏其他 Apache 顶级项目的 PMC 或者 Committer。Apache DolphinScheduler 开源社区不断发展壮大，微信用户群已达 6000+ 人，600 + 家公司及机构已在生产环境中采用 Apache DolphinScheduler。\nTWOS\n在“2021OSCAR 开源产业大会”上，中国信通院正式成立可信开源社区共同体（TWOS）。可信开源社区共同体由众多开源项目和开源社区组成，目的是引导建立健康可信且可持续发展的开源社区，旨在搭建交流平台，提供全套的开源风险监测与生态监测服务。\n\n\n\n为帮助企业降低开源软件的使用风险，推动建立可信开源生态，中国信通院建立了可信开源标准体系，对企业开源治理能力、开源项目合规性、开源社区成熟度、开源工具检测能力、商业产品开源风险管理能力开展测评。其中，对于开源社区的评估，是开源社区=人+项目+基础设施平台，一个好的开源社区有助于开源项目营造良好的开源生态并扩大影响力。可信开源社区评估从基础设施、社区治理、社区运营与社区开发等角度，梳理开源社区应关注的内容及指标，聚焦于如何构建活跃的开发者生态与可信的开源社区。\n经过可信开源社区共同体（TWOS）的重重评估标准的筛选，批准 Apache DolphinScheduler 入选预备成员，证明了其对 Apache DolphinScheduler 的开源运营方式、成熟度及贡献的认可，激励社区提升活跃度。\n\n\n\n可信开源标准体系 开源社区评估标准\n2021 年 9 月 17 日，可信开源社区共同体（TWOS）第一批成员加入。目前，可信开源社区共同体包括 openEuler、openGauss、MindSpore、openLookeng 等在内的 25 名正式成员，以及 Apache RocketMQ、Dcloud、Fluid、FastReID 等在内的 27 名预备成员，总数为 52 名：\n\n\n\n第二批预备成员仅有两个项目——Apache DolphinScheduler 与阿里云开源云原生态分布式数据库 PolarDB 入选。\nApache DolphinScheduler 社区十分荣幸能够入选可信开源社区共同体（TWOS）预备成员，这是整个行业对 Apache DolphinScheduler 社区建设的肯定与激励，社区将再接再厉，争取早日成为正式成员，与中国信息通信研究院领导的可信开源社区共同体一起，为中国开源生态建设提供更多价值！\n",
    "title": "恭喜 Apache DolphinScheduler 入选可信开源社区共同体（TWOS）预备成员！",
    "time": "2022-1-11"
  },
  {
    "name": "Zhendao_Group_builds_an_intelligent_marketing_cloud_platform_based_on_Apache_DolphinScheduler",
    "content": "珍岛集团致力于打造全球领先的智能营销云平台，在国内率先推出的Marketingforce（营销力）平台，专注于人工智能、大数据、云计算在数字营销及企业数字化智能化领域的创新与实践，面向全球企业提供营销力软件及服务，以一站式智能营销生态助力企业进行数字化转型。\n\n之前，珍岛集团使用完全开源的Apache DolphinScheduler任务调度框架，随着业务的发展，以及数据集成平台和GMA，算法计算平台越来越多的业务需求，开源版本的Apache DolphinScheduler已经不能完全满足需求，迫切地需要对Apache DolphinScheduler做一些定制化的开发。以下是珍岛集团团队最近一年在开源版本的基础上进行的优化和改进。\n业务需求\n技术方面\n1.期待简单易用，低代码的方式；\n2.Plug-in足够多，能够符合各业务模块需求；\n3.活跃的开源社区，优秀的人才；\n4.技术栈能够和珍岛现有各业务模块高度吻合；\n5.后期新建业务模块时，不需要过多的二次开发。\n业务方面\n\n\n对调度系统的稳定性要求高；\n\n\n高并发情况下，任务能够正常执行。\n\n\n拿一个简单的业务来举例，当用户通过配置设置好受众的特征，需要能够在指定的时间内，通过离线数据和算法，将具体的目标受众计算出来，并且将优惠卷或者消息通过各种渠道推送给目标受众，如果产生延迟，或者错误，就会使用户产生损失。另外，还要满足能够抗住夜间4000个流程实例同时启动，并且正常运行的需求。\n架构设计\n在珍岛集团，业务层以数据集为例，数据集将业务数据转换成二次开发后暴露的公共接口需要的参数，业务执行过程通过MQ将实例信息发送给业务方，当流程定义满足于配置的告警策略，将发送自定义邮件和企微信息，如果是DataX类型任务，则调用K8S的API，并通过异步的方式获取日志信息。\n\n\n二次开发实践\nMQ解耦\n背景：\n在珍岛内部，很多模块都需要调度中心调度，例如数据的ETL，标签模块、算法模型模块都需要系统调度，各模块需要Apache DolphinScheduler的状态，历史记录等数据，并在自己的模块实现自己的业务，然而各模块和Apache DolphinScheduler使用的不是同一个数据库。于是我们就需要将Apache DolphinScheduler中的一些信息推送给各个模块，使得各模块能够更灵活的使用这些信息。\n当有新的业务模块需要使用Apache DolphinScheduler，只要按照给定的结构去保存，并接收处理Apache DolphinScheduler返回的MQ消息即可。\n解决方案：\n通过MQ将状态，流程实例，任务实例等状态传递给各模块。\n我们新增了四个自定义注解，用于流程实例和任务实例的新增与状态变更。\n\n以创建流程实例为例：\n\n\n对于Apache DolphinScheduler而言，某个任务实例失败了，那么该任务实例的下游就不执行，也不生成记录了，但是对于有些业务来说，可能会想知道整个任务执行时涉及到了哪些节点（用户未运行整个画布），所以我们又稍微做了一些改造，将本次流程实例涉及到的节点也全部返回了。\n用户自定义邮件\n背景：\nApache DolphinScheduler 默认的邮件消息格式过于死板，对于客户来说不够友好。\n解决方案：\n对于用户来说，用户是看不懂源生Apache DolphinScheduler的邮件的，我们将Apache DolphinScheduler 以及业务中的一些参数交给用户，使得用户能够自定义邮件内容，业务数据相关的占位符由业务解析，解析完成之后将数据再移交给Apache DolphinScheduler，Apache DolphinScheduler再接着处理和自身相关的占位符变量。\nDataX on K8S与在本地运行的融合\n背景：\n\n\n夜间定时任务多，并发量大，在该时间段内需要的资源多，但是日间需要资源相对少很多，如果使用K8S可以释放出来给其他模块使用；\n\n\nDataX节点运行在worker 机器上，并发大的时候，资源使用率陡增，参数设置不正确，可能会导致OOM 的情况，交给K8S运行可保证其稳定性；\n\n\nworker应该只做提交任务的工作，而不是负责具体任务的运行；\n\n\n减少给予worker的配置资源。消耗资源小的可以运行在本地，消耗资源大的运行在k8s上。\n\n\n解决方案：\n改造DataX组件，再生成job 的JSON 之后交给K8S处理，k8s调用自定义的DataX镜像，并异步获取K8S 的job日志。当job完成之后就释放掉该job的资源。\nDataX改造\n对于数据集成平台来说，用户一定会有许多非JDBC类型的数据源需要抽取。所以我们做了以下改造：\n\n\n解耦DataX的reader和writer 的生成方式\n\n\n支持动态更新自定义模版，原先的DataX类型任务，开启自定义模版，保存后存储的信息就固定了。如果数据源的账号密码或者其他信息改变。再次执行还是历史保存的信息。\n\n\n背景：\n1. 目前客户的一些数据源信息，不一定是JDBC类型的数据源，例如抖音粉丝数据属于API接口调用，HDFS。又或者说HBase的结构和普通的RDBMS的结构不一样。所以目前的DataX节点不足以满足我们全部的需求。\n2.对于业务模块来说，更倾向于使用JSON形式，因为JSON更灵活，能够完全使用DataX的功能，但是使用自定义模版会出现以下情况：用户新建数据源a，数据源a 的密码是123，并用数据源a生成流程定义A，运行A，没有问题能够正常执行，此时将数据源a的密码修改为456，再次执行A，报错。如果a关联了很多很多流程定义，手动改就会很累，而且找到a关联的流程定义也需要额外的接口去做，这不符合用户习惯。\n解决方案：\n将数据源Id 也放入JSON中，运行时重新拼接JSON，自定义抽象类，将DataX的reader 和writer 生成分离，JDBC走公共的一套，其他非JDBC或特殊类型走自己派生的逻辑。\n加密指定类型的日志信息\n在Apache DolphinScheduler中各类型节点使用同一套正则表达式隐藏日志，不够灵活。共用一套也可能产生互相影响。于是我们根据节点的类型来配置各自需要隐藏的内容。\nApache DolphinScheduler 优化实践\n另外，在使用Apache DolphinScheduler的过程中，我们还发现了一些或大或小的使用问题，为了更好地使用，我们对其中一些问题进行了优化修改，并提交了PR。比如：\n\n\n今年Apache DolphinScheduler刚上线时，我们发现某些项目的SUB_PROCESS节点，明明已经运行完毕，但是还是运行中的状态。通过排查发现父流程实例的失败策略为结束，并且选择并行策略。当subprocess类型节点很多的时候，一部分子流程实例已经运行成功，但是subprocess 在父流程中的任务实例还未更新成完成状态，此时一个subprocess 类型节点失败了，父流程会杀死所有存活的任务实例，所有存活的subprocess关联的子流程实例会被强制更新为ready stop 状态，导致父流程中的任务实例无法更新，所以一直显示提交成功，或者运行中状态。对此，添加subprocess 任务状态的判断条件，即可解决此问题。#14169\n\n\n当任务有有问题，会造成死循环，疯狂打日志，最后磁盘打满，Apache DolphinScheduler不再执行任务的问题，同时编辑画布导致任务死循环报错的问题。对此，建议当业务出现问题时跳出死循环，以此减少日志疯狂打印，避免磁盘刷满的问题。 #13045 #13053\n\n\nSwitch类型的任务，当switch的分支的扭转，再次交合时，执行的流程不符合预期。对此，当Switch 节点执行后续节点的分析出现了问题，可以将不该跳过执行的节点放入了skipTaskNodes中，修改switch的编排逻辑，以此解决该问题。#14537\n\n\n针对当任务执行后，只会在执行完成后更新pid的问题，我们新增一个事件类型，在进程阻塞等待任务执行完毕之前，通过netty 将pid 传递给master 并及时更新到数据库。#13201\n\n\n此外，在参数调优上，我们还调整了quartz 框架的org.quartz.jobStore.misfireThreshold参数，可以在夜间承载更多的并发。\n\n\n还修改了check4timeOut的超时时间，以便能够减少日志大小，减少磁盘消耗。\n\n\n\n用户收益\nApache DolphinScheduler稳定的赋能业务增长的客户全域数据平台，整合客户全渠道数据，统一用户档案与标签体系，赋能用户洞察与业务增长。同时，它还支持多种数据源集成，如实时数据、离线数据，并实现了跨类型、跨源数据的全链路打通。利用Apache DolphinScheduler给类型节点，通过低代码的形式使得业务部门可参与编码，显著节省了 ETL 人力，降低了维护成本。\n通过近一年我们对 Apache DolphinScheduler 进行优化，近半年公司没有出现高级别的BUG。\n用户简介\n珍岛信息技术（上海）股份有限公司，简称珍岛集团，成立于2009年，致力于打造全球领先的智能营销云平台，也是国内率先推出的Marketingforce（营销力）平台，专注于人工智能、大数据、云计算在数字营销及企业数字化智能化领域的创新与实践，面向全球企业提供营销力软件及服务，现已形成IaaS（云计算）、PaaS（开放中台）、All-in-one AI SaaS智能营销云平台。以营销云、销售云两大业务体系为支撑，珍岛以一站式智能营销生态助力企业开启数字化转型之旅。\n致谢\n感谢韩玉林，顾琦琦，以及珍岛运维部和数据平台部的协助与优化意见\n感谢Radeity，ruanwenjun，以及DolphinScheduler 社区的帮助与建议\n作者简介\n伏长海\n珍岛集团开发工程师\nApache Dolphinscheduler Contributor\n",
    "title": " 珍岛集团基于 Apache DolphinScheduler 打造智能营销云平台",
    "time": "2023-10-24"
  },
  {
    "name": "Zuoyebang’s_data_development_platform_practice_based_on_DolphinScheduler",
    "content": "1. 背景\n首先介绍下我们的大数据平台架构：\n\n数据计算层承接了全公司的数据开发需求，负责运行各类指标计算任务。\n其中批计算任务运行在 UDA 数据开发平台，支持任务全链路的开发场景：开发、调试、环境隔离、运维、监控。这些功能的支持、任务的稳定运行，强依赖底层的调度系统。\n原有调度系统是 2015 年 (抑或更早) 自研的，随着任务类型新增、任务数量增多，暴露出诸多问题：\n\n\n稳定性：频繁出现 mysql 连接不释放、锁超时等问题；数据库压力进一步导致调度性能瓶颈，任务无法及时调度。\n可维护性：核心调度器通过 php 开发，代码古老又经历多次交接，外围模块实现时采用了 go java python 多种语言；再加上功能上也存在单点，维护成本很高。\n扩展性：业务高速发展，不同任务类型需求越来越多，但是调度作为底层服务在支撑上一直力不从心。\n可观测性：由于是定时nohup启动任务进程的方式，经常出现任务跑飞了的情况，系统暴露出来的可观测指标几乎为 0。\n\n对调度系统的核心诉求，我觉得分为功能和系统两部分：\n\n功能上，调度系统的核心能力是解决数仓构建的依赖调度问题，因此需要支持多种依赖形式；支持丰富的任务类型，同时可扩展自定义新的任务类型。以及上线管控、历史版本回滚、任务血缘等提高易用性的能力。\n系统上，稳定性是第一位的，因此需要具备高可用的能力。同时支持租户隔离、线性扩展、可观测，以方便的对系统进行开发、维护和预警。\n历史上我们调研过Airflow、DolphinScheduler 等多种选型，在过去大概一年的时间里，我们将大部分任务从自研调度系统迁移到了 DolphinScheduler 上。\n当前调度系统概况如下：\n\n任务类型上：HiveSQL、SparkSQL、DorisSQL、PrestoSQL、部分 shell 任务，均通过 DolphinScheduler 调度；遗留部分 shell 任务在原调度系统。\n任务数量上：DolphinScheduler 天级别调度数万工作流实例，数十万任务实例，高峰时期同时运行 4K+ 工作流实例。迁移完成后，预计工作流实例实例数翻倍。\n\n2. 数据开发平台实践\n2.1. 基于 DolphinScheduler 的改造\n对 DolphinScheduler 的改造围绕稳定性和易用性展开，对于原有调度系统设计良好的功能，需要兼容以降低任务迁移成本。\n我们基于 DolphinScheduler 做了如下升级：\n\n由于 DolphinScheduler 的架构设计比较好，优化基本上可以围绕单点或者复用现有能力展开，而无需对架构进行大刀阔斧的改造。\n我们的 SQL 任务都是多个 SQL 组成，但是原生的 SQL 任务只能提交单个。为了确保系统简洁，我没有引入各类 client(hive-client、spark-client 等)，而是通过 SQL 解析、连接池管理方式重构等方式，通过 JDBC 协议支持了单任务多 SQL 的提交。\n同时充分复用了 DolphinScheduler 对于数据源的设计，赋予数据源更多的属性，比如连接不同的 HiveServer2、Kyubbi、Presto Coordinator 等，对于计算运行在 Yarn 上的任务，单个数据源也只允许使用单个队列。对数据源增加权限控制，这样不同任务就只能使用有权限的集群资源。\n我们将资源文件、DQL运行的结果数据，都统一上传到了腾讯云的 COS 对象存储，以确保做到 Worker 真正的无状态。(注：日志上传进行中)\n此外包括对负载均衡进行优化、多业务线的租户调度隔离、数据库使用优化等。\n2.2. 平滑的大规模迁移\n尽管两个调度系统，在功能以及架构上存在巨大差异，但是需要做到平滑的迁移，主要三个原因：\n\n原有调度系统服务多年，用户对于功能设计、系统专有字段名词等都已经养成习惯\n2W+ 工作流的迁移预计耗时较久，涵盖公司众多重要数据流，问题影响程度高\n用户覆盖了公司众多业务线 (平台、直播课、硬件、图书)，问题影响面广\n\n如此大规模的迁移我们做到了对用户几乎无感知，主要依赖新旧调度系统的打通和 DIFF。\n接下来介绍下具体是怎么做的。\n2.2.1. 新旧调度系统打通\n任务迁移阶段，一部分任务运行在新的调度系统上，一部分运行在原有调度系统上，就需要解决两个问题：\n\n用户能够查看所有任务实例的运行情况，包括一些内部已经习惯的调度名词 (run_index、result_ftp、log_ftp、csv_result_path 等)，这部分信息在 DolphinScheduler 调度里显然没有\n任务和任务之间有依赖关系，两个系统间调度任务时，也需要查询对方系统调度的任务实例状态，用于判断当前任务依赖是否就绪。\n\n因此，我们在迁移阶段，架构是这样：\n\n核心设计有两处。\n首先任务实例状态统一到原调度系统数据库，对平台而言：\n\n查询方式、字段、API 跟之前一致\n任务更新时，如果该任务已经迁移到了新调度系统，则同时更新 DolphinScheduler 里的工作流定义\n\n因此平台在使用上，对用户没有感知。\n其次我们修改了 DolphinScheduler DependentTaskProcessor 的代码，支持查询 DolphinScheduler 及原有调度系统的任务实例状态。这样 DolphinScheduler 调度的任务，就可以自由依赖两个调度系统的任务实例了。\n因此在调度能力上，也做到了对用户没有感知。\n上述架构，未来在迁移完成后，就可以仅通过 UDA-API + DolphinScheduler 提供完整的调度能力了。\n同时，我们在配置依赖的易用性上也做了优化，历史上支持了多种依赖方式：文件依赖、任务依赖、hql依赖、prestosql 依赖等。后两者都需要用户手动配置查询对应表，我们都优化为了表依赖。平台解析用户的 sql，针对读取的表，自动添加对应的依赖。既提高了易用性，也对用户屏蔽了底层具体表存储类型 (Hive/Presto/Iceberg/...) 的细节：\n\n对任务依赖，也支持了全局搜索、偏移量、偏移单位以进一步提高易用性。\n2.2.2. 新旧调度系统 DIFF\n其次是新旧调度系统的 DIFF.\n作为基础平台，服务的业务线众多，再加上 YARN 资源极其紧张，因此我们对调度系统的稳定性要求很高。为了确保迁移顺利，专门基于 DolphinScheduler DryRun 的能力做了一版定制：\n\n所谓镜像任务，是指我们在迁移新调度之前，会先在 DolphinScheduler 镜像一份完全相同的任务，任务同样经过变量替换等操作，只是该任务标记了不真正执行。\n这样我们就可以比较两个系统间的 DIFF，主要包括：\n\n调度时间是否基本一致：用于验证依赖配置、定时设置等的兼容性\nSQL 是否完全一致：验证变量替换、SQL 屏蔽、队列配置后，真正提交的 SQL 是否完全相同\n\n经过上述空跑观察一段时间，确保无 diff 后，线上任务就真正迁移到新的调度引擎上了。\n2.2.3. 系统的可观测性\n在有限的时间里，我们做了上述准备，但是仍然不够充分。\n系统需要具备良好的可观测性，DolphinScheduler 对外提供了 Prometheus 格式的基础指标。我们增加了一些高优指标，同时转化为 Falcon 格式对接到公司内部的监控系统。\n通过监控大盘来查看调度系统的健康状况，并针对不同级别的指标和阈值，配置电话 / 钉钉报警：\n\n可观测性提高后，分析问题的人力成本也得到控制，例如对于这种曲线:\n\n容易观察到在非工作时间曲线值基本为 0，因此就能判断指标异常 (=1) 很可能是用户修改后触发的，相比之前出现问题只能靠猜和逐台机器登录分析日志的方式，通过 metrics 分析能够更早发现和预警问题。\n在迁移启动后，对于 misfire、worker 线程池饱和度、连接池饱和度、io-util、overload 等指标，都重点关注和评估，以确保迁移顺利。\n2.3. 迁移收益\n目前迁移已经进行了一大半，我们针对新旧调度系统的数据库以及调度机资源使用做了对比：\n\n\n数据库：\n\n\nQPS: 10000+ -&gt; 500\n\n\n负载：4.0 -&gt; 1.0\n\n\n\n\n资源使用降低 65%\n\n\n我们在迁移过程中，通过 DolphinScheduler 以极低的开发成本支持了 SparkSQL、DorisSQL，以及高版本 PrestoSQL 这类业务新的调度需求。\n功能上的其他对比：\n\n3. 未来规划\n\n例行任务、调试能力全部迁移 DolphinScheduler，沉淀线上操作SOP\n结合社区的容器化进度，实现模块 K8S 部署。当前 API 模块已经在生产环境使用，Worker、Master 进行中\n全链路的一键数据回溯能力\n离线、实时平台打通\n\n",
    "title": "作业帮基于 DolphinScheduler 的数据开发平台实践",
    "time": "2024-01-11"
  },
  {
    "name": "dolphinscheduler_json",
    "content": "dolphinscheduler json拆解\n1、为什么拆解json\n在dolphinscheduler 1.3.x及以前的工作流中的任务及任务关系保存时是以大json的方式保存到数据库中process_definiton表的process_definition_json字段，如果某个工作流很大比如有100或者1000个任务，这个json字段也就非常大，在使用时需要解析json，非常耗费性能，且任务没法重用；基于大json，在工作流版本及任务版本上也没有好的实现方案，否则会导致数据大量冗余。\n故社区计划启动json拆解项目，实现的需求目标：\n\n大json完全拆分\n新增工作流及任务版本\n引入全局唯一键(code)\n\n2、如何设计拆解后的表\n1、1.3.6版本工作流\n1、比如在当前1.3.6版本创建个a--&gt;b的工作流\n\n以下是processDefiniton save 接口在controller入口打印的入参日志\ncreate  process definition, project name: hadoop, process definition name: ab, process_definition_json: {&quot;globalParams&quot;:[],&quot;tasks&quot;:[{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-77643&quot;,&quot;name&quot;:&quot;a&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[{&quot;prop&quot;:&quot;yesterday&quot;,&quot;direct&quot;:&quot;IN&quot;,&quot;type&quot;:&quot;VARCHAR&quot;,&quot;value&quot;:&quot;${system.biz.date}&quot;}],&quot;rawScript&quot;:&quot;echo ${yesterday}&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[]},{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-99814&quot;,&quot;name&quot;:&quot;b&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[{&quot;prop&quot;:&quot;today&quot;,&quot;direct&quot;:&quot;IN&quot;,&quot;type&quot;:&quot;VARCHAR&quot;,&quot;value&quot;:&quot;${system.biz.curdate}&quot;}],&quot;rawScript&quot;:&quot;echo ${today}&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;a&quot;]}],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}, desc:  locations:{&quot;tasks-77643&quot;:{&quot;name&quot;:&quot;a&quot;,&quot;targetarr&quot;:&quot;&quot;,&quot;nodenumber&quot;:&quot;1&quot;,&quot;x&quot;:251,&quot;y&quot;:166},&quot;tasks-99814&quot;:{&quot;name&quot;:&quot;b&quot;,&quot;targetarr&quot;:&quot;tasks-77643&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:533,&quot;y&quot;:161}}, connects:[{&quot;endPointSourceId&quot;:&quot;tasks-77643&quot;,&quot;endPointTargetId&quot;:&quot;tasks-99814&quot;}]\n\n2、依赖节点的工作流，dep是依赖节点\n\n以下是processDefiniton save 接口在controller入口打印的入参日志\n create  process definition, project name: hadoop, process definition name: dep_c, process_definition_json: {&quot;globalParams&quot;:[],&quot;tasks&quot;:[{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-69503&quot;,&quot;name&quot;:&quot;c&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 11&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;dep&quot;]},{&quot;type&quot;:&quot;DEPENDENT&quot;,&quot;id&quot;:&quot;tasks-22756&quot;,&quot;name&quot;:&quot;dep&quot;,&quot;params&quot;:{},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{&quot;relation&quot;:&quot;AND&quot;,&quot;dependTaskList&quot;:[{&quot;relation&quot;:&quot;AND&quot;,&quot;dependItemList&quot;:[{&quot;projectId&quot;:1,&quot;definitionId&quot;:1,&quot;depTasks&quot;:&quot;b&quot;,&quot;cycle&quot;:&quot;day&quot;,&quot;dateValue&quot;:&quot;today&quot;}]}]},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[]}],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}, desc:  locations:{&quot;tasks-69503&quot;:{&quot;name&quot;:&quot;c&quot;,&quot;targetarr&quot;:&quot;tasks-22756&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:597,&quot;y&quot;:166},&quot;tasks-22756&quot;:{&quot;name&quot;:&quot;dep&quot;,&quot;targetarr&quot;:&quot;&quot;,&quot;nodenumber&quot;:&quot;1&quot;,&quot;x&quot;:308,&quot;y&quot;:164}}, connects:[{&quot;endPointSourceId&quot;:&quot;tasks-22756&quot;,&quot;endPointTargetId&quot;:&quot;tasks-69503&quot;}]\n\n3、条件判断的工作流\n\n以下是processDefiniton save 接口在controller入口打印的入参日志\ncreate  process definition, project name: hadoop, process definition name: condition_test, process_definition_json: {&quot;globalParams&quot;:[],&quot;tasks&quot;:[{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-68456&quot;,&quot;name&quot;:&quot;d&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 11&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[]},{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-58183&quot;,&quot;name&quot;:&quot;e&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 22&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;cond&quot;]},{&quot;type&quot;:&quot;SHELL&quot;,&quot;id&quot;:&quot;tasks-43996&quot;,&quot;name&quot;:&quot;f&quot;,&quot;params&quot;:{&quot;resourceList&quot;:[],&quot;localParams&quot;:[],&quot;rawScript&quot;:&quot;echo 33&quot;},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;&quot;],&quot;failedNode&quot;:[&quot;&quot;]},&quot;dependence&quot;:{},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;cond&quot;]},{&quot;type&quot;:&quot;CONDITIONS&quot;,&quot;id&quot;:&quot;tasks-38972&quot;,&quot;name&quot;:&quot;cond&quot;,&quot;params&quot;:{},&quot;description&quot;:&quot;&quot;,&quot;timeout&quot;:{&quot;strategy&quot;:&quot;&quot;,&quot;interval&quot;:null,&quot;enable&quot;:false},&quot;runFlag&quot;:&quot;NORMAL&quot;,&quot;conditionResult&quot;:{&quot;successNode&quot;:[&quot;e&quot;],&quot;failedNode&quot;:[&quot;f&quot;]},&quot;dependence&quot;:{&quot;relation&quot;:&quot;AND&quot;,&quot;dependTaskList&quot;:[{&quot;relation&quot;:&quot;AND&quot;,&quot;dependItemList&quot;:[{&quot;depTasks&quot;:&quot;d&quot;,&quot;status&quot;:&quot;SUCCESS&quot;}]}]},&quot;maxRetryTimes&quot;:&quot;0&quot;,&quot;retryInterval&quot;:&quot;1&quot;,&quot;taskInstancePriority&quot;:&quot;MEDIUM&quot;,&quot;workerGroup&quot;:&quot;default&quot;,&quot;preTasks&quot;:[&quot;d&quot;]}],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}, desc:  locations:{&quot;tasks-68456&quot;:{&quot;name&quot;:&quot;d&quot;,&quot;targetarr&quot;:&quot;&quot;,&quot;nodenumber&quot;:&quot;1&quot;,&quot;x&quot;:168,&quot;y&quot;:158},&quot;tasks-58183&quot;:{&quot;name&quot;:&quot;e&quot;,&quot;targetarr&quot;:&quot;tasks-38972&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:573,&quot;y&quot;:82},&quot;tasks-43996&quot;:{&quot;name&quot;:&quot;f&quot;,&quot;targetarr&quot;:&quot;tasks-38972&quot;,&quot;nodenumber&quot;:&quot;0&quot;,&quot;x&quot;:591,&quot;y&quot;:288},&quot;tasks-38972&quot;:{&quot;name&quot;:&quot;cond&quot;,&quot;targetarr&quot;:&quot;tasks-68456&quot;,&quot;nodenumber&quot;:&quot;2&quot;,&quot;x&quot;:382,&quot;y&quot;:175}}, connects:[{&quot;endPointSourceId&quot;:&quot;tasks-68456&quot;,&quot;endPointTargetId&quot;:&quot;tasks-38972&quot;},{&quot;endPointSourceId&quot;:&quot;tasks-38972&quot;,&quot;endPointTargetId&quot;:&quot;tasks-58183&quot;},{&quot;endPointSourceId&quot;:&quot;tasks-38972&quot;,&quot;endPointTargetId&quot;:&quot;tasks-43996&quot;}]\n\n从以上三个案例中，我们知道controller的入口参数的每个参数都可以在t_ds_process_definition表中找到对应，故表中数据如下图\n\n2、拆解后的表设计思路\n工作流只是dag的展现形式，任务通过工作流进行组织，组织的同时存在了任务之间的关系，也就是依赖。就好比一个画板，画板上有些图案，工作流就是画板，图案就是任务，图案之间的关系就是依赖。而调度的核心是调度任务，依赖只是表述调度的先后顺序。当前定时还是对整个工作流进行的定时，拆解后就方便对单独任务进行调度。正是基于这个思想设计了拆解的思路，所以这就需要三张表，工作流定义表、任务定义表、任务关系表。\n\n工作流定义表：描述工作流的基本信息，比如全局参数、dag中节点的位置信息\n任务定义表：描述任务的详情信息，比如任务类别、任务容错信息、优先级等\n任务关系表：描述任务的关系信息，比如当前节点、前置节点等\n\n基于这个设计思想再扩展到版本，无非是对于这三张表，每张表新增个保存版本的日志表。\n工作流定义表\n现在看案例中save接口日志，现有字段(project、process_definition_name、desc、locations、connects)，对于json中除了task之外的还剩下\n{&quot;globalParams&quot;:[],&quot;tenantId&quot;:1,&quot;timeout&quot;:0}\n\n所以可知工作流定义表：\nCREATE TABLE `t_ds_process_definition` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;self-increasing id&#x27;,\n  `code` bigint(20) NOT NULL COMMENT &#x27;encoding&#x27;,\n  `name` varchar(200) DEFAULT NULL COMMENT &#x27;process definition name&#x27;,\n  `version` int(11) DEFAULT NULL COMMENT &#x27;process definition version&#x27;,\n  `description` text COMMENT &#x27;description&#x27;,\n  `project_code` bigint(20) NOT NULL COMMENT &#x27;project code&#x27;,\n  `release_state` tinyint(4) DEFAULT NULL COMMENT &#x27;process definition release state：0:offline,1:online&#x27;,\n  `user_id` int(11) DEFAULT NULL COMMENT &#x27;process definition creator id&#x27;,\n  `global_params` text COMMENT &#x27;global parameters&#x27;,\n  `flag` tinyint(4) DEFAULT NULL COMMENT &#x27;0 not available, 1 available&#x27;,\n  `locations` text COMMENT &#x27;Node location information&#x27;,\n  `connects` text COMMENT &#x27;Node connection information&#x27;,\n  `warning_group_id` int(11) DEFAULT NULL COMMENT &#x27;alert group id&#x27;,\n  `timeout` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;time out, unit: minute&#x27;,\n  `tenant_id` int(11) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;tenant id&#x27;,\n  `create_time` datetime NOT NULL COMMENT &#x27;create time&#x27;,\n  `update_time` datetime DEFAULT NULL COMMENT &#x27;update time&#x27;,\n  PRIMARY KEY (`id`,`code`),\n  UNIQUE KEY `process_unique` (`name`,`project_code`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_process_definition_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;self-increasing id&#x27;,\n  `code` bigint(20) NOT NULL COMMENT &#x27;encoding&#x27;,\n  `name` varchar(200) DEFAULT NULL COMMENT &#x27;process definition name&#x27;,\n  `version` int(11) DEFAULT NULL COMMENT &#x27;process definition version&#x27;,\n  `description` text COMMENT &#x27;description&#x27;,\n  `project_code` bigint(20) NOT NULL COMMENT &#x27;project code&#x27;,\n  `release_state` tinyint(4) DEFAULT NULL COMMENT &#x27;process definition release state：0:offline,1:online&#x27;,\n  `user_id` int(11) DEFAULT NULL COMMENT &#x27;process definition creator id&#x27;,\n  `global_params` text COMMENT &#x27;global parameters&#x27;,\n  `flag` tinyint(4) DEFAULT NULL COMMENT &#x27;0 not available, 1 available&#x27;,\n  `locations` text COMMENT &#x27;Node location information&#x27;,\n  `connects` text COMMENT &#x27;Node connection information&#x27;,\n  `warning_group_id` int(11) DEFAULT NULL COMMENT &#x27;alert group id&#x27;,\n  `timeout` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;time out,unit: minute&#x27;,\n  `tenant_id` int(11) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;tenant id&#x27;,\n  `operator` int(11) DEFAULT NULL COMMENT &#x27;operator user id&#x27;,\n  `operate_time` datetime DEFAULT NULL COMMENT &#x27;operate time&#x27;,\n  `create_time` datetime NOT NULL COMMENT &#x27;create time&#x27;,\n  `update_time` datetime DEFAULT NULL COMMENT &#x27;update time&#x27;,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n从表字段可以看出 日志表仅仅比主表多了两个字段operator、operate_time\n任务定义表\n案例中ab工作流task的json\n\t&quot;tasks&quot;: [{\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-77643&quot;,\n\t\t&quot;name&quot;: &quot;a&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [{\n\t\t\t\t&quot;prop&quot;: &quot;yesterday&quot;,\n\t\t\t\t&quot;direct&quot;: &quot;IN&quot;,\n\t\t\t\t&quot;type&quot;: &quot;VARCHAR&quot;,\n\t\t\t\t&quot;value&quot;: &quot;${system.biz.date}&quot;\n\t\t\t}],\n\t\t\t&quot;rawScript&quot;: &quot;echo ${yesterday}&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: []\n\t}, {\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-99814&quot;,\n\t\t&quot;name&quot;: &quot;b&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [{\n\t\t\t\t&quot;prop&quot;: &quot;today&quot;,\n\t\t\t\t&quot;direct&quot;: &quot;IN&quot;,\n\t\t\t\t&quot;type&quot;: &quot;VARCHAR&quot;,\n\t\t\t\t&quot;value&quot;: &quot;${system.biz.curdate}&quot;\n\t\t\t}],\n\t\t\t&quot;rawScript&quot;: &quot;echo ${today}&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;a&quot;]\n\t}]\n\ndep_c工作流task的json\n\t&quot;tasks&quot;: [{\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-69503&quot;,\n\t\t&quot;name&quot;: &quot;c&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 11&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;dep&quot;]\n\t}, {\n\t\t&quot;type&quot;: &quot;DEPENDENT&quot;,\n\t\t&quot;id&quot;: &quot;tasks-22756&quot;,\n\t\t&quot;name&quot;: &quot;dep&quot;,\n\t\t&quot;params&quot;: {},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {\n\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t&quot;dependTaskList&quot;: [{\n\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t&quot;dependItemList&quot;: [{\n\t\t\t\t\t&quot;projectId&quot;: 1,\n\t\t\t\t\t&quot;definitionId&quot;: 1,\n\t\t\t\t\t&quot;depTasks&quot;: &quot;b&quot;,\n\t\t\t\t\t&quot;cycle&quot;: &quot;day&quot;,\n\t\t\t\t\t&quot;dateValue&quot;: &quot;today&quot;\n\t\t\t\t}]\n\t\t\t}]\n\t\t},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: []\n\t}]\n\ncondition_test工作流task的json\n\t&quot;tasks&quot;: [{\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-68456&quot;,\n\t\t&quot;name&quot;: &quot;d&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 11&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: []\n\t}, {\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-58183&quot;,\n\t\t&quot;name&quot;: &quot;e&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 22&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;cond&quot;]\n\t}, {\n\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t&quot;id&quot;: &quot;tasks-43996&quot;,\n\t\t&quot;name&quot;: &quot;f&quot;,\n\t\t&quot;params&quot;: {\n\t\t\t&quot;resourceList&quot;: [],\n\t\t\t&quot;localParams&quot;: [],\n\t\t\t&quot;rawScript&quot;: &quot;echo 33&quot;\n\t\t},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;cond&quot;]\n\t}, {\n\t\t&quot;type&quot;: &quot;CONDITIONS&quot;,\n\t\t&quot;id&quot;: &quot;tasks-38972&quot;,\n\t\t&quot;name&quot;: &quot;cond&quot;,\n\t\t&quot;params&quot;: {},\n\t\t&quot;description&quot;: &quot;&quot;,\n\t\t&quot;timeout&quot;: {\n\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t&quot;interval&quot;: null,\n\t\t\t&quot;enable&quot;: false\n\t\t},\n\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t&quot;conditionResult&quot;: {\n\t\t\t&quot;successNode&quot;: [&quot;e&quot;],\n\t\t\t&quot;failedNode&quot;: [&quot;f&quot;]\n\t\t},\n\t\t&quot;dependence&quot;: {\n\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t&quot;dependTaskList&quot;: [{\n\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t&quot;dependItemList&quot;: [{\n\t\t\t\t\t&quot;depTasks&quot;: &quot;d&quot;,\n\t\t\t\t\t&quot;status&quot;: &quot;SUCCESS&quot;\n\t\t\t\t}]\n\t\t\t}]\n\t\t},\n\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t&quot;workerGroup&quot;: &quot;default&quot;,\n\t\t&quot;preTasks&quot;: [&quot;d&quot;]\n\t}]\n\n从案例中可以知道SHELL/DEPENDENT/CONDITIONS类型的节点的json构成（其他任务类似SHELL），preTasks标识前置依赖节点。conditionResult结构比较固定，而dependence结构复杂，DEPENDENT和CONDITIONS类型任务的dependence结构还不一样，所以为了统一，我们将conditionResult和dependence整体放到params中，params对应表字段的task_params。\n这样我们就确定了t_ds_task_definition表\nCREATE TABLE `t_ds_task_definition` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `code` bigint(20) NOT NULL COMMENT 'encoding',\n  `name` varchar(200) DEFAULT NULL COMMENT 'task definition name',\n  `version` int(11) DEFAULT NULL COMMENT 'task definition version',\n  `description` text COMMENT 'description',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `user_id` int(11) DEFAULT NULL COMMENT 'task definition creator id',\n  `task_type` varchar(50) NOT NULL COMMENT 'task type',\n  `task_params` text COMMENT 'job custom parameters',\n  `flag` tinyint(2) DEFAULT NULL COMMENT '0 not available, 1 available',\n  `task_priority` tinyint(4) DEFAULT NULL COMMENT 'job priority',\n  `worker_group` varchar(200) DEFAULT NULL COMMENT 'worker grouping',\n  `fail_retry_times` int(11) DEFAULT NULL COMMENT 'number of failed retries',\n  `fail_retry_interval` int(11) DEFAULT NULL COMMENT 'failed retry interval',\n  `timeout_flag` tinyint(2) DEFAULT '0' COMMENT 'timeout flag:0 close, 1 open',\n  `timeout_notify_strategy` tinyint(4) DEFAULT NULL COMMENT 'timeout notification policy: 0 warning, 1 fail',\n  `timeout` int(11) DEFAULT '0' COMMENT 'timeout length,unit: minute',\n  `delay_time` int(11) DEFAULT '0' COMMENT 'delay execution time,unit: minute',\n  `resource_ids` varchar(255) DEFAULT NULL COMMENT 'resource id, separated by comma',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`,`code`),\n  UNIQUE KEY `task_unique` (`name`,`project_code`) USING BTREE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_task_definition_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `code` bigint(20) NOT NULL COMMENT 'encoding',\n  `name` varchar(200) DEFAULT NULL COMMENT 'task definition name',\n  `version` int(11) DEFAULT NULL COMMENT 'task definition version',\n  `description` text COMMENT 'description',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `user_id` int(11) DEFAULT NULL COMMENT 'task definition creator id',\n  `task_type` varchar(50) NOT NULL COMMENT 'task type',\n  `task_params` text COMMENT 'job custom parameters',\n  `flag` tinyint(2) DEFAULT NULL COMMENT '0 not available, 1 available',\n  `task_priority` tinyint(4) DEFAULT NULL COMMENT 'job priority',\n  `worker_group` varchar(200) DEFAULT NULL COMMENT 'worker grouping',\n  `fail_retry_times` int(11) DEFAULT NULL COMMENT 'number of failed retries',\n  `fail_retry_interval` int(11) DEFAULT NULL COMMENT 'failed retry interval',\n  `timeout_flag` tinyint(2) DEFAULT '0' COMMENT 'timeout flag:0 close, 1 open',\n  `timeout_notify_strategy` tinyint(4) DEFAULT NULL COMMENT 'timeout notification policy: 0 warning, 1 fail',\n  `timeout` int(11) DEFAULT '0' COMMENT 'timeout length,unit: minute',\n  `delay_time` int(11) DEFAULT '0' COMMENT 'delay execution time,unit: minute',\n  `resource_ids` varchar(255) DEFAULT NULL COMMENT 'resource id, separated by comma',\n  `operator` int(11) DEFAULT NULL COMMENT 'operator user id',\n  `operate_time` datetime DEFAULT NULL COMMENT 'operate time',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n注意：dev版本和1.3.6版本区别，dev版本已将description换成desc，并且新增了delayTime\n{\n\t&quot;globalParams&quot;: [],\n\t&quot;tasks&quot;: [{\n\t\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-18200&quot;,\n\t\t\t&quot;name&quot;: &quot;d&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {\n\t\t\t\t&quot;resourceList&quot;: [],\n\t\t\t\t&quot;localParams&quot;: [],\n\t\t\t\t&quot;rawScript&quot;: &quot;echo 5&quot;\n\t\t\t},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [],\n\t\t\t&quot;depList&quot;: null\n\t\t},\n\t\t{\n\t\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-55225&quot;,\n\t\t\t&quot;name&quot;: &quot;e&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {\n\t\t\t\t&quot;resourceList&quot;: [],\n\t\t\t\t&quot;localParams&quot;: [],\n\t\t\t\t&quot;rawScript&quot;: &quot;echo 6&quot;\n\t\t\t},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [\n\t\t\t\t&quot;def&quot;\n\t\t\t],\n\t\t\t&quot;depList&quot;: null\n\t\t},\n\t\t{\n\t\t\t&quot;type&quot;: &quot;SHELL&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-67639&quot;,\n\t\t\t&quot;name&quot;: &quot;f&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {\n\t\t\t\t&quot;resourceList&quot;: [],\n\t\t\t\t&quot;localParams&quot;: [],\n\t\t\t\t&quot;rawScript&quot;: &quot;echo 7&quot;\n\t\t\t},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [\n\t\t\t\t&quot;def&quot;\n\t\t\t],\n\t\t\t&quot;depList&quot;: null\n\t\t},\n\t\t{\n\t\t\t&quot;type&quot;: &quot;CONDITIONS&quot;,\n\t\t\t&quot;id&quot;: &quot;tasks-67387&quot;,\n\t\t\t&quot;name&quot;: &quot;def&quot;,\n\t\t\t&quot;code&quot;: &quot;&quot;,\n\t\t\t&quot;params&quot;: {},\n\t\t\t&quot;desc&quot;: &quot;&quot;,\n\t\t\t&quot;runFlag&quot;: &quot;NORMAL&quot;,\n\t\t\t&quot;conditionResult&quot;: {\n\t\t\t\t&quot;successNode&quot;: [\n\t\t\t\t\t&quot;e&quot;\n\t\t\t\t],\n\t\t\t\t&quot;failedNode&quot;: [\n\t\t\t\t\t&quot;f&quot;\n\t\t\t\t]\n\t\t\t},\n\t\t\t&quot;dependence&quot;: {\n\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t&quot;dependTaskList&quot;: [{\n\t\t\t\t\t&quot;relation&quot;: &quot;AND&quot;,\n\t\t\t\t\t&quot;dependItemList&quot;: [{\n\t\t\t\t\t\t\t&quot;depTasks&quot;: &quot;d&quot;,\n\t\t\t\t\t\t\t&quot;status&quot;: &quot;SUCCESS&quot;\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t&quot;depTasks&quot;: &quot;d&quot;,\n\t\t\t\t\t\t\t&quot;status&quot;: &quot;FAILURE&quot;\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}]\n\t\t\t},\n\t\t\t&quot;maxRetryTimes&quot;: &quot;0&quot;,\n\t\t\t&quot;retryInterval&quot;: &quot;1&quot;,\n\t\t\t&quot;delayTime&quot;: &quot;0&quot;,\n\t\t\t&quot;timeout&quot;: {\n\t\t\t\t&quot;strategy&quot;: &quot;&quot;,\n\t\t\t\t&quot;interval&quot;: null,\n\t\t\t\t&quot;enable&quot;: false\n\t\t\t},\n\t\t\t&quot;waitStartTimeout&quot;: {},\n\t\t\t&quot;taskInstancePriority&quot;: &quot;MEDIUM&quot;,\n\t\t\t&quot;workerGroup&quot;: &quot;hadoop&quot;,\n\t\t\t&quot;preTasks&quot;: [\n\t\t\t\t&quot;d&quot;\n\t\t\t],\n\t\t\t&quot;depList&quot;: null\n\t\t}\n\t],\n\t&quot;tenantId&quot;: 1,\n\t&quot;timeout&quot;: 0\n}\n\n任务关系表\npreTasks标识前置依赖节点，当前节点在关系表中使用postTask标识。由于当前节点肯定存在而前置节点不一定存在，所以post不可能为空，而preTask有可能为空\nCREATE TABLE `t_ds_process_task_relation` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `name` varchar(200) DEFAULT NULL COMMENT 'relation name',\n  `process_definition_version` int(11) DEFAULT NULL COMMENT 'process version',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `process_definition_code` bigint(20) NOT NULL COMMENT 'process code',\n  `pre_task_code` bigint(20) NOT NULL COMMENT 'pre task code',\n  `pre_task_version` int(11) NOT NULL COMMENT 'pre task version',\n  `post_task_code` bigint(20) NOT NULL COMMENT 'post task code',\n  `post_task_version` int(11) NOT NULL COMMENT 'post task version',\n  `condition_type` tinyint(2) DEFAULT NULL COMMENT 'condition type : 0 none, 1 judge 2 delay',\n  `condition_params` text COMMENT 'condition params(json)',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_process_task_relation_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `name` varchar(200) DEFAULT NULL COMMENT 'relation name',\n  `process_definition_version` int(11) DEFAULT NULL COMMENT 'process version',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `process_definition_code` bigint(20) NOT NULL COMMENT 'process code',\n  `pre_task_code` bigint(20) NOT NULL COMMENT 'pre task code',\n  `pre_task_version` int(11) NOT NULL COMMENT 'pre task version',\n  `post_task_code` bigint(20) NOT NULL COMMENT 'post task code',\n  `post_task_version` int(11) NOT NULL COMMENT 'post task version',\n  `condition_type` tinyint(2) DEFAULT NULL COMMENT 'condition type : 0 none, 1 judge 2 delay',\n  `condition_params` text COMMENT 'condition params(json)',\n  `operator` int(11) DEFAULT NULL COMMENT 'operator user id',\n  `operate_time` datetime DEFAULT NULL COMMENT 'operate time',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n对于依赖关系复杂的场景\n\n3、API模块如何改造\n\n\n[ ] api模块进行save操作时\n\n\n通过雪花算法生成13位的数字作为process_definition_code，工作流定义保存至process_definition（主表）和process_definition_log（日志表），这两个表保存的是同样的数据，工作流定义版本为1\n通过雪花算法生成13位的数字作为task_definition_code，任务定义表保存至task_definition（主表）和task_definition_log（日志表），也是保存同样的数据，任务定义版本为1\n工作流任务关系保存在 process_task_relation（主表）和process_task_relation_log（日志表），该表保存的code和version是工作流的code和version，因为任务是通过工作流进行组织，以工作流来画dag。也是通过post_task_code和post_task_version标识dag的当前节点，这个节点的前置依赖通过pre_task_code和pre_task_version来标识，如果没有依赖，pre_task_code和pre_task_version为0\n\n\n\n[ ] api模块进行update操作时，工作流定义和任务定义直接更新主表数据，更新后的数据insert到日志表。关系表主表先删除然后再插入新的关系，关系表日志表直接插入新的关系\n\n\n[ ] api模块进行delete操作时，工作流定义、任务定义和关系表直接删除主表数据，日志表数据不变动\n\n\n[ ] api模块进行switch操作时，直接将日志表中对应version数据覆盖到主表\n\n\n4、数据交互如何改造\n\n\n\n[ ] 在json拆分一期api模块controller层整体未变动，传入的大json还是在service层映射为ProcessData对象。insert或update操作在公共Service模块通过ProcessService.saveProcessDefiniton()入口完成保存数据库操作，按照task_definition、process_task_relation、process_definition的顺序保存。保存时，如果该任务已经存在并且关联的工作流未上线，则更改任务；如果任务关联的工作流已上线，则不允许更改任务\n\n\n[ ] api查询操作时，当前还是通过工作流id来查询，在公共Service模块通过ProcessService.genTaskNodeList()入口完成数据组装，还是组装为ProcessData对象，进而生成json返回\n\n\n[ ] Server模块（Master）也是通过公共Service模块ProcessService.genTaskNodeList()获得TaskNodeList生成调度dag，把当前任务所有信息放到 MasterExecThread.readyToSubmitTaskQueue队列，以便生成taskInstance，dispatch给worker\n\n\n5、当前json还需做什么\n\ncontroller对外restAPI接口改造\nui模块dag改造\nui模块新增task操作页面\n\nprocessDefinition\n\n\n\n接口名称\n参数名称\n参数说明\n数据类型\n\n\n\n\n\nprojectName\n项目名称\nstring\n\n\n\nname\n流程定义名称\nstring\n\n\n\ndescription\n流程定义描述信息\nstring\n\n\n\nglobalParams\n全局参数\nstring\n\n\nsave\nconnects\n流程定义节点图标连接信息(json格式)\nstring\n\n\n\nlocations\n流程定义节点坐标位置信息(json格式)\nstring\n\n\n\ntimeout\n超时分钟数\nint\n\n\n\ntenantId\n租户id\nint\n\n\n\ntaskRelationJson\n任务关系(json格式)\nstring\n\n\n--\n--\n--\n--\n\n\n\nprojectName\n项目名称\nstring\n\n\n\ncode\n流程定义code\nlong\n\n\n\nname\n流程定义名称\nstring\n\n\n\ndescription\n流程定义描述信息\nstring\n\n\nupdate\nreleaseState\n发布流程定义,可用值:OFFLINE,ONLINE\nref\n\n\n\nconnects\n流程定义节点图标连接信息(json格式)\nstring\n\n\n\nlocations\n流程定义节点坐标位置信息(json格式)\nstring\n\n\n\ntimeout\n超时分钟数\nint\n\n\n\ntenantId\n租户id\nint\n\n\n\ntaskRelationJson\n任务关系(json格式)\nstring\n\n\n\n--\n--\n--\n\n\n\ncode\n流程定义code\nlong\n\n\nswitch/deleteCode\nprojectName\n项目名称\nstring\n\n\n\nversion\n版本号\nstring\n\n\n\n备注：taskRelationJson格式：[{&quot;name&quot;:&quot;&quot;,&quot;pre_task_code&quot;:0,&quot;pre_task_version&quot;:0,&quot;post_task_code&quot;:123456789,&quot;post_task_version&quot;:1,&quot;condition_type&quot;:0,&quot;condition_params&quot;:{}},{&quot;name&quot;:&quot;&quot;,&quot;pre_task_code&quot;:123456789,&quot;pre_task_version&quot;:1,&quot;post_task_code&quot;:123451234,&quot;post_task_version&quot;:1,&quot;condition_type&quot;:0,&quot;condition_params&quot;:{}}]\n同理其他接口请求参数processDefinitionId换成code\nschedule\n\n\n\n接口名称\n参数名称\n参数说明\n数据类型\n\n\n\n\n\ncode\n流程定义code\nlong\n\n\n\nprojectName\n项目名称\nstring\n\n\n\nfailureStrategy\n失败策略,可用值:END,CONTINUE\nstring\n\n\ncreateSchedule\nprocessInstancePriority\n流程实例优先级,可用值:HIGHEST,HIGH,MEDIUM,LOW,LOWEST\nstring\n\n\n\nschedule\n定时\nstring\n\n\n\nwarningGroupId\n发送组ID\nint\n\n\n\nwarningType\n发送策略,可用值:NONE,SUCCESS,FAILURE,ALL\nstring\n\n\n\nworkerGroup\nworkerGroup\nstring\n\n\n--\n--\n--\n--\n\n\n\ncode\n流程定义code\nlong\n\n\n\nprojectName\n项目名称\nstring\n\n\nqueryScheduleListPaging\npageNo\n页码号\nint\n\n\n\npageSize\n页大小\nint\n\n\n\nsearchVal\n搜索值\nstring\n\n\n\ntaskDefinition(新增)\n\n\n\n接口名称\n参数名称\n参数说明\n数据类型\n\n\n\n\nsave\nprojectName\n项目名称\n\n\n\n\ntaskDefinitionJson\ntask信息\nstring\n\n\n--\n--\n--\n--\n\n\n\nprojectName\n项目名称\nstring\n\n\nupdate\ncode\ntask code\nlong\n\n\n\ntaskDefinitionJson\ntask信息\nstring\n\n\n--\n--\n--\n--\n\n\n\nprojectName\n项目名称\nstring\n\n\nswitch/deleteCode\ntask_code\ntaskCode\nlong\n\n\n\nversion\n版本号\nint\n\n\n--\n--\n--\n--\n\n\nquery/delete\nprojectName\n项目名称\nstring\n\n\n\ntask_code\ntask code\nlong\n\n\n--\n--\n--\n--\n\n\nqueryTaskListPaging\n\n\n\n\n\n\ntaskDefinitionJson：[{&quot;name&quot;:&quot;test&quot;,&quot;description&quot;:&quot;&quot;,&quot;task_type&quot;:&quot;SHELL&quot;,&quot;task_params&quot;:[],&quot;flag&quot;:0,&quot;task_priority&quot;:0,&quot;worker_group&quot;:&quot;default&quot;,&quot;fail_retry_times&quot;:0,&quot;fail_retry_interval&quot;:0,&quot;timeout_flag&quot;:0,&quot;timeout_notify_strategy&quot;:0,&quot;timeout&quot;:0,&quot;delay_time&quot;:0,&quot;resource_ids&quot;:&quot;&quot;}]\n对应需求issue\n[Feature][JsonSplit-api] api module controller design #5498 \n1. [Feature][JsonSplit-api]processDefinition save/update interface  #5499 \n2. [Feature][JsonSplit-api]processDefinition switch interface #5501 \n3. [Feature][JsonSplit-api]processDefinition delete interface #5502 \n4. [Feature][JsonSplit-api]processDefinition copy interface #5503 \n5. [Feature][JsonSplit-api]processDefinition export interface #5504 \n6. [Feature][JsonSplit-api]processDefinition list-paging interface #5505 \n7. [Feature][JsonSplit-api]processDefinition move interface #5506 \n8. [Feature][JsonSplit-api]processDefinition queryProcessDefinitionAllByProjectId interface #5507 \n9. [Feature][JsonSplit-api]processDefinition select-by-id interface #5508 \n10. [Feature][JsonSplit-api]processDefinition view-tree interface #5509 \n11. [Feature][JsonSplit-api]schedule create interface #5510 \n12. [Feature][JsonSplit-api]schedule list-paging interface #5511 \n13. [Feature][JsonSplit-api]schedule update interface #5512 \n14. [Feature][JsonSplit-api]taskDefinition save interface #5513 \n15. [Feature][JsonSplit-api]taskDefinition update interface #5514 \n16. [Feature][JsonSplit-api]taskDefinition switch interface #5515 \n17. [Feature][JsonSplit-api]taskDefinition query interface #5516 \n18. [Feature][JsonSplit-api]taskDefinition delete interface #5517 \n19. [Feature][JsonSplit-api]WorkFlowLineage interface #5518 \n20. [Feature][JsonSplit-api]analysis interface #5519 \n21. [Feature][JsonSplit-api]executors interface #5520 \n22. [Feature][JsonSplit-api]processInstance interface #5521 \n23. [Feature][JsonSplit-api]project interface #5522 \n\n",
    "title": "DolphinScheduler 核心之 DAG 大 JSON 拆分详解",
    "time": "2021-05-29"
  },
  {
    "name": "ipalfish_tech_platform",
    "content": "伴鱼数据质量平台实践\n伴鱼少儿英语是目前飞速成长的互联网在线英语教育品牌，期望打造更创新、更酷、让学英语更有效的新一代互联网产品。博客官网\n日常工作中，数据开发、数仓开发工程师开发上线完一个任务后并不是就可以高枕无忧了，时常会因为上游链路数据异常或者自身处理逻辑的 BUG 导致产出的数据结果不可信。而这个问题的发现可能会经历一个较长的周期（尤其是离线场景），往往是业务方通过上层数据报表发现数据异常后 push 数据方去定位问题（对于一个较冷的报表，这个周期可能会更长）。同时，由于数据加工链路较长需要借助数据的血缘关系逐个任务排查，也会导致问题的定位难度增大，严重影响开发人员的工作效率。更有甚者，如果数据问题没有被及时发现，可能导致业务方作出错误的决策。此类问题可统一归属为大数据领域数据质量的问题。\n本文将向大家介绍伴鱼基础架构数据团队在应对该类问题时推出的平台化产品 - 数据质量中心（Data Quality Center, DQC）的设计与实现，但这与 DolphinScheduler 有什么关系呢？且听伴鱼的伙伴细细道来。\n调研\n业内关于数据质量平台化的产品介绍不多，我们主要对两个开源产品和一个云平台产品进行了调研，下面将主要介绍开源产品。\nApache Griffin\nApache Griffin是 eBay 开源的一款基于 Apache Hadoop 和 Apache Spark 的数据质量服务平台。其架构图如下：\n\n架构图从 High Level 层面清晰地展示了数据质量平台的三个核心流程：\n\n\nDefine：数据质检规则（指标）的定义。\n\n\nMeasure：数据质检任务的执行，基于 Spark 引擎实现。\n\n\nAnalyze：数据质检结果量化及可视化展示。同时，平台对数据质检规则进行了分类（这也是目前业内普遍认可的数据质量的六大标准）:\n\n\nAccuracy：准确性。如是否符合表的加工逻辑。\n\n\nCompleteness：完备性。如数据是否存在丢失。\n\n\nTimeliness：及时性。如表数据是否按时产生。\n\n\nUniqueness：唯一性。如主键字段是否唯一。\n\n\nValidity：合规性。如字段长度是否合规、枚举值集合是否合规。\n\n\nConsistency：一致性。如表与表之间在某些字段上是否存在矛盾。\n\n\n目前该开源项目仅在 Accuracy 类的规则上进行了实现。\nGriffin 是一个完全闭环的平台化产品。其质检任务的执行依赖于内置定时调度器的调度，调度执行时间由用户在 UI 上设定。任务将通过 Apache Livy 组件提交至配置的 Spark 集群。这也就意味着质检的实时性难以保障，我们无法对产出异常数据的任务进行强行阻断，二者不是在同一个调度平台被调度，时序上也不能保持串行。\nQualitis\nQualitis  是微众银行开源的一款数据质量管理系统。同样，它提供了一整套统一的流程来定义和检测数据集的质量并及时报告问题。从整个流程上看我们依然可以用 Define、Measure 和 Analyze 描述。它是基于其开源的另一款组件 Linkis 进行计算任务的代理分发，底层依赖 Spark 引擎，同时可以与其开源的 DataSphereStudio 任务开发平台无缝衔接，也就实现了在任务执行的工作流中嵌入质检任务，满足质检时效性的要求。可见，Qualitis 需要借助微众银行开源的一系列产品才能达到满意的效果。\nDataWorks 数据质量\nDataWorks 是阿里云上提供的一站式大数据工场，其中就包括了数据质量在内的产品解决方案。同样，它的实现依赖于阿里云上其他产品组件的支持。不过不得不说 DataWorks 数据质量部分的使用介绍从产品形态上给了我们很大的帮助，对于我们的产品设计非常具有指导性的作用。\n设计目标\n经过一番调研，我们确定了 DQC 的设计目标，主要包括以下几点：\n\n目前暂且只支持离线部分的数据质量管理。\n支持通用的规则描述和规则管理。\n质检任务由公司内部统一的调度引擎调度执行，可支持对质检结果异常的任务进行强阻断。同时，尽量降低质检功能对调度引擎的代码侵入。\n支持质检结果的可视化。\n\nDQC 系统设计\n背景补充\n伴鱼离线调度开发平台是基于 Apache DolphinScheduler 实现的。它是一个分布式去中心化，易扩展的可视化 DAG 调度系统，支持包括 Shell、Python、Spark、Flink 等多种类型的 Task 任务，并具有很好的扩展性。架构如下图所示：\n\nMaster 节点负责任务的监听和调度，Worker 节点则负责任务的执行。值得注意的是，每一个需要被调度的任务必然需要设置一个调度时间的表达式（cron 表达式），由 Quartz 定时为任务生成待执行的 DAG Command，有且仅有一个 Master 节点获得执行权，掌管该 DAG 各任务节点的调度执行。\n数据质量平台整体架构\n以下是数据质量平台整体的架构图：\n\n由以下几部分组成：\n\nDQC Web UI：质检规则等前端操作页面。\nDQC(GO)：简单的实体元数据管理后台。主要包括：规则、规则模板、质检任务和质检结果几个实体。\nDS(数据质量部分)：质检任务依赖 DolphinScheduler 调度执行，需要对 DolphinScheduler 进行一定的改造。\nDQC SDK(JAR)：DolphinScheduler 调度执行任务时，检测到任务绑定了质检规则，将生成一类新的任务 DQC Task （与 DolphinScheduler 中其他类型的 Task 同级，DolphinScheduler 对于 Task 进行了很好的抽象可以方便扩展），本质上该 Task 将以脚本形式调用执行 DQC SDK 的逻辑。DQC SDK 涵盖了规则解析、执行的全部逻辑。\n\n下文主要阐述我们在各模块设计上的一些思考和权衡。\n规则表述\n标准与规则\n前文在调研部分提及了业内普遍认可的数据质量的六大标准。那么问题来了：\n\n如何将标准与平台的规则对应起来？\n标准中涉及到的现实场景是否我们可以一一枚举？\n即便我们可以将标准一一细化，数据开发人员是否可以轻松的理解？\n\n可以将这些问题统一归类为：平台在规则设定上是否需要和业界数据质量标准所抽象出来的概念进行绑定。很遗憾我们并没有找到有关数据质量标准更加细化和指导性的描述，事实上作为一个开发人员这些概念对于我来说是比较费解的，而更贴近程序员视角的方式是「show me the code」，因此我们决定将这一层概念弱化。未来更深入的实践过程后再做更细化的思考。\n标量化\n接下来我们着重讨论下另一个问题：如何对规则提供一种通用的描述（or maybe a kind of DSL）？\n其实当我们跳脱出前文所描述的一切背景和概念，仔细思考下数据质检的过程，会发现本质上就是通过一次真实的任务执行产出结果，然后对比输出结果与期望是否满足，以验证任务逻辑的正确性。这个过程可形象得和 Unit Testing 进行类比，只不过 Unit Testing 是通过模拟数据构造的一次代码逻辑的执行。另外数据任务执行产生的结果是一张二维结构的 Hive 表，需要进行加工方能获取到想要的统计结果，这也是两者的区别之一。\n顺着这个思路，我们可以利用 Unit Testing 的概念从以下三方面继续深入：\nActual Value\n数据任务执行产出的结果是一张 Hive 表，我们需要对这张 Hive 表的数据进行加工、提取以获得需要的 Actual Value。涉及到对 Hive 表的加工，必然想到是以 SQL 的方式来实现，通过 Query 和 一系列 Aggregation 操作拿到结果，此结果的结构又可分为以下三类：\n\n\n二维数组\n\n\n单行或者单列的一维数组\n\n\n单行且单列的标量\n\n\n显然单行且单列的标量是我们期望得到的，因为它更易于结果的比较（事实上就目前我们所能想到的规则，都可以通过 SQL 方式提取为一个标量结果）。因此，在规则设计中，需要规则创建者输入一段用于结果提取的 SQL，该段 SQL 的执行结果需要为一个标量。\nExpected Value\n既然 Actual Value 是一个标量，那么 Expected Value 同样也是一个标量，需要规则创建者在平台输入。\nAssert\n上述标量的类型决定了断言的比较方式。目前我们只支持了数值型标量的比较方式，包含「大于」、「等于」及「小于」三种比较算子。如出现其他类型标量，需要扩充比较的方式。\n以上三要素即可完整的描述规则想要表达的核心逻辑。如我们想要表述「字段为空异常」的规则（潜在含义：字段为空的行数大于 0 时判定异常），就可以通过以下设定满足：\n\n\nActual Value ：出现字段为空的行数\ncount(case when ${field} is null then 1 else null end)\n\n\n\nExpected Value : 0\n\n\nAssert：「大于」\n\n\n规则管理\n规则模板\n规则模板是为了规则复用抽象出的一个概念，模板中包含规则的 SQL 定义、规则的比较方式、参数定义（注：SQL 中包含一些占位符，这些占位符将以参数的形式被定义，在规则实体定义时需要用户明确具体含义）以及其他的一些元信息。下图为「字段空值的行数」模板的示例：\n\n规则实体\n规则实体是基于规则模板构建的，是规则的具象表达。在规则实体中将明确规则的 Expected Value、比较方式中具体的比较算子、参数的含义以及其他的一些元信息。基于同一个规则模板，可以构造多个规则实体。下图为「某表 user_id 唯一性校验」规则的示例：\n\n值得一提的是，规则可能不仅仅只是针对单表的校验，对于多表的情况我们这套规则模板同样是适用的，只要我们可以将逻辑使用 SQL 表达。\n规则绑定\n在 DolphinScheduler 的前端交互上支持为任务直接绑定校验规则，规则列表通过 API 从 DQC 获取，这种方式在用户的使用体验上存在一定的割裂（规则创建和绑定在两个平台完成）。同时，在 DQC 的前端亦可以直接设置关联调度，为已有任务绑定质检规则，任务列表通过 API 从 DolphinScheduler 获取。同一个任务可绑定多个质检规则，这些信息将存储至 DolphinScheduler 的 DAG 元信息中。那么这里需要考虑几个问题：\n\n规则的哪些信息应该存储至 DAG 的元信息中？\n规则的更新 DAG 元信息是否可以实时同步？\n\n主要有两种方式：\n\n以大 Json 方式将规则信息打包存储，计算时解析 Json 逐个执行校验。在规则更新时，需要同步调用修改 Json 信息。\n以 List 方式存储规则 ID，计算时需执行一次 Pull 操作获取规则具体信息然后执行校验。规则更新，无须同步更新 List 信息。\n\n我们选择了后者，ID List 方式可以使对 DolphinScheduler 的侵入降到最低。\n\n规则执行\n规则的强弱性质由用户为任务绑定规则时设定，此性质决定了规则执行的方式。\n强规则\n和当前所执行的任务节点同步执行，一旦规则检测失败整个任务节点将置为执行失败的状态，后续任务节点的执行会被阻断。对应 DolphinScheduler 中的执行过程表述如下：\n\n某一个 Master 节点获取 DAG 的执行权，将 DAG 拆分成不同的 Job Task 先后下发给 Worker 节点执行。\n执行 Job Task 逻辑，并设置 Job Task 的 ExitStatusCode。\n判断 Job Task 是否绑定了强规则。若是，则生成 DQC Task 并触发执行，最后根据执行结果修正 Job Task 的 ExitStatusCode。\nMaster 节点根据 Job Task 的 ExitStatusCode 判定任务是否成功执行，继续进入后续的调度逻辑。\n\n弱规则\n和当前所执行的任务节点异步执行，规则检测结果对于原有的任务执行状态无影响，从而也就不能阻断后续任务的执行。对应 DolphinScheduler 中的执行过程表述如下：\n\n某一个 Master 节点获取 DAG 的执行权，将 DAG 拆分成不同的 Job Task 先后下发给 Worker 节点执行。\n执行 Job Task 逻辑，并设置 Job Task 的 ExitStatusCode。\n判断 Job Task 是否绑定了弱规则。若是，则在 Job Task 的 Context 中设置弱规则的标记 。\nMaster 节点根据 Job Task 的 ExitStatusCode 判定任务是否成功执行，若成功执行再判定是否 Context 中带有弱规则标记，若有则生成一个新的 DAG（有且仅有一个 DQC Task，且新生成的 DAG 与 当前执行的 DAG 没有任何的关联） 然后继续进入后续的调度逻辑。\n各 Master 节点竞争新生成的 DAG 的执行权。\n\n可以看出在强弱规则的执行方式上，对 DolphinScheduler 调度部分的代码有一定的侵入，但这个改动不大，成本是可以接受的。\nDQC Task &amp; DQC SDK\n上文提及到一个 Job Task 绑定的规则（可能有多个）将被转换为一个 DQC Task 被 DolphinScheduler 调度执行，接下来我们就讨论下 DQC Task 的实现细节以及由此引出的 DQC SDK 的设计和实现。\nDQC Task 继承自 DolphinScheduler 中的抽象类 AbstractTask，只需要实现抽象方法 handle（任务执行的具体实现）即可。那么对于我们的质检任务，实际上执行逻辑可以拆分成以下几步：\n\n提取 Job Task 绑定的待执行的 Rule ID List。\n拉取各个 Rule ID 对应的详情信息。\n构建完整的执行 Query 语句（将规则参数填充至模板 SQL 中）。\n执行 Query。\n执行 Asset。\n\n最核心的步骤为 Query 的执行。Query 的实现方式又可分为两种：\nSpark 实现\n\n优点：实现可控，灵活性更高。\n缺点：配置性要求较高。\n\nPresto SQL 实现\n\n优点：不需要额外配置，开发量少，拼接 SQL 即可。\n缺点：速度没有 Spark 快。\n\n我们选择了后者，这种方式最易实现，离线场景这部分的计算耗时也可以接受。同时由于一个 DQC Task 包含多条规则，在拼接 SQL 时将同表的规则聚合以减少 IO 次数。不同的 SQL 交由不同的线程并行执行。\n上述执行逻辑其实是一个完整且闭环的功能模块，因此我们想到将其作为一个单独的 SDK 对外提供，并以 Jar 包的形式被 DolphinScheduler 依赖，后续即便是更换调度引擎，这部分的逻辑可直接迁移使用（当然概率很低）。那么 DolphinScheduler 中 DQC Task 的 handle 逻辑也就变得异常简单，直接以 Shell 形式调用 SDK ，进一步降低对 DolphinScheduler 代码的侵入。\n执行结果\n单条规则的质检结果将在平台上直接展现，目前我们还未对任务级的规则进行聚合汇总，这是接下来需要完善的。对于质检失败的任务将向报警接收人发送报警。\n\n实践中的问题\n平台解决了规则创建、规则执行的问题，而在实践过程中，对用户而言更关心的问题是：\n\n一个任务应该需要涵盖哪些的规则才能有效地保证数据的质量？\n我们不可能对全部的表和字段都添加规则，那么到底哪些是需要添加的？\n\n这些是很难通过平台自动实现的，因为平台理解不了业务的信息，平台能做的只能是通过质量检测报告给与用户反馈。因此这个事情需要具体的开发人员对核心场景进行梳理，在充分理解业务场景后根据实际情况进行设定。话又说回来，平台只是工具，每一个数据开发人员应当提升保证数据质量的意识，这又涉及到组织内规范落地的问题了。\n未来工作\n数据质量管理是一个长期的过程，未来在平台化方向我们还有几个关键的部分有待继续推进：\n\n基于血缘关系建立全链路的数据质量监控。当前的监控粒度是任务级的，如果规则设置的是弱规则，下游对于数据问题依旧很难感知。\n数据质量的结果量化。需要建立起一套指标用于定量地衡量数据的质量。\n支持实时数据的质量检测。\n\n",
    "title": "伴鱼数据质量平台实践",
    "time": "2021-07-06"
  },
  {
    "name": "json_split",
    "content": "为什么要把 DolphinScheduler 工作流定义中保存任务及关系的大 json 给拆了?\n背景\n当前 DolphinScheduler 的工作流中的任务及关系保存时是以大 json 的方式保存到数据库中 process_definiton 表的 process_definition_json 字段，如果某个工作流很大比如有 1000 个任务，这个 json 字段也就随之变得非常大，在使用时需要解析 json，非常耗费性能，且任务没法重用，故社区计划启动 json 拆分项目。可喜的是目前我们已经完成了这个工作的大部分，因此总结一下，供大家参考学习。\n总结\njson split 项目从 2021-01-12 开始启动，到 2021-04-25 初步完成主要开发。代码已合入 dev 分支。感谢 lenboo、JinyLeeChina、simon824、wen-hemin 四位伙伴参与 coding。\n主要变化以及贡献如下：\n\n代码变更 12793 行\n修改/增加 168 个文件\n共 145 次 Commits\n有 85 个 PR\n\n拆分方案回顾\n\n\n[ ] api 模块进行 save 操作时\n\n\n通过雪花算法生成 13 位的数字作为 process_definition_code，工作流定义保存至 process_definition（主表）和 process_definition_log（日志表），这两个表保存的是同样的数据，工作流定义版本为 1\n通过雪花算法生成 13 位的数字作为 task_definition_code，任务定义表保存至 task_definition（主表）和 task_definition_log（日志表），也是保存同样的数据，任务定义版本为 1\n工作流任务关系保存在 process_task_relation（主表）和 process_task_relation_log（日志表），该表保存的 code 和 version 是工作流的 code 和 version，因为任务是通过工作流进行组织，以工作流来画 dag。也是通过 post_task_code 和 post_task_version 标识 dag 的当前节点，这个节点的前置依赖通过 pre_task_code 和 pre_task_version 来标识，如果没有依赖，pre_task_code 和 pre_task_version 为 0\n\n\n\n[ ] api 模块进行 update 操作时，工作流定义和任务定义直接更新主表数据，更新后的数据 insert 到日志表。关系表主表先删除然后再插入新的关系，关系表日志表直接插入新的关系\n\n\n[ ] api 模块进行 delete 操作时，工作流定义、任务定义和关系表直接删除主表数据，日志表数据不变动\n\n\n[ ] api 模块进行 switch 操作时，直接将日志表中对应 version 数据覆盖到主表\n\n\nJson 存取方案\n\n\n\n[ ] 当前一期拆分方案，api 模块 controller 层整体未变动，传入的大 json 还是在 service 层映射为 ProcessData 对象。insert 或 update 操作在公共 Service 模块通过 ProcessService.saveProcessDefiniton() 入口完成保存数据库操作，按照 task_definition、process_task_relation、process_definition 的顺序保存。保存时，如果该任务已经存在并且关联的工作流未上线，则更改任务；如果任务关联的工作流已上线，则不允许更改任务\n\n\n[ ] api 查询操作时，当前还是通过工作流 id 来查询，在公共 Service 模块通过 ProcessService.genTaskNodeList() 入口完成数据组装，还是组装为 ProcessData 对象，进而生成 json 返回\n\n\n[ ] Server 模块（Master）也是通过公共 Service 模块 ProcessService.genTaskNodeList() 获得 TaskNodeList 生成调度 dag，把当前任务所有信息放到 MasterExecThread.readyToSubmitTaskQueue 队列，以便生成 taskInstance、dispatch 给 worker\n\n\n二期规划\napi/ui 模块改造\n\n[ ] processDefinition 接口通过 processDefinitionId 请求后端的替换为 processDefinitonCode\n[ ] 支持 task 的单独定义，当前 task 的插入及修改是通过工作流来操作的，二期需要支持单独定义\n[ ] 前端及后端 controller 层 json 拆分，一期已完成 api 模块 service 层到 dao 的 json 拆分，二期需要完成前端及 controller 层 json 拆分\n\nserver 模块改造\n\n\n[ ] t_ds_command、t_ds_error_command、t_ds_schedules 中 process_definition_id 替换为 process_definition_code\n\n\n[ ] 生成 taskInstance 流程改造\n\n\n当前生成 process_instance，是通过 process_definition 和 schedules 及 command 表生成，而生成 taskInstance 还是来源于 MasterExecThread.readyToSubmitTaskQueue 队列，并且队列中数据来源于 dag 对象。此时，该队列及 dag 中保存 taskInstance 的所有信息，这种方式非常占用内存。可改造为如下图的数据流转方式，readyToSubmitTaskQueue 队列及 dag 中保存任务 code 和版本信息，在生成 task_instance 前，查询 task_definition\n\n\n附录：雪花算法（snowflake）\n雪花算法（snowflake）： 是一种生成分布式全剧唯一 ID 的算法，生成的 ID 称为 snowflake，这种算法是由 Twitter 创建，并用于推文的 ID。\n一个 Snowflake ID 有 64 bit。前 41 bit 是时间戳，表示了自选定的时期以来的毫秒数。 接下来的 10 bit 代表计算机 ID，防止冲突。 其余 12 bit 代表每台机器上生成 ID 的序列号，这允许在同一毫秒内创建多个 Snowflake ID。SnowflakeID 基于时间生成，故可以按时间排序。此外，一个 ID 的生成时间可以由其自身推断出来，反之亦然。该特性可以用于按时间筛选 ID，以及与之联系的对象。\n雪花算法的结构：\n\n主要分为 5 个部分：\n\n是 1 个 bit：0，这个是无意义的；\n是 41 个 bit：表示的是时间戳；\n是 10 个 bit：表示的是机房 id，0000000000，因为此时传入的是 0；\n是 12 个 bit：表示的序号，就是某个机房某台机器上这一毫秒内同时生成的 id 的序号，0000 0000 0000。\n\n接下去我们来解释一下四个部分：\n1 bit，是无意义的：\n因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。\n41 bit：表示的是时间戳，单位是毫秒。\n41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2 ^ 41 - 1 个毫秒值，换算成年就是表示 69 年的时间。\n10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。\n但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2 ^ 5 个机房（32 个机房），每个机房里可以代表 2 ^ 5 个机器（32 台机器），这里可以随意拆分，比如拿出 4 位标识业务号，其他 6 位作为机器号。可以随意组合。\n12 bit：这个是用来记录同一个毫秒内产生的不同 id。\n12 bit 可以代表的最大正整数是 2 ^ 12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。也就是同一毫秒内同一台机器所生成的最大 ID 数量为 4096\n简单来说，你的某个服务假设要生成一个全局唯一 id，那么就可以发送一个请求给部署了 SnowFlake 算法的系统，由这个 SnowFlake 算法系统来生成唯一 id。这个 SnowFlake 算法系统首先肯定是知道自己所在的机器号，（这里姑且讲 10bit 全部作为工作机器 ID）接着 SnowFlake 算法系统接收到这个请求之后，首先就会用二进制位运算的方式生成一个 64 bit 的 long 型 id，64 个 bit 中的第一个 bit 是无意义的。接着用当前时间戳（单位到毫秒）占用 41 个 bit，然后接着 10 个 bit 设置机器 id。最后再判断一下，当前这台机房的这台机器上这一毫秒内，这是第几个请求，给这次生成 id 的请求累加一个序号，作为最后的 12 个 bit。\nSnowFlake 的特点是：\n\n毫秒数在高位，自增序列在低位，整个 ID 都是趋势递增的。\n不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成 ID 的性能也是非常高的。\n可以根据自身业务特性分配 bit 位，非常灵活。\n\n",
    "title": "为什么要把 DolphinScheduler 工作流定义中保存任务及关系的大 json 给拆了?",
    "time": "2021-05-25"
  },
  {
    "name": "Meetup_2022_02_26",
    "content": "直播报名火热启动 | 2022 年 Apache DolphinScheduler Meetup 首秀！\n\n\n\n各位关注 Apache DolphinScheduler 的小伙伴们大家好呀！相信大家都已经从热闹的春节里回过神来，重新投入到忙碌的工作学习生活中去。海豚调度在这里祝福大家虎年大吉，万事顺利。\n在这早春时节， Apache DolphinScheduler 于 2022 年的第一场 Meetup 也是即将到来，相信社区里的小伙伴早就已经翘首以盼了吧！\n在此次的Meetup中，四位来自不同公司的嘉宾将会为我们讲述他们在使用 Apache DolphinScheduler 时的心得体会，相信不论你是 Apache Dolphin Scheduler 的使用者抑或是正在观望的朋友都能从中收获到许多。\n我们相信未来将会有更多的小伙伴加入我们，也会有越来越多的使用者互相分享实践经验使用和体会， Apache DolphinScheduler 的未来离不开大家的贡献与参与，愿我们一起越变越好。\n此次 Apache DolphinScheduler 2月首秀将于2月26日下午14:00准时开播，大家别忘了扫码预约哦~\n\n\n\n扫码预约直播\n小伙伴们也可以点击 https://www.slidestalk.com/m/679 直达直播预约界面！\n1 活动简介\n主题：Apache DolphinScheduler 用户实践分享\n时间：2022年2月26日 14：00-17：00\n形式：线上直播\n02 活动亮点\n此次 Apache DolphinScheduler 的活动我们将迎来四位分享嘉宾，他们都是来自于360数科、杭州思科以及途家的大咖，相信他们对于 Apache DolphinScheduler的实际体验将会有高度的代表性以及典型性。他们与我们分享的实践经验包含但不限于在 K8S 集群上的探索与应用以及对Alert 模块的改造等，对于解决大家在实际使用中遇到的困难具有很大的意义，希望大家都可以从中得到独属于自己的宝贵财富。\n03 活动议程\n\n\n\n04 议题介绍\n\n\n\n刘建敏/360数科/大数据工程师\n演讲主题：Apache DolphinScheduler 在360数科的实践\n演讲概要：大数据调度从 Azkaban 到 Apache DolphinScheduler 的演变， Apache  DolphinScheduler 的使用与改造。\n\n\n\n李庆旺/杭州思科/大数据工程师\n演讲主题：Apache DolphinScheduler Alert 模块的改造\n演讲概要：切换到 Apache DolphinScheduler 探索，针对于不同的告警场景，对Apache DolphinScheduler 的 Alert 模块进行一些改造和探索。\n\n\n\n昝绪超/途家/大数据工程师\n演讲主题：Apache DolphinScheduler 的探索与应用\n演讲概要：引入 Apache DolphinScheduler ，搭建了途家数据调度系统，完成数据服务的接入，具体包括离线表，邮件，以及数据同步等。详细的介绍了我们对调度系统做了一些功能的开发。\n\n\n\n刘千/杭州思科/大数据工程师\n演讲主题：Apache DolphinScheduler 在 K8S 集群上的探索与应用\n演讲概要：切换到 Apache DolphinScheduler 探索，以及在基于 Apache DolphinScheduler二次开发支持任务提交在k8s上。目前可以运行镜像，spark ， flink 等任务，以及日志监控报警的探索。\nApache DolphinScheduler 的小伙伴们，我们2月26日下午14:00不见不散！\n",
    "title": "直播报名火热启动 | 2022 年 Apache DolphinScheduler Meetup 首秀！",
    "time": "2022-2-18"
  },
  {
    "name": "meetup_2019_10_26",
    "content": "\nApache Dolphin Scheduler(Incubating) Meetup 会议 2019 年 10 月 26 日在上海成功举行。\n地址：上海长宁愚园路 1107 号创邑 SPACE(弘基)3R20\n会议时间：下午 2 点开始，5 点结束.\n议程如下：\n\nDolphinScheduler 简介/概述（易观 CTO-郭炜 14:00-14:20）PPT\nDolphinScheduler 内部原理和架构设计（易观-乔占卫 14:20-15:00）PPT\n从开源用户到 PPMC——谈我和 DolphinScheduler（观远数据-吴宝琪 15:10-15:50）PPT\nDolphinScheduler 在国泰产险的应用与实践 （张宗耀 15:50-16:30）PPT\n即将发布的特性和路线图（易观-代立冬 16:30-17:00）PPT\n自由讨论\n\n",
    "title": "Apache Dolphin Scheduler(Incubating)将于2019.10在上海举行见面会",
    "time": "2019-9-27"
  }
]