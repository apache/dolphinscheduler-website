{
  "__html": "<h1>Quick Start with Apache DolphinScheduler Machine Learning Workflow</h1>\n<p><img src=\"/img/media/16720405454837/16720405586499.jpg\" alt=\"\"></p>\n<h2>Abstract</h2>\n<p>With the release of Apache DolphinScheduler 3.1.0, many AI components have been added to help users to build machine learning workflows on Apache DolphinScheduler more efficiently.</p>\n<p>This article describes in detail how to set up DolphinScheduler with some Machine Learning environments. It also introduces the use of the MLflow component and the DVC component with experimental examples.</p>\n<h2>DolphinScheduler and Machine Learning Environment</h2>\n<p>Test Program\nAll code can be found at https://github.com/jieguangzhou/dolphinscheduler-ml-tutorial</p>\n<p>Get the code</p>\n<pre><code class=\"language-git\">git checkout dev\n</code></pre>\n<h3>Installation environment</h3>\n<p><strong>Conda</strong>\nSimply install it following the official website and add the path to Conda to the environment variables</p>\n<p>After installation mlflow and dvc commands will be installed in conda’s bin directory.</p>\n<pre><code>pip install mlflow==1.30.0 dvc\n</code></pre>\n<p><strong>Java8 environment</strong></p>\n<pre><code class=\"language-sudo\">sudo apt-get install openjdk-8-jdk\njava -version\n</code></pre>\n<p>Configure the Java environment variable, ~/.bashrc or ~/.zshrc</p>\n<pre><code class=\"language-#\">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\nexport PATH=$PATH:$JAVA_HOME/bin\n</code></pre>\n<p><strong>Apache DolphinScheduler 3.1.0</strong></p>\n<p>Download DolphinScheduler 3.1.0</p>\n<pre><code># Go to the following directory (you can install in other directories, for the convenience of replication, in this case, the installation is performed in the following directory)\ncd first-example/install_dolphinscheduler\n## install DolphinScheduler\nwget &lt;https://dlcdn.apache.org/dolphinscheduler/3.1.0/apache-dolphinscheduler-3.1.0-bin.tar.gz&gt;\ntar -zxvf apache-dolphinscheduler-3.1.0-bin.tar.gz\nrm apache-dolphinscheduler-3.1.0-bin.tar.gz\n</code></pre>\n<p>Configuring the Conda environment and Python environment in DolphinScheduler</p>\n<pre><code>## Configure conda environment and default python environment\ncp common.properties apache-dolphinscheduler-3.1.0-bin/standalone-server/conf\necho &quot;export PATH=$(which conda)/bin:\\\\$PATH&quot; &gt;&gt; apache-dolphinscheduler-3.1.0-bin/bin/env/dolphinscheduler_env.sh\necho &quot;export PYTHON_HOME=$(dirname $(which conda))/python&quot; &gt;&gt; apache-dolphinscheduler-3.1.0-bin/bin/env/dolphinscheduler_env.sh\n</code></pre>\n<ul>\n<li>dolphinscheduler-mlflow configuration\nWhen using the MLFLOW component, the dolphinscheduler-mlflow project on GitHub will be used as a reference, so if you can’t get a proper network connection, you can replace the repository source by following these steps</li>\n</ul>\n<p>Firstly execute git clone <a href=\"https://github.com/apache/dolphinscheduler-mlflow.git\">https://github.com/apache/dolphinscheduler-mlflow.git</a></p>\n<p>Then change the value of the ml.mlflow.preset_repository field in common.properties to the default path for the download</p>\n<p>Start DolphinScheduler</p>\n<pre><code>## start DolphinScheduler\ncd apache-dolphinscheduler-3.1.0-bin\nbash bin/dolphinscheduler-daemon.sh start standalone-server\n## You can view the log using the following command\n# tail -500f standalone-server/logs/dolphinscheduler-standalone.log\n</code></pre>\n<p>Once started, wait a moment for the service to boot up and you will be taken to the DolphinScheduler page</p>\n<p>Open http://localhost:12345/dolphinscheduler/ui and you will see the DolphinScheduler page</p>\n<p>Account: admin, Password: dolphinscheduler123\n<img src=\"/img/media/16720405454837/16720407528096.jpg\" alt=\"\">\n<strong>MLflow</strong>\nThe MLflow Tracking Server is relatively simple to start up, and can simply be started by using the command docker run — name mlflow -p 5000:5000 -d jalonzjg/mlflow:latest</p>\n<p>Open http://localhost:5000, and you will be able to find the MLflow model and test management page</p>\n<p><img src=\"/img/media/16720405454837/16720407653742.jpg\" alt=\"\">\nThe Dockerfile for this mirror image can be found at first-example/docker-mlflow/Dockerfile</p>\n<p><strong>Components Introduction</strong>\nThere are 5 main types of components used in this article</p>\n<p><strong>SHELL component</strong>\nThe SHELL component is used to run shell-type tasks</p>\n<p><strong>PYTHON component</strong>\nThe PYTHON component is used to run python-type tasks</p>\n<p><strong>CONDITIONS component</strong>\nCONDITIONS is a conditional node that determines which downstream task should be run based on the running status of the upstream task.</p>\n<p><strong>MLFLOW component</strong>\nMLFLOW component is used to run the MLflow Project on DolphinScheduler based on the dolphinscheduler-mlflow library to implement pre-built algorithms and AutoML functionality for classification scenarios and to deploy models on the MLflow tracking server</p>\n<p><strong>DVC component</strong>\nDVC component is used for data versioning in machine learning on DolphinScheduler, such as registering specific data as a specific version and downloading specific versions of data.</p>\n<p>Among the above five components</p>\n<ul>\n<li>\n<p>SHELL component and PYTHON component are the base components, which can run a wide range of tasks.</p>\n</li>\n<li>\n<p>CONDITIONS are logical components that can dynamically control the logic of the workflow’s operation.</p>\n</li>\n<li>\n<p>The MLFLOW component and DVC component are machine learning type components that can be used to facilitate the ease of use of machine learning scenario feature capabilities within the workflow.\nMachine learning workflow\nThe workflow consists of three parts.</p>\n</li>\n<li>\n<p>The first part is the preliminary preparation, such as data download, data versioning management repository, etc.; it is a one-time preparation.</p>\n</li>\n<li>\n<p>The second part is the training model workflow: it includes data pre-processing, training model, and model evaluation</p>\n</li>\n<li>\n<p>The third part is the deployment workflow, which includes model deployment and interface testing.</p>\n</li>\n</ul>\n<p>Preliminary preparation workflow\nCreate a directory to store all the process data mkdir /tmp/ds-ml-example</p>\n<p>At the beginning of the program, we need to download the test data and initialize the DVC repository for data versioning</p>\n<p>All the following commands are run in the dolphinscheduler-ml-tutorial/first-example directory</p>\n<p>Since we are submitting the workflow via pydolphinscheduler, let’s install pip install apache-dolphinscheduler==3.1.0</p>\n<p>Workflow(download-data): Downloading test data</p>\n<p>Command: pydolphinscheduler yaml -f pyds/download_data.yaml</p>\n<p>Execute the following two tasks in order</p>\n<ol>\n<li>\n<p>Install-dependencies: install the python dependencies packages needed in the download script</p>\n</li>\n<li>\n<p>Download-data: download the dataset to /tmp/ds-ml-example/raw</p>\n</li>\n</ol>\n<p><img src=\"/img/media/16720405454837/16720408372893.jpg\" alt=\"\">\nWorkflow(dvc_init_local): Initialize the dvc data versioning management repository</p>\n<p>Command: pydolphinscheduler yaml -f pyds/init_dvc_repo.yaml</p>\n<p>Execute the following tasks in order</p>\n<ol>\n<li>\n<p>create_git_repo: Create an empty git repository in the local environment</p>\n</li>\n<li>\n<p>init_dvc: convert the repository to a dvc-type repository for data versioning</p>\n</li>\n<li>\n<p>condition: determine the status of the init_dvc task, if successful then execute report_success_message, otherwise execute report_error_message</p>\n</li>\n</ol>\n<p><img src=\"/img/media/16720405454837/16720408471707.jpg\" alt=\"\">\nTraining model workflow\nIn the training model part, the workflow includes data pre-processing, model training, and model evaluation.</p>\n<p>Workflow(download-data): data preprocessing</p>\n<p>Command: pydolphinscheduler yaml -f pyds/prepare_data.yaml</p>\n<p><img src=\"/img/media/16720405454837/16720408537181.jpg\" alt=\"\">\nPerform the following tasks in order</p>\n<ol>\n<li>\n<p>data_preprocessing: preprocesses the data, for demo purposes, we’ve only perform a simple truncation procedure here</p>\n</li>\n<li>\n<p>upload_data: uploads data to the repository and registers it as a specific version v1</p>\n</li>\n</ol>\n<p>The following image shows the information in the git repository</p>\n<p><img src=\"/img/media/16720405454837/16720408664980.jpg\" alt=\"\">\nWorkflow(train_model): Training model</p>\n<p>Command: pydolphinscheduler yaml -f pyds/train_model.yaml</p>\n<p>Perform the following tasks in order</p>\n<ol>\n<li>\n<p>clean_exists_data: Delete the historical data generated by potentially repeated tests /tmp/ds-ml-example/train_data</p>\n</li>\n<li>\n<p>pull_data: pull v1 data to /tmp/ds-ml-example/train_data</p>\n</li>\n<li>\n<p>train_automl: Uses the MLFLOW component’s AutoML function to train the classification model and register it with the MLflow Tracking Server, if the current model version F1 is the highest, then register it as the Production version.</p>\n</li>\n<li>\n<p>inference: import a small part of the data for batch inference using the mlflow CLI</p>\n</li>\n<li>\n<p>evaluate: Obtain the results of the inference and perform a simple evaluation of the model again, which includes the metrics of the new data, the projected label distribution, etc.\n<img src=\"/img/media/16720405454837/16720408742949.jpg\" alt=\"\"></p>\n</li>\n</ol>\n<p>The results of the test and the model can be viewed in the MLflow Tracking Server ( http://localhost:5000 ) after train_automl has completed its operation.</p>\n<p><img src=\"/img/media/16720405454837/16720408868765.jpg\" alt=\"\">\nThe operation logs for the evaluation task can be viewed after it has completed its operation.</p>\n<p><img src=\"/img/media/16720405454837/16720408963992.jpg\" alt=\"\">\nDeployment Process Workflow\nWorkflow(deploy_model): Deployment model</p>\n<p>Run: pydolphinscheduler yaml -f pyds/deploy.yaml</p>\n<p>Run the following tasks in order.</p>\n<ol>\n<li>\n<p>kill-server: Shut down the previous server</p>\n</li>\n<li>\n<p>deploy-model: Deploy the model</p>\n</li>\n<li>\n<p>test-server: Test the server</p>\n</li>\n</ol>\n<p><img src=\"/img/media/16720405454837/16720409057879.jpg\" alt=\"\">\nIf this workflow is started manually, the interface will look as follows, just enter the port number and the model version number.</p>\n<p><img src=\"/img/media/16720405454837/16720409115839.jpg\" alt=\"\">\nIntegrate the workflows\nFor practical use, after obtaining stable workflow iterations, the whole process needs to be linked together, for example after getting a new version, then train the model, and if it performs better, then deploy the model.</p>\n<p>For example, we switch to the production version git checkout first-example-production</p>\n<p>The differences between the two versions are:</p>\n<ol>\n<li>\n<p>there is an additional workflow definition in train_and_deploy.yaml, which is used to combine the various workflows</p>\n</li>\n<li>\n<p>modify the pre-processing script to get the v2 data</p>\n</li>\n<li>\n<p>change the flag in the definition of each sub-workflow to false and let train_and_deploy.yaml run in unison.</p>\n</li>\n</ol>\n<p>Run: pydolphinscheduler yaml -f pyds/train_and_deploy.yaml</p>\n<p>Each task in the diagram below is a sub-workflow task, which corresponds to the three workflows described above.</p>\n<p><img src=\"/img/media/16720405454837/16720409204499.jpg\" alt=\"\">\nAs below, the new version of the model, version2, is obtained after the operation and has been registered as the Production version</p>\n<p><img src=\"/img/media/16720405454837/16720409274430.jpg\" alt=\"\"></p>\n",
  "time": "2022-12-5",
  "author": "Leonard Nie",
  "title": "Quick Start with Apache DolphinScheduler Machine Learning Workflow",
  "type": "tech",
  "label": "Tech Sharing"
}