{
  "__html": "<h1>Fast Task Type Expanding On Apache DolphinScheduler | Tutorial</h1>\n<div align=center>\n<img src=\"/img/2022-03-29/En/1.png\"/>\n</div>\n<h2>Background</h2>\n<p>At present, the scheduler plays an indispensable role in big data ecology. The Apache DolphinScheduler, a top-tier Apache project, is one of the most stable and easy-to-use scheduling systems. With scheduling, distribution, high availability, and ease of use in place, it is only natural that users will want to quickly, easily, and concisely expand the Apache Dolphinscheduler task types as their business grows or as more components are used for various needs. This article shows you how to expand an Apache DolphinScheduler Task easily and quickly.</p>\n<h2>Author Bio</h2>\n<div align=center>\n<img src=\"/img/2022-03-29/En/2.png\"/>\n</div>\n<p>Baiqiang Zhang</p>\n<p>Baiqiang Zhang is a big data development engineer, who is interested in researching real-time computing, metadata governance, and big data basic components.</p>\n<h2>1 What is SPI?</h2>\n<p>SPI (Service Provider Interface) is a service delivery discovery mechanism built into the JDK. Most people will probably rarely use it, as it is positioned primarily for development vendors, and is described in more detail in the java.util.ServiceLoader files. The abstract concept of SPI refers to the dynamic loading of service implementation.</p>\n<h2>2 Why did we introduce SPI?</h2>\n<p>Different enterprises may have their components that need to be executed by tasks, for example, enterprises use Hive, the most commonly used tool in the big data ecosystem, in different ways. Some enterprises execute tasks through HiveServer2, and some use HiveClient to execute tasks. Considering the out-of-the-box Task provided by Apache DolphinScheduler does not support HiveClient’s Task, so most users will execute through the Shell. However, a Shell doesn’t work well compared with a TaskTemplate. So, Apache DolphinScheduler supports TaskSPI to enable users to better customize different Tasks according to their business needs.</p>\n<p>First of all, we need to understand the history of the Task iteration of Apache DolphinScheduler. In DS 1.3.x, expanding a Task required recompiling the whole Apache DolphinScheduler, which was heavily coupled, so in Apache DolphinScheduler 2.0.x, we introduced SPI. As we mentioned earlier, the essence of SPI is to dynamically load the implementation of a service, so let’s make it more concrete and consider the Task of Apache DolphinScheduler as an execution service, and we need to execute different services according to the user’s choice. If there is no service, we need to expand it ourselves. Compared to 1.3.x we only need to complete our Task implementation logic, then follow the SPI rules, compile it into a Jar and upload it to the specified directory, and use our own written Task.</p>\n<h2>3 Who is using it?</h2>\n<p><strong>a. Apache DolphinScheduler</strong></p>\n<p>i. task</p>\n<p>ii. datasource</p>\n<p><strong>b. Apache Flink</strong></p>\n<p>i. Flink sql connector, after the user has implemented a flink-connector, Flink is also dynamically loaded via SPI</p>\n<p><strong>c. Spring boot</strong></p>\n<p>i. spring boot spi</p>\n<p><strong>d. Jdbc</strong></p>\n<p>i. Before jdbc4.0, developers need to load the driver based on Class by forName(“xxx”), jdbc4 also based on the spi mechanism to discover the driver provider, you can expose the driver provider by specifying the implementation class in the META-INF/services/java. sql. Driver file</p>\n<p><strong>e. More</strong></p>\n<ul>\n<li><strong>dubbo</strong></li>\n<li><strong>common-logging</strong></li>\n<li><strong>……</strong></li>\n</ul>\n<h2>4 What’s the Apache DolphinScheduler SPI Process?</h2>\n<div align=center>\n<img src=\"/img/2022-03-29/En/3.png\"/>\n</div>\n<p><em>Note: SPI Rules</em></p>\n<p><em>When compiling the specific implementation of the service into a JAR, we need to create the META-INF/services/ folder in the dir of the resource, and then create a fully qualified class name with the file name of the service, which is the fully qualified class name of the integrated interface. The content inside is the fully qualified class name of the implementing class.</em></p>\n<p>To explain the above diagram, I have divided Apache DolphinScheduler into logical tasks and physical tasks, logical tasks refer to DependTask, SwitchTask, and physical tasks refer to ShellTask, SQLTask, which are the Task for executing tasks. In Apache DolphinScheduler, we generally expand the physical tasks, which are handed over to the Worker to execute, so what we need to understand is that when we have more than one Worker, we have to distribute the custom task to each machine with Worker, and when we start the worker service, the worker will start a ClassLoader to load the corresponding task lib that implements the rules. Note that HiveClient and SeatunnelTasks are user-defined, but only HiveTasks are loaded by Apache DolphinScheduler TaskPluginManage. The reason is that SeatunnelTask does not follow SPI rules. The SPI rules are also described on the diagram, or you can refer to the class java.util.ServiceLoader, which has a simple reference below (part of the code is extracted):</p>\n<pre><code class=\"language-plain\">public final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt; {\n    //scanning dir prefix\n    private static final String PREFIX = &quot;META-INF/services/&quot;;\n    //The class or interface representing the service being loaded\n    private final Class&lt;S&gt; service;\n    //The class loader used to locate, load, and instantiate providers\n    private final ClassLoader loader;\n    //Private inner class implementing fully-lazy provider lookup\n    private class LazyIterator implements Iterator&lt;S&gt; {\n        Class&lt;S&gt; service;\n        ClassLoader loader;\n        Enumeration&lt;URL&gt; configs = null;\n        String nextName = null;\n        //......\n        private boolean hasNextService() {\n            if (configs == null) {\n                try {\n                    //get dir all class\n                    String fullName = PREFIX + service.getName();\n                    if (loader == null)\n                        configs = ClassLoader.getSystemResources(fullName);\n                    else\n                        configs = loader.getResources(fullName);\n                } catch (IOException x) {\n                    //......\n                }\n                //......\n            }\n        }\n    }\n}\n</code></pre>\n<h2>5 How to extend a data source Task or DataSource ?</h2>\n<h3>5.1 Creating a Maven project</h3>\n<pre><code class=\"language-plain\">mvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.dolphinscheduler \\\n    -DarchetypeArtifactId=dolphinscheduler-hive-client-task \\\n    -DarchetypeVersion=1.10.0 \\\n    -DgroupId=org.apache.dolphinscheduler \\\n    -DartifactId=dolphinscheduler-hive-client-task \\\n    -Dversion=0.1 \\\n    -Dpackage=org.apache.dolphinscheduler \\\n    -DinteractiveMode=false\n</code></pre>\n<h3>5.2 Maven dependencies</h3>\n<pre><code class=\"language-plain\">&lt;! --dolphinscheduler spi basic core denpendence --&gt;\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-spi&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency\n &lt;dependency&gt;\n     &lt;groupId&gt;org.apache.dolphinscheduler&lt;/groupId&gt;\n     &lt;artifactId&gt;dolphinscheduler-task-api&lt;/artifactId&gt;\n     &lt;version&gt;${dolphinscheduler.lib.version}&lt;/version\n     &lt;scope&gt;${common.lib.scope}&lt;/scope&gt;\n &lt;/dependency\n</code></pre>\n<h3>5.3 Creating a TaskChannelFactory</h3>\n<p>First, we need to create the factory for the task service, which mainly targets to help build the TaskChannel and TaskPlugin parameters, and to give the unique identity of the task. The ChannelFactory connects the Task service group of Apache DolphinScheduler, and helps the front and back end interaction to build the TaskChannel.</p>\n<pre><code class=\"language-plain\">package org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.spi.params.base.PluginParams;\nimport org.apache.dolphinscheduler.spi.task.TaskChannel;\nimport org.apache.dolphinscheduler.spi.task.TaskChannelFactory;\nimport java.util.List;\npublic class HiveClientTaskChannelFactory implements TaskChannelFactory {\n    /**\n     * Create a task channel and execute tasks based on it\n     * @return Task Channel\n     */\n    @Override\n    public TaskChannel create() {\n        return new HiveClientTaskChannel();\n    }\n    /**\n     * Returns the globally unique identifier of the current task\n     * @return Task type name\n     */\n    @Override\n    public String getName() {\n        return &quot;HIVE CLIENT&quot;;\n    }\n    /**\n     * The front-end pages need to be rendered, mainly into\n\n     * @return\n     */\n    @Override\n    public List&lt;PluginParams&gt; getParams() {\n        List&lt;PluginParams&gt; pluginParams = new ArrayList&lt;&gt;();\n        InputParam nodeName = InputParam.newBuilder(&quot;name&quot;, &quot;$t('Node name')&quot;)\n                .addValidate(Validate.newBuilder()\n                        .setRequired(true)\n                        .build())\n                .build();\n        PluginParams runFlag = RadioParam.newBuilder(&quot;runFlag&quot;, &quot;RUN_FLAG&quot;)\n                .addParamsOptions(new ParamsOptions(&quot;NORMAL&quot;, &quot;NORMAL&quot;, false))\n                .addParamsOptions(new ParamsOptions(&quot;FORBIDDEN&quot;, &quot;FORBIDDEN&quot;, false))\n                .build();\n        PluginParams build = CheckboxParam.newBuilder(&quot;Hive SQL&quot;, &quot;Test HiveSQL&quot;)\n                .setDisplay(true)\n                .setValue(&quot;-- author: \\n --desc:&quot;)\n                .build();\n        pluginParams.add(nodeName);\n        pluginParams.add(runFlag);\n        pluginParams.add(build);\n        return pluginParams;\n    }\n}\n</code></pre>\n<h3>5.4 Creating a TaskChannel</h3>\n<p>After we have a factory, we will create a TaskChannel based on it. The TaskChannel contains two methods, canceling and creating, currently, we only need to focus on creating tasks.</p>\n<pre><code class=\"language-plain\">void cancelApplication(boolean status);\n    /**\n     * Build executable tasks\n     */\n    AbstractTask createTask(TaskRequest taskRequest);\npublic class HiveClientTaskChannel implements TaskChannel {\n    @Override\n    public void cancelApplication(boolean b) {\n        //do nothing\n    }\n    @Override\n    public AbstractTask createTask(TaskRequest taskRequest) {\n        return new HiveClientTask(taskRequest);\n    }\n}\n</code></pre>\n<h3>5.5 Building a Task Implementation</h3>\n<p>With TaskChannel we get the physical task that can be executed, but we need to add the corresponding implementation to the current task to allow Apache DolphinScheduler to execute your task.</p>\n<p>We can see from the above figure that the tasks based on Yarn execution will inherit AbstractYarnTask, and those that do not need to be executed by Yarn will directly inherit AbstractTaskExecutor, which mainly contains an AppID, and CanalApplication setMainJar. As you can see above, our HiveClient needs to inherit AbstractYarnTask, and before building the task, we need to build the parameters object that fits the HiveClient to deserialize the JsonParam.</p>\n<pre><code class=\"language-plain\">package org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.ResourceInfo;\nimport java.util.List;\npublic class HiveClientParameters extends AbstractParameters {\n    /**\n     * The easiest way to execute with HiveClient is to just paste in all the SQL, so we only need one SQL parameter\n     */\n    private String sql;\n    public String getSql() {\n        return sql;\n    }\n    public void setSql(String sql) {\n        this.sql = sql;\n    }\n    @Override\n    public boolean checkParameters() {\n        return sql ! = null;\n    }\n    @Override\n    public List&lt;ResourceInfo&gt; getResourceFilesList() {\n        return null;\n    }\n}\n</code></pre>\n<p>After implementing the parameters object, let’s implement the Task. The implementation in the example is relatively simple, which is to write the user’s parameters to a file and execute the task via Hive -f.</p>\n<pre><code class=\"language-plain\">package org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.request.TaskRequest;\nimport org.apache.dolphinscheduler.spi.utils.JSONUtils;\nimport java.io;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\npublic class HiveClientTask extends AbstractYarnTask {\n    /**\n     * hive client parameters\n     */\n    private HiveClientParameters hiveClientParameters;\n    /**\n     * taskExecutionContext\n     */\n    private final TaskRequest taskExecutionContext;\n    public HiveClientTask(TaskRequest taskRequest) {\n        super(taskRequest);\n        this.taskExecutionContext = taskRequest;\n    }\n    /**\n     * task init method\n     */\n    @Override\n    public void init() {\n        logger.info(&quot;hive client task param is {}&quot;, JSONUtils.toJsonString(taskExecutionContext));\n        this.hiveClientParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), HiveClientParameters.class);\n        if (this.hiveClientParameters ! = null &amp;&amp; !hiveClientParameters.checkParameters()) {\n            throw new RuntimeException(&quot;hive client task params is not valid&quot;);\n        }\n    }\n    /**\n     * build task execution command\n     *\n     * @return task execution command or null\n     */\n    @Override\n    protected String buildCommand() {\n        String filePath = getFilePath();\n        if (writeExecutionContentToFile(filePath)) {\n            return &quot;hive -f &quot; + filePath;\n        }\n        return null;\n    }\n    /**\n     * get hive sql write path\n     *\n     * @return file write path\n     */\n    private String getFilePath() {\n        return String.format(&quot;%s/hive-%s-%s.sql&quot;, this.taskExecutionContext.getExecutePath(), this.taskExecutionContext.getTaskName(), this. taskExecutionContext.getTaskInstanceId());\n    }\n    @Override\n    protected void setMainJarName() {\n        //do nothing\n    }\n    /**\n     * write hive sql to filepath\n     *\n     * @param filePath file path\n     * @return write success?\n     */\n    private boolean writeExecutionContentToFile(String filePath) {\n        Path path = Paths.get(filePath);\n        try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {\n            writer.write(this.hiveClientParameters.getSql());\n            logger.info(&quot;file:&quot; + filePath + &quot;write success.&quot;);\n            return true;\n        } catch (IOException e) {\n            logger.error(&quot;file:&quot; + filePath + &quot;write failed. please path auth.&quot;);\n            e.printStackTrace();\n            return false;\n        }\n    }\n    @Override\n    public AbstractParameters getParameters() {\n        return this.hiveClientParameters;\n    }\n}\n</code></pre>\n<h3>5.6 Compliance with SPI Rules</h3>\n<pre><code class=\"language-plain\"># 1,Create META-INF/services folder under Resource, create the file with the same full class name of the interface\nzhang@xiaozhang resources % tree . /\n. /\n└── META-INF\n    └── services\n        └─ org.apache.dolphinscheduler.spi.task.TaskChannelFactory\n# 2, write the fully qualified class name of the implemented class in the file\nzhang@xiaozhang resources % more META-INF/services/org.apache.dolphinscheduler.spi.task.TaskChannelFactory\norg.apache.dolphinscheduler.plugin.task.hive.HiveClientTaskChannelFactory\n</code></pre>\n<h3>5.7 Packaging and Deployment</h3>\n<pre><code class=\"language-plain\">## 1,Packing\nmvn clean install\n## 2, Deployment\ncp . /target/dolphinscheduler-task-hiveclient-1.0.jar $DOLPHINSCHEDULER_HOME/lib/\n## 3,restart dolphinscheduler server\n</code></pre>\n<p>After the above operation, we check the worker log tail -200f $Apache DolphinScheduler_HOME/log/Apache DolphinScheduler-worker.log.\nThat’s all~ The front-end modifications involved above can be found in Apache DolphinScheduler-ui/src/js/conf/home/pages/dag/_source/formModel/</p>\n<h2>Join the Community</h2>\n<p>There are many ways to participate and contribute to the DolphinScheduler community, including:</p>\n<p><strong>Documents, translation, Q&amp;A, tests, codes, articles, keynote speeches, etc.</strong></p>\n<p>We assume the first PR (document, code) to contribute to be simple and should be used to familiarize yourself with the submission process and community collaboration style.</p>\n<p>So the community has compiled the following list of issues suitable for novices: <a href=\"https://github.com/apache/dolphinscheduler/issues/5689\">https://github.com/apache/dolphinscheduler/issues/5689</a></p>\n<p>List of non-newbie issues: <a href=\"https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22\">https://github.com/apache/dolphinscheduler/issues?q=is%3Aopen+is%3Aissue+label%3A%22volunteer+wanted%22</a></p>\n<p>How to participate in the contribution:  https://dolphinscheduler.apache.org/en-us/community</p>\n<p><strong>GitHub Code Repository:</strong> <a href=\"https://github.com/apache/dolphinscheduler\">https://github.com/apache/dolphinscheduler</a></p>\n<p><strong>Official Website</strong>：<a href=\"https://dolphinscheduler.apache.org/\">https://dolphinscheduler.apache.org/</a></p>\n<p><strong>MailList</strong>：dev@dolphinscheduler@apache.org</p>\n<p><strong>Twitter</strong>：@DolphinSchedule</p>\n<p><strong>YouTube：</strong><a href=\"https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA\">https://www.youtube.com/channel/UCmrPmeE7dVqo8DYhSLHa0vA</a></p>\n<p><strong>Slack：</strong><a href=\"https://s.apache.org/dolphinscheduler-slack\">https://s.apache.org/dolphinscheduler-slack</a></p>\n<p><strong>Contributor Guide：</strong><a href=\"https://dolphinscheduler.apache.org/en-us/community\">https://dolphinscheduler.apache.org/en-us/community</a></p>\n<p>Your Star for the project is important, don’t hesitate to lighten a Star for Apache DolphinScheduler ❤️</p>\n",
  "time": "2022-4-14",
  "author": "Debra Chen",
  "title": "Fast Task Type Expanding On Apache DolphinScheduler | Tutorial",
  "type": "tutorial",
  "label": "Tutorial"
}