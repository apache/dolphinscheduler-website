{
  "__html": "<h1>Application transformation of the FinTech data center based on DolphinScheduler</h1>\n<p><img src=\"/img/media/16720400637574/16720400704016.jpg\" alt=\"\">\nOn Apache DolphinScheduler Meetup last week, Feng Mingxia, a big data engineer from Chengfang FinTech, brought us the application practice of DolphinScheduler in the field of FinTech. The following is the presentation.</p>\n<p><img src=\"/img/media/16720400637574/16720400759248.jpg\" alt=\"\">\nFeng Mingxia, Chengfang Financial Technology Big Data Engineer</p>\n<p>Focusing on real-time and offline data processing and analysis in the field of big data, at present, he is mainly responsible for the research and development of data middle platforms.</p>\n<p>Speech summary:</p>\n<p>· Use background</p>\n<p>· Secondary transformation based on DolphinScheduler</p>\n<p>· DolphinScheduler plug-in expansion</p>\n<p>· Future and outlook</p>\n<h2>Use Background</h2>\n<h3>Data Center Construction</h3>\n<p>At present, big data technology is widely used in the financial field, and the big data platform has become a financial infrastructure. In the construction of a big data platform, the data center is the brightest star, which is the entrance and interface for business systems to use big data, when various business systems are connected to the data center, the data middle office needs to provide unified management and unified access to ensure the security, reliability, efficiency, and reliability of the service.</p>\n<p>As shown in the figure below, the data middle office is in the middle link between the business systems and the big data platform, each business system accesses the big data platform through the services provided by the data center.</p>\n<p><img src=\"/img/media/16720400637574/16720401185208.jpg\" alt=\"\">\nThe core concept of the data middle office is to realize four modernizations, namely, business data, data asset, asset service, and service business. From business to data, and back to the complete closed loop formed by business, support the digital transformation of enterprises.</p>\n<p><img src=\"/img/media/16720400637574/16720401253440.jpg\" alt=\"\">\nThe logical architecture of the data center is shown in the figure above, analyzing from bottom to top, First, the bottom layer is the data resource layer, which is the original data generated by various business systems; The next layer is data integration, and the methods of data integration include offline collection and real-time collection, of which the technologies used include Flume, CDC real-time collection, etc.</p>\n<p>The next layer is the data lake, which puts data in the lake through data integration, stored in Hadoop distributed storage or MPP architecture database.</p>\n<p>The next layer is the data engine layer, which processes and analyzes the data in the data lake through real-time and offline computing engines like Flink and Spark, form service data is available for the upper layer.</p>\n<p>The next layer is the data service that the data center needs to provide. At present, the data service includes data development service and data sharing service, providing data development and sharing capabilities for the upper business systems.</p>\n<p>The data application layer is the specific application of data, including data anomaly detection, data governance, AI decision-making, and BI analysis.</p>\n<p>In the construction of the whole data middle platform, the scheduling engine is the core position in the data engine layer and is also an important function in the construction of the data middle platform.</p>\n<h3>Problems and challenges faced by the data center</h3>\n<p>The data middle office will face some problems and challenges.</p>\n<p>First of all, the execution and scheduling of data tasks are the core and key of data development services provided by the data center.</p>\n<p>Secondly, the data center provides unified data service management, service development, service invocation, and service monitoring.</p>\n<p>Third, ensuring the security of financial data is the primary task of FinTech, and the data middle office needs to ensure the security and reliability of data services.</p>\n<p>Under the above problems and challenges, we investigated some open-source scheduling engines.</p>\n<p><img src=\"/img/media/16720400637574/16720401472681.jpg\" alt=\"\"></p>\n<p>At present, we use a variety of scheduling engines in the production process, such as oozie, XXL job, and DolphinScheduler, which we introduced through research and analysis in 2022, and plays a very important role in the construction of the entire data center.</p>\n<p>First of all, DolphinScheduler partially addresses our requirements for unified service management, service development, service invocation, and service management.</p>\n<p>Secondly, it has its own unique design in task fault tolerance, supporting HA, elastic expansion, fault tolerance, and basically ensuring the safe operation of tasks.</p>\n<p>Third, it supports task and node monitoring.</p>\n<p>Fourth, it supports multi-tenant and permission control.</p>\n<p>Finally, its community is very active, with rapid version change and problem repair.</p>\n<p>Through the analysis of DolphinScheduler’s architecture and source code, we believe that its architecture conforms to the mainstream big data framework design and has similar architecture patterns and designs with excellent foreign products such as Hbase and Kafka.</p>\n<h3>Re-development based on DolphinScheduler</h3>\n<p>To make DolphinScheduler more suitable for our application scenarios, we have made a second transformation based on DolphinScheduler, it includes 6 aspects.</p>\n<ul>\n<li>Add asynchronous service call function</li>\n<li>Add Metabase Oracle adaptation</li>\n<li>Add multi-environment configuration capability</li>\n<li>Add log and historical data-cleaning strategy</li>\n<li>Add access to Yarn logs</li>\n<li>Add service security strategy</li>\n</ul>\n<h3>Add asynchronous service calling function</h3>\n<p>First, the asynchronous service invocation function is added, the figure above shows the architecture of DolphinScheduler version 2.0.5, and most of them are service components of the native DolphinScheduler. GateWay marked in red is a gateway service added based on DolphinScheduler. It realizes flow control, black and white list, and is also the access for users to access service development. By optimizing the startup interface of the process and returning the unique code of the process, we have added the function of service mapping.</p>\n<p><img src=\"/img/media/16720400637574/16720402083983.jpg\" alt=\"\">\nIn the classic DolphinScheduler access mode, the workflow execution instructions submitted by users will enter the command table in the original database, after getting the zk lock, the master component obtains commands from the Metabase, performs DAG parsing, generates actual process instances, delivers the decomposed tasks to the work node for execution through RPC, and then synchronously waits for the execution results.</p>\n<p>In the native DolphinScheduler request, After the user submits the instruction, The return code for executing the workflow is missing, Therefore, we have added a unique return ID, through which users can query the subsequent process status, download logs, and download data.</p>\n<h3>Add Metabase Oracle adaptation</h3>\n<p>Our second transformation is to adapt DolphinScheduler to the Oracle database. At present, the metadatabase of the native DolphinScheduler is MySQL, and we need to convert the original database into an Oracle database according to our production needs. To achieve this, it is necessary to complete the adaptation of the data initialization module and the data operation module.</p>\n<p><img src=\"/img/media/16720400637574/16720402291980.jpg\" alt=\"\"></p>\n<p>First, for the data initialization module, we modified the install_ config. Conf configuration file to change it to the configuration of Oracle.</p>\n<p>Secondly, the Oracle application needs to be added Yml, we are in dolphinscheduler-2.0*/ the application. yml of Oracle is added to the apache-dolphinscheduler-2.0. * — bin/conf/directory.</p>\n<p>Finally, we convert the data operation module, Modify the mapper file and the file, Because the Dolphinscheduler-dao module is a database operation module, other modules will reference this module to implement database operations. It uses Mybatis for database connection, so you need to change the mapper file, all mapper files are in the resources directory.</p>\n<h3>Multi-environment configuration capability</h3>\n<p>The installation of the native DolphinScheduler version cannot be configured according to the environment, Generally, relevant parameters need to be adjusted according to the actual environment. We want to enhance the environment selection and configuration through the installation script, to reduce the cost of manual online modification, Automated installation. It is believed that all partners have encountered similar difficulties. In order to use DolphinScheduler in a development environment, test environment, joint debugging environment, performance environment, quasi-production environment, and production environment, a large number of environment-related configuration parameters need to be modified.</p>\n<p>We modify the install Sh.file, add the input parameter [dev|test|product], and select the appropriate install_ config_$ {evn}. Conf can be installed to automatically select the environment.</p>\n<p>In addition, DolphinScheduler’s workflow is strongly bound to the environment, and workflows in different environments cannot be shared. The following figure shows the JSON file of a workflow exported by the native DolphinScheduler. The grayed part represents the resource resources on which the process depends. The ID is a number, which is generated by the auto-increment of the database. However, if the process instances generated by environment a are placed in environment b, there may be ID primary key conflicts. In other words, workflows generated in different environments cannot be shared.</p>\n<p><img src=\"/img/media/16720400637574/16720402508893.jpg\" alt=\"\">\nWe solve this problem by generating the absolute path of the resource as the unique ID of the resource.</p>\n<h3>Log and historical data cleaning policy</h3>\n<p>The DolphinScheduler generates a lot of data. The database will generate instance data in the instance table, which will continue to grow with the running of instance tasks. Our strategy is to clean up the data of these tables according to the agreed save cycle by defining the scheduled task of DolphinScheduler.</p>\n<p>Secondly, the data of DolphinScheduler mainly includes log data and task execution directory, including the service log data of the worker, master, API, and the directory executed by the worker. These data will not be automatically deleted at the end of task execution, but also need to be deleted through scheduled tasks. By running the log cleanup script, we can automatically delete logs.</p>\n<p><img src=\"/img/media/16720400637574/16720402711565.jpg\" alt=\"\">\n<img src=\"/img/media/16720400637574/16720402758234.jpg\" alt=\"\"></p>\n<h3>Increased access to Yarn logs</h3>\n<p>The native DolphinScheduler can obtain the log information executed on the worker node, but for tasks on Yarn, you need to log in to the Yarn cluster and obtain it through the command or interface. We obtain the Yarn task ID by analyzing the YARNID tag in the log and obtain the task log through the yarnclient. The process of manually viewing logs is reduced.</p>\n<p><img src=\"/img/media/16720400637574/16720403297820.jpg\" alt=\"\"></p>\n<h3>Service security policy</h3>\n<p>Add Monitor component monitoring</p>\n<p><img src=\"/img/media/16720400637574/16720403572773.jpg\" alt=\"\"></p>\n<p>The above figure shows the interaction between the master and worker, the two core components of DolphinScheduler, and Zookeeper. When the MasterServer service starts, it will register a temporary node with Zookeeper, and conduct fault tolerance processing by listening for changes in Zookeeper temporary nodes. WorkerServer is mainly responsible for task execution. When the WorkerServer service starts, it registers a temporary node with Zookeeper and maintains the heartbeat. At present, Zookeeper plays a very important role, mainly in service registration and heartbeat detection.</p>\n<p>The relevant parameters can be seen when the master and worker connect to Zookeeper, including connection timeout, session timeout, and a maximum number of retries.</p>\n<p>Due to network jitter and other factors, master and worker nodes may lose connection with zk. After the loss of connection, because the temporary information registered on the zk by the worker and master disappears, it will be determined that the zk is lost from the master and worker, affecting the task execution. Without human intervention, the task will be delayed. We added the monitor component to monitor the service status. Through the scheduled task cron, we run the monitor program every 5 minutes to check whether the worker process and master process are alive. If they are down, they will be restarted.</p>\n<ul>\n<li>Add Kerberos authentication link for service components using zk</li>\n</ul>\n<p>The second security policy is to add the Kerberos authentication link for service components using zk. Kerberos is a network authentication protocol designed to provide powerful authentication services for client/server applications through a key system. Master service components, API service components, and worker service components complete Kerberos authentication at startup, and then use zk for relevant service registration and heartbeat connection to ensure service security.</p>\n<h3>DolphinScheduler-based plugin extension</h3>\n<p>In addition, we have extended the plug-in based on DolphinScheduler. We have extended four types of operators, including Richshell, SparkSQL, Dataexport, and GBase operators.</p>\n<h3>Add a new task type Richshell</h3>\n<p>First of all, Richshell, a new task type, has enhanced the native Shell function. It mainly realizes the dynamic replacement of script parameters through the template engine. Users can replace script parameters through service calls, making users more flexible in using parameters. It is a supplement to global parameters.</p>\n<p><img src=\"/img/media/16720400637574/16720403977529.jpg\" alt=\"\"></p>\n<h3>Add a new task type SparkSQL</h3>\n<p>The second operator added is SparkSQL. Users can execute Spark tasks by writing SQL so that tasks can be scheduled on Yarn. DolphinScheduler natively also supports SparkSQL execution in JDBC mode, but there is a situation of resource contention because the number of JDBC connections is limited. The Yarn cluster mode cannot be used for execution through tools such as SparkSQL/Spark beer. By using this task type, SparkSQL programs can be run on the Yarn cluster in cluster mode to maximize the use of cluster resources and reduce the use of client resources.</p>\n<h3>Add a new task type Dataexport</h3>\n<p>The third addition is Dataexport, which is also a data export operator. Users can export data stored in components by selecting different storage components. Components include ES, Hive, Hbase, etc.</p>\n<p><img src=\"/img/media/16720400637574/16720404142720.jpg\" alt=\"\">\nThe data in the big data platform may be used for BI display, statistical analysis, machine learning, and other data preparation after being exported. Most of these scenarios require data export, and Spark’s data processing capability is used to achieve the export function of different data sources.</p>\n<h3>Add a new task type GBase</h3>\n<p>The fourth plug-in added is Gbase. GBase 8a MPP Cluster is a distributed parallel database cluster with column storage and shared nothing architecture. It has the characteristics of high performance, high availability, high expansion, etc. It is suitable for OLAP scenarios (query scenarios), can provide a cost-effective general computing platform for large-scale data management, and is widely used to support various data warehouse systems, BI systems, and decision support systems.</p>\n<p><img src=\"/img/media/16720400637574/16720404412259.jpg\" alt=\"\">\nAs an application scenario of data entering the lake, we have added a GBase operator, which supports the import, export, and execution of GBase data.</p>\n",
  "time": "2022-12-6",
  "author": "Leonard Nie",
  "title": "Application transformation of the FinTech data center based on DolphinScheduler",
  "type": "user",
  "label": "Use Cases"
}