{
  "__html": "<h1 id=\"quickstart-in-kubernetes\">QuickStart in Kubernetes</h1>\n<p>Kubernetes deployment is DolphinScheduler deployment in a Kubernetes cluster, which can schedule massive tasks and can be used in production.</p>\n<p>If you are a new hand and want to experience DolphinScheduler functions, we recommend you install follow <a href=\"/en-us/docs/3.3.1/guide/installation/standalone\">Standalone deployment</a>. If you want to experience more complete functions and schedule massive tasks, we recommend you install follow <a href=\"/en-us/docs/3.3.1/guide/installation/pseudo-cluster\">pseudo-cluster deployment</a>. If you want to deploy DolphinScheduler in production, we recommend you follow <a href=\"/en-us/docs/3.3.1/guide/installation/cluster\">cluster deployment</a> or <a href=\"/en-us/docs/3.3.1/guide/installation/kubernetes\">Kubernetes deployment</a>.</p>\n<blockquote>\n<p><strong>Tip</strong>: You can also try <a href=\"https://github.com/apache/dolphinscheduler-operator\">DolphinScheduler K8S Operator</a>ï¼Œwhich is current on alpha1 stage</p>\n</blockquote>\n<h2 id=\"prerequisites\">Prerequisites</h2>\n<ul>\n<li><a href=\"https://helm.sh/\">Helm</a> version 3.1.0+</li>\n<li><a href=\"https://kubernetes.io/\">Kubernetes</a> version 1.12+</li>\n<li>PV provisioner support in the underlying infrastructure</li>\n</ul>\n<h2 id=\"install-dolphinscheduler\">Install DolphinScheduler</h2>\n<pre><code class=\"language-bash\"><span class=\"hljs-comment\"># Choose the corresponding version yourself</span>\nhelm upgrade --install dolphinscheduler --create-namespace --namespace dolphinscheduler oci://registry-1.docker.io/apache/dolphinscheduler-helm --version 3.3.1\n</code></pre>\n<p>These commands are used to deploy DolphinScheduler on the Kubernetes cluster by default. The <a href=\"#appendix-configuration\">Appendix-Configuration</a> section lists the parameters that can be configured during installation.</p>\n<blockquote>\n<p><strong>Tip</strong>: List all releases using <code>helm list</code></p>\n</blockquote>\n<p>The <strong>PostgreSQL</strong> (with username <code>root</code>, password <code>root</code> and database <code>dolphinscheduler</code>) and <strong>ZooKeeper</strong> services will start by default.</p>\n<h2 id=\"access-dolphinscheduler-ui\">Access DolphinScheduler UI</h2>\n<p>If <code>ingress.enabled</code> in <code>values.yaml</code> is set to <code>true</code>, you could access <code>http://${ingress.host}/dolphinscheduler</code> in browser.</p>\n<blockquote>\n<p><strong>Tip</strong>: If there is a problem with ingress access, please contact the Kubernetes administrator and refer to the <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a>.</p>\n</blockquote>\n<p>Otherwise, when <code>api.service.type=ClusterIP</code> you need to execute <code>port-forward</code> commands:</p>\n<pre><code class=\"language-bash\">$ kubectl port-forward --address 0.0.0.0 svc/dolphinscheduler-api 12345:12345\n$ kubectl port-forward --address 0.0.0.0 -n <span class=\"hljs-built_in\">test</span> svc/dolphinscheduler-api 12345:12345 <span class=\"hljs-comment\"># with test namespace</span>\n</code></pre>\n<blockquote>\n<p><strong>Tip</strong>: If the error of <code>unable to do port forwarding: socat not found</code> appears, you need to install <code>socat</code> first.</p>\n</blockquote>\n<p>Access the web: <code>http://localhost:12345/dolphinscheduler/ui</code> (Modify the IP address if needed).</p>\n<p>Or when <code>api.service.type=NodePort</code> you need to execute the command:</p>\n<pre><code class=\"language-bash\">NODE_IP=$(kubectl get no -n {{ .Release.Namespace }} -o jsonpath=<span class=\"hljs-string\">&quot;{.items[0].status.addresses[0].address}&quot;</span>)\nNODE_PORT=$(kubectl get svc {{ template <span class=\"hljs-string\">&quot;dolphinscheduler.fullname&quot;</span> . }}-api -n {{ .Release.Namespace }} -o jsonpath=<span class=\"hljs-string\">&quot;{.spec.ports[0].nodePort}&quot;</span>)\n<span class=\"hljs-built_in\">echo</span> http://<span class=\"hljs-variable\">$NODE_IP</span>:<span class=\"hljs-variable\">$NODE_PORT</span>/dolphinscheduler\n</code></pre>\n<p>Access the web: <code>http://$NODE_IP:$NODE_PORT/dolphinscheduler</code>.</p>\n<p>The default username is <code>admin</code> and the default password is <code>dolphinscheduler123</code>.</p>\n<p>Please refer to the <code>Quick Start</code> in the chapter <a href=\"/en-us/docs/3.3.1/guide/start/quick-start\">Quick Start</a> to explore how to use DolphinScheduler.</p>\n<h2 id=\"uninstall-the-chart\">Uninstall the Chart</h2>\n<p>To uninstall or delete the <code>dolphinscheduler</code> deployment:</p>\n<pre><code class=\"language-bash\">$ helm uninstall dolphinscheduler\n</code></pre>\n<p>The command removes all the Kubernetes components (except PVC) associated with the <code>dolphinscheduler</code> and deletes the release.</p>\n<p>Run the command below to delete the PVC's associated with <code>dolphinscheduler</code>:</p>\n<pre><code class=\"language-bash\">$ kubectl delete pvc -l app.kubernetes.io/instance=dolphinscheduler\n</code></pre>\n<blockquote>\n<p><strong>Note</strong>: Deleting the PVC's will delete all data as well. Please be cautious before doing it.</p>\n</blockquote>\n<h2 id=\"[experimental]-worker-autoscaling\">[Experimental] Worker Autoscaling</h2>\n<blockquote>\n<p><strong>Warning</strong>: Currently this is an experimental feature and may not be suitable for production!</p>\n</blockquote>\n<p><code>DolphinScheduler</code> uses <a href=\"https://github.com/kedacore/keda\">KEDA</a> for worker autoscaling. However, <code>DolphinScheduler</code> disables\nthis feature by default. To turn on worker autoscaling:</p>\n<p>Firstly, you need to create a namespace for <code>KEDA</code> and install it with <code>helm</code>:</p>\n<pre><code class=\"language-bash\">helm repo add kedacore https://kedacore.github.io/charts\n\nhelm repo update\n\nkubectl create namespace keda\n\nhelm install keda kedacore/keda \\\n    --namespace keda \\\n    --version <span class=\"hljs-string\">&quot;v2.0.0&quot;</span>\n</code></pre>\n<p>Secondly, you need to set <code>worker.keda.enabled</code> to <code>true</code> in <code>values.yaml</code> or install the chart by:</p>\n<pre><code class=\"language-bash\">helm upgrade --install dolphinscheduler --create-namespace --namespace dolphinscheduler oci://registry-1.docker.io/apache/dolphinscheduler-helm --version 3.3.1 --<span class=\"hljs-built_in\">set</span> worker.keda.enabled=<span class=\"hljs-literal\">true</span>\n</code></pre>\n<p>Once autoscaling enabled, the number of workers will scale between <code>minReplicaCount</code> and <code>maxReplicaCount</code> based on the states\nof your tasks. For example, when there is no tasks running in your <code>DolphinScheduler</code> instance, there will be no workers,\nwhich will significantly save the resources.</p>\n<p>Worker autoscaling feature is compatible with <code>postgresql</code> and <code>mysql</code> shipped with <code>DolphinScheduler official helm chart</code>. If you\nuse external database, worker autoscaling feature only supports external <code>mysql</code> and <code>postgresql</code> databases.</p>\n<p>If you need to change the value of worker <code>WORKER_EXEC_THREADS</code> when using autoscaling feature,\nplease change <code>worker.env.WORKER_EXEC_THREADS</code> in <code>values.yaml</code> instead of through <code>configmap</code>.</p>\n<h2 id=\"configuration\">Configuration</h2>\n<p>The configuration file is <code>values.yaml</code>, and the <a href=\"#appendix-configuration\">Appendix-Configuration</a> tables lists the configurable parameters of the DolphinScheduler and their default values.</p>\n<h2 id=\"support-matrix\">Support Matrix</h2>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Support</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Shell</td>\n<td>Yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Python2</td>\n<td>Yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Python3</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Hadoop2</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Hadoop3</td>\n<td>Not Sure</td>\n<td>Not tested</td>\n</tr>\n<tr>\n<td>Spark-Local(client)</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Spark-YARN(cluster)</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Spark-Standalone(cluster)</td>\n<td>Not Yet</td>\n<td></td>\n</tr>\n<tr>\n<td>Spark-Kubernetes(cluster)</td>\n<td>Not Yet</td>\n<td></td>\n</tr>\n<tr>\n<td>Flink-Local(local&gt;=1.11)</td>\n<td>Not Yet</td>\n<td>Generic CLI mode is not yet supported</td>\n</tr>\n<tr>\n<td>Flink-YARN(yarn-cluster)</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Flink-YARN(yarn-session/yarn-per-job/yarn-application&gt;=1.11)</td>\n<td>Not Yet</td>\n<td>Generic CLI mode is not yet supported</td>\n</tr>\n<tr>\n<td>Flink-Standalone(default)</td>\n<td>Not Yet</td>\n<td></td>\n</tr>\n<tr>\n<td>Flink-Standalone(remote&gt;=1.11)</td>\n<td>Not Yet</td>\n<td>Generic CLI mode is not yet supported</td>\n</tr>\n<tr>\n<td>Flink-Kubernetes(default)</td>\n<td>Not Yet</td>\n<td></td>\n</tr>\n<tr>\n<td>Flink-Kubernetes(remote&gt;=1.11)</td>\n<td>Not Yet</td>\n<td>Generic CLI mode is not yet supported</td>\n</tr>\n<tr>\n<td>Flink-NativeKubernetes(kubernetes-session/application&gt;=1.11)</td>\n<td>Not Yet</td>\n<td>Generic CLI mode is not yet supported</td>\n</tr>\n<tr>\n<td>MapReduce</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Kerberos</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>HTTP</td>\n<td>Yes</td>\n<td></td>\n</tr>\n<tr>\n<td>DataX</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>Sqoop</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-MySQL</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-PostgreSQL</td>\n<td>Yes</td>\n<td></td>\n</tr>\n<tr>\n<td>SQL-Hive</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-Spark</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-ClickHouse</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-Oracle</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-SQLServer</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n<tr>\n<td>SQL-DB2</td>\n<td>Indirect Yes</td>\n<td>Refer to FAQ</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"faq\">FAQ</h2>\n<h3 id=\"how-to-view-the-logs-of-a-pod-container?\">How to View the Logs of a Pod Container?</h3>\n<p>List all pods (aka <code>po</code>):</p>\n<pre><code>kubectl get po\nkubectl get po -n test # with test namespace\n</code></pre>\n<p>View the logs of a pod container named <code>dolphinscheduler-master-0</code>:</p>\n<pre><code>kubectl logs dolphinscheduler-master-0\nkubectl logs -f dolphinscheduler-master-0 # follow log output\nkubectl logs --tail 10 dolphinscheduler-master-0 -n test # show last 10 lines from the end of the logs\n</code></pre>\n<h3 id=\"how-to-scale-api,-master-and-worker-on-kubernetes?\">How to Scale API, master and worker on Kubernetes?</h3>\n<p>List all deployments (aka <code>deploy</code>):</p>\n<pre><code>kubectl get deploy\nkubectl get deploy -n test # with test namespace\n</code></pre>\n<p>Scale api to 3 replicas:</p>\n<pre><code>kubectl scale --replicas=3 deploy dolphinscheduler-api\nkubectl scale --replicas=3 deploy dolphinscheduler-api -n test # with test namespace\n</code></pre>\n<p>List all stateful sets (aka <code>sts</code>):</p>\n<pre><code>kubectl get sts\nkubectl get sts -n test # with test namespace\n</code></pre>\n<p>Scale master to 2 replicas:</p>\n<pre><code>kubectl scale --replicas=2 sts dolphinscheduler-master\nkubectl scale --replicas=2 sts dolphinscheduler-master -n test # with test namespace\n</code></pre>\n<p>Scale worker to 6 replicas:</p>\n<pre><code>kubectl scale --replicas=6 sts dolphinscheduler-worker\nkubectl scale --replicas=6 sts dolphinscheduler-worker -n test # with test namespace\n</code></pre>\n<h3 id=\"how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?\">How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?</h3>\n<blockquote>\n<p>Because of the commercial license, we cannot directly use the driver of MySQL.</p>\n<p>If you want to use MySQL, you can build a new image based on the <code>apache/dolphinscheduler-&lt;service&gt;</code> image follow the following instructions:</p>\n<p>Since version 3.0.0, dolphinscheduler has been microserviced and the change of metadata storage requires replacing all services with MySQL driver, which including dolphinscheduler-tools, dolphinscheduler-master, dolphinscheduler-worker, dolphinscheduler-api, dolphinscheduler-alert-server</p>\n</blockquote>\n<ol>\n<li>\n<p>Download the MySQL driver <a href=\"https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.16/mysql-connector-java-8.0.16.jar\">mysql-connector-java-8.0.16.jar</a>.</p>\n</li>\n<li>\n<p>Create a new <code>Dockerfile</code> to add MySQL driver:</p>\n</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-&lt;service&gt;:3.3.1\n# For example\n# FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-tools:3.3.1\n\n# Attention Please, If the build is dolphinscheduler-tools image\n# You need to change the following line to: COPY mysql-connector-java-8.0.16.jar /opt/dolphinscheduler/tools/libs\n# The other services don't need any changes\nCOPY mysql-connector-java-8.0.16.jar /opt/dolphinscheduler/libs\n</code></pre>\n<ol start=\"3\">\n<li>Build a new docker image including MySQL driver:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler-&lt;service&gt;:mysql-driver .\n</code></pre>\n<ol start=\"4\">\n<li>\n<p>Push the docker image <code>apache/dolphinscheduler-&lt;service&gt;:mysql-driver</code> to a docker registry.</p>\n</li>\n<li>\n<p>Modify image <code>repository</code> and update <code>tag</code> to <code>mysql-driver</code> in <code>values.yaml</code>.</p>\n</li>\n<li>\n<p>Modify postgresql <code>enabled</code> to <code>false</code> in <code>values.yaml</code>.</p>\n</li>\n<li>\n<p>Modify externalDatabase (especially modify <code>host</code>, <code>username</code> and <code>password</code>) in <code>values.yaml</code>:</p>\n</li>\n</ol>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">externalDatabase:</span>\n  <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">&quot;mysql&quot;</span>\n  <span class=\"hljs-attr\">host:</span> <span class=\"hljs-string\">&quot;localhost&quot;</span>\n  <span class=\"hljs-attr\">port:</span> <span class=\"hljs-string\">&quot;3306&quot;</span>\n  <span class=\"hljs-attr\">username:</span> <span class=\"hljs-string\">&quot;root&quot;</span>\n  <span class=\"hljs-attr\">password:</span> <span class=\"hljs-string\">&quot;root&quot;</span>\n  <span class=\"hljs-attr\">database:</span> <span class=\"hljs-string\">&quot;dolphinscheduler&quot;</span>\n  <span class=\"hljs-attr\">params:</span> <span class=\"hljs-string\">&quot;useUnicode=true&amp;characterEncoding=UTF-8&quot;</span>\n</code></pre>\n<ol start=\"8\">\n<li>Run a DolphinScheduler release in Kubernetes (See <strong>Install DolphinScheduler</strong>).</li>\n</ol>\n<h3 id=\"how-to-support-mysql-or-oracle-datasource-in-<code>datasource-manage</code>?\">How to Support MySQL or Oracle Datasource in <code>Datasource manage</code>?</h3>\n<blockquote>\n<p>Because of the commercial license, we cannot directly use the driver of MySQL or Oracle.</p>\n<p>If you want to add MySQL or Oracle datasource, you can build a new image based on the <code>apache/dolphinscheduler-&lt;service&gt;</code> image follow the following instructions:</p>\n<p>You need to change the two service images including dolphinscheduler-worker, dolphinscheduler-api.</p>\n</blockquote>\n<ol>\n<li>\n<p>Download the MySQL driver <a href=\"https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.16/mysql-connector-java-8.0.16.jar\">mysql-connector-java-8.0.16.jar</a>.\nor download the Oracle driver <a href=\"https://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/\">ojdbc8.jar</a> (such as <code>ojdbc8-19.9.0.0.jar</code>)</p>\n</li>\n<li>\n<p>Create a new <code>Dockerfile</code> to add MySQL or Oracle driver:</p>\n</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-&lt;service&gt;:3.3.1\n# For example\n# FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-worker:3.3.1\n\n# If you want to support MySQL Datasource\nCOPY mysql-connector-java-8.0.16.jar /opt/dolphinscheduler/libs\n\n# If you want to support Oracle Datasource\nCOPY ojdbc8-19.9.0.0.jar /opt/dolphinscheduler/libs\n</code></pre>\n<ol start=\"3\">\n<li>Build a new docker image including MySQL or Oracle driver:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler-&lt;service&gt;:new-driver .\n</code></pre>\n<ol start=\"4\">\n<li>\n<p>Push the docker image <code>apache/dolphinscheduler-&lt;service&gt;:new-driver</code> to a docker registry.</p>\n</li>\n<li>\n<p>Modify image <code>repository</code> and update <code>tag</code> to <code>new-driver</code> in <code>values.yaml</code>.</p>\n</li>\n<li>\n<p>Run a DolphinScheduler release in Kubernetes (See <strong>Install DolphinScheduler</strong>).</p>\n</li>\n<li>\n<p>Add a MySQL or Oracle datasource in <code>Datasource manage</code>.</p>\n</li>\n</ol>\n<h3 id=\"how-to-support-python-2-pip-and-custom-requirements.txt?\">How to Support Python 2 pip and Custom requirements.txt?</h3>\n<blockquote>\n<p>Just change the image of the dolphinscheduler-worker service.</p>\n</blockquote>\n<ol>\n<li>Create a new <code>Dockerfile</code> to install pip:</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-worker:3.3.1\nCOPY requirements.txt /tmp\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends python-pip &amp;&amp; \\\n    pip install --no-cache-dir -r /tmp/requirements.txt &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre>\n<p>The command will install the default <strong>pip 18.1</strong>. If you upgrade the pip, just add the following command.</p>\n<pre><code>pip install --no-cache-dir -U pip &amp;&amp; \\\n</code></pre>\n<ol start=\"2\">\n<li>Build a new docker image including pip:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler-worker:pip .\n</code></pre>\n<ol start=\"3\">\n<li>\n<p>Push the docker image <code>apache/dolphinscheduler-worker:pip</code> to a docker registry.</p>\n</li>\n<li>\n<p>Modify image <code>repository</code> and update <code>tag</code> to <code>pip</code> in <code>values.yaml</code>.</p>\n</li>\n<li>\n<p>Run a DolphinScheduler release in Kubernetes (See <strong>Install DolphinScheduler</strong>).</p>\n</li>\n<li>\n<p>Verify pip under a new Python task.</p>\n</li>\n</ol>\n<h3 id=\"how-to-support-python-3?\">How to Support Python 3?</h3>\n<blockquote>\n<p>Just change the image of the dolphinscheduler-worker service.</p>\n</blockquote>\n<ol>\n<li>Create a new <code>Dockerfile</code> to install Python 3:</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-worker:3.3.1\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends python3 &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre>\n<p>The command will install the default <strong>Python 3.7.3</strong>. If you also want to install <strong>pip3</strong>, just replace <code>python3</code> with <code>python3-pip</code> like:</p>\n<pre><code>apt-get install -y --no-install-recommends python3-pip &amp;&amp; \\\n</code></pre>\n<ol start=\"2\">\n<li>Build a new docker image including Python 3:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler-worker:python3 .\n</code></pre>\n<ol start=\"3\">\n<li>\n<p>Push the docker image <code>apache/dolphinscheduler-worker:python3</code> to a docker registry.</p>\n</li>\n<li>\n<p>Modify image <code>repository</code> and update <code>tag</code> to <code>python3</code> in <code>values.yaml</code>.</p>\n</li>\n<li>\n<p>Modify <code>PYTHON_LAUNCHER</code> to <code>/usr/bin/python3</code> in <code>values.yaml</code>.</p>\n</li>\n<li>\n<p>Run a DolphinScheduler release in Kubernetes (See <strong>Install DolphinScheduler</strong>).</p>\n</li>\n<li>\n<p>Verify Python 3 under a new Python task.</p>\n</li>\n</ol>\n<h3 id=\"how-to-support-hadoop,-spark,-flink,-hive-or-datax?\">How to Support Hadoop, Spark, Flink, Hive or DataX?</h3>\n<p>Take Spark 2.4.7 as an example:</p>\n<ol>\n<li>\n<p>Download the Spark 2.4.7 release binary <code>spark-2.4.7-bin-hadoop2.7.tgz</code>.</p>\n</li>\n<li>\n<p>Ensure that <code>common.sharedStoragePersistence.enabled</code> is turned on.</p>\n</li>\n<li>\n<p>Run a DolphinScheduler release in Kubernetes (See <strong>Install DolphinScheduler</strong>).</p>\n</li>\n<li>\n<p>Copy the Spark 2.4.7 release binary into the Docker container.</p>\n</li>\n</ol>\n<pre><code class=\"language-bash\">kubectl <span class=\"hljs-built_in\">cp</span> spark-2.4.7-bin-hadoop2.7.tgz dolphinscheduler-worker-0:/opt/soft\nkubectl <span class=\"hljs-built_in\">cp</span> -n <span class=\"hljs-built_in\">test</span> spark-2.4.7-bin-hadoop2.7.tgz dolphinscheduler-worker-0:/opt/soft <span class=\"hljs-comment\"># with test namespace</span>\n</code></pre>\n<p>Because the volume <code>sharedStoragePersistence</code> is mounted on <code>/opt/soft</code>, all files in <code>/opt/soft</code> will not be lost.</p>\n<ol start=\"5\">\n<li>Attach the container and ensure that <code>SPARK_HOME</code> exists.</li>\n</ol>\n<pre><code class=\"language-bash\">kubectl <span class=\"hljs-built_in\">exec</span> -it dolphinscheduler-worker-0 bash\nkubectl <span class=\"hljs-built_in\">exec</span> -n <span class=\"hljs-built_in\">test</span> -it dolphinscheduler-worker-0 bash <span class=\"hljs-comment\"># with test namespace</span>\n<span class=\"hljs-built_in\">cd</span> /opt/soft\ntar zxf spark-2.4.7-bin-hadoop2.7.tgz\n<span class=\"hljs-built_in\">rm</span> -f spark-2.4.7-bin-hadoop2.7.tgz\n<span class=\"hljs-built_in\">ln</span> -s spark-2.4.7-bin-hadoop2.7 spark2 <span class=\"hljs-comment\"># or just mv</span>\n<span class=\"hljs-variable\">$SPARK_HOME</span>/bin/spark-submit --version\n</code></pre>\n<p>The last command will print the Spark version if everything goes well.</p>\n<ol start=\"6\">\n<li>Verify Spark under a Shell task.</li>\n</ol>\n<pre><code>$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.7.jar\n</code></pre>\n<p>Check whether the task log contains the output like <code>Pi is roughly 3.146015</code>.</p>\n<ol start=\"7\">\n<li>Verify Spark under a Spark task.</li>\n</ol>\n<p>The file <code>spark-examples_2.11-2.4.7.jar</code> needs to be uploaded to the resources first, and then create a Spark task with:</p>\n<ul>\n<li>Main Class: <code>org.apache.spark.examples.SparkPi</code></li>\n<li>Main Package: <code>spark-examples_2.11-2.4.7.jar</code></li>\n<li>Deploy Mode: <code>local</code></li>\n</ul>\n<p>Similarly, check whether the task log contains the output like <code>Pi is roughly 3.146015</code>.</p>\n<ol start=\"8\">\n<li>Verify Spark on YARN.</li>\n</ol>\n<p>Spark on YARN (Deploy Mode is <code>cluster</code> or <code>client</code>) requires Hadoop support. Similar to Spark support, the operation of supporting Hadoop is almost the same as the previous steps.</p>\n<p>Ensure that <code>$HADOOP_HOME</code> and <code>$HADOOP_CONF_DIR</code> exists.</p>\n<h3 id=\"how-to-support-shared-storage-between-master,-worker-and-api-server?\">How to Support Shared Storage Between Master, Worker and Api Server?</h3>\n<p>For example, Master, Worker and API server may use Hadoop at the same time.</p>\n<ol>\n<li>Modify the following configurations in <code>values.yaml</code></li>\n</ol>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">sharedStoragePersistence:</span>\n    <span class=\"hljs-attr\">enabled:</span> <span class=\"hljs-literal\">false</span>\n    <span class=\"hljs-attr\">mountPath:</span> <span class=\"hljs-string\">&quot;/opt/soft&quot;</span>\n    <span class=\"hljs-attr\">accessModes:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">&quot;ReadWriteMany&quot;</span>\n    <span class=\"hljs-attr\">storageClassName:</span> <span class=\"hljs-string\">&quot;-&quot;</span>\n    <span class=\"hljs-attr\">storage:</span> <span class=\"hljs-string\">&quot;20Gi&quot;</span>\n</code></pre>\n<p>Modify <code>storageClassName</code> and <code>storage</code> to actual environment values.</p>\n<blockquote>\n<p><strong>Note</strong>: <code>storageClassName</code> must support the access mode: <code>ReadWriteMany</code>.</p>\n</blockquote>\n<ol start=\"2\">\n<li>\n<p>Copy the Hadoop into the directory <code>/opt/soft</code>.</p>\n</li>\n<li>\n<p>Ensure that <code>$HADOOP_HOME</code> and <code>$HADOOP_CONF_DIR</code> are correct.</p>\n</li>\n</ol>\n<h3 id=\"how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?\">How to Support Local File Resource Storage Instead of HDFS and S3?</h3>\n<p>Modify the following configurations in <code>values.yaml</code>:</p>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">configmap:</span>\n    <span class=\"hljs-attr\">RESOURCE_STORAGE_TYPE:</span> <span class=\"hljs-string\">&quot;HDFS&quot;</span>\n    <span class=\"hljs-attr\">RESOURCE_UPLOAD_PATH:</span> <span class=\"hljs-string\">&quot;/dolphinscheduler&quot;</span>\n    <span class=\"hljs-attr\">FS_DEFAULT_FS:</span> <span class=\"hljs-string\">&quot;file:///&quot;</span>\n  <span class=\"hljs-attr\">fsFileResourcePersistence:</span>\n    <span class=\"hljs-attr\">enabled:</span> <span class=\"hljs-literal\">true</span>\n    <span class=\"hljs-attr\">accessModes:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">&quot;ReadWriteMany&quot;</span>\n    <span class=\"hljs-attr\">storageClassName:</span> <span class=\"hljs-string\">&quot;-&quot;</span>\n    <span class=\"hljs-attr\">storage:</span> <span class=\"hljs-string\">&quot;20Gi&quot;</span>\n</code></pre>\n<p>Modify <code>storageClassName</code> and <code>storage</code> to actual environment values.</p>\n<blockquote>\n<p><strong>Note</strong>: <code>storageClassName</code> must support the access mode: <code>ReadWriteMany</code>.</p>\n</blockquote>\n<h3 id=\"how-to-support-s3-resource-storage-like-minio?\">How to Support S3 Resource Storage Like MinIO?</h3>\n<p>Take MinIO as an example: Modify the following configurations in <code>values.yaml</code>:</p>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">configmap:</span>\n    <span class=\"hljs-attr\">RESOURCE_STORAGE_TYPE:</span> <span class=\"hljs-string\">&quot;S3&quot;</span>\n    <span class=\"hljs-string\">...</span>\n</code></pre>\n<p>For detailed explanation of specific fields, please see: <a href=\"/en-us/docs/3.3.1/guide/resource/configuration\">Resource Center Configuration</a></p>\n<h3 id=\"how-to-deploy-specific-components-separately?\">How to deploy specific components separately?</h3>\n<p>Modify the <code>api.enabled</code>, <code>alert.enabled</code>, <code>master.enabled</code>, or <code>worker.enabled</code> configuration items in the <code>values.yaml</code> file.</p>\n<p>For example, if you need to deploy worker to both CPU and GPU servers in a cluster, and the worker uses different images, you can do the following:</p>\n<pre><code class=\"language-bash\"><span class=\"hljs-comment\"># Install master, api-server, alert-server, and other default components, but do not install worker</span>\nhelm upgrade --install dolphinscheduler --create-namespace --namespace dolphinscheduler oci://registry-1.docker.io/apache/dolphinscheduler-helm --version 3.3.1 --<span class=\"hljs-built_in\">set</span> worker.enabled=<span class=\"hljs-literal\">false</span>\n<span class=\"hljs-comment\"># Disable the installation of other components, only install worker, use the self-built CPU image, deploy to CPU servers with the `x86` label through nodeselector, and use zookeeper as the external registry center</span>\nhelm upgrade --install dolphinscheduler-cpu-worker --create-namespace --namespace dolphinscheduler oci://registry-1.docker.io/apache/dolphinscheduler-helm --version 3.3.1 \\\n     --<span class=\"hljs-built_in\">set</span> minio.enabled=<span class=\"hljs-literal\">false</span> --<span class=\"hljs-built_in\">set</span> postgresql.enabled=<span class=\"hljs-literal\">false</span> --<span class=\"hljs-built_in\">set</span> zookeeper.enabled=<span class=\"hljs-literal\">false</span> \\\n     --<span class=\"hljs-built_in\">set</span> master.enabled=<span class=\"hljs-literal\">false</span>  --<span class=\"hljs-built_in\">set</span> api.enabled=<span class=\"hljs-literal\">false</span> --<span class=\"hljs-built_in\">set</span> alert.enabled=<span class=\"hljs-literal\">false</span> \\\n     --<span class=\"hljs-built_in\">set</span> worker.enabled=<span class=\"hljs-literal\">true</span> --<span class=\"hljs-built_in\">set</span> image.tag=latest-cpu --<span class=\"hljs-built_in\">set</span> worker.nodeSelector.cpu=<span class=\"hljs-string\">&quot;x86&quot;</span> \\\n     --<span class=\"hljs-built_in\">set</span> externalRegistry.registryPluginName=zookeeper --<span class=\"hljs-built_in\">set</span> externalRegistry.registryServers=dolphinscheduler-zookeeper:2181\n<span class=\"hljs-comment\"># Disable the installation of other components, only install worker, use the self-built GPU image, deploy to GPU servers with the `a100` label through nodeselector, and use zookeeper as the external registry center</span>\nhelm upgrade --install dolphinscheduler-gpu-worker --create-namespace --namespace dolphinscheduler oci://registry-1.docker.io/apache/dolphinscheduler-helm --version 3.3.1 \\\n     --<span class=\"hljs-built_in\">set</span> minio.enabled=<span class=\"hljs-literal\">false</span> --<span class=\"hljs-built_in\">set</span> postgresql.enabled=<span class=\"hljs-literal\">false</span> --<span class=\"hljs-built_in\">set</span> zookeeper.enabled=<span class=\"hljs-literal\">false</span> \\\n     --<span class=\"hljs-built_in\">set</span> master.enabled=<span class=\"hljs-literal\">false</span>  --<span class=\"hljs-built_in\">set</span> api.enabled=<span class=\"hljs-literal\">false</span> --<span class=\"hljs-built_in\">set</span> alert.enabled=<span class=\"hljs-literal\">false</span> \\\n     --<span class=\"hljs-built_in\">set</span> worker.enabled=<span class=\"hljs-literal\">true</span> --<span class=\"hljs-built_in\">set</span> image.tag=latest-gpu --<span class=\"hljs-built_in\">set</span> worker.nodeSelector.gpu=<span class=\"hljs-string\">&quot;a100&quot;</span> \\\n     --<span class=\"hljs-built_in\">set</span> externalRegistry.registryPluginName=zookeeper --<span class=\"hljs-built_in\">set</span> externalRegistry.registryServers=dolphinscheduler-zookeeper:2181\n</code></pre>\n<blockquote>\n<p><strong>Note</strong>: the above steps are for reference only, and specific operations need to be adjusted according to the actual situation.\n<strong>Note</strong>: DS uses the /tmp/dolphinscheduler directory as the resource center by default. If you need to change the directory of the resource center, change the resource items in the conf/common.properties file</p>\n</blockquote>\n<h2 id=\"appendix-configuration\">Appendix-Configuration</h2>\n<p>Ref: <a href=\"https://github.com/apache/dolphinscheduler/blob/dev/deploy/kubernetes/dolphinscheduler/README.md\">DolphinScheduler Helm Charts</a></p>\n",
  "location": [
    "Installation",
    "Kubernetes Deployment"
  ],
  "time": "2025-8-24",
  "structure": [
    {
      "title": "Prerequisites",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "prerequisites"
    },
    {
      "title": "Install DolphinScheduler",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "install-dolphinscheduler"
    },
    {
      "title": "Access DolphinScheduler UI",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "access-dolphinscheduler-ui"
    },
    {
      "title": "Uninstall the Chart",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "uninstall-the-chart"
    },
    {
      "title": "[Experimental] Worker Autoscaling",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "[experimental]-worker-autoscaling"
    },
    {
      "title": "Configuration",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "configuration"
    },
    {
      "title": "Support Matrix",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "support-matrix"
    },
    {
      "title": "FAQ",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "faq"
    },
    {
      "title": "Appendix-Configuration",
      "children": [
        {
          "title": "How to View the Logs of a Pod Container?",
          "children": [],
          "anchor": "how-to-view-the-logs-of-a-pod-container?"
        },
        {
          "title": "How to Scale API, master and worker on Kubernetes?",
          "children": [],
          "anchor": "how-to-scale-api,-master-and-worker-on-kubernetes?"
        },
        {
          "title": "How to Use MySQL as the DolphinScheduler's Database Instead of PostgreSQL?",
          "children": [],
          "anchor": "how-to-use-mysql-as-the-dolphinscheduler's-database-instead-of-postgresql?"
        },
        {
          "title": "How to Support MySQL or Oracle Datasource in Datasource manage?",
          "children": [],
          "anchor": "how-to-support-mysql-or-oracle-datasource-in-datasource-manage?"
        },
        {
          "title": "How to Support Python 2 pip and Custom requirements.txt?",
          "children": [],
          "anchor": "how-to-support-python-2-pip-and-custom-requirements.txt?"
        },
        {
          "title": "How to Support Python 3?",
          "children": [],
          "anchor": "how-to-support-python-3?"
        },
        {
          "title": "How to Support Hadoop, Spark, Flink, Hive or DataX?",
          "children": [],
          "anchor": "how-to-support-hadoop,-spark,-flink,-hive-or-datax?"
        },
        {
          "title": "How to Support Shared Storage Between Master, Worker and Api Server?",
          "children": [],
          "anchor": "how-to-support-shared-storage-between-master,-worker-and-api-server?"
        },
        {
          "title": "How to Support Local File Resource Storage Instead of HDFS and S3?",
          "children": [],
          "anchor": "how-to-support-local-file-resource-storage-instead-of-hdfs-and-s3?"
        },
        {
          "title": "How to Support S3 Resource Storage Like MinIO?",
          "children": [],
          "anchor": "how-to-support-s3-resource-storage-like-minio?"
        },
        {
          "title": "How to deploy specific components separately?",
          "children": [],
          "anchor": "how-to-deploy-specific-components-separately?"
        }
      ],
      "anchor": "appendix-configuration"
    }
  ],
  "title": "QuickStart in Kubernetes",
  "link": "/guide/installation/kubernetes"
}