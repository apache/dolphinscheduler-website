{
  "__html": "<h1 id=\"spark节点\">SPARK节点</h1>\n<ul>\n<li>通过SPARK节点，可以直接直接执行SPARK程序，对于spark节点，worker会使用<code>spark-submit</code>方式提交任务</li>\n</ul>\n<blockquote>\n<p>拖动工具栏中的<img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/toolbar_SPARK.png\" alt=\"PNG\">任务节点到画板中，如下图所示：</p>\n</blockquote>\n<p align=\"center\">\n   <img src=\"/img/spark_edit.png\" width=\"80%\" />\n </p>\n<ul>\n<li>程序类型：支持JAVA、Scala和Python三种语言</li>\n<li>主函数的class：是Spark程序的入口Main Class的全路径</li>\n<li>主jar包：是Spark的jar包</li>\n<li>部署方式：支持yarn-cluster、yarn-client和local三种模式</li>\n<li>Driver内核数：可以设置Driver内核数及内存数</li>\n<li>Executor数量：可以设置Executor数量、Executor内存数和Executor内核数</li>\n<li>命令行参数：是设置Spark程序的输入参数，支持自定义参数变量的替换。</li>\n<li>其他参数：支持 --jars、--files、--archives、--conf格式</li>\n<li>资源：如果其他参数中引用了资源文件，需要在资源中选择指定</li>\n<li>自定义参数：是MR局部的用户自定义参数，会替换脚本中以${变量}的内容</li>\n</ul>\n<p>注意：JAVA和Scala只是用来标识，没有区别，如果是Python开发的Spark则没有主函数的class，其他都是一样</p>\n",
  "location": [
    "功能介绍",
    "任务类型",
    "Spark"
  ],
  "time": "2022-1-5",
  "structure": [],
  "title": "SPARK节点",
  "link": "/guide/task/spark"
}