{
  "__html": "<h1 id=\"快速试用-kubernetes-部署\">快速试用 Kubernetes 部署</h1>\n<p>Kubernetes部署目的是在Kubernetes集群中部署 DolphinScheduler 服务，能调度大量任务，可用于在生产中部署。</p>\n<p>如果你是新手，想要体验 DolphinScheduler 的功能，推荐使用<a href=\"/zh-cn/docs/2.0.7/guide/installation/standalone\">Standalone</a>方式体检。如果你想体验更完整的功能，或者更大的任务量，推荐使用<a href=\"/zh-cn/docs/2.0.7/guide/installation/pseudo-cluster\">伪集群部署</a>。如果你是在生产中使用，推荐使用<a href=\"/zh-cn/docs/2.0.7/guide/installation/cluster\">集群部署</a>或者<a href=\"/zh-cn/docs/2.0.7/guide/installation/kubernetes\">kubernetes</a></p>\n<h2 id=\"先决条件\">先决条件</h2>\n<ul>\n<li><a href=\"https://helm.sh/\">Helm</a> 3.1.0+</li>\n<li><a href=\"https://kubernetes.io/\">Kubernetes</a> 1.12+</li>\n<li>PV 供应(需要基础设施支持)</li>\n</ul>\n<h2 id=\"安装-dolphinscheduler\">安装 dolphinscheduler</h2>\n<blockquote>\n<p>注意：您需要在本地更改 <code>Chart.yaml</code> 文件才能使其正常工作。 由于 Bitnami 存储库的更改，https://charts.bitnami.com/bitnami 被截断，\n仅包含最近 6 个月（从 2022 年 1 月起）的条目。 只有这个 url 才包含了：https://raw.githubusercontent.com/bitnami/charts/archive-full-index/bitnami 完整的 index.yaml。\n如果您想了解更多细节，请访问：https://github.com/bitnami/charts/issues/10833。</p>\n<p>下载源代码后，更改路径 <code>apache-dolphinscheduler-2.0.7-src/docker/kubernetes/dolphinscheduler</code> 中的 <code>Chart.yaml</code> 文件，需要同时修改两个地方，\n将 <code>repository: https://charts.bitnami.com/bitnami</code> 替换成 <code>repository: https://raw.githubusercontent.com/bitnami/charts/archive-full-index/bitnami</code></p>\n</blockquote>\n<p>请下载源码包 apache-dolphinscheduler-2.0.7-src.tar.gz，下载地址: <a href=\"/zh-cn/download/download.html\">下载</a></p>\n<p>发布一个名为 <code>dolphinscheduler</code> 的版本(release)，请执行以下命令：</p>\n<pre><code>$ tar -zxvf apache-dolphinscheduler-2.0.7-src.tar.gz\n$ cd apache-dolphinscheduler-2.0.7-src/docker/kubernetes/dolphinscheduler\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\n$ helm dependency update .\n$ helm install dolphinscheduler . --set image.tag=2.0.7\n</code></pre>\n<p>将名为 <code>dolphinscheduler</code> 的版本(release) 发布到 <code>test</code> 的命名空间中：</p>\n<pre><code class=\"language-bash\">$ helm install dolphinscheduler . -n <span class=\"hljs-built_in\">test</span>\n</code></pre>\n<blockquote>\n<p><strong>提示</strong>: 如果名为 <code>test</code> 的命名空间被使用, 选项参数 <code>-n test</code> 需要添加到 <code>helm</code> 和 <code>kubectl</code> 命令中</p>\n</blockquote>\n<p>这些命令以默认配置在 Kubernetes 集群上部署 DolphinScheduler，<a href=\"#appendix-configuration\">附录-配置</a>部分列出了可以在安装过程中配置的参数  <!-- markdown-link-check-disable-line --></p>\n<blockquote>\n<p><strong>提示</strong>: 列出所有已发布的版本，使用 <code>helm list</code></p>\n</blockquote>\n<p><strong>PostgreSQL</strong> (用户 <code>root</code>, 密码 <code>root</code>, 数据库 <code>dolphinscheduler</code>) 和 <strong>ZooKeeper</strong> 服务将会默认启动</p>\n<h2 id=\"访问-dolphinscheduler-前端页面\">访问 DolphinScheduler 前端页面</h2>\n<p>如果 <code>values.yaml</code> 文件中的 <code>ingress.enabled</code> 被设置为 <code>true</code>, 在浏览器中访问 <code>http://${ingress.host}/dolphinscheduler</code> 即可</p>\n<blockquote>\n<p><strong>提示</strong>: 如果 ingress 访问遇到问题，请联系 Kubernetes 管理员并查看 <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a></p>\n</blockquote>\n<p>否则，当 <code>api.service.type=ClusterIP</code> 时，你需要执行 port-forward 端口转发命令：</p>\n<pre><code class=\"language-bash\">$ kubectl port-forward --address 0.0.0.0 svc/dolphinscheduler-api 12345:12345\n$ kubectl port-forward --address 0.0.0.0 -n <span class=\"hljs-built_in\">test</span> svc/dolphinscheduler-api 12345:12345 <span class=\"hljs-comment\"># 使用 test 命名空间</span>\n</code></pre>\n<blockquote>\n<p><strong>提示</strong>: 如果出现 <code>unable to do port forwarding: socat not found</code> 错误, 需要先安装 <code>socat</code></p>\n</blockquote>\n<p>访问前端页面：http://localhost:12345/dolphinscheduler，如果有需要请修改成对应的 IP 地址</p>\n<p>或者当 <code>api.service.type=NodePort</code> 时，你需要执行命令：</p>\n<pre><code class=\"language-bash\">NODE_IP=$(kubectl get no -n {{ .Release.Namespace }} -o jsonpath=<span class=\"hljs-string\">&quot;{.items[0].status.addresses[0].address}&quot;</span>)\nNODE_PORT=$(kubectl get svc {{ template <span class=\"hljs-string\">&quot;dolphinscheduler.fullname&quot;</span> . }}-api -n {{ .Release.Namespace }} -o jsonpath=<span class=\"hljs-string\">&quot;{.spec.ports[0].nodePort}&quot;</span>)\n<span class=\"hljs-built_in\">echo</span> http://<span class=\"hljs-variable\">$NODE_IP</span>:<span class=\"hljs-variable\">$NODE_PORT</span>/dolphinscheduler\n</code></pre>\n<p>然后访问前端页面: http://localhost:12345/dolphinscheduler</p>\n<p>默认的用户是<code>admin</code>，默认的密码是<code>dolphinscheduler123</code></p>\n<p>请参考用户手册章节的<a href=\"../quick-start.md\">快速上手</a>查看如何使用DolphinScheduler</p>\n<h2 id=\"卸载-dolphinscheduler\">卸载 dolphinscheduler</h2>\n<p>卸载名为 <code>dolphinscheduler</code> 的版本(release)，请执行：</p>\n<pre><code class=\"language-bash\">$ helm uninstall dolphinscheduler\n</code></pre>\n<p>该命令将删除与 <code>dolphinscheduler</code> 相关的所有 Kubernetes 组件（但PVC除外），并删除版本(release)</p>\n<p>要删除与 <code>dolphinscheduler</code> 相关的PVC，请执行：</p>\n<pre><code class=\"language-bash\">$ kubectl delete pvc -l app.kubernetes.io/instance=dolphinscheduler\n</code></pre>\n<blockquote>\n<p><strong>注意</strong>: 删除PVC也会删除所有数据，请谨慎操作！</p>\n</blockquote>\n<h2 id=\"配置\">配置</h2>\n<p>配置文件为 <code>values.yaml</code>，<a href=\"#appendix-configuration\">附录-配置</a> 表格列出了 DolphinScheduler 的可配置参数及其默认值  <!-- markdown-link-check-disable-line --></p>\n<h2 id=\"支持矩阵\">支持矩阵</h2>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>支持</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Shell</td>\n<td>是</td>\n<td></td>\n</tr>\n<tr>\n<td>Python2</td>\n<td>是</td>\n<td></td>\n</tr>\n<tr>\n<td>Python3</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Hadoop2</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Hadoop3</td>\n<td>尚未确定</td>\n<td>尚未测试</td>\n</tr>\n<tr>\n<td>Spark-Local(client)</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Spark-YARN(cluster)</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Spark-Standalone(cluster)</td>\n<td>尚不</td>\n<td></td>\n</tr>\n<tr>\n<td>Spark-Kubernetes(cluster)</td>\n<td>尚不</td>\n<td></td>\n</tr>\n<tr>\n<td>Flink-Local(local&gt;=1.11)</td>\n<td>尚不</td>\n<td>Generic CLI 模式尚未支持</td>\n</tr>\n<tr>\n<td>Flink-YARN(yarn-cluster)</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Flink-YARN(yarn-session/yarn-per-job/yarn-application&gt;=1.11)</td>\n<td>尚不</td>\n<td>Generic CLI 模式尚未支持</td>\n</tr>\n<tr>\n<td>Flink-Standalone(default)</td>\n<td>尚不</td>\n<td></td>\n</tr>\n<tr>\n<td>Flink-Standalone(remote&gt;=1.11)</td>\n<td>尚不</td>\n<td>Generic CLI 模式尚未支持</td>\n</tr>\n<tr>\n<td>Flink-Kubernetes(default)</td>\n<td>尚不</td>\n<td></td>\n</tr>\n<tr>\n<td>Flink-Kubernetes(remote&gt;=1.11)</td>\n<td>尚不</td>\n<td>Generic CLI 模式尚未支持</td>\n</tr>\n<tr>\n<td>Flink-NativeKubernetes(kubernetes-session/application&gt;=1.11)</td>\n<td>尚不</td>\n<td>Generic CLI 模式尚未支持</td>\n</tr>\n<tr>\n<td>MapReduce</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Kerberos</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>HTTP</td>\n<td>是</td>\n<td></td>\n</tr>\n<tr>\n<td>DataX</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>Sqoop</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-MySQL</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-PostgreSQL</td>\n<td>是</td>\n<td></td>\n</tr>\n<tr>\n<td>SQL-Hive</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-Spark</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-ClickHouse</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-Oracle</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-SQLServer</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n<tr>\n<td>SQL-DB2</td>\n<td>间接支持</td>\n<td>详见 FAQ</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"faq\">FAQ</h2>\n<h3 id=\"如何查看一个-pod-容器的日志？\">如何查看一个 pod 容器的日志？</h3>\n<p>列出所有 pods (别名 <code>po</code>):</p>\n<pre><code>kubectl get po\nkubectl get po -n test # with test namespace\n</code></pre>\n<p>查看名为 dolphinscheduler-master-0 的 pod 容器的日志:</p>\n<pre><code>kubectl logs dolphinscheduler-master-0\nkubectl logs -f dolphinscheduler-master-0 # 跟随日志输出\nkubectl logs --tail 10 dolphinscheduler-master-0 -n test # 显示倒数10行日志\n</code></pre>\n<h3 id=\"如何在-kubernetes-上扩缩容-api,-master-和-worker？\">如何在 Kubernetes 上扩缩容 api, master 和 worker？</h3>\n<p>列出所有 deployments (别名 <code>deploy</code>):</p>\n<pre><code>kubectl get deploy\nkubectl get deploy -n test # with test namespace\n</code></pre>\n<p>扩缩容 api 至 3 个副本:</p>\n<pre><code>kubectl scale --replicas=3 deploy dolphinscheduler-api\nkubectl scale --replicas=3 deploy dolphinscheduler-api -n test # with test namespace\n</code></pre>\n<p>列出所有 statefulsets (别名 <code>sts</code>):</p>\n<pre><code>kubectl get sts\nkubectl get sts -n test # with test namespace\n</code></pre>\n<p>扩缩容 master 至 2 个副本:</p>\n<pre><code>kubectl scale --replicas=2 sts dolphinscheduler-master\nkubectl scale --replicas=2 sts dolphinscheduler-master -n test # with test namespace\n</code></pre>\n<p>扩缩容 worker 至 6 个副本:</p>\n<pre><code>kubectl scale --replicas=6 sts dolphinscheduler-worker\nkubectl scale --replicas=6 sts dolphinscheduler-worker -n test # with test namespace\n</code></pre>\n<h3 id=\"如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？\">如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？</h3>\n<blockquote>\n<p>由于商业许可证的原因，我们不能直接使用 MySQL 的驱动包.</p>\n<p>如果你要使用 MySQL, 你可以基于官方镜像 <code>apache/dolphinscheduler</code> 进行构建.</p>\n</blockquote>\n<ol>\n<li>下载 MySQL 驱动包 <a href=\"https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.16/mysql-connector-java-8.0.16.jar\">mysql-connector-java-8.0.16.jar</a></li>\n<li>创建一个新的 <code>Dockerfile</code>，用于添加 MySQL 的驱动包:</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler:2.0.7\nCOPY mysql-connector-java-8.0.16.jar /opt/dolphinscheduler/lib\n</code></pre>\n<ol start=\"3\">\n<li>构建一个包含 MySQL 驱动包的新镜像:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler:mysql-driver .\n</code></pre>\n<ol start=\"4\">\n<li>\n<p>推送 docker 镜像 <code>apache/dolphinscheduler:mysql-driver</code> 到一个 docker registry 中</p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中 image 的 <code>repository</code> 字段，并更新 <code>tag</code> 为 <code>mysql-driver</code></p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中 postgresql 的 <code>enabled</code> 为 <code>false</code></p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中的 externalDatabase 配置 (尤其修改 <code>host</code>, <code>username</code> 和 <code>password</code>)</p>\n</li>\n</ol>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">externalDatabase:</span>\n  <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">&quot;mysql&quot;</span>\n  <span class=\"hljs-attr\">driver:</span> <span class=\"hljs-string\">&quot;com.mysql.jdbc.Driver&quot;</span>\n  <span class=\"hljs-attr\">host:</span> <span class=\"hljs-string\">&quot;localhost&quot;</span>\n  <span class=\"hljs-attr\">port:</span> <span class=\"hljs-string\">&quot;3306&quot;</span>\n  <span class=\"hljs-attr\">username:</span> <span class=\"hljs-string\">&quot;root&quot;</span>\n  <span class=\"hljs-attr\">password:</span> <span class=\"hljs-string\">&quot;root&quot;</span>\n  <span class=\"hljs-attr\">database:</span> <span class=\"hljs-string\">&quot;dolphinscheduler&quot;</span>\n  <span class=\"hljs-attr\">params:</span> <span class=\"hljs-string\">&quot;useUnicode=true&amp;characterEncoding=UTF-8&quot;</span>\n</code></pre>\n<ol start=\"8\">\n<li>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</li>\n</ol>\n<h3 id=\"如何在数据源中心支持-mysql-数据源？\">如何在数据源中心支持 MySQL 数据源？</h3>\n<blockquote>\n<p>由于商业许可证的原因，我们不能直接使用 MySQL 的驱动包.</p>\n<p>如果你要添加 MySQL 数据源, 你可以基于官方镜像 <code>apache/dolphinscheduler</code> 进行构建.</p>\n</blockquote>\n<ol>\n<li>\n<p>下载 MySQL 驱动包 <a href=\"https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.16/mysql-connector-java-8.0.16.jar\">mysql-connector-java-8.0.16.jar</a> (要求 <code>&gt;=8.0.1</code>)</p>\n</li>\n<li>\n<p>创建一个新的 <code>Dockerfile</code>，用于添加 MySQL 驱动包:</p>\n</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler:2.0.7\nCOPY mysql-connector-java-8.0.16.jar /opt/dolphinscheduler/lib\n</code></pre>\n<ol start=\"3\">\n<li>构建一个包含 MySQL 驱动包的新镜像:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler:mysql-driver .\n</code></pre>\n<ol start=\"4\">\n<li>\n<p>推送 docker 镜像 <code>apache/dolphinscheduler:mysql-driver</code> 到一个 docker registry 中</p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中 image 的 <code>repository</code> 字段，并更新 <code>tag</code> 为 <code>mysql-driver</code></p>\n</li>\n<li>\n<p>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</p>\n</li>\n<li>\n<p>在数据源中心添加一个 MySQL 数据源</p>\n</li>\n</ol>\n<h3 id=\"如何在数据源中心支持-oracle-数据源？\">如何在数据源中心支持 Oracle 数据源？</h3>\n<blockquote>\n<p>由于商业许可证的原因，我们不能直接使用 Oracle 的驱动包.</p>\n<p>如果你要添加 Oracle 数据源, 你可以基于官方镜像 <code>apache/dolphinscheduler</code> 进行构建.</p>\n</blockquote>\n<ol>\n<li>\n<p>下载 Oracle 驱动包 <a href=\"https://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/\">ojdbc8.jar</a> (例如 <code>ojdbc8-19.9.0.0.jar</code>)</p>\n</li>\n<li>\n<p>创建一个新的 <code>Dockerfile</code>，用于添加 Oracle 驱动包:</p>\n</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler:2.0.7\nCOPY ojdbc8-19.9.0.0.jar /opt/dolphinscheduler/lib\n</code></pre>\n<ol start=\"3\">\n<li>构建一个包含 Oracle 驱动包的新镜像:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler:oracle-driver .\n</code></pre>\n<ol start=\"4\">\n<li>\n<p>推送 docker 镜像 <code>apache/dolphinscheduler:oracle-driver</code> 到一个 docker registry 中</p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中 image 的 <code>repository</code> 字段，并更新 <code>tag</code> 为 <code>oracle-driver</code></p>\n</li>\n<li>\n<p>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</p>\n</li>\n<li>\n<p>在数据源中心添加一个 Oracle 数据源</p>\n</li>\n</ol>\n<h3 id=\"如何支持-python-2-pip-以及自定义-requirements.txt？\">如何支持 Python 2 pip 以及自定义 requirements.txt？</h3>\n<ol>\n<li>创建一个新的 <code>Dockerfile</code>，用于安装 pip:</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler:2.0.7\nCOPY requirements.txt /tmp\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends python-pip &amp;&amp; \\\n    pip install --no-cache-dir -r /tmp/requirements.txt &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre>\n<p>这个命令会安装默认的 <strong>pip 18.1</strong>. 如果你想升级 pip, 只需添加一行</p>\n<pre><code>    pip install --no-cache-dir -U pip &amp;&amp; \\\n</code></pre>\n<ol start=\"2\">\n<li>构建一个包含 pip 的新镜像:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler:pip .\n</code></pre>\n<ol start=\"3\">\n<li>\n<p>推送 docker 镜像 <code>apache/dolphinscheduler:pip</code> 到一个 docker registry 中</p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中 image 的 <code>repository</code> 字段，并更新 <code>tag</code> 为 <code>pip</code></p>\n</li>\n<li>\n<p>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</p>\n</li>\n<li>\n<p>在一个新 Python 任务下验证 pip</p>\n</li>\n</ol>\n<h3 id=\"如何支持-python-3？\">如何支持 Python 3？</h3>\n<ol>\n<li>创建一个新的 <code>Dockerfile</code>，用于安装 Python 3:</li>\n</ol>\n<pre><code>FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler:2.0.7\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends python3 &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre>\n<p>这个命令会安装默认的 <strong>Python 3.7.3</strong>. 如果你也想安装 <strong>pip3</strong>, 将 <code>python3</code> 替换为 <code>python3-pip</code> 即可</p>\n<pre><code>    apt-get install -y --no-install-recommends python3-pip &amp;&amp; \\\n</code></pre>\n<ol start=\"2\">\n<li>构建一个包含 Python 3 的新镜像:</li>\n</ol>\n<pre><code>docker build -t apache/dolphinscheduler:python3 .\n</code></pre>\n<ol start=\"3\">\n<li>\n<p>推送 docker 镜像 <code>apache/dolphinscheduler:python3</code> 到一个 docker registry 中</p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中 image 的 <code>repository</code> 字段，并更新 <code>tag</code> 为 <code>python3</code></p>\n</li>\n<li>\n<p>修改 <code>values.yaml</code> 文件中的 <code>PYTHON_HOME</code> 为 <code>/usr/bin/python3</code></p>\n</li>\n<li>\n<p>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</p>\n</li>\n<li>\n<p>在一个新 Python 任务下验证 Python 3</p>\n</li>\n</ol>\n<h3 id=\"如何支持-hadoop,-spark,-flink,-hive-或-datax？\">如何支持 Hadoop, Spark, Flink, Hive 或 DataX？</h3>\n<p>以 Spark 2.4.7 为例:</p>\n<ol>\n<li>\n<p>下载 Spark 2.4.7 发布的二进制包 <code>spark-2.4.7-bin-hadoop2.7.tgz</code></p>\n</li>\n<li>\n<p>确保 <code>common.sharedStoragePersistence.enabled</code> 开启</p>\n</li>\n<li>\n<p>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</p>\n</li>\n<li>\n<p>复制 Spark 3.1.1 二进制包到 Docker 容器中</p>\n</li>\n</ol>\n<pre><code class=\"language-bash\">kubectl <span class=\"hljs-built_in\">cp</span> spark-2.4.7-bin-hadoop2.7.tgz dolphinscheduler-worker-0:/opt/soft\nkubectl <span class=\"hljs-built_in\">cp</span> -n <span class=\"hljs-built_in\">test</span> spark-2.4.7-bin-hadoop2.7.tgz dolphinscheduler-worker-0:/opt/soft <span class=\"hljs-comment\"># with test namespace</span>\n</code></pre>\n<p>因为存储卷 <code>sharedStoragePersistence</code> 被挂载到 <code>/opt/soft</code>, 因此 <code>/opt/soft</code> 中的所有文件都不会丢失</p>\n<ol start=\"5\">\n<li>登录到容器并确保 <code>SPARK_HOME2</code> 存在</li>\n</ol>\n<pre><code class=\"language-bash\">kubectl <span class=\"hljs-built_in\">exec</span> -it dolphinscheduler-worker-0 bash\nkubectl <span class=\"hljs-built_in\">exec</span> -n <span class=\"hljs-built_in\">test</span> -it dolphinscheduler-worker-0 bash <span class=\"hljs-comment\"># with test namespace</span>\n<span class=\"hljs-built_in\">cd</span> /opt/soft\ntar zxf spark-2.4.7-bin-hadoop2.7.tgz\n<span class=\"hljs-built_in\">rm</span> -f spark-2.4.7-bin-hadoop2.7.tgz\n<span class=\"hljs-built_in\">ln</span> -s spark-2.4.7-bin-hadoop2.7 spark2 <span class=\"hljs-comment\"># or just mv</span>\n<span class=\"hljs-variable\">$SPARK_HOME2</span>/bin/spark-submit --version\n</code></pre>\n<p>如果一切执行正常，最后一条命令将会打印 Spark 版本信息</p>\n<ol start=\"6\">\n<li>在一个 Shell 任务下验证 Spark</li>\n</ol>\n<pre><code>$SPARK_HOME2/bin/spark-submit --class org.apache.spark.examples.SparkPi $SPARK_HOME2/examples/jars/spark-examples_2.11-2.4.7.jar\n</code></pre>\n<p>检查任务日志是否包含输出 <code>Pi is roughly 3.146015</code></p>\n<ol start=\"7\">\n<li>在一个 Spark 任务下验证 Spark</li>\n</ol>\n<p>文件 <code>spark-examples_2.11-2.4.7.jar</code> 需要先被上传到资源中心，然后创建一个 Spark 任务并设置:</p>\n<ul>\n<li>Spark版本: <code>SPARK2</code></li>\n<li>主函数的Class: <code>org.apache.spark.examples.SparkPi</code></li>\n<li>主程序包: <code>spark-examples_2.11-2.4.7.jar</code></li>\n<li>部署方式: <code>local</code></li>\n</ul>\n<p>同样地, 检查任务日志是否包含输出 <code>Pi is roughly 3.146015</code></p>\n<ol start=\"8\">\n<li>验证 Spark on YARN</li>\n</ol>\n<p>Spark on YARN (部署方式为 <code>cluster</code> 或 <code>client</code>) 需要 Hadoop 支持. 类似于 Spark 支持, 支持 Hadoop 的操作几乎和前面的步骤相同</p>\n<p>确保 <code>$HADOOP_HOME</code> 和 <code>$HADOOP_CONF_DIR</code> 存在</p>\n<h3 id=\"如何支持-spark-3？\">如何支持 Spark 3？</h3>\n<p>事实上，使用 <code>spark-submit</code> 提交应用的方式是相同的, 无论是 Spark 1, 2 或 3. 换句话说，<code>SPARK_HOME2</code> 的语义是第二个 <code>SPARK_HOME</code>, 而非 <code>SPARK2</code> 的 <code>HOME</code>, 因此只需设置 <code>SPARK_HOME2=/path/to/spark3</code> 即可</p>\n<p>以 Spark 3.1.1 为例:</p>\n<ol>\n<li>\n<p>下载 Spark 3.1.1 发布的二进制包 <code>spark-3.1.1-bin-hadoop2.7.tgz</code></p>\n</li>\n<li>\n<p>确保 <code>common.sharedStoragePersistence.enabled</code> 开启</p>\n</li>\n<li>\n<p>部署 dolphinscheduler (详见<strong>安装 dolphinscheduler</strong>)</p>\n</li>\n<li>\n<p>复制 Spark 3.1.1 二进制包到 Docker 容器中</p>\n</li>\n</ol>\n<pre><code class=\"language-bash\">kubectl <span class=\"hljs-built_in\">cp</span> spark-3.1.1-bin-hadoop2.7.tgz dolphinscheduler-worker-0:/opt/soft\nkubectl <span class=\"hljs-built_in\">cp</span> -n <span class=\"hljs-built_in\">test</span> spark-3.1.1-bin-hadoop2.7.tgz dolphinscheduler-worker-0:/opt/soft <span class=\"hljs-comment\"># with test namespace</span>\n</code></pre>\n<ol start=\"5\">\n<li>登录到容器并确保 <code>SPARK_HOME2</code> 存在</li>\n</ol>\n<pre><code class=\"language-bash\">kubectl <span class=\"hljs-built_in\">exec</span> -it dolphinscheduler-worker-0 bash\nkubectl <span class=\"hljs-built_in\">exec</span> -n <span class=\"hljs-built_in\">test</span> -it dolphinscheduler-worker-0 bash <span class=\"hljs-comment\"># with test namespace</span>\n<span class=\"hljs-built_in\">cd</span> /opt/soft\ntar zxf spark-3.1.1-bin-hadoop2.7.tgz\n<span class=\"hljs-built_in\">rm</span> -f spark-3.1.1-bin-hadoop2.7.tgz\n<span class=\"hljs-built_in\">ln</span> -s spark-3.1.1-bin-hadoop2.7 spark2 <span class=\"hljs-comment\"># or just mv</span>\n<span class=\"hljs-variable\">$SPARK_HOME2</span>/bin/spark-submit --version\n</code></pre>\n<p>如果一切执行正常，最后一条命令将会打印 Spark 版本信息</p>\n<ol start=\"6\">\n<li>在一个 Shell 任务下验证 Spark</li>\n</ol>\n<pre><code>$SPARK_HOME2/bin/spark-submit --class org.apache.spark.examples.SparkPi $SPARK_HOME2/examples/jars/spark-examples_2.12-3.1.1.jar\n</code></pre>\n<p>检查任务日志是否包含输出 <code>Pi is roughly 3.146015</code></p>\n<h3 id=\"如何在-master、worker-和-api-服务之间支持共享存储？\">如何在 Master、Worker 和 Api 服务之间支持共享存储？</h3>\n<p>例如, Master、Worker 和 Api 服务可能同时使用 Hadoop</p>\n<ol>\n<li>修改 <code>values.yaml</code> 文件中下面的配置项</li>\n</ol>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">sharedStoragePersistence:</span>\n    <span class=\"hljs-attr\">enabled:</span> <span class=\"hljs-literal\">false</span>\n    <span class=\"hljs-attr\">mountPath:</span> <span class=\"hljs-string\">&quot;/opt/soft&quot;</span>\n    <span class=\"hljs-attr\">accessModes:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">&quot;ReadWriteMany&quot;</span>\n    <span class=\"hljs-attr\">storageClassName:</span> <span class=\"hljs-string\">&quot;-&quot;</span>\n    <span class=\"hljs-attr\">storage:</span> <span class=\"hljs-string\">&quot;20Gi&quot;</span>\n</code></pre>\n<p><code>storageClassName</code> 和 <code>storage</code> 需要被修改为实际值</p>\n<blockquote>\n<p><strong>注意</strong>: <code>storageClassName</code> 必须支持访问模式: <code>ReadWriteMany</code></p>\n</blockquote>\n<ol start=\"2\">\n<li>\n<p>将 Hadoop 复制到目录 <code>/opt/soft</code></p>\n</li>\n<li>\n<p>确保 <code>$HADOOP_HOME</code> 和 <code>$HADOOP_CONF_DIR</code> 正确</p>\n</li>\n</ol>\n<h3 id=\"如何支持本地文件存储而非-hdfs-和-s3？\">如何支持本地文件存储而非 HDFS 和 S3？</h3>\n<p>修改 <code>values.yaml</code> 文件中下面的配置项</p>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">configmap:</span>\n    <span class=\"hljs-attr\">RESOURCE_STORAGE_TYPE:</span> <span class=\"hljs-string\">&quot;HDFS&quot;</span>\n    <span class=\"hljs-attr\">RESOURCE_UPLOAD_PATH:</span> <span class=\"hljs-string\">&quot;/dolphinscheduler&quot;</span>\n    <span class=\"hljs-attr\">FS_DEFAULT_FS:</span> <span class=\"hljs-string\">&quot;file:///&quot;</span>\n  <span class=\"hljs-attr\">fsFileResourcePersistence:</span>\n    <span class=\"hljs-attr\">enabled:</span> <span class=\"hljs-literal\">true</span>\n    <span class=\"hljs-attr\">accessModes:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">&quot;ReadWriteMany&quot;</span>\n    <span class=\"hljs-attr\">storageClassName:</span> <span class=\"hljs-string\">&quot;-&quot;</span>\n    <span class=\"hljs-attr\">storage:</span> <span class=\"hljs-string\">&quot;20Gi&quot;</span>\n</code></pre>\n<p><code>storageClassName</code> 和 <code>storage</code> 需要被修改为实际值</p>\n<blockquote>\n<p><strong>注意</strong>: <code>storageClassName</code> 必须支持访问模式: <code>ReadWriteMany</code></p>\n</blockquote>\n<h3 id=\"如何支持-s3-资源存储，例如-minio？\">如何支持 S3 资源存储，例如 MinIO？</h3>\n<p>以 MinIO 为例: 修改 <code>values.yaml</code> 文件中下面的配置项</p>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">configmap:</span>\n    <span class=\"hljs-attr\">RESOURCE_STORAGE_TYPE:</span> <span class=\"hljs-string\">&quot;S3&quot;</span>\n    <span class=\"hljs-attr\">RESOURCE_UPLOAD_PATH:</span> <span class=\"hljs-string\">&quot;/dolphinscheduler&quot;</span>\n    <span class=\"hljs-attr\">FS_DEFAULT_FS:</span> <span class=\"hljs-string\">&quot;s3a://BUCKET_NAME&quot;</span>\n    <span class=\"hljs-attr\">FS_S3A_ENDPOINT:</span> <span class=\"hljs-string\">&quot;http://MINIO_IP:9000&quot;</span>\n    <span class=\"hljs-attr\">FS_S3A_ACCESS_KEY:</span> <span class=\"hljs-string\">&quot;MINIO_ACCESS_KEY&quot;</span>\n    <span class=\"hljs-attr\">FS_S3A_SECRET_KEY:</span> <span class=\"hljs-string\">&quot;MINIO_SECRET_KEY&quot;</span>\n</code></pre>\n<p><code>BUCKET_NAME</code>, <code>MINIO_IP</code>, <code>MINIO_ACCESS_KEY</code> 和 <code>MINIO_SECRET_KEY</code> 需要被修改为实际值</p>\n<blockquote>\n<p><strong>注意</strong>: <code>MINIO_IP</code> 只能使用 IP 而非域名, 因为 DolphinScheduler 尚不支持 S3 路径风格访问 (S3 path style access)</p>\n</blockquote>\n<h3 id=\"如何配置-skywalking？\">如何配置 SkyWalking？</h3>\n<p>修改 <code>values.yaml</code> 文件中的 SKYWALKING 配置项</p>\n<pre><code class=\"language-yaml\"><span class=\"hljs-attr\">common:</span>\n  <span class=\"hljs-attr\">configmap:</span>\n    <span class=\"hljs-attr\">SKYWALKING_ENABLE:</span> <span class=\"hljs-string\">&quot;true&quot;</span>\n    <span class=\"hljs-attr\">SW_AGENT_COLLECTOR_BACKEND_SERVICES:</span> <span class=\"hljs-string\">&quot;127.0.0.1:11800&quot;</span>\n    <span class=\"hljs-attr\">SW_GRPC_LOG_SERVER_HOST:</span> <span class=\"hljs-string\">&quot;127.0.0.1&quot;</span>\n    <span class=\"hljs-attr\">SW_GRPC_LOG_SERVER_PORT:</span> <span class=\"hljs-string\">&quot;11800&quot;</span>\n</code></pre>\n<h2 id=\"附录-配置\">附录-配置</h2>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Description</th>\n<th>Default</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>timezone</code></td>\n<td>World time and date for cities in all time zones</td>\n<td><code>Asia/Shanghai</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>image.repository</code></td>\n<td>Docker image repository for the DolphinScheduler</td>\n<td><code>apache/dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>image.tag</code></td>\n<td>Docker image version for the DolphinScheduler</td>\n<td><code>latest</code></td>\n</tr>\n<tr>\n<td><code>image.pullPolicy</code></td>\n<td>Image pull policy. One of Always, Never, IfNotPresent</td>\n<td><code>IfNotPresent</code></td>\n</tr>\n<tr>\n<td><code>image.pullSecret</code></td>\n<td>Image pull secret. An optional reference to secret in the same namespace to use for pulling any of the images</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>postgresql.enabled</code></td>\n<td>If not exists external PostgreSQL, by default, the DolphinScheduler will use a internal PostgreSQL</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>postgresql.postgresqlUsername</code></td>\n<td>The username for internal PostgreSQL</td>\n<td><code>root</code></td>\n</tr>\n<tr>\n<td><code>postgresql.postgresqlPassword</code></td>\n<td>The password for internal PostgreSQL</td>\n<td><code>root</code></td>\n</tr>\n<tr>\n<td><code>postgresql.postgresqlDatabase</code></td>\n<td>The database for internal PostgreSQL</td>\n<td><code>dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>postgresql.persistence.enabled</code></td>\n<td>Set <code>postgresql.persistence.enabled</code> to <code>true</code> to mount a new volume for internal PostgreSQL</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>postgresql.persistence.size</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td><code>postgresql.persistence.storageClass</code></td>\n<td>PostgreSQL data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.type</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database type will use it</td>\n<td><code>postgresql</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.driver</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database driver will use it</td>\n<td><code>org.postgresql.Driver</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.host</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database host will use it</td>\n<td><code>localhost</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.port</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database port will use it</td>\n<td><code>5432</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.username</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database username will use it</td>\n<td><code>root</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.password</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database password will use it</td>\n<td><code>root</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.database</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database database will use it</td>\n<td><code>dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>externalDatabase.params</code></td>\n<td>If exists external PostgreSQL, and set <code>postgresql.enabled</code> value to false. DolphinScheduler's database params will use it</td>\n<td><code>characterEncoding=utf8</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>zookeeper.enabled</code></td>\n<td>If not exists external Zookeeper, by default, the DolphinScheduler will use a internal Zookeeper</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>zookeeper.fourlwCommandsWhitelist</code></td>\n<td>A list of comma separated Four Letter Words commands to use</td>\n<td><code>srvr,ruok,wchs,cons</code></td>\n</tr>\n<tr>\n<td><code>zookeeper.persistence.enabled</code></td>\n<td>Set <code>zookeeper.persistence.enabled</code> to <code>true</code> to mount a new volume for internal Zookeeper</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>zookeeper.persistence.size</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td><code>zookeeper.persistence.storageClass</code></td>\n<td>Zookeeper data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>zookeeper.zookeeperRoot</code></td>\n<td>Specify dolphinscheduler root directory in Zookeeper</td>\n<td><code>/dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>externalZookeeper.zookeeperQuorum</code></td>\n<td>If exists external Zookeeper, and set <code>zookeeper.enabled</code> value to false. Specify Zookeeper quorum</td>\n<td><code>127.0.0.1:2181</code></td>\n</tr>\n<tr>\n<td><code>externalZookeeper.zookeeperRoot</code></td>\n<td>If exists external Zookeeper, and set <code>zookeeper.enabled</code> value to false. Specify dolphinscheduler root directory in Zookeeper</td>\n<td><code>/dolphinscheduler</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>common.configmap.DOLPHINSCHEDULER_OPTS</code></td>\n<td>The jvm options for dolphinscheduler, suitable for all servers</td>\n<td><code>&quot;&quot;</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.DATA_BASEDIR_PATH</code></td>\n<td>User data directory path, self configuration, please make sure the directory exists and have read write permissions</td>\n<td><code>/tmp/dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.RESOURCE_STORAGE_TYPE</code></td>\n<td>Resource storage type: HDFS, S3, NONE</td>\n<td><code>HDFS</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.RESOURCE_UPLOAD_PATH</code></td>\n<td>Resource store on HDFS/S3 path, please make sure the directory exists on hdfs and have read write permissions</td>\n<td><code>/dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.FS_DEFAULT_FS</code></td>\n<td>Resource storage file system like <code>file:///</code>, <code>hdfs://mycluster:8020</code> or <code>s3a://dolphinscheduler</code></td>\n<td><code>file:///</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.FS_S3A_ENDPOINT</code></td>\n<td>S3 endpoint when <code>common.configmap.RESOURCE_STORAGE_TYPE</code> is set to <code>S3</code></td>\n<td><code>s3.xxx.amazonaws.com</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.FS_S3A_ACCESS_KEY</code></td>\n<td>S3 access key when <code>common.configmap.RESOURCE_STORAGE_TYPE</code> is set to <code>S3</code></td>\n<td><code>xxxxxxx</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.FS_S3A_SECRET_KEY</code></td>\n<td>S3 secret key when <code>common.configmap.RESOURCE_STORAGE_TYPE</code> is set to <code>S3</code></td>\n<td><code>xxxxxxx</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.HADOOP_SECURITY_AUTHENTICATION_STARTUP_STATE</code></td>\n<td>Whether to startup kerberos</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.JAVA_SECURITY_KRB5_CONF_PATH</code></td>\n<td>The java.security.krb5.conf path</td>\n<td><code>/opt/krb5.conf</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.LOGIN_USER_KEYTAB_USERNAME</code></td>\n<td>The login user from keytab username</td>\n<td><code>hdfs@HADOOP.COM</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.LOGIN_USER_KEYTAB_PATH</code></td>\n<td>The login user from keytab path</td>\n<td><code>/opt/hdfs.keytab</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.KERBEROS_EXPIRE_TIME</code></td>\n<td>The kerberos expire time, the unit is hour</td>\n<td><code>2</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.HDFS_ROOT_USER</code></td>\n<td>The HDFS root user who must have the permission to create directories under the HDFS root path</td>\n<td><code>hdfs</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.RESOURCE_MANAGER_HTTPADDRESS_PORT</code></td>\n<td>Set resource manager httpaddress port for yarn</td>\n<td><code>8088</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.YARN_RESOURCEMANAGER_HA_RM_IDS</code></td>\n<td>If resourcemanager HA is enabled, please set the HA IPs</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.YARN_APPLICATION_STATUS_ADDRESS</code></td>\n<td>If resourcemanager is single, you only need to replace ds1 to actual resourcemanager hostname, otherwise keep default</td>\n<td><code>http://ds1:%s/ws/v1/cluster/apps/%s</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.SKYWALKING_ENABLE</code></td>\n<td>Set whether to enable skywalking</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.SW_AGENT_COLLECTOR_BACKEND_SERVICES</code></td>\n<td>Set agent collector backend services for skywalking</td>\n<td><code>127.0.0.1:11800</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.SW_GRPC_LOG_SERVER_HOST</code></td>\n<td>Set grpc log server host for skywalking</td>\n<td><code>127.0.0.1</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.SW_GRPC_LOG_SERVER_PORT</code></td>\n<td>Set grpc log server port for skywalking</td>\n<td><code>11800</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.HADOOP_HOME</code></td>\n<td>Set <code>HADOOP_HOME</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/hadoop</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.HADOOP_CONF_DIR</code></td>\n<td>Set <code>HADOOP_CONF_DIR</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/hadoop/etc/hadoop</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.SPARK_HOME1</code></td>\n<td>Set <code>SPARK_HOME1</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/spark1</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.SPARK_HOME2</code></td>\n<td>Set <code>SPARK_HOME2</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/spark2</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.PYTHON_HOME</code></td>\n<td>Set <code>PYTHON_HOME</code> for DolphinScheduler's task environment</td>\n<td><code>/usr/bin/python</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.JAVA_HOME</code></td>\n<td>Set <code>JAVA_HOME</code> for DolphinScheduler's task environment</td>\n<td><code>/usr/local/openjdk-8</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.HIVE_HOME</code></td>\n<td>Set <code>HIVE_HOME</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/hive</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.FLINK_HOME</code></td>\n<td>Set <code>FLINK_HOME</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/flink</code></td>\n</tr>\n<tr>\n<td><code>common.configmap.DATAX_HOME</code></td>\n<td>Set <code>DATAX_HOME</code> for DolphinScheduler's task environment</td>\n<td><code>/opt/soft/datax</code></td>\n</tr>\n<tr>\n<td><code>common.sharedStoragePersistence.enabled</code></td>\n<td>Set <code>common.sharedStoragePersistence.enabled</code> to <code>true</code> to mount a shared storage volume for Hadoop, Spark binary and etc</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>common.sharedStoragePersistence.mountPath</code></td>\n<td>The mount path for the shared storage volume</td>\n<td><code>/opt/soft</code></td>\n</tr>\n<tr>\n<td><code>common.sharedStoragePersistence.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes, must be <code>ReadWriteMany</code></td>\n<td><code>[ReadWriteMany]</code></td>\n</tr>\n<tr>\n<td><code>common.sharedStoragePersistence.storageClassName</code></td>\n<td>Shared Storage persistent volume storage class, must support the access mode: ReadWriteMany</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>common.sharedStoragePersistence.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td><code>common.fsFileResourcePersistence.enabled</code></td>\n<td>Set <code>common.fsFileResourcePersistence.enabled</code> to <code>true</code> to mount a new file resource volume for <code>api</code> and <code>worker</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>common.fsFileResourcePersistence.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes, must be <code>ReadWriteMany</code></td>\n<td><code>[ReadWriteMany]</code></td>\n</tr>\n<tr>\n<td><code>common.fsFileResourcePersistence.storageClassName</code></td>\n<td>Resource persistent volume storage class, must support the access mode: ReadWriteMany</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>common.fsFileResourcePersistence.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>master.podManagementPolicy</code></td>\n<td>PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down</td>\n<td><code>Parallel</code></td>\n</tr>\n<tr>\n<td><code>master.replicas</code></td>\n<td>Replicas is the desired number of replicas of the given Template</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>master.annotations</code></td>\n<td>The <code>annotations</code> for master server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>master.affinity</code></td>\n<td>If specified, the pod's scheduling constraints</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>master.nodeSelector</code></td>\n<td>NodeSelector is a selector which must be true for the pod to fit on a node</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>master.tolerations</code></td>\n<td>If specified, the pod's tolerations</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>master.resources</code></td>\n<td>The <code>resource</code> limit and request config for master server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_SERVER_OPTS</code></td>\n<td>The jvm options for master server</td>\n<td><code>-Xms1g -Xmx1g -Xmn512m</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_EXEC_THREADS</code></td>\n<td>Master execute thread number to limit process instances</td>\n<td><code>100</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_EXEC_TASK_NUM</code></td>\n<td>Master execute task number in parallel per process instance</td>\n<td><code>20</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_DISPATCH_TASK_NUM</code></td>\n<td>Master dispatch task number per batch</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_HOST_SELECTOR</code></td>\n<td>Master host selector to select a suitable worker, optional values include Random, RoundRobin, LowerWeight</td>\n<td><code>LowerWeight</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_HEARTBEAT_INTERVAL</code></td>\n<td>Master heartbeat interval, the unit is second</td>\n<td><code>10</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_TASK_COMMIT_RETRYTIMES</code></td>\n<td>Master commit task retry times</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_TASK_COMMIT_INTERVAL</code></td>\n<td>master commit task interval, the unit is second</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_MAX_CPULOAD_AVG</code></td>\n<td>Master max cpuload avg, only higher than the system cpu load average, master server can schedule</td>\n<td><code>-1</code> (<code>the number of cpu cores * 2</code>)</td>\n</tr>\n<tr>\n<td><code>master.configmap.MASTER_RESERVED_MEMORY</code></td>\n<td>Master reserved memory, only lower than system available memory, master server can schedule, the unit is G</td>\n<td><code>0.3</code></td>\n</tr>\n<tr>\n<td><code>master.livenessProbe.enabled</code></td>\n<td>Turn on and off liveness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>master.livenessProbe.initialDelaySeconds</code></td>\n<td>Delay before liveness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>master.livenessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>master.livenessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>master.livenessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>master.livenessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>master.readinessProbe.enabled</code></td>\n<td>Turn on and off readiness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>master.readinessProbe.initialDelaySeconds</code></td>\n<td>Delay before readiness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>master.readinessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>master.readinessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>master.readinessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>master.readinessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>master.persistentVolumeClaim.enabled</code></td>\n<td>Set <code>master.persistentVolumeClaim.enabled</code> to <code>true</code> to mount a new volume for <code>master</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>master.persistentVolumeClaim.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes</td>\n<td><code>[ReadWriteOnce]</code></td>\n</tr>\n<tr>\n<td><code>master.persistentVolumeClaim.storageClassName</code></td>\n<td><code>Master</code> logs data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>master.persistentVolumeClaim.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>worker.podManagementPolicy</code></td>\n<td>PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down</td>\n<td><code>Parallel</code></td>\n</tr>\n<tr>\n<td><code>worker.replicas</code></td>\n<td>Replicas is the desired number of replicas of the given Template</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>worker.annotations</code></td>\n<td>The <code>annotations</code> for worker server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>worker.affinity</code></td>\n<td>If specified, the pod's scheduling constraints</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>worker.nodeSelector</code></td>\n<td>NodeSelector is a selector which must be true for the pod to fit on a node</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>worker.tolerations</code></td>\n<td>If specified, the pod's tolerations</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>worker.resources</code></td>\n<td>The <code>resource</code> limit and request config for worker server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>worker.configmap.LOGGER_SERVER_OPTS</code></td>\n<td>The jvm options for logger server</td>\n<td><code>-Xms512m -Xmx512m -Xmn256m</code></td>\n</tr>\n<tr>\n<td><code>worker.configmap.WORKER_SERVER_OPTS</code></td>\n<td>The jvm options for worker server</td>\n<td><code>-Xms1g -Xmx1g -Xmn512m</code></td>\n</tr>\n<tr>\n<td><code>worker.configmap.WORKER_EXEC_THREADS</code></td>\n<td>Worker execute thread number to limit task instances</td>\n<td><code>100</code></td>\n</tr>\n<tr>\n<td><code>worker.configmap.WORKER_HEARTBEAT_INTERVAL</code></td>\n<td>Worker heartbeat interval, the unit is second</td>\n<td><code>10</code></td>\n</tr>\n<tr>\n<td><code>worker.configmap.WORKER_MAX_CPULOAD_AVG</code></td>\n<td>Worker max cpuload avg, only higher than the system cpu load average, worker server can be dispatched tasks</td>\n<td><code>-1</code> (<code>the number of cpu cores * 2</code>)</td>\n</tr>\n<tr>\n<td><code>worker.configmap.WORKER_RESERVED_MEMORY</code></td>\n<td>Worker reserved memory, only lower than system available memory, worker server can be dispatched tasks, the unit is G</td>\n<td><code>0.3</code></td>\n</tr>\n<tr>\n<td><code>worker.configmap.WORKER_GROUPS</code></td>\n<td>Worker groups</td>\n<td><code>default</code></td>\n</tr>\n<tr>\n<td><code>worker.livenessProbe.enabled</code></td>\n<td>Turn on and off liveness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>worker.livenessProbe.initialDelaySeconds</code></td>\n<td>Delay before liveness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>worker.livenessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>worker.livenessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>worker.livenessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>worker.livenessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>worker.readinessProbe.enabled</code></td>\n<td>Turn on and off readiness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>worker.readinessProbe.initialDelaySeconds</code></td>\n<td>Delay before readiness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>worker.readinessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>worker.readinessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>worker.readinessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>worker.readinessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.enabled</code></td>\n<td>Set <code>worker.persistentVolumeClaim.enabled</code> to <code>true</code> to enable <code>persistentVolumeClaim</code> for <code>worker</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.dataPersistentVolume.enabled</code></td>\n<td>Set <code>worker.persistentVolumeClaim.dataPersistentVolume.enabled</code> to <code>true</code> to mount a data volume for <code>worker</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.dataPersistentVolume.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes</td>\n<td><code>[ReadWriteOnce]</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.dataPersistentVolume.storageClassName</code></td>\n<td><code>Worker</code> data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.dataPersistentVolume.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.logsPersistentVolume.enabled</code></td>\n<td>Set <code>worker.persistentVolumeClaim.logsPersistentVolume.enabled</code> to <code>true</code> to mount a logs volume for <code>worker</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.logsPersistentVolume.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes</td>\n<td><code>[ReadWriteOnce]</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.logsPersistentVolume.storageClassName</code></td>\n<td><code>Worker</code> logs data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>worker.persistentVolumeClaim.logsPersistentVolume.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>alert.replicas</code></td>\n<td>Replicas is the desired number of replicas of the given Template</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>alert.strategy.type</code></td>\n<td>Type of deployment. Can be &quot;Recreate&quot; or &quot;RollingUpdate&quot;</td>\n<td><code>RollingUpdate</code></td>\n</tr>\n<tr>\n<td><code>alert.strategy.rollingUpdate.maxSurge</code></td>\n<td>The maximum number of pods that can be scheduled above the desired number of pods</td>\n<td><code>25%</code></td>\n</tr>\n<tr>\n<td><code>alert.strategy.rollingUpdate.maxUnavailable</code></td>\n<td>The maximum number of pods that can be unavailable during the update</td>\n<td><code>25%</code></td>\n</tr>\n<tr>\n<td><code>alert.annotations</code></td>\n<td>The <code>annotations</code> for alert server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>alert.affinity</code></td>\n<td>If specified, the pod's scheduling constraints</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>alert.nodeSelector</code></td>\n<td>NodeSelector is a selector which must be true for the pod to fit on a node</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>alert.tolerations</code></td>\n<td>If specified, the pod's tolerations</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>alert.resources</code></td>\n<td>The <code>resource</code> limit and request config for alert server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.ALERT_SERVER_OPTS</code></td>\n<td>The jvm options for alert server</td>\n<td><code>-Xms512m -Xmx512m -Xmn256m</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.XLS_FILE_PATH</code></td>\n<td>XLS file path</td>\n<td><code>/tmp/xls</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_SERVER_HOST</code></td>\n<td>Mail <code>SERVER HOST </code></td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_SERVER_PORT</code></td>\n<td>Mail <code>SERVER PORT</code></td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_SENDER</code></td>\n<td>Mail <code>SENDER</code></td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_USER</code></td>\n<td>Mail <code>USER</code></td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_PASSWD</code></td>\n<td>Mail <code>PASSWORD</code></td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_SMTP_STARTTLS_ENABLE</code></td>\n<td>Mail <code>SMTP STARTTLS</code> enable</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_SMTP_SSL_ENABLE</code></td>\n<td>Mail <code>SMTP SSL</code> enable</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.MAIL_SMTP_SSL_TRUST</code></td>\n<td>Mail <code>SMTP SSL TRUST</code></td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.ENTERPRISE_WECHAT_ENABLE</code></td>\n<td><code>Enterprise Wechat</code> enable</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.ENTERPRISE_WECHAT_CORP_ID</code></td>\n<td><code>Enterprise Wechat</code> corp id</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.ENTERPRISE_WECHAT_SECRET</code></td>\n<td><code>Enterprise Wechat</code> secret</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.ENTERPRISE_WECHAT_AGENT_ID</code></td>\n<td><code>Enterprise Wechat</code> agent id</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.configmap.ENTERPRISE_WECHAT_USERS</code></td>\n<td><code>Enterprise Wechat</code> users</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>alert.livenessProbe.enabled</code></td>\n<td>Turn on and off liveness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>alert.livenessProbe.initialDelaySeconds</code></td>\n<td>Delay before liveness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>alert.livenessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>alert.livenessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>alert.livenessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>alert.livenessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>alert.readinessProbe.enabled</code></td>\n<td>Turn on and off readiness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>alert.readinessProbe.initialDelaySeconds</code></td>\n<td>Delay before readiness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>alert.readinessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>alert.readinessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>alert.readinessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>alert.readinessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>alert.persistentVolumeClaim.enabled</code></td>\n<td>Set <code>alert.persistentVolumeClaim.enabled</code> to <code>true</code> to mount a new volume for <code>alert</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>alert.persistentVolumeClaim.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes</td>\n<td><code>[ReadWriteOnce]</code></td>\n</tr>\n<tr>\n<td><code>alert.persistentVolumeClaim.storageClassName</code></td>\n<td><code>Alert</code> logs data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>alert.persistentVolumeClaim.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>api.replicas</code></td>\n<td>Replicas is the desired number of replicas of the given Template</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>api.strategy.type</code></td>\n<td>Type of deployment. Can be &quot;Recreate&quot; or &quot;RollingUpdate&quot;</td>\n<td><code>RollingUpdate</code></td>\n</tr>\n<tr>\n<td><code>api.strategy.rollingUpdate.maxSurge</code></td>\n<td>The maximum number of pods that can be scheduled above the desired number of pods</td>\n<td><code>25%</code></td>\n</tr>\n<tr>\n<td><code>api.strategy.rollingUpdate.maxUnavailable</code></td>\n<td>The maximum number of pods that can be unavailable during the update</td>\n<td><code>25%</code></td>\n</tr>\n<tr>\n<td><code>api.annotations</code></td>\n<td>The <code>annotations</code> for api server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>api.affinity</code></td>\n<td>If specified, the pod's scheduling constraints</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>api.nodeSelector</code></td>\n<td>NodeSelector is a selector which must be true for the pod to fit on a node</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>api.tolerations</code></td>\n<td>If specified, the pod's tolerations</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>api.resources</code></td>\n<td>The <code>resource</code> limit and request config for api server</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td><code>api.configmap.API_SERVER_OPTS</code></td>\n<td>The jvm options for api server</td>\n<td><code>-Xms512m -Xmx512m -Xmn256m</code></td>\n</tr>\n<tr>\n<td><code>api.livenessProbe.enabled</code></td>\n<td>Turn on and off liveness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>api.livenessProbe.initialDelaySeconds</code></td>\n<td>Delay before liveness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>api.livenessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>api.livenessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>api.livenessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>api.livenessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>api.readinessProbe.enabled</code></td>\n<td>Turn on and off readiness probe</td>\n<td><code>true</code></td>\n</tr>\n<tr>\n<td><code>api.readinessProbe.initialDelaySeconds</code></td>\n<td>Delay before readiness probe is initiated</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>api.readinessProbe.periodSeconds</code></td>\n<td>How often to perform the probe</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>api.readinessProbe.timeoutSeconds</code></td>\n<td>When the probe times out</td>\n<td><code>5</code></td>\n</tr>\n<tr>\n<td><code>api.readinessProbe.failureThreshold</code></td>\n<td>Minimum consecutive successes for the probe</td>\n<td><code>3</code></td>\n</tr>\n<tr>\n<td><code>api.readinessProbe.successThreshold</code></td>\n<td>Minimum consecutive failures for the probe</td>\n<td><code>1</code></td>\n</tr>\n<tr>\n<td><code>api.persistentVolumeClaim.enabled</code></td>\n<td>Set <code>api.persistentVolumeClaim.enabled</code> to <code>true</code> to mount a new volume for <code>api</code></td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>api.persistentVolumeClaim.accessModes</code></td>\n<td><code>PersistentVolumeClaim</code> access modes</td>\n<td><code>[ReadWriteOnce]</code></td>\n</tr>\n<tr>\n<td><code>api.persistentVolumeClaim.storageClassName</code></td>\n<td><code>api</code> logs data persistent volume storage class. If set to &quot;-&quot;, storageClassName: &quot;&quot;, which disables dynamic provisioning</td>\n<td><code>-</code></td>\n</tr>\n<tr>\n<td><code>api.persistentVolumeClaim.storage</code></td>\n<td><code>PersistentVolumeClaim</code> size</td>\n<td><code>20Gi</code></td>\n</tr>\n<tr>\n<td><code>api.service.type</code></td>\n<td><code>type</code> determines how the Service is exposed. Valid options are ExternalName, ClusterIP, NodePort, and LoadBalancer</td>\n<td><code>ClusterIP</code></td>\n</tr>\n<tr>\n<td><code>api.service.clusterIP</code></td>\n<td><code>clusterIP</code> is the IP address of the service and is usually assigned randomly by the master</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>api.service.nodePort</code></td>\n<td><code>nodePort</code> is the port on each node on which this service is exposed when type=NodePort</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>api.service.externalIPs</code></td>\n<td><code>externalIPs</code> is a list of IP addresses for which nodes in the cluster will also accept traffic for this service</td>\n<td><code>[]</code></td>\n</tr>\n<tr>\n<td><code>api.service.externalName</code></td>\n<td><code>externalName</code> is the external reference that kubedns or equivalent will return as a CNAME record for this service</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>api.service.loadBalancerIP</code></td>\n<td><code>loadBalancerIP</code> when service.type is LoadBalancer. LoadBalancer will get created with the IP specified in this field</td>\n<td><code>nil</code></td>\n</tr>\n<tr>\n<td><code>api.service.annotations</code></td>\n<td><code>annotations</code> may need to be set when service.type is LoadBalancer</td>\n<td><code>{}</code></td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><code>ingress.enabled</code></td>\n<td>Enable ingress</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>ingress.host</code></td>\n<td>Ingress host</td>\n<td><code>dolphinscheduler.org</code></td>\n</tr>\n<tr>\n<td><code>ingress.path</code></td>\n<td>Ingress path</td>\n<td><code>/dolphinscheduler</code></td>\n</tr>\n<tr>\n<td><code>ingress.tls.enabled</code></td>\n<td>Enable ingress tls</td>\n<td><code>false</code></td>\n</tr>\n<tr>\n<td><code>ingress.tls.secretName</code></td>\n<td>Ingress tls secret name</td>\n<td><code>dolphinscheduler-tls</code></td>\n</tr>\n</tbody>\n</table>\n",
  "location": [
    "部署指南",
    "Kubernetes部署(Kubernetes)"
  ],
  "time": "2022-10-24",
  "structure": [
    {
      "title": "先决条件",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "先决条件"
    },
    {
      "title": "安装 dolphinscheduler",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "安装-dolphinscheduler"
    },
    {
      "title": "访问 DolphinScheduler 前端页面",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "访问-dolphinscheduler-前端页面"
    },
    {
      "title": "卸载 dolphinscheduler",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "卸载-dolphinscheduler"
    },
    {
      "title": "配置",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "配置"
    },
    {
      "title": "支持矩阵",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "支持矩阵"
    },
    {
      "title": "FAQ",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "faq"
    },
    {
      "title": "附录-配置",
      "children": [
        {
          "title": "如何查看一个 pod 容器的日志？",
          "children": [],
          "anchor": "如何查看一个-pod-容器的日志？"
        },
        {
          "title": "如何在 Kubernetes 上扩缩容 api, master 和 worker？",
          "children": [],
          "anchor": "如何在-kubernetes-上扩缩容-api,-master-和-worker？"
        },
        {
          "title": "如何用 MySQL 替代 PostgreSQL 作为 DolphinScheduler 的数据库？",
          "children": [],
          "anchor": "如何用-mysql-替代-postgresql-作为-dolphinscheduler-的数据库？"
        },
        {
          "title": "如何在数据源中心支持 MySQL 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-mysql-数据源？"
        },
        {
          "title": "如何在数据源中心支持 Oracle 数据源？",
          "children": [],
          "anchor": "如何在数据源中心支持-oracle-数据源？"
        },
        {
          "title": "如何支持 Python 2 pip 以及自定义 requirements.txt？",
          "children": [],
          "anchor": "如何支持-python-2-pip-以及自定义-requirements.txt？"
        },
        {
          "title": "如何支持 Python 3？",
          "children": [],
          "anchor": "如何支持-python-3？"
        },
        {
          "title": "如何支持 Hadoop, Spark, Flink, Hive 或 DataX？",
          "children": [],
          "anchor": "如何支持-hadoop,-spark,-flink,-hive-或-datax？"
        },
        {
          "title": "如何支持 Spark 3？",
          "children": [],
          "anchor": "如何支持-spark-3？"
        },
        {
          "title": "如何在 Master、Worker 和 Api 服务之间支持共享存储？",
          "children": [],
          "anchor": "如何在-master、worker-和-api-服务之间支持共享存储？"
        },
        {
          "title": "如何支持本地文件存储而非 HDFS 和 S3？",
          "children": [],
          "anchor": "如何支持本地文件存储而非-hdfs-和-s3？"
        },
        {
          "title": "如何支持 S3 资源存储，例如 MinIO？",
          "children": [],
          "anchor": "如何支持-s3-资源存储，例如-minio？"
        },
        {
          "title": "如何配置 SkyWalking？",
          "children": [],
          "anchor": "如何配置-skywalking？"
        }
      ],
      "anchor": "附录-配置"
    }
  ],
  "title": "快速试用 Kubernetes 部署",
  "link": "/guide/installation/kubernetes"
}