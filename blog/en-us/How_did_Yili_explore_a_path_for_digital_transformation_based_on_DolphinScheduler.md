---
title:# How did Yili explore a “path” for digital transformation based on DolphinScheduler?
keywords: Apache,DolphinScheduler,scheduler,big data,ETL,airflow,hadoop,orchestration,dataops,Meetup
description:People in general know about Yili. Every year, 1.3 billion...
---
# How did Yili explore a “path” for digital transformation based on DolphinScheduler?
![](https://miro.medium.com/max/1400/1*m9up--yqrOqWkvVCAF7QWA.png)

# Background and introduction of application

## Introduction of Yili
People in general know about Yili. Every year, 1.3 billion national consumers consume an average of 100 million packs of Yili products every day. The starting point of Yili’s business can be traced back to the cultivation of grass, the breeding of a cow, the production of a cup of milk, and finally delivered to consumers through a complex supply chain system, which involves the primary, secondary and tertiary industries.

## Background of the application

There is a complex application matrix behind the huge business volume, which poses a great challenge to our technical architecture.

**· The current multi-cloud distribution of applications makes cross-cloud data migration and multi-cloud unified scheduling a rigid requirement**
Currently, in the process of cooperation with cloud service providers, Yili does not only consider the IaaS and PaaS abilities supported by cloud service providers, but also the ecology behind them, including private domain ecology, e-commerce, logistics, etc. Therefore, our applications are more distributed on multiple clouds, and the data generated by these applications are also found on multiple clouds. The current centralized data structure requires a large amount of physical data relocation, and we need to migrate 8,000 tables per day, involving more than 80 systems. Stable and scalable multi-cloud data integration is thus crucial.

· Unified technical architecture to combat entropy increase, reduce cost and increase efficiency

The first challenge we faced was the ease of use, stability, and scalability of data integration and scheduling abilities due to massive relocation requirements. The second one came from the effect of entropy increase brought by the landing of a large number of applications. In the case of limited technical resources, it was difficult for us to effectively control our costs and improve our efficiency, especially in the face of data integration. We were facing the following core issues:

• Duplicate construction of similar tools and products: Due to a lack of unified planning, duplicate construction of similar functional products exists.

• Miscellaneous technology selection: AirFlow, Azkaban, Oozie, self-developed scheduling, etc.

• High construction and derivative costs: costs such as personnel resource reserves, operation and maintenance, and operation training for multiple technology stacks.

• Extensibility issues: extensive support for localized individual requirements.

Before the introduction of Apache DolphinScheduler, our overall technical architecture was not unified in scheduling and data integration. To meet our internal rigid requirements, and with the consideration of scalability and stability, we planned to build a unified scheduling & data integration service system. After researching a large number of scheduling products, we decided to use Apache DolphinScheduler as our core engine and make localized transformations on top of it.

## The positioning and application of the status of the Yili Big Data Scheduling System
With limited technical resources, it is challenging to initiate the development of a tool. We must be clear about who our core users are, what the core positioning of this product is, and in what circumstances is this product used.

After sufficient research, we have clarified the core positioning of this tool. It is not a scheduling platform for simple workflow, but a development platform of integrated data that suits the multi-cloud business of Yili. Application developers support the integration of internal and external data, scheduling requirements of the application, visual orchestration, and scheduling of data tasks.

Judging from the current application of Yili, the number of daily scheduling tasks has reached 13,000, there are 15 nodes of clusters and more than 8000 tables are relocated every day.

We have built a unified data platform for data integration, development, scheduling, operation, and maintenance on the multi-cloud infrastructure for big data. Through this platform, we can hide the differences between multi-cloud big data platforms and provide users with a unified development experience.

**· System Overview of the Data Scheduling Service Platform of Yili
**
We have re-abstracted the 2.0.2 version of the entire model of DolphinScheduler, with the project as the top model, abstracting the three concepts of resources, roles, and tools, and bound them to the project. Among them, the toolset is the most important. All the tools used in the data development process would be expanded here. Users can complete data integration, development, operation and maintenance, asset management, and other tasks by authorizing projects in the platform. We have also integrated the tenants of Apache DolphinScheduler with the tenants of the big data platform and completed the fine management of resource quotas and permissions through the tenant management system of Ranger+Ldap+Kerberos on the big data platform.

# Practice & Exploration
We have upgraded three times based on Apache DolphinScheduler 2.0.2 version with many function optimization, but there is a logic that runs through our product design and development process, that is, to meet the core needs of users to build new functions, improve user experience and to optimize the old functions. We aim to make the product easy to use, easy to get used to, user-friendly, and with reasonable moving lines.

## Scenario-oriented data integration
What is scenario-oriented data integration? Our products are completely open to users. When developers do data integration, they have a very clear context, such as synchronizing Mysql data to Hive, ES data to StarRocks, etc. Therefore, we provide a large number of integration components, which can complete the integration based on the configuration method. In the whole process, no code is needed. This improves the efficiency of data integration development.

Before planning the core functions of data integration, we had the following main problems:

1. Multiple data integration components or technologies make it difficult for users to choose

2. An inconsistent technical structure that leads to increased maintenance costs

3. The configuration of data integration is troublesome

4. Poor scalability for secondary development

We implemented the following strategies for these four problems :

1. We only gave users the best choice — there are often no choices to choose from, so we will just provide the best one.

2. Based on the structure of DataX unified data integration, we solved problems related to resources and reduced operation and maintenance costs

3. For business applications, we used the template generator to generate task templates to simplify operations

4. We implemented a reasonable code design

## A lightened asset management
The core requirement corresponding to this function is as follows: when applying the native big data capability to build a lightened data warehouse, it is necessary to gain a clear insight into the technical metadata information, including library, table, field, and basic attributes. We listed some business tag information, including subject domains, topics, etc., to facilitate the use of data by analysts. Based on such demands, we extended this lightened asset management function on the platform.

**Forward and reverse table building are the core functions**. Forward table building refers to supporting users to create table models on the platform and materialize the table models on the big data platform. Reverse table building is used more often. In many cases, users still use traditional development methods to write scripts on the ETL server and submit them to the big data platform for calculation. At this time, the table already exists in advance, rather than being created visually. To reverse building a table is to automatically scan the metadata and register it on the asset platform.

Asset retrieval provides asset retrieval functions for development and business analysis. It can perform lightweight retrieval based on subject domains, tags, field names, etc., and display asset cards, and upstream and downstream bloodline information.

##  Scheduling of multi-cloud tasks
Scheduling of multi-cloud tasks is to solve the integrated scheduling problem of multi-cloud platforms. As mentioned above, Yili’s applications are distributed in multiple clouds, and this function can complete cross-cloud task scheduling based on a canvas. We made some extensions and transformations based on Apache DolphinScheduler. The binding relationship between the environment and the worker group is flexibly used, and the big data infrastructure is used to solve the problem of Kerberos cross-domain and mutual trust of multi-cluster Keytab.

We can separately trigger data integration tasks, including data processing tasks on different clouds. After the processing is completed, data calculation and business application-oriented planning are triggered on another cloud, and data services are provided finally. In this way, multi-cloud scheduling is done.

## Optimization of monitoring dashboard
Based on the original Apache DolphinScheduler dashboard, we did some optimization and upgrade. By focusing on operation and maintenance, we built the core indicators that the operation and maintenance work focuses on and displayed the indicators visually.

The entire monitoring board is divided into task monitoring and cluster service monitoring.

For task monitoring, we design key indicators like the number of projects, workflow (online/offline), the number of tasks to be scheduled, the number of tasks that have been successfully scheduled, an overview of the hourly operation, the TOP5 statistics on running failures, and the TOP5 statistics on running time.

For service monitoring, key indicators include CPU, memory, average load, worker load in each cloud environment, and hourly worker operation overview.

## Upgrade and optimization of users’ experience

We also upgraded and optimized the user experience, such as the optimization of users’ timed configuration and the overall layout, which aims to optimize the user experience and make it easier to use.

# Plans
Lastly, these are the plans for DolphinScheduler.

## Long-term planning
In the long run, both the community and our company need to have a clear positioning for DolphinScheduler. It is not a scheduling platform for simple workflow, but a one-stop intelligent data development platform. There are at least three important characteristics of this platform:

1. Multi-cloud unification: It can perfectly fit with the new decentralized data structure, which is a very important direction;

2. Low-code: From the user’s point of view, it can support one-stop visual modeling, development, and analysis experience for the whole link;

3. Intelligent management: The current data structure is based on reducing the complexity of data management, metadata-driven governance, and realizing intelligent data quality management will also be an important proposition.

## Short and mid-range plans:
Three important points will be focused on in the short term :

1. The combination of cloud-native: The DolphinScheduler community has provided deployment services based on k8S, so our master, worker, and other services will be transformed based on containerization;

2. Introduce the testing and online process: add roles such as developers and reviewers in the project, and control the online actions, and the metadata will be automatically synchronized after getting online;

3. Quality module: DolphinScheduler 3.0 has added the data quality module. We will integrate this part and carry out the localized expansion, including pre-defining data quality, auditing, introducing data blood relationships, etc.

# Reflects on open-source
During the process of introducing DolphinScheduler, Yili was driven and inspired by some thoughts on collaboration. That is it’s very important to build a good collaboration between the company and open-source.

Internally, we will assign specific personnel to follow up on the dynamics of open-source communities. The R&D team will proactively find bugs and submit them to the community, and submit PRs after they are fixed.

At the same time, we will also actively interact with the community and check the trends through weekly and monthly meetings. Companies with limited technical research and development resources should find an open-source product that fits the actual use, and truly understand and implement the meaning of open-source. It is a powerful tool, as it can solve many problems internally, and notice issues that were never noticed before. Therefore, companies need to implement a suitable process for open-source and grow with the community.

That’s all of my talks. Thank you!

# How to contribute:

* GitHub Code Repository: https://github.com/apache/dolphinscheduler

* Official Website:https://dolphinscheduler.apache.org/

* Mail List:dev@dolphinscheduler@apache.org

* Twitter:@DolphinSchedule