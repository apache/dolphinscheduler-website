# 中国联通改造 Apache DolphinScheduler 资源中心，实现计费环境跨集群调用与数据脚本一站式访问

<div align=center>
<img src="/img/2022-05-07/ch/0.png"/>
</div>

截止2022年，中国联通用户规模达到4.6亿，占据了全中国人口的30%，随着5G的推广普及，运营商IT系统普遍面临着海量用户、海量话单、多样化业务、组网模式等一系列变革的冲击。

当前，联通每天处理话单量超过400亿条。在这样的体量基础上，提高服务水平，为客户提供更有针对性的服务，也成为了联通品牌追求的终极目标。而中国联通在海量数据汇集、加工、脱敏、加密等技术与应用方面已崭露头角，在行业中具有一定的先发优势，未来势必成为大数据赋能数字经济发展的重要推动者。

在 Apache DolphinScheduler 4月 Meetup 上，我们邀请到了联通软件研究院的柏雪松，他为我们分享了《DolphinScheduler在联通计费环境中的应用》。

本次演讲主要包括三个部分：

- DolphinScheduler在联通的总体使用情况
    
- 联通计费业务专题分享
    
- 下一步的规划
    

<div align=center>
<img src="/img/2022-05-07/ch/0-1.png"/>
</div>

柏雪松 联通软研院 大数据工程师

毕业于中国农业大学，从事于大数据平台构建和 AI 平台构建，为 Apache DolphinScheduler 贡献 Apache SeaTunnel(Incubating) 插件，并为 Apache SeaTunnel(Incubating) 共享 alluxio 插件

## 01  总体使用情况

首先给大家说明一下联通在DolphinScheduler的总体使用情况：

- 现在我们的业务主要运行在3地4集群
    
- 总体任务流数量大概在300左右
    
- 日均任务运行差不多5000左右
    

我们使用到的DolphinScheduler组件包括Spark、Flink、SeaTunnel（原Waterdrop），以及存储过程中的Presto和一些Shell脚本，涵盖的业务则包含稽核，收入分摊，计费业务，还有其他一些需要自动化的业务等。

<div align=center>
<img src="/img/2022-05-07/ch/1.jpg"/>
</div>

## 02 业务专题分享

### **​01 跨集群双活业务调用**

上文说过，我们的业务运行在3地4集群上，这样就免不了集群之间的互相的数据交换和业务调用。如何统一管理和调度这些跨集群的数据传输任务是一个重要的问题，我们数据在生产集群，对于集群网络带宽十分敏感，必须有组织地对数据传输进行管理。

另一方面，我们有一些业务需要跨集群去调用，例如A集群数据到位后B集群要启动统计任务等，我们选择 Apache DolphinScheduler作为调度和控制，来解决这两个问题。

首先说明下我们跨集群数据传输的流程在AB两个集群上进行，我们均使用HDFS进行底层的数据存储，在跨集群的HDFS数据交换上，根据数据量大小和用途，我们将使用的数据分为小批量和大批量数据，向结构表，配置表等。

对于小批量数据，我们直接将其挂载到同一个Alluxio上进行数据共享，这样不会发生数据同步不及时导致的版本问题。

- 像明细表和其他大文件，我们使用Distcp和Spark混合进行处理；
    
- 对于结构表数据，使用SeaTunnel on Spark的方式；
    
- 通过Yarn队列的方式进行限速设置；
    
- 非结构数据使用Distcp传输，通过自带的参数Bandwidth进行速度限制；
    

这些传输任务都是运行在DolphinScheduler平台上面，我们整体的数据流程主要是A集群的数据到位检测，A集群的数据完整性校验，AB集群之间的数据传输，B集群的数据稽核和到位通知。

强调一点：其中我们重点用到了DolphinScheduler自带的补数重跑，对失败的任务或者不完整的数据进行修复。

<div align=center>
<img src="/img/2022-05-07/ch/2.jpg"/>
</div>

在完成了跨集群的数据同步和访问，我们还会使用DolphinScheduler进行跨地域和集群的任务调用。

我们在A地有两个集群，分别是测试A1和生产A2，在B地有生产B1集群，我们会在每个集群上拿出两台具有内网IP的机器作为接口机，通过在6台接口机上搭建DolphinScheduler建立一个虚拟集群，从而可以在统一页面上操作三个集群的内容；

Q：如何实现由测试到生产上线？

A：在A1测试上进行任务开发，并且通过测试之后，直接将worker节点改动到A2生产上；

Q：遇到A2生产出了问题，数据未到位等情况怎么办？

A：我们可以直接切换到B1生产上，实现手动的双活容灾切换；

<div align=center>
<img src="/img/2022-05-07/ch/3.jpg"/>
</div>

最后我们还有些任务比较大，为满足任务时效性，需要利用两个集群同时计算，我们会将数据拆分两份分别放到A2和B1上面，之后同时运行任务，最后将运行结果传回同一集群进行合并，这些任务流程基本都是通过DolphinScheduler来进行调用的。

请大家注意，在这个过程中，我们使用DolphinScheduler解决了几个问题：

- 项目跨集群的任务依赖校验；
    
- 控制节点级别的任务环境变量；
    

### **02 AI开发同步任务运行**

#### **1、统一数据访问方式**

我们现在已经有一个简易的AI开发平台，主要为用户提供一些Tensorflow和Spark ML的计算环境。在业务需求下，我们需要将用户训练的本地文件模型和集群文件系统打通，并且能够提供统一的访问方式和部署方法，为解决这个问题，我们使用了Alluxio-fuse和DolphinScheduler这两个工具。

- Alluxio-fuse打通本地和集群存储
    
- DolphinScheduler共享本地和集群存储
    

由于我们搭建的AI平台集群和数据集群是两个数据集群，所以在数据集群上我们进行一个数据的存储，利用Spark SQL或者Hive进行一些数据的预加工处理，之后我们将处理完的数据挂载到Alluxio上，最后通过Alluxio fuse跨级群映射到本地文件，这样我们基于Conda的开发环境，就可以直接访问这些数据，这样就可以做到统一数据的访问方式，以访问本地数据的方法访问集群的数据。

<div align=center>
<img src="/img/2022-05-07/ch/4.jpg"/>
</div>

#### **2、数据脚本一站式访问**

分离资源之后，通过预处理大数据内容通过数据集群，通过我们的AI集群去处理训练模型和预测模型，在这里，我们使用Alluxio-fuse对DolphinScheduler的资源中心进行了二次改动，我们将DolphinScheduler资源中心连接到Alluxio上，再通过Alluxio-fuse同时挂载本地文件和集群文件，这样在DolphinSchedule上面就可以同时访问在本地的训练推理脚本，又可以访问到存储在hdfs上的训练推理数据，实现数据脚本一站式访问。

<div align=center>
<img src="/img/2022-05-07/ch/5.jpg"/>
</div>

### **03 业务查询逻辑持久化**

第三个场景是我们用Presto和Hue为用户提供了一个前台的即时查询界面，因为有些用户通过前台写完SQL，并且测试完成之后，需要定时运行一些加工逻辑和存储过程，所以这就需要打通从前台SQL到后台定时运行任务的流程。

<div align=center>
<img src="/img/2022-05-07/ch/6.jpg"/>
</div>

另一个问题是Presto原生没有租户间的资源隔离问题。我们也是对比了几个方案之后，最后结合实际情况选择了Presto on Spark方案。

因为我们是一个多租户平台，最开始给用户提供的方案是前端用Hue界面，后端直接使用原生的Presto跑在物理集群上，这导致了用户资源争抢占的问题。当有某些大查询或者大的加工逻辑存在时，会导致其他租户业务长时间处于等待状态。

为此，我们对比了Presto on Yarn和Presto on Spark，综合对比性能之后发现Presto on Spark资源使用效率会更高一些，这里大家也可以根据自己的需求选择对应的方案。

<div align=center>
<img src="/img/2022-05-07/ch/7.jpg"/>
</div>

另一方面，我们使用了原生Presto和Presto on spark共存的方式，对于一些数据量较小，加工逻辑较为简单的SQL，我们直接将其在原生Presto上运行，而对于一些加工逻辑比较复杂，运行时间比较长的SQL，则在Presto on spark上运行，这样用户用一套SQL就可以切换到不同的底层引擎上。

此外，我们还打通了Hue到DolphinScheduler定时任务调度流程。我们在Hue上进行SQL开发调制后，通过存储到本地Serve文件，连接到Git进行版本控制。

我们将本地文件挂载到Alluxio fuse上，作为SQL的同步挂载，最后我们使用Hue，通过DolphinScheduler的API创建任务和定时任务，实现从SQL开发到定时运行的流程控制。

<div align=center>
<img src="/img/2022-05-07/ch/8.jpg"/>
</div>

### **04 数据湖数据统一治理**

最后一个场景是数据湖数据统一管理，在我们自研的数据集成平台上，使用分层治理的方式对数据湖数据进行统一的管理和访问，其中使用了DolphinScheduler作为入湖调度和监控引擎。

在数据集成平台上，对于数据集成、数据入湖、数据分发这些批量的和实时的任务的，我们使用DolphinScheduler进行调度。

底层运行在Spark和Flink上，对于数据查询和数据探索这些需要即时反馈的业务需求，我们使用嵌入Hue接入Spark和Presto的方法，对数据进行探索查询；对于数据资产登记同步和数据稽核等，直接对数据源文件信息进行查询，直接同步底层数据信息。

最后一个场景是数据湖数据统一管理，在我们自研的数据集成平台上，使用分层治理的方式对数据湖数据进行统一的管理和访问，其中使用了DolphinScheduler作为入湖调度和监控引擎。

在数据集成平台上，对于数据集成、数据入湖、数据分发这些批量的和实时的任务的，我们使用DolphinScheduler进行调度。

底层运行在Spark和Flink上，对于数据查询和数据探索这些需要即时反馈的业务需求，我们使用嵌入Hue接入Spark和Presto的方法，对数据进行探索查询；对于数据资产登记同步和数据稽核等，直接对数据源文件信息进行查询，直接同步底层数据信息。

<div align=center>
<img src="/img/2022-05-07/ch/9.jpg"/>
</div>

目前我们集成平台基本上管理着460张数据表的质量管理，对数据准确性和准时性提供统一的管理。

## 03 下一步计划与需求

<div align=center>
<img src="/img/2022-05-07/ch/10.jpg"/>
</div>

### **01 资源中心**

在资源中心层面，为了方便用户之间的文件共享，我们计划为全用户提供资源授权，同时根据它的归属租户，分配租户级别的共享文件，使得对于一个多租户的平台更为友善。

### **02 用户管理**

其次与用户传权限相关，我们只提供租户级别的管理员账账户，后续的用户账户由租户管理员账户创建，同时租户组内的用户管理也是由租户管理员去控制，以方便租户内部的管理。

### **03 任务节点**

最后是我们的任务节点相关的计划，现在已在进行之中：一方面是完成SQL节点的优化，让用户能够选择一个资源中心的SQL文件，而不需要手动复制SQL；另一方面是HTTP节点对返回的json自定义解析提取字段判断，对复杂返回值进行更为友好的处理。

