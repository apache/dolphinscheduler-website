{
  "filename": "spark.md",
  "__html": "<h1>SPARK节点</h1>\n<h2>综述</h2>\n<p>Spark  任务类型，用于执行 Spark 程序。对于 Spark 节点，worker 会通过使用 spark 命令 <code>spark submit</code> 方式提交任务。更多详情查看 <a href=\"https://spark.apache.org/docs/3.2.1/submitting-applications.html#launching-applications-with-spark-submit\">spark-submit</a>。</p>\n<h2>创建任务</h2>\n<ul>\n<li>\n<p>点击项目管理 -&gt; 项目名称 -&gt; 工作流定义，点击”创建工作流”按钮，进入 DAG 编辑页面：</p>\n</li>\n<li>\n<p>拖动工具栏的 <img src=\"/img/tasks/icons/spark.png\" width=\"15\"/> 任务节点到画板中。</p>\n</li>\n</ul>\n<h2>任务参数</h2>\n<ul>\n<li>节点名称：设置任务的名称。一个工作流定义中的节点名称是唯一的。</li>\n<li>运行标志：标识这个节点是否能正常调度,如果不需要执行，可以打开禁止执行开关。</li>\n<li>描述：描述该节点的功能。</li>\n<li>任务优先级：worker 线程数不足时，根据优先级从高到低依次执行，优先级一样时根据先进先出原则执行。</li>\n<li>Worker 分组：任务分配给 worker 组的机器执行，选择 Default 会随机选择一台 worker 机执行。</li>\n<li>环境名称：配置运行脚本的环境。</li>\n<li>失败重试次数：任务失败重新提交的次数。</li>\n<li>失败重试间隔：任务失败重新提交任务的时间间隔，以分为单位。</li>\n<li>延迟执行时间：任务延迟执行的时间，以分为单位。</li>\n<li>超时警告：勾选超时警告、超时失败，当任务超过“超时时长”后，会发送告警邮件并且任务执行失败。</li>\n<li>程序类型：支持 Java、Scala 和 Python 三种语言。</li>\n<li>Spark 版本：支持 Spark1 和 Spark2。</li>\n<li>主函数的 Class：Spark 程序的入口 Main class 的全路径。</li>\n<li>主程序包：执行 Spark 程序的 jar 包（通过资源中心上传）。</li>\n<li>部署方式：支持 yarn-clusetr、yarn-client 和 local 三种模式。</li>\n<li>任务名称（可选）：Spark 程序的名称。</li>\n<li>Driver 核心数：用于设置 Driver 内核数，可根据实际生产环境设置对应的核心数。</li>\n<li>Driver 内存数：用于设置 Driver 内存数，可根据实际生产环境设置对应的内存数。</li>\n<li>Executor 数量：用于设置 Executor 的数量，可根据实际生产环境设置对应的内存数。</li>\n<li>Executor 内存数：用于设置 Executor 内存数，可根据实际生产环境设置对应的内存数。</li>\n<li>主程序参数：设置 Spark 程序的输入参数，支持自定义参数变量的替换。</li>\n<li>选项参数：支持 <code>--jar</code>、<code>--files</code>、<code>--archives</code>、<code>--conf</code> 格式。</li>\n<li>资源：如果其他参数中引用了资源文件，需要在资源中选择指定。</li>\n<li>自定义参数：是 Spark 局部的用户自定义参数，会替换脚本中以 ${变量} 的内容。</li>\n<li>前置任务：选择当前任务的前置任务，会将被选择的前置任务设置为当前任务的上游。</li>\n</ul>\n<h2>任务样例</h2>\n<h3>执行 WordCount 程序</h3>\n<p>本案例为大数据生态中常见的入门案例，常应用于 MapReduce、Flink、Spark 等计算框架。主要为统计输入的文本中，相同的单词的数量有多少。</p>\n<h4>在 DolphinScheduler 中配置 Spark 环境</h4>\n<p>若生产环境中要是使用到 Spark 任务类型，则需要先配置好所需的环境。配置文件如下：<code>/dolphinscheduler/conf/env/dolphinscheduler_env.sh</code>。</p>\n<p><img src=\"/img/tasks/demo/spark_task01.png\" alt=\"spark_configure\"></p>\n<h4>上传主程序包</h4>\n<p>在使用 Spark 任务节点时，需要利用资源中心上传执行程序的 jar 包，可参考<a href=\"../resource.md\">资源中心</a>。</p>\n<p>当配置完成资源中心之后，直接使用拖拽的方式，即可上传所需目标文件。</p>\n<p><img src=\"/img/tasks/demo/upload_jar.png\" alt=\"resource_upload\"></p>\n<h4>配置 Spark 节点</h4>\n<p>根据上述参数说明，配置所需的内容即可。</p>\n<p><img src=\"/img/tasks/demo/spark_task02.png\" alt=\"demo-spark-simple\"></p>\n<h2>注意事项：</h2>\n<p>注意：JAVA 和 Scala 只是用来标识，没有区别，如果是 Python 开发的 Spark 则没有主函数的 class，其他都是一样。</p>\n",
  "link": "/dist/zh-cn/docs/dev/user_doc/guide/task/spark.html",
  "meta": {}
}