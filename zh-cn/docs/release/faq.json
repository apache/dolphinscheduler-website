{
  "filename": "faq.md",
  "__html": "<!-- markdown-link-check-disable -->\n<h2>Q：项目的名称是？</h2>\n<p>A：DolphinScheduler</p>\n<hr>\n<h2>Q：DolphinScheduler 服务介绍及建议运行内存</h2>\n<p>A：DolphinScheduler 由 5 个服务组成，MasterServer、WorkerServer、ApiServer、AlertServer、LoggerServer 和 UI。</p>\n<table>\n<thead>\n<tr>\n<th>服务</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MasterServer</td>\n<td>主要负责 <strong>DAG</strong> 的切分和任务状态的监控</td>\n</tr>\n<tr>\n<td>WorkerServer/LoggerServer</td>\n<td>主要负责任务的提交、执行和任务状态的更新。LoggerServer 用于 Rest Api 通过 <strong>RPC</strong> 查看日志</td>\n</tr>\n<tr>\n<td>ApiServer</td>\n<td>提供 Rest Api 服务，供 UI 进行调用</td>\n</tr>\n<tr>\n<td>AlertServer</td>\n<td>提供告警服务</td>\n</tr>\n<tr>\n<td>UI</td>\n<td>前端页面展示</td>\n</tr>\n</tbody>\n</table>\n<p>注意：<strong>由于服务比较多，建议单机部署最好是 4 核 16G 以上</strong></p>\n<hr>\n<h2>Q：系统支持哪些邮箱？</h2>\n<p>A：支持绝大多数邮箱，qq、163、126、139、outlook、aliyun 等皆支持。支持 <strong>TLS 和 SSL</strong> 协议，可以在 alert.properties 中选择性配置</p>\n<hr>\n<h2>Q：常用的系统变量时间参数有哪些，如何使用？</h2>\n<p>A：请参考<a href=\"https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/guide/parameter/built-in.html\">使用手册</a> 第8小节</p>\n<hr>\n<h2>Q：pip install kazoo 这个安装报错。是必须安装的吗？</h2>\n<p>A： 这个是 python 连接 Zookeeper 需要使用到的，用于删除Zookeeper中的master/worker临时节点信息。所以如果是第一次安装，就可以忽略错误。在1.3.0之后，kazoo不再需要了，我们用程序来代替kazoo所做的</p>\n<hr>\n<h2>Q：怎么指定机器运行任务</h2>\n<p>A：使用 <strong>管理员</strong> 创建 Worker 分组，在 <strong>流程定义启动</strong> 的时候可<strong>指定Worker分组</strong>或者在<strong>任务节点上指定Worker分组</strong>。如果不指定，则使用 Default，<strong>Default默认是使用的集群里所有的Worker中随机选取一台来进行任务提交、执行</strong></p>\n<hr>\n<h2>Q：任务的优先级</h2>\n<p>A：我们同时 <strong>支持流程和任务的优先级</strong>。优先级我们有 <strong>HIGHEST、HIGH、MEDIUM、LOW 和 LOWEST</strong> 五种级别。<strong>可以设置不同流程实例之间的优先级，也可以设置同一个流程实例中不同任务实例的优先级</strong>。详细内容请参考任务优先级设计 <a href=\"https://analysys.github.io/easyscheduler_docs_cn/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1.html#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1\">https://analysys.github.io/easyscheduler_docs_cn/系统架构设计.html#系统架构设计</a></p>\n<hr>\n<h2>Q：dolphinscheduler-grpc 报错</h2>\n<p>A：在 1.2 及以前版本中，在根目录下执行：mvn -U clean package assembly:assembly -Dmaven.test.skip=true,然后刷新下整个项目就好，1.3版本中不再使用 GRPC 进行通信了</p>\n<hr>\n<h2>Q：DolphinScheduler 支持 windows 上运行么</h2>\n<p>A： 理论上只有 <strong>Worker 是需要在 Linux 上运行的</strong>，其它的服务都是可以在 windows 上正常运行的。但是还是建议最好能在 linux 上部署使用</p>\n<hr>\n<h2>Q：UI 在 linux 编译 node-sass 提示：Error：EACCESS:permission denied，mkdir xxxx</h2>\n<p>A：单独安装 <strong>npm install node-sass --unsafe-perm</strong>，之后再 <strong>npm install</strong></p>\n<hr>\n<h2>Q：UI 不能正常登陆访问</h2>\n<p>A：     1，如果是 node 启动的查看 dolphinscheduler-ui 下的 .env 文件里的 API_BASE 配置是否是 Api Server 服务地址</p>\n<p>​       2，如果是 nginx 启动的并且是通过 <strong><a href=\"http://install-dolphinscheduler-ui.sh\">install-dolphinscheduler-ui.sh</a></strong> 安装的，查看</p>\n<p>​              <strong>/etc/nginx/conf.d/dolphinscheduler.conf</strong> 中的 proxy_pass 配置是否是 Api Server 服务地址</p>\n<p>​       3，如果以上配置都是正确的，那么请查看 Api Server 服务是否是正常的，</p>\n<p>​\t\t\tcurl <a href=\"http://localhost:12345/dolphinscheduler/users/get-user-info\">http://localhost:12345/dolphinscheduler/users/get-user-info</a> 查看 Api Server 日志，</p>\n<p>​\t\t\t如果提示 cn.dolphinscheduler.api.interceptor.LoginHandlerInterceptor:[76] - session info is null，则证明 Api Server 服务是正常的</p>\n<p>​       4，如果以上都没有问题，需要查看一下 <strong>application.properties</strong> 中的 <strong>server.context-path 和 server.port 配置</strong>是否正确\n注意：1.3 版本直接使用 Jetty 进行前端代码的解析，无需再安装配置 nginx 了</p>\n<hr>\n<h2>Q：流程定义手动启动或调度启动之后，没有流程实例生成</h2>\n<p>A： \t  1，首先通过 <strong>jps 查看MasterServer服务是否存在</strong>，或者从服务监控直接查看 zk 中是否存在 master 服务</p>\n<p>​\t   2，如果存在 master 服务，查看 <strong>命令状态统计</strong> 或者 <strong>t_ds_error_command</strong> 中是否增加的新记录，如果增加了，<strong>请查看 message 字段定位启动异常原因</strong></p>\n<hr>\n<h2>Q：任务状态一直处于提交成功状态</h2>\n<p>A：        1，首先通过 <strong>jps 查看 WorkerServer 服务是否存在</strong>，或者从服务监控直接查看 zk 中是否存在 worker 服务</p>\n<p>​          2，如果 <strong>WorkerServer</strong> 服务正常，需要 <strong>查看 MasterServer 是否把 task 任务放到 zk 队列中</strong> ，<strong>需要查看 MasterServer 日志及 zk 队列中是否有任务阻塞</strong></p>\n<p>​\t   3，如果以上都没有问题，需要定位是否指定了 Worker 分组，但是 <strong>Worker 分组的机器不是在线状态</strong></p>\n<hr>\n<h2>Q：<a href=\"http://install.sh\">install.sh</a> 中需要注意问题</h2>\n<p>A：  \t   1，如果替换变量中包含特殊字符，<strong>请用 \\ 转移符进行转移</strong></p>\n<p>​\t    2，installPath=&quot;/data1_1T/dolphinscheduler&quot;，<strong>这个目录不能和当前要一键安装的 <a href=\"http://install.sh\">install.sh</a> 目录是一样的</strong></p>\n<p>​\t    3，deployUser=&quot;dolphinscheduler&quot;，<strong>部署用户必须具有 sudo 权限</strong>，因为 worker 是通过 sudo -u 租户 sh xxx.command 进行执行的</p>\n<p>​\t    4，monitorServerState=&quot;false&quot;，服务监控脚本是否启动，默认是不启动服务监控脚本的。<strong>如果启动服务监控脚本，则每 5 分钟定时来监控 master 和 worker 的服务是否 down 机，如果 down 机则会自动重启</strong></p>\n<p>​\t    5，hdfsStartupSate=&quot;false&quot;，是否开启 HDFS 资源上传功能。默认是不开启的，<strong>如果不开启则资源中心是不能使用的</strong>。如果开启，需要 conf/common/hadoop/hadoop.properties 中配置 fs.defaultFS 和 yarn 的相关配置，如果使用 namenode HA，需要将 core-site.xml 和 hdfs-site.xml 复制到conf根目录下</p>\n<p>​\t注意：<strong>1.0.x 版本是不会自动创建 hdfs 根目录的，需要自行创建，并且需要部署用户有hdfs的操作权限</strong></p>\n<hr>\n<h2>Q：流程定义和流程实例下线异常</h2>\n<p>A ： 对于 <strong>1.0.4 以前的版本中</strong>，修改 dolphinscheduler-api cn.dolphinscheduler.api.quartz 包下的代码即可</p>\n<pre><code>public boolean deleteJob(String jobName, String jobGroupName) {\n    lock.writeLock().lock();\n    try {\n      JobKey jobKey = new JobKey(jobName,jobGroupName);\n      if(scheduler.checkExists(jobKey)){\n        logger.info(&quot;try to delete job, job name: {}, job group name: {},&quot;, jobName, jobGroupName);\n        return scheduler.deleteJob(jobKey);\n      }else {\n        return true;\n      }\n\n    } catch (SchedulerException e) {\n      logger.error(String.format(&quot;delete job : %s failed&quot;,jobName), e);\n    } finally {\n      lock.writeLock().unlock();\n    }\n    return false;\n  }\n</code></pre>\n<hr>\n<h2>Q：HDFS 启动之前创建的租户，能正常使用资源中心吗</h2>\n<p>A： 不能。因为在未启动 HDFS 创建的租户，不会在 HDFS 中注册租户目录。所以上次资源会报错</p>\n<h2>Q：多 Master 和多 Worker 状态下，服务掉了，怎么容错</h2>\n<p>A：  <strong>注意：Master 监控 Master 及 Worker 服务。</strong></p>\n<p>​\t1，如果 Master 服务掉了，其它的 Master 会接管挂掉的 Master 的流程，继续监控 Worker task 状态</p>\n<p>​\t2，如果 Worker 服务掉了，Master 会监控到 Worker 服务掉了，如果存在 Yarn 任务，Kill Yarn 任务之后走重试</p>\n<p>具体请看容错设计：<a href=\"https://analysys.github.io/easyscheduler_docs_cn/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1.html#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1\">https://analysys.github.io/easyscheduler_docs_cn/系统架构设计.html#系统架构设计</a></p>\n<hr>\n<h2>Q：对于 Master 和 Worker 一台机器伪分布式下的容错</h2>\n<p>A ： 1.0.3 版本只实现了 Master 启动流程容错，不走 Worker 容错。也就是说如果 Worker 挂掉的时候，没有 Master 存在。这流程将会出现问题。我们会在 <strong>1.1.0</strong> 版本中增加 Master 和 Worker 启动自容错，修复这个问题。如果想手动修改这个问题，需要针对 <strong>跨重启正在运行流程</strong> <strong>并且已经掉的正在运行的 Worker 任务，需要修改为失败</strong>，<strong>同时跨重启正在运行流程设置为失败状态</strong>。然后从失败节点进行流程恢复即可</p>\n<hr>\n<h2>Q：定时容易设置成每秒执行</h2>\n<p>A ： 设置定时的时候需要注意，如果第一位（* * * * * ? *）设置成 * ，则表示每秒执行。<strong>我们将会在 1.1.0 版本中加入显示最近调度的时间列表</strong> ，使用 <a href=\"http://cron.qqe2.com/\">http://cron.qqe2.com/</a>  可以在线看近 5 次运行时间</p>\n<h2>Q：定时有有效时间范围吗</h2>\n<p>A：有的，<strong>如果定时的起止时间是同一个时间，那么此定时将是无效的定时</strong>。<strong>如果起止时间的结束时间比当前的时间小，很有可能定时会被自动删除</strong></p>\n<h2>Q：任务依赖有几种实现</h2>\n<p>A：  1，<strong>DAG</strong> 之间的任务依赖关系，是从 <strong>入度为零</strong> 进行 DAG 切分的</p>\n<p>​\t 2，有 <strong>任务依赖节点</strong> ，可以实现跨流程的任务或者流程依赖，具体请参考 依赖(DEPENDENT)节点：<a href=\"https://analysys.github.io/easyscheduler_docs_cn/%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C.html#%E4%BB%BB%E5%8A%A1%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE\">https://analysys.github.io/easyscheduler_docs_cn/系统使用手册.html#任务节点类型和参数设置</a></p>\n<h2>Q：流程定义有几种启动方式</h2>\n<p>A： 1，在 <strong>流程定义列表</strong>，点击 <strong>启动</strong> 按钮</p>\n<p>​\t\t2，<strong>流程定义列表添加定时器</strong>，调度启动流程定义</p>\n<p>​\t\t3，流程定义 <strong>查看或编辑</strong> DAG 页面，任意 <strong>任务节点右击</strong> 启动流程定义</p>\n<p>​\t\t4，可以对流程定义 DAG 编辑，设置某些任务的运行标志位 <strong>禁止运行</strong>，则在启动流程定义的时候，将该节点的连线将从 DAG 中去掉</p>\n<h2>Q：Python 任务设置 Python 版本</h2>\n<p>A：  只需要修改 conf/env/dolphinscheduler_env.sh 中的 PYTHON_HOME</p>\n<pre><code>export PYTHON_HOME=/bin/python\n</code></pre>\n<p>注意：这了 <strong>PYTHON_HOME</strong> ，是 python 命令的绝对路径，而不是单纯的 PYTHON_HOME，还需要注意的是 export PATH 的时候，需要直接</p>\n<pre><code>export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH\n</code></pre>\n<h2>Q：Worker Task 通过 sudo -u 租户 sh xxx.command 会产生子进程，在 kill 的时候，是否会杀掉</h2>\n<p>A： 我们会在 1.0.4 中增加 kill 任务同时，kill 掉任务产生的各种所有子进程</p>\n<h2>Q：DolphinScheduler 中的队列怎么用，用户队列和租户队列是什么意思</h2>\n<p>A ： DolphinScheduler 中的队列可以在用户或者租户上指定队列，<strong>用户指定的队列优先级是高于租户队列的优先级的。</strong>，例如：对 MR 任务指定队列，是通过 mapreduce.job.queuename 来指定队列的。</p>\n<p>注意：MR 在用以上方法指定队列的时候，传递参数请使用如下方式：</p>\n<pre><code>\tConfiguration conf = new Configuration();\n        GenericOptionsParser optionParser = new GenericOptionsParser(conf, args);\n        String[] remainingArgs = optionParser.getRemainingArgs();\n</code></pre>\n<p>如果是 Spark 任务 --queue 方式指定队列</p>\n<h2>Q：Master 或者 Worker 报如下告警</h2>\n<p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/master_worker_lack_res.png\" width=\"60%\" />\n </p>\n<p>A ： 修改 conf 下的 master.properties <strong>master.reserved.memory</strong> 的值为更小的值，比如说 0.1 或者</p>\n<p>worker.properties <strong>worker.reserved.memory</strong> 的值为更小的值，比如说 0.1</p>\n<h2>Q：hive 版本是 1.1.0+cdh5.15.0，SQL hive 任务连接报错</h2>\n<p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/cdh_hive_error.png\" width=\"60%\" />\n </p>\n<p>A： 将 hive pom</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;\n    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;\n    &lt;version&gt;2.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>修改为</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;\n    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;\n    &lt;version&gt;1.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<hr>\n<h2>Q：如何增加一台工作服务器</h2>\n<p>A： 1，参考官网<a href=\"https://dolphinscheduler.apache.org/zh-cn/docs/laster/user_doc/installation/cluster.html\">部署文档</a> 1.3 小节，创建部署用户和 hosts 映射</p>\n<p>​\t2，参考官网<a href=\"https://dolphinscheduler.apache.org/zh-cn/docs/laster/user_doc/installation/cluster.html\">部署文档</a> 1.4 小节，配置 hosts 映射和 ssh 打通及修改目录权限.\n​          1.4 小节的最后一步是在当前新增机器上执行的，即需要给部署目录部署用户的权限</p>\n<p>​\t3，复制正在运行的服务器上的部署目录到新机器的同样的部署目录下</p>\n<p>​\t4，到 bin 下，启动 worker server 和 logger server</p>\n<pre><code>        ./dolphinscheduler-daemon.sh start worker-server\n        ./dolphinscheduler-daemon.sh start logger-server\n</code></pre>\n<hr>\n<h2>Q：DolphinScheduler 什么时候发布新版本，同时新旧版本区别，以及如何升级，版本号规范</h2>\n<p>A：1，Apache 项目的发版流程是通过邮件列表完成的。 你可以订阅 DolphinScheduler 的邮件列表，订阅之后如果有发版，你就可以收到邮件。请参照这篇<a href=\"https://github.com/apache/dolphinscheduler#get-help\">指引</a>来订阅 DolphinScheduler 的邮件列表。</p>\n<p>2，当项目发版的时候，会有发版说明告知具体的变更内容，同时也会有从旧版本升级到新版本的升级文档。</p>\n<p>3，版本号为 x.y.z, 当 x 增加时代表全新架构的版本。当 y 增加时代表与 y 版本之前的不兼容需要升级脚本或其他人工处理才能升级。当 z 增加代表是 bug 修复，升级完全兼容。无需额外处理。之前有个问题 1.0.2 的升级不兼容 1.0.1 需要升级脚本。</p>\n<hr>\n<h2>Q：后续任务在前置任务失败情况下仍旧可以执行</h2>\n<p>A：在启动工作流的时候，你可以设置失败策略：继续还是失败。\n<img src=\"https://user-images.githubusercontent.com/15833811/80368215-ee378080-88be-11ea-9074-01a33d012b23.png\" alt=\"设置任务失败策略\"></p>\n<hr>\n<h2>Q：工作流模板 DAG、工作流实例、工作任务及实例之间是什么关系 工作流模板 DAG、工作流实例、工作任务及实例之间是什么关系，一个 dag 支持最大并发 100，是指产生 100 个工作流实例并发运行吗？一个 dag 中的任务节点，也有并发数的配置，是指任务也可以并发多个线程运行吗？最大数 100 吗？</h2>\n<p>A：</p>\n<p>1.2.1 version</p>\n<pre><code>   master.properties\n   设置 master 节点并发执行的最大工作流数\n   master.exec.threads=100\n   \n   Control the number of parallel tasks in each workflow\n   设置每个工作流可以并发执行的最大任务数\n   master.exec.task.number=20\n   \n   worker.properties\n   设置 worker 节点并发执行的最大任务数\n   worker.exec.threads=100\n</code></pre>\n<hr>\n<h2>Q：工作组管理页面没有展示按钮</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/39816903/81903776-d8cb9180-95f4-11ea-98cb-94ca1e6a1db5.png\" width=\"60%\" />\n</p>\nA：1.3.0 版本，为了支持 k8s，worker ip 一直变动，因此我们不能在 UI 界面上配置，工作组可以配置在 worker.properties 上配置名称。\n<hr>\n<h2>Q：为什么不把 mysql 的 jdbc 连接包添加到 docker 镜像里面</h2>\n<p>A：Mysql jdbc 连接包的许可证和 apache v2 的许可证不兼容，因此它不能被加入到 docker 镜像里面。</p>\n<hr>\n<h2>Q：当一个任务提交多个 yarn 程序的时候经常失败</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/16174111/81312485-476e9380-90b9-11ea-9aad-ed009db899b1.png\" width=\"60%\" />\n</p>\nA：这个 Bug 在 dev 分支已修复，并加入到需求/待做列表。\n<hr>\n<h2>Q：Master 服务和 Worker 服务在运行几天之后停止了</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/18378986/81293969-c3101680-90a0-11ea-87e5-ac9f0dd53f5e.png\" width=\"60%\" />\n</p>\nA：会话超时时间太短了，只有 0.3 秒，修改 zookeeper.properties 的配置项：\n<pre><code>   zookeeper.session.timeout=60000\n   zookeeper.connection.timeout=30000\n</code></pre>\n<hr>\n<h2>Q：使用 docker-compose 默认配置启动，显示 zookeeper 错误</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/42579056/80374318-13c98780-88c9-11ea-8d5f-53448b957f02.png\" width=\"60%\" />\n </p>\nA：这个问题在 dev-1.3.0 版本解决了。这个 [pr](https://github.com/apache/dolphinscheduler/pull/2595) 已经解决了这个 bug，主要的改动点：\n<pre><code>    在docker-compose.yml文件中增加zookeeper的环境变量ZOO_4LW_COMMANDS_WHITELIST。\n    把minLatency,avgLatency and maxLatency的类型从int改成float。\n</code></pre>\n<hr>\n<h2>Q：界面上显示任务一直运行，结束不了，从日志上看任务实例为空</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/51871547/80302626-b1478d00-87dd-11ea-97d4-08aa2244a6d0.jpg\" width=\"60%\" />\n </p>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/51871547/80302626-b1478d00-87dd-11ea-97d4-08aa2244a6d0.jpg\" width=\"60%\" />\n </p>\nA：这个 [bug](https://github.com/apache/dolphinscheduler/issues/1477)  描述了问题的详情。这个问题在 1.2.1 版本已经被修复了。\n对于 1.2.1 以下的版本，这种情况的一些提示：\n<pre><code>1，清空 zk 下这个路径的任务：/dolphinscheduler/task_queue\n2，修改任务状态为失败（int 值 6）\n3，运行工作流来从失败中恢复\n</code></pre>\n<hr>\n<h2>Q：zk 中注册的 master 信息 ip 地址是 127.0.0.1，而不是配置的域名所对应或者解析的 ip 地址，可能导致不能查看任务日志</h2>\n<p>A：修复 bug：</p>\n<pre><code>   1、confirm hostname\n   $hostname\n   hadoop1\n   2、hostname -i\n   127.0.0.1 10.3.57.15\n   3、edit /etc/hosts,delete hadoop1 from 127.0.0.1 record\n   $cat /etc/hosts\n   127.0.0.1 localhost\n   10.3.57.15 ds1 hadoop1\n   4、hostname -i\n   10.3.57.15\n</code></pre>\n<p>hostname 命令返回服务器主机名，hostname -i 返回的是服务器主机名在 /etc/hosts 中所有匹配的ip地址。所以我把 /etc/hosts 中 127.0.0.1 中的主机名删掉，只保留内网 ip 的解析就可以了，没必要把 127.0.0.1 整条注释掉, 只要 hostname 命令返回值在 /etc/hosts 中对应的内网 ip 正确就可以，ds 程序取了第一个值，我理解上 ds 程序不应该用 hostname -i 取值这样有点问题，因为好多公司服务器的主机名都是运维配置的，感觉还是直接取配置文件的域名解析的返回 ip 更准确，或者 znode 中存域名信息而不是 /etc/hosts。</p>\n<hr>\n<h2>Q：调度系统设置了一个秒级的任务，导致系统挂掉</h2>\n<p>A：调度系统不支持秒级任务。</p>\n<hr>\n<h2>Q：编译前后端代码 (dolphinscheduler-ui) 报错不能下载&quot;<a href=\"https://github.com/sass/node-sass/releases/download/v4.13.1/darwin-x64-72_binding.node\">https://github.com/sass/node-sass/releases/download/v4.13.1/darwin-x64-72_binding.node</a>&quot;</h2>\n<p>A：1，cd dolphinscheduler-ui 然后删除 node_modules 目录</p>\n<pre><code>sudo rm -rf node_modules\n</code></pre>\n<p>​\t2，通过 <a href=\"http://npm.taobao.org\">npm.taobao.org</a> 下载 node-sass</p>\n<pre><code>sudo npm uninstall node-sass\nsudo npm i node-sass --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\n</code></pre>\n<p>3，如果步骤 2 报错，请重新构建 node-saas <a href=\"https://dolphinscheduler.apache.org/en-us/development/frontend-development.html\">参考链接</a></p>\n<pre><code> sudo npm rebuild node-sass\n</code></pre>\n<p>当问题解决之后，如果你不想每次编译都下载这个 node，你可以设置系统环境变量：SASS_BINARY_PATH= /xxx/xxx/xxx/xxx.node。</p>\n<hr>\n<h2>Q：当使用 mysql 作为 ds 数据库需要如何配置</h2>\n<p>A：1，修改项目根目录 maven 配置文件，移除 scope 的 test 属性，这样 mysql 的包就可以在其它阶段被加载</p>\n<pre><code>&lt;dependency&gt;\n\t&lt;groupId&gt;mysql&lt;/groupId&gt;\n\t&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n\t&lt;version&gt;${mysql.connector.version}&lt;/version&gt;\n\t&lt;scope&gt;test&lt;scope&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>​\t2，修改 application-dao.properties 和 quzrtz.properties 来使用 mysql 驱动\n默认驱动是 postgres 主要由于许可证原因。</p>\n<hr>\n<h2>Q：shell 任务是如何运行的</h2>\n<p>A：1，被执行的服务器在哪里配置，以及实际执行的服务器是哪台? 要指定在某个 worker 上去执行，可以在 worker 分组中配置，固定 IP，这样就可以把路径写死。如果配置的 worker 分组有多个 worker，实际执行的服务器由调度决定的，具有随机性。</p>\n<p>​\t2，如果是服务器上某个路径的一个 shell 文件，怎么指向这个路径？服务器上某个路径下的 shell 文件，涉及到权限问题，不建议这么做。建议你可以使用资源中心的存储功能，然后在 shell 编辑器里面使用资源引用就可以，系统会帮助你把脚本下载到执行目录下。如果以 hdfs 作为资源中心，在执行的时候，调度器会把依赖的 jar 包，文件等资源拉到 worker 的执行目录上，我这边是 /tmp/escheduler/exec/process，该配置可以在 <a href=\"http://install.sh\">install.sh</a> 中进行指定。</p>\n<p>3，以哪个用户来执行任务？执行任务的时候，调度器会采用 sudo -u 租户的方式去执行，租户是一个 linux 用户。</p>\n<hr>\n<h2>Q：生产环境部署方式有推荐的最佳实践吗</h2>\n<p>A：1，如果没有很多任务要运行，出于稳定性考虑我们建议使用 3 个节点，并且最好把 Master/Worder 服务部署在不同的节点。如果你只有一个节点，当然只能把所有的服务部署在同一个节点！通常来说，需要多少节点取决于你的业务，海豚调度系统本身不需要很多的资源。充分测试之后，你们将找到使用较少节点的合适的部署方式。</p>\n<hr>\n<h2>Q：DEPENDENT 节点</h2>\n<p>A：1，DEPENDENT 节点实际是没有执行体的，是专门用来配置数据周期依赖逻辑，然后再把执行节点挂载后面，来实现任务间的周期依赖。</p>\n<hr>\n<h2>Q：如何改变 Master 服务的启动端口</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/8263441/62352160-0f3e9100-b53a-11e9-95ba-3ae3dde49c72.png\" width=\"60%\" />\n </p>\nA：1，修改 application_master.properties 配置文件，例如：server.port=12345。\n<hr>\n<h2>Q：调度任务不能上线</h2>\n<p>A：1，我们可以成功创建调度任务，并且表 t_scheduler_schedules 中也成功加入了一条记录，但当我点击上线后，前端页面无反应且会把 t_scheduler_schedules 这张表锁定，我测试过将 t_scheduler_schedules 中的 RELEASE_state 字段手动更新为 1 这样前端会显示为上线状态。DS 版本 1.2+ 表名是 t_ds_schedules，其它版本表名是 t_scheduler_schedules。</p>\n<hr>\n<h2>Q：请问 swagger ui 的地址是什么</h2>\n<p>A：1，1.2+ 版本地址是：<a href=\"http://apiServerIp\">http://apiServerIp</a>:apiServerPort/dolphinscheduler/doc.html?language=zh_CN&amp;lang=cn，其它版本是 <a href=\"http://apiServerIp\">http://apiServerIp</a>:apiServerPort/escheduler/doc.html?language=zh_CN&amp;lang=cn。</p>\n<hr>\n<h2>Q：前端安装包缺少文件</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/41460919/61437083-d960b080-a96e-11e9-87f1-297ba3aca5e3.png\" width=\"60%\" />\n </p>\n <p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/41460919/61437218-1b89f200-a96f-11e9-8e48-3fac47eb2389.png\" width=\"60%\" />\n  </p>\nA： 1，用户修改了 api server 配置文件中的![apiServerContextPath](https://user-images.githubusercontent.com/41460919/61678323-1b09a680-ad35-11e9-9707-3ba68bbc70d6.png)配置项，导致了这个问题，恢复成默认配置之后问题解决。\n<hr>\n<h2>Q：上传比较大的文件卡住</h2>\n<p align=\"center\">\n   <img src=\"https://user-images.githubusercontent.com/21357069/58231400-805b0e80-7d69-11e9-8107-7f37b06a95df.png\" width=\"60%\" />\n </p>\nA：1，编辑 ngnix 配置文件 vi /etc/nginx/nginx.conf，更改上传大小 client_max_body_size 1024m。\n<p>​\t2，更新 google chrome 版本到最新版本。</p>\n<hr>\n<h2>Q：创建 spark 数据源，点击“测试连接”，系统回退回到登入页面</h2>\n<p>A：1，edit /etc/nginx/conf.d/escheduler.conf</p>\n<pre><code>     proxy_connect_timeout 300s;\n     proxy_read_timeout 300s;\n     proxy_send_timeout 300s;\n</code></pre>\n<hr>\n<h2>Q：欢迎订阅 DolphinScheduler 开发邮件列表</h2>\n<p>A：在使用 DolphinScheduler 的过程中，如果您有任何问题或者想法、建议，都可以通过 Apache 邮件列表参与到 DolphinScheduler 的社区建设中来。\n发送订阅邮件也非常简单，步骤如下:</p>\n<p>1，用自己的邮箱向 <a href=\"mailto:dev-subscribe@dolphinscheduler.apache.org\">dev-subscribe@dolphinscheduler.apache.org</a> 发送一封邮件，主题和内容任意。</p>\n<p>2， 接收确认邮件并回复。 完成步骤1后，您将收到一封来自 <a href=\"mailto:dev-help@dolphinscheduler.apache.org\">dev-help@dolphinscheduler.apache.org</a> 的确认邮件（如未收到，请确认邮件是否被自动归入垃圾邮件、推广邮件、订阅邮件等文件夹）。然后直接回复该邮件，或点击邮件里的链接快捷回复即可，主题和内容任意。</p>\n<p>3， 接收欢迎邮件。 完成以上步骤后，您会收到一封主题为 WELCOME to <a href=\"mailto:dev@dolphinscheduler.apache.org\">dev@dolphinscheduler.apache.org</a> 的欢迎邮件，至此您已成功订阅 Apache DolphinScheduler的邮件列表。</p>\n<hr>\n<h2>Q：工作流依赖</h2>\n<p>A：1，目前是按照自然天来判断，上月末：判断时间是工作流 A start_time/scheduler_time between '2019-05-31 00:00:00' and '2019-05-31 23:59:59'。上月：是判断上个月从 1 号到月末每天都要有完成的A实例。上周： 上周 7 天都要有完成的 A 实例。前两天： 判断昨天和前天，两天都要有完成的 A 实例。</p>\n<hr>\n<h2>Q：DS 后端接口文档</h2>\n<p>A：1，<a href=\"http://106.75.43.194:8888/dolphinscheduler/doc.html?language=zh_CN&amp;lang=zh%E3%80%82\">http://106.75.43.194:8888/dolphinscheduler/doc.html?language=zh_CN&amp;lang=zh。</a></p>\n<h2>dolphinscheduler 在运行过程中，ip 地址获取错误的问题</h2>\n<p>master 服务、worker 服务在 zookeeper 注册时，会以 ip:port 的形式创建相关信息</p>\n<p>如果 ip 地址获取错误，请检查网络信息，如 Linux 系统通过 <code>ifconfig</code> 命令查看网络信息，以下图为例：</p>\n<p align=\"center\">\n  <img src=\"/img/network/network_config.png\" width=\"60%\" />\n</p>\n<p>可以使用 dolphinscheduler 提供的三种策略，获取可用 ip：</p>\n<ul>\n<li>default: 优先获取内网网卡获取 ip 地址，其次获取外网网卡获取 ip 地址，在前两项失效情况下，使用第一块可用网卡的地址</li>\n<li>inner: 使用内网网卡获取 ip地址，如果获取失败抛出异常信息</li>\n<li>outer: 使用外网网卡获取 ip地址，如果获取失败抛出异常信息</li>\n</ul>\n<p>配置方式是在 <code>common.properties</code> 中修改相关配置：</p>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> network IP gets priority, default: inner outer</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> dolphin.scheduler.network.priority.strategy=default</span>\n</code></pre>\n<p>以上配置修改后重启服务生效</p>\n<p>如果 ip 地址获取依然错误，请下载 <a href=\"/asset/dolphinscheduler-netutils.jar\">dolphinscheduler-netutils.jar</a> 到相应机器，执行以下命令以进一步排障，并反馈给社区开发人员：</p>\n<pre><code class=\"language-shell\">java -jar target/dolphinscheduler-netutils.jar\n</code></pre>\n<h2>配置 sudo 免密，用于解决默认配置 sudo 权限过大或不能申请 root 权限的使用问题</h2>\n<p>配置 dolphinscheduler OS 账号的 sudo 权限为部分普通用户范围内的一个普通用户管理者，限制指定用户在指定主机上运行某些命令，详细配置请看 sudo 权限管理\n例如 sudo 权限管理配置 dolphinscheduler OS 账号只能操作用户 userA,userB,userC 的权限（其中用户 userA,userB,userC 用于多租户向大数据集群提交作业）</p>\n<pre><code class=\"language-shell\">echo &#x27;dolphinscheduler  ALL=(userA,userB,userC)  NOPASSWD: NOPASSWD: ALL&#x27; &gt;&gt; /etc/sudoers\nsed -i &#x27;s/Defaults    requirett/#Defaults    requirett/g&#x27; /etc/sudoers\n</code></pre>\n<hr>\n<h2>Q：Yarn多集群支持</h2>\n<p>A：将Worker节点分别部署至多个Yarn集群，步骤如下（例如AWS EMR）：</p>\n<ol>\n<li>\n<p>将 Worker 节点部署至 EMR 集群的 Master 节点</p>\n</li>\n<li>\n<p>将 <code>conf/common.properties</code> 中的 <code>yarn.application.status.address</code> 修改为当前集群的 Yarn 的信息</p>\n</li>\n<li>\n<p>通过 <code>bin/dolphinscheduler-daemon.sh start worker-server</code> 和 <code>bin/dolphinscheduler-daemon.sh start logger-server</code> 分别启动 worker-server 和 logger-server</p>\n</li>\n</ol>\n<hr>\n<h2>Q：Update process definition error: Duplicate key TaskDefinition</h2>\n<p>A：在DS 2.0.4之前（2.0.0-alpha之后），可能存在版本切换的重复键问题，导致更新工作流失败；可参考如下SQL进行重复数据的删除，以MySQL为例：（注意：操作前请务必备份原数据，SQL来源于pr <a href=\"https://github.com/apache/dolphinscheduler/pull/8408\">#8408</a>）</p>\n<pre><code class=\"language-SQL\"><span class=\"hljs-keyword\">DELETE</span> <span class=\"hljs-keyword\">FROM</span> t_ds_process_task_relation_log <span class=\"hljs-keyword\">WHERE</span> id <span class=\"hljs-keyword\">IN</span>\n(\n <span class=\"hljs-keyword\">SELECT</span>\n     x.id\n <span class=\"hljs-keyword\">FROM</span>\n     (\n         <span class=\"hljs-keyword\">SELECT</span>\n             aa.id\n         <span class=\"hljs-keyword\">FROM</span>\n             t_ds_process_task_relation_log aa\n                 <span class=\"hljs-keyword\">JOIN</span>\n             (\n                 <span class=\"hljs-keyword\">SELECT</span>\n                     a.process_definition_code\n                      ,<span class=\"hljs-built_in\">MAX</span>(a.id) <span class=\"hljs-keyword\">as</span> min_id\n                      ,a.pre_task_code\n                      ,a.pre_task_version\n                      ,a.post_task_code\n                      ,a.post_task_version\n                      ,a.process_definition_version\n                      ,<span class=\"hljs-built_in\">COUNT</span>(<span class=\"hljs-operator\">*</span>) cnt\n                 <span class=\"hljs-keyword\">FROM</span>\n                     t_ds_process_task_relation_log a\n                         <span class=\"hljs-keyword\">JOIN</span> (\n                         <span class=\"hljs-keyword\">SELECT</span>\n                             code\n                         <span class=\"hljs-keyword\">FROM</span>\n                             t_ds_process_definition\n                         <span class=\"hljs-keyword\">GROUP</span> <span class=\"hljs-keyword\">BY</span> code\n                     )b <span class=\"hljs-keyword\">ON</span> b.code <span class=\"hljs-operator\">=</span> a.process_definition_code\n                 <span class=\"hljs-keyword\">WHERE</span> <span class=\"hljs-number\">1</span><span class=\"hljs-operator\">=</span><span class=\"hljs-number\">1</span>\n                 <span class=\"hljs-keyword\">GROUP</span> <span class=\"hljs-keyword\">BY</span> a.pre_task_code\n                        ,a.post_task_code\n                        ,a.pre_task_version\n                        ,a.post_task_version\n                        ,a.process_definition_code\n                        ,a.process_definition_version\n                 <span class=\"hljs-keyword\">HAVING</span> <span class=\"hljs-built_in\">COUNT</span>(<span class=\"hljs-operator\">*</span>) <span class=\"hljs-operator\">&gt;</span> <span class=\"hljs-number\">1</span>\n             )bb <span class=\"hljs-keyword\">ON</span> bb.process_definition_code <span class=\"hljs-operator\">=</span> aa.process_definition_code\n                 <span class=\"hljs-keyword\">AND</span> bb.pre_task_code <span class=\"hljs-operator\">=</span> aa.pre_task_code\n                 <span class=\"hljs-keyword\">AND</span> bb.post_task_code <span class=\"hljs-operator\">=</span> aa.post_task_code\n                 <span class=\"hljs-keyword\">AND</span> bb.process_definition_version <span class=\"hljs-operator\">=</span> aa.process_definition_version\n                 <span class=\"hljs-keyword\">AND</span> bb.pre_task_version <span class=\"hljs-operator\">=</span> aa.pre_task_version\n                 <span class=\"hljs-keyword\">AND</span> bb.post_task_version <span class=\"hljs-operator\">=</span> aa.post_task_version\n                 <span class=\"hljs-keyword\">AND</span> bb.min_id <span class=\"hljs-operator\">!=</span> aa.id\n     )x\n)\n;\n\n<span class=\"hljs-keyword\">DELETE</span> <span class=\"hljs-keyword\">FROM</span> t_ds_task_definition_log <span class=\"hljs-keyword\">WHERE</span> id <span class=\"hljs-keyword\">IN</span>\n(\n   <span class=\"hljs-keyword\">SELECT</span>\n       x.id\n   <span class=\"hljs-keyword\">FROM</span>\n       (\n           <span class=\"hljs-keyword\">SELECT</span>\n               a.id\n           <span class=\"hljs-keyword\">FROM</span>\n               t_ds_task_definition_log a\n                   <span class=\"hljs-keyword\">JOIN</span>\n               (\n                   <span class=\"hljs-keyword\">SELECT</span>\n                       code\n                        ,name\n                        ,version\n                        ,<span class=\"hljs-built_in\">MAX</span>(id) <span class=\"hljs-keyword\">AS</span> min_id\n                   <span class=\"hljs-keyword\">FROM</span>\n                       t_ds_task_definition_log\n                   <span class=\"hljs-keyword\">GROUP</span> <span class=\"hljs-keyword\">BY</span> code\n                          ,name\n                          ,version\n                   <span class=\"hljs-keyword\">HAVING</span> <span class=\"hljs-built_in\">COUNT</span>(<span class=\"hljs-operator\">*</span>) <span class=\"hljs-operator\">&gt;</span> <span class=\"hljs-number\">1</span>\n               )b <span class=\"hljs-keyword\">ON</span> b.code <span class=\"hljs-operator\">=</span> a.code\n                   <span class=\"hljs-keyword\">AND</span> b.name <span class=\"hljs-operator\">=</span> a.name\n                   <span class=\"hljs-keyword\">AND</span> b.version <span class=\"hljs-operator\">=</span> a.version\n                   <span class=\"hljs-keyword\">AND</span> b.min_id <span class=\"hljs-operator\">!=</span> a.id\n       )x\n)\n;\n</code></pre>\n<hr>\n<p>我们会持续收集更多的 FAQ。</p>\n",
  "link": "/dist/zh-cn/docs/release/faq.html",
  "meta": {}
}