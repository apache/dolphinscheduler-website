{
  "filename": "deployparam.md",
  "__html": "<h1>Dolphin Scheduler 1.2.0 部署参数分析</h1>\n<h3>Dolphin Scheduler目录配置文件解读</h3>\n<p>（讲解配置文件的作用，具体配置在install.sh部署文件中完成）<br /><img src=\"/img/doc-img/1.2.0/deployparam-img/deploydir.png\" alt=\"image.png\"></p>\n<ul>\n<li>bin 启动脚本</li>\n<li>conf 配置文件</li>\n<li>lib ds依赖的jar包</li>\n<li>script 数据库创建升级脚本，部署分发脚本</li>\n<li>sql ds的元数据创建升级sql文件</li>\n<li>install脚本 部署ds主要的配置文件修改处\n<a name=\"poKCK\"></a></li>\n</ul>\n<h4>bin</h4>\n<p>bin目录下比较重要的是dolphinscheduler-daemon文件，之前版本中极容易出现的找不到jdk问题来源，当前版本的jdk已经export了本机的$JAVA_HOME，再也不用担心找不到jdk了。<br /><img src=\"/img/doc-img/1.2.0/deployparam-img/daemon-120.png\" alt=\"image.png\">\n<a name=\"lmmR2\"></a></p>\n<h4>conf</h4>\n<p>非常重要的配置文件目录！！！<br />非常重要的配置文件目录！！！<br />非常重要的配置文件目录！！！<br /><img src=\"/img/doc-img/1.2.0/deployparam-img/conf-120.png\" alt=\"image.png\"></p>\n<ul>\n<li>env目录下的.dolphinscheduller_env.sh文件中记录了所有跟ds-task相关的环境变量,1.2.0版本的Spark不具备指定Spark版本的功能，可以注释掉SPARK_HOME1或者将SPARK_HOME1和SPARK_HOME2均配置为集群中的Spark2。下面给出CDH中的配置，测试环境中没有部署Flink，请忽略Flink的配置。（特别注意这是个隐藏文件，需要ls -al）</li>\n</ul>\n<pre><code class=\"language-shell\">export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop\nexport HADOOP_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hadoop/etc/hadoop\n<span class=\"hljs-meta\">#</span><span class=\"bash\">可以注释掉，也可以配置为SPARK_HOME2</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"><span class=\"hljs-built_in\">export</span> SPARK_HOME1=/opt/cloudera/parcels/SPARK2/lib/spark2</span>\nexport SPARK_HOME2=/opt/cloudera/parcels/SPARK2/lib/spark2\nexport PYTHON_HOME=/usr/local/anaconda3/bin/python\nexport JAVA_HOME=/usr/java/jdk1.8.0_131\nexport HIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive\nexport FLINK_HOME=/opt/soft/flink\nexport PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH:$FLINK_HOME/bin:$PATH\n</code></pre>\n<p><a name=\"chqEB\"></a></p>\n<h4>common目录</h4>\n<p>common目录包含：common.properties和hadoop/hadoop.properties</p>\n<ul>\n<li>common.properies\n<ul>\n<li>ds的task队列实现方式，默认是Zookeeper</li>\n<li>ds的task和资源的worker执行路径</li>\n<li>资源中心\n<ul>\n<li>资源中心可选择HDFS和S3</li>\n</ul>\n</li>\n<li>资源文件类型</li>\n<li>kerberos</li>\n<li>开发状态\n<ul>\n<li>开发测试可以开启，生产环境建议设置为false</li>\n</ul>\n</li>\n<li>ds的环境变量配置，本地调试的时候，需要保证dolphinscheduler.env.path存在</li>\n</ul>\n</li>\n<li>hadoop.properties\n<ul>\n<li>hdfs namenode配置\n<ul>\n<li>单点可以直接写namenode的ip</li>\n<li>hdfsHA需要将集群的core-site.xml和hdfs-site.xml文件拷贝到ds的conf目录下</li>\n</ul>\n</li>\n<li>s3配置</li>\n<li>yarn resourcemanager配置\n<ul>\n<li>单点配置yarn.application.status.address</li>\n<li>HA配置yarn.resourcemanager.ha.rm.ids\n<a name=\"ggGBc\"></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4>config目录</h4>\n<p>config目录包含install_config.conf和run_config.conf</p>\n<ul>\n<li>install_config.conf\n<ul>\n<li>ds的安装路径</li>\n<li>部署用户</li>\n<li>部署ds的机器组ip</li>\n</ul>\n</li>\n<li>run_config.conf\n<ul>\n<li>指定ds的masters，workers，alertServer，apiServer部署在哪些机器上\n<a name=\"AC8Jm\"></a></li>\n</ul>\n</li>\n</ul>\n<h4>alert.properties</h4>\n<ul>\n<li>邮件告警配置</li>\n<li>excel下载目录</li>\n<li>企业微信配置\n<a name=\"1qiHb\"></a></li>\n</ul>\n<h4>application-api.properties</h4>\n<ul>\n<li>apiserver端口，上下文，日志等\n<a name=\"IiX6U\"></a></li>\n</ul>\n<h4>application-dao.properties</h4>\n<p>敲黑板，重点！！！ds的元数据库配置，在ds-1.2.0中默认的数据库是pg，如果要使用MySQL，需要将MySQL的jdbc包放到lib目录下。</p>\n<ul>\n<li>ds元数据库配置\n<a name=\"oomWN\"></a></li>\n</ul>\n<h4>master.properties</h4>\n<ul>\n<li>master执行线程数</li>\n<li>master并行任务上限</li>\n<li>master资源CPU和内存阈值，超出阈值不会进行dag切分\n<a name=\"ZeAdP\"></a></li>\n</ul>\n<h4>worker.properties</h4>\n<ul>\n<li>worker执行线程数</li>\n<li>worker一次提交任务数</li>\n<li>worker资源CPU和内存阈值，超出不会去task队列拉取task\n<a name=\"saeo8\"></a></li>\n</ul>\n<h4>Zookeeper.properties</h4>\n<ul>\n<li>zk集群</li>\n<li>ds所需zk的znode，包含dag和task的分布式锁和master和worker的容错\n<a name=\"2eTCI\"></a></li>\n</ul>\n<h4>quartz.properties</h4>\n<p>ds的定时由quartz框架完成，特别注意里边有quartz的数据库配置！！！</p>\n<ul>\n<li>quartz的基本属性，线程池和job配置</li>\n<li>quartz元数据库配置\n<a name=\"vWF4U\"></a></li>\n</ul>\n<h3>install脚本</h3>\n<p>install.sh部署脚本是ds部署中的重头戏，下面将参数分组进行分析。\n<a name=\"rYEds\"></a></p>\n<h4>数据库配置</h4>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> <span class=\"hljs-keyword\">for</span> example postgresql or mysql ...</span>\ndbtype=&quot;postgresql&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> db config</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> db address and port</span>\ndbhost=&quot;192.168.xx.xx:5432&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> db name</span>\ndbname=&quot;dolphinscheduler&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> db username</span>\nusername=&quot;xx&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> db passwprd</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note: <span class=\"hljs-keyword\">if</span> there are special characters, please use the \\ transfer character to transfer</span>\npassowrd=&quot;xx&quot;\n</code></pre>\n<ul>\n<li>dbtype参数可以设置postgresql和mysql，这里指定了ds连接元数据库的jdbc相关信息\n<a name=\"K4u2S\"></a></li>\n</ul>\n<h4>部署用户&amp;目录</h4>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> conf/config/install_config.conf config</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note: the installation path is not the same as the current path (<span class=\"hljs-built_in\">pwd</span>)</span>\ninstallPath=&quot;/data1_1T/dolphinscheduler&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> deployment user</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note: the deployment user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled, the root directory needs to be created by itself</span>\ndeployUser=&quot;dolphinscheduler&quot;\n</code></pre>\n<ul>\n<li>installPath是安装路径，在执行install.sh之后，会把ds安装到指定目录，如/opt/ds-agent。installPath不要和当前要一键安装的install.sh是同一目录。</li>\n<li>deployUser是指ds的部署用户，该用户需要在部署ds的机器上打通sudo免密，并且需要具有操作hdfs的权限，建议挂到hadoop的supergroup组下。\n<a name=\"6rEDt\"></a></li>\n</ul>\n<h4>zk集群&amp;角色指定</h4>\n<ul>\n<li>配置zk集群的时候，特别注意：要用ip:2181的方式配置上去，一定要把端口带上。</li>\n<li>ds一共包括master worker alert api四种角色，其中alert api只需指定一台机器即可，master和worker可以部署多态机器。下面的例子就是在4台机器中，部署2台master，2台worker，1台alert，1台api</li>\n<li>ips参数，填写所有需要部署机器的hostname</li>\n<li>masters，填写部署master机器的hostname</li>\n<li>workers，填写部署worker机器的hostname</li>\n<li>alertServer，填写部署alert机器的hostname</li>\n<li>apiServers，填写部署api机器的hostname</li>\n<li>zkroot参数可以通过调整，在一套zk集群中，托管多个ds集群，如配置zkRoot=&quot;/dspro&quot;,zkRoot=&quot;/dstest&quot;</li>\n</ul>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> zk cluster</span>\nzkQuorum=&quot;192.168.xx.xx:2181,192.168.xx.xx:2181,192.168.xx.xx:2181&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> install hosts</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note: install the scheduled hostname list. If it is pseudo-distributed, just write a pseudo-distributed hostname</span>\nips=&quot;ark0,ark1,ark2,ark3&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> conf/config/run_config.conf config</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> run master machine</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note: list of hosts hostname <span class=\"hljs-keyword\">for</span> deploying master</span>\nmasters=&quot;ark0,ark1&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> run worker machine</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> note: list of machine hostnames <span class=\"hljs-keyword\">for</span> deploying workers</span>\nworkers=&quot;ark2,ark3&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> run alert machine</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> note: list of machine hostnames <span class=\"hljs-keyword\">for</span> deploying alert server</span>\nalertServer=&quot;ark3&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> run api machine</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> note: list of machine hostnames <span class=\"hljs-keyword\">for</span> deploying api server</span>\napiServers=&quot;ark1&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk config</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> zk root directory</span>\nzkRoot=&quot;/dolphinscheduler&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> used to record the zk directory of the hanging machine</span>\nzkDeadServers=&quot;$zkRoot/dead-servers&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> masters directory</span>\nzkMasters=&quot;$zkRoot/masters&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> workers directory</span>\nzkWorkers=&quot;$zkRoot/workers&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk master distributed lock</span>\nmastersLock=&quot;$zkRoot/lock/masters&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk worker distributed lock</span>\nworkersLock=&quot;$zkRoot/lock/workers&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk master fault-tolerant distributed lock</span>\nmastersFailover=&quot;$zkRoot/lock/failover/masters&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk worker fault-tolerant distributed lock</span>\nworkersFailover=&quot;$zkRoot/lock/failover/workers&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk master start fault tolerant distributed lock</span>\nmastersStartupFailover=&quot;$zkRoot/lock/failover/startup-masters&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk session timeout</span>\nzkSessionTimeout=&quot;300&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk connection timeout</span>\nzkConnectionTimeout=&quot;300&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk retry interval</span>\nzkRetrySleep=&quot;100&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> zk retry maximum number of <span class=\"hljs-built_in\">times</span></span>\nzkRetryMaxtime=&quot;5&quot;\n</code></pre>\n<p><a name=\"7aGb8\"></a></p>\n<h4>邮件配置&amp;excel文件路径</h4>\n<ul>\n<li>邮件配置这块也是大家非常容易出问题的，建议可以拉一下ds的代码，跑一下alert.MailUtilisTest这个测试类，下面给出QQ邮箱配置方式。如果是内网邮箱，需要注意的是ssl是否需要关闭，以及mail.user登陆用户是否需要去掉邮箱后缀。</li>\n<li>excel路径则需要保证该路径的写入权限</li>\n</ul>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\">QQ邮箱配置</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> alert config</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> mail protocol</span>\nmailProtocol=&quot;SMTP&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> mail server host</span>\nmailServerHost=&quot;smtp.qq.com&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> mail server port</span>\nmailServerPort=&quot;465&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> sender</span>\nmailSender=&quot;783xx8369@qq.com&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> user</span>\nmailUser=&quot;783xx8369@qq.com&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> sender password</span>\nmailPassword=&quot;邮箱授权码&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> TLS mail protocol support</span>\nstarttlsEnable=&quot;false&quot;\n\nsslTrust=&quot;smtp.qq.com&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> SSL mail protocol support</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> note: The SSL protocol is enabled by default.</span> \n<span class=\"hljs-meta\">#</span><span class=\"bash\"> only one of TLS and SSL can be <span class=\"hljs-keyword\">in</span> the <span class=\"hljs-literal\">true</span> state.</span>\nsslEnable=&quot;true&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> download excel path</span>\nxlsFilePath=&quot;/tmp/xls&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> alert port</span>\nalertPort=7789\n</code></pre>\n<p><a name=\"h0lJz\"></a></p>\n<h4>apiServer配置</h4>\n<ul>\n<li>apiServer这里可以关注一下，apiserver的端口和上下文即apiServerPort和apiServerContextPath参数</li>\n</ul>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> api config</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> api server port</span>\napiServerPort=&quot;12345&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> api session timeout</span>\napiServerSessionTimeout=&quot;7200&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> api server context path</span>\napiServerContextPath=&quot;/dolphinscheduler/&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> spring max file size</span>\nspringMaxFileSize=&quot;1024MB&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> spring max request size</span>\nspringMaxRequestSize=&quot;1024MB&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> api max http post size</span>\napiMaxHttpPostSize=&quot;5000000&quot;\n</code></pre>\n<p><a name=\"T4u9c\"></a></p>\n<h4>资源中心&amp;YARN</h4>\n<ul>\n<li>ds的资源中心支持HDFS和S3.</li>\n<li>resUploadStartupType=&quot;HDFS&quot;则开启hdfs作为资源中心。</li>\n<li>defaultFS，如果hdfs没有配置HA则需要在这里写上单点namenode的ip，如果HDFS是HA则需要将集群的core-site.xml文件和hdfs-site.xml文件拷贝到conf目录下</li>\n<li>yarnHaIps，如果yarn启用了HA，配置两个resourcemanager的ip，如果是单点，配置空字符串</li>\n<li>singleYarnIp，如果yarn是单点，配置resourcemanager的ip</li>\n<li>hdfsPath，HDFS上ds存储资源的根路径，可采用默认值，如果是从1.1.0版本进行升级，需要注意这个地方，改为/escheduler</li>\n</ul>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> resource Center upload and select storage method：HDFS,S3,NONE</span>\nresUploadStartupType=&quot;NONE&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> <span class=\"hljs-keyword\">if</span> resUploadStartupType is HDFS，defaultFS write namenode address，HA you need to put core-site.xml and hdfs-site.xml <span class=\"hljs-keyword\">in</span> the conf directory.</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> <span class=\"hljs-keyword\">if</span> S3，write S3 address，HA，<span class=\"hljs-keyword\">for</span> example ：s3a://dolphinscheduler，</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note，s3 be sure to create the root directory /dolphinscheduler</span>\ndefaultFS=&quot;hdfs://mycluster:8020&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> <span class=\"hljs-keyword\">if</span> S3 is configured, the following configuration is required.</span>\ns3Endpoint=&quot;http://192.168.xx.xx:9010&quot;\ns3AccessKey=&quot;xxxxxxxxxx&quot;\ns3SecretKey=&quot;xxxxxxxxxx&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> resourcemanager HA configuration, <span class=\"hljs-keyword\">if</span> it is a single resourcemanager, here is yarnHaIps=<span class=\"hljs-string\">&quot;&quot;</span></span>\nyarnHaIps=&quot;192.168.xx.xx,192.168.xx.xx&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> <span class=\"hljs-keyword\">if</span> it is a single resourcemanager, you only need to configure one host name. If it is resourcemanager HA, the default configuration is fine.</span>\nsingleYarnIp=&quot;ark1&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> hdfs root path, the owner of the root path must be the deployment user.</span> \n<span class=\"hljs-meta\">#</span><span class=\"bash\"> versions prior to 1.1.0 <span class=\"hljs-keyword\">do</span> not automatically create the hdfs root directory, you need to create it yourself.</span>\nhdfsPath=&quot;/dolphinscheduler&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> have users who create directory permissions under hdfs root path /</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\"> Note: <span class=\"hljs-keyword\">if</span> kerberos is enabled, hdfsRootUser=<span class=\"hljs-string\">&quot;&quot;</span> can be used directly.</span>\nhdfsRootUser=&quot;hdfs&quot;\n</code></pre>\n<p><a name=\"0bSHO\"></a></p>\n<h4>开发状态</h4>\n<ul>\n<li>devState在测试环境部署的时候可以调为true，生产环境部署建议调为false</li>\n</ul>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> development status, <span class=\"hljs-keyword\">if</span> <span class=\"hljs-literal\">true</span>, <span class=\"hljs-keyword\">for</span> the SHELL script, you can view the encapsulated SHELL script <span class=\"hljs-keyword\">in</span> the execPath directory.</span> \n<span class=\"hljs-meta\">#</span><span class=\"bash\"> If it is <span class=\"hljs-literal\">false</span>, execute the direct delete</span>\ndevState=&quot;true&quot;\n</code></pre>\n<p><a name=\"x9hTX\"></a></p>\n<h4>角色参数</h4>\n<ul>\n<li>下面的参数主要是调整的application.properties里边的配置，涉及master,worker和apiserver</li>\n<li>apiServerPort可以自定义修改apiserver的端口，注意需要跟前端保持一致。</li>\n<li>master和worker的参数，初次部署建议保持默认值，如果在运行当中出现性能问题在作调整，有条件可以压一下自身环境中的master和worker的最佳线程数。</li>\n<li>worker.reserved.memory是worker的内存阈值，masterReservedMemory是master的内存阈值，建议调整为0.1</li>\n<li>masterMaxCpuLoadAvg建议注释掉，ds-1.2.0master和worker的CPU负载给出了默认cpu线程数 * 2的默认值</li>\n</ul>\n<pre><code class=\"language-shell\"><span class=\"hljs-meta\">#</span><span class=\"bash\"> master config</span> \n<span class=\"hljs-meta\">#</span><span class=\"bash\"> master execution thread maximum number, maximum parallelism of process instance</span>\nmasterExecThreads=&quot;100&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> the maximum number of master task execution threads, the maximum degree of parallelism <span class=\"hljs-keyword\">for</span> each process instance</span>\nmasterExecTaskNum=&quot;20&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master heartbeat interval</span>\nmasterHeartbeatInterval=&quot;10&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master task submission retries</span>\nmasterTaskCommitRetryTimes=&quot;5&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master task submission retry interval</span>\nmasterTaskCommitInterval=&quot;100&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master maximum cpu average load, used to determine whether the master has execution capability</span>\n<span class=\"hljs-meta\">#</span><span class=\"bash\">masterMaxCpuLoadAvg=<span class=\"hljs-string\">&quot;10&quot;</span></span>\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master reserve memory to determine <span class=\"hljs-keyword\">if</span> the master has execution capability</span>\nmasterReservedMemory=&quot;1&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master port</span>\nmasterPort=5566\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> worker config</span> \n<span class=\"hljs-meta\">#</span><span class=\"bash\"> worker execution thread</span>\nworkerExecThreads=&quot;100&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> worker heartbeat interval</span>\nworkerHeartbeatInterval=&quot;10&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> worker number of fetch tasks</span>\nworkerFetchTaskNum=&quot;3&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> worker reserve memory to determine <span class=\"hljs-keyword\">if</span> the master has execution capability</span>\nworkerReservedMemory=&quot;1&quot;\n<span class=\"hljs-meta\">\n#</span><span class=\"bash\"> master port</span>\nworkerPort=7788\n</code></pre>\n<p><a name=\"3QaMD\"></a></p>\n<h3>特别注意</h3>\n<ul>\n<li>ds需要启用资源中心之后，才可以创建租户，因此资源中心的配置一定要正确</li>\n<li>ds老版本部署需要配置JDK的问题已经解决</li>\n<li>installPath不要和当前要一键安装的install.sh是同一目录</li>\n<li>ds的task运行都依赖env目录下的环境变量文件，需要正确配置</li>\n<li>HDFS高可用，需要把core-site.xml和hdfs-site.xml文件拷贝到conf目录下</li>\n<li>邮件配置中mailUser和mailSender的区别</li>\n</ul>\n",
  "link": "/dist/zh-cn/docs/1.2.0/user_doc/deployparam.html",
  "meta": {}
}