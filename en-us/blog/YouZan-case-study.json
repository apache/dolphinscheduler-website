{
  "filename": "YouZan-case-study.md",
  "__html": "<h1>From Airflow to Apache DolphinScheduler, the Roadmap of Scheduling System On Youzan Big Data Development Platform</h1>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1639383815755.png\"/>\n</div>\n<p>At the recent Apache DolphinScheduler Meetup 2021, Zheqi Song, the Director of Youzan Big Data Development Platform\nshared the design scheme and production environment practice of its scheduling system migration from Airflow to Apache\nDolphinScheduler.</p>\n<p>This post-90s young man from Hangzhou, Zhejiang Province joined Youzan in September 2019, where he is engaged in the\nresearch and development of data development platforms, scheduling systems, and data synchronization modules. When he\nfirst joined, Youzan used Airflow, which is also an Apache open source project, but after research and production\nenvironment testing, Youzan decided to switch to DolphinScheduler.</p>\n<p>How does the Youzan big data development platform use the scheduling system? Why did Youzan decide to switch to Apache\nDolphinScheduler? The message below will uncover the truth.</p>\n<h2>Youzan Big Data Development Platform（DP）</h2>\n<p>As a retail technology SaaS service provider, Youzan is aimed to help online merchants open stores, build data products\nand digital solutions through social marketing and expand the omnichannel retail business, and provide better SaaS\ncapabilities for driving merchants' digital growth.</p>\n<p>At present, Youzan has established a relatively complete digital product matrix with the support of the data center:</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_Jjgx5qQfjo559_oaJP-DAQ.png\"/>\n</div>\n<p>Youzan has established a big data development platform (hereinafter referred to as DP platform) to support the\nincreasing demand for data processing services. This is a big data offline development platform that provides users with\nthe environment, tools, and data needed for the big data tasks development.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_G9znZGQ1XBhJva0tjWa6Bg.png\"/>\n</div>\n<p>Youzan Big Data Development Platform Architecture</p>\n<p>Youzan Big Data Development Platform is mainly composed of five modules: basic component layer, task component layer,\nscheduling layer, service layer, and monitoring layer. Among them, the service layer is mainly responsible for the job\nlife cycle management, and the basic component layer and the task component layer mainly include the basic environment\nsuch as middleware and big data components that the big data development platform depends on. The service deployment of\nthe DP platform mainly adopts the master-slave mode, and the master node supports HA. The scheduling layer is\nre-developed based on Airflow, and the monitoring layer performs comprehensive monitoring and early warning of the\nscheduling cluster.</p>\n<h3>1 Scheduling layer architecture design</h3>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_UDNCmMrZtcswj62aqNXA1g.png\"/>\n</div>\n<p>Youzan Big Data Development Platform Scheduling Layer Architecture Design</p>\n<p>In 2017, our team investigated the mainstream scheduling systems, and finally adopted Airflow (1.7) as the task\nscheduling module of DP. In the design of architecture, we adopted the deployment plan of Airflow + Celery + Redis +\nMySQL based on actual business scenario demand, with Redis as the dispatch queue, and implemented distributed deployment\nof any number of workers through Celery.</p>\n<p>In the HA design of the scheduling node, it is well known that Airflow has a single point problem on the scheduled node.\nTo achieve high availability of scheduling, the DP platform uses the Airflow Scheduler Failover Controller, an\nopen-source component, and adds a Standby node that will periodically monitor the health of the Active node. Once the\nActive node is found to be unavailable, Standby is switched to Active to ensure the high availability of the schedule.</p>\n<h3>2 Worker nodes load balancing strategy</h3>\n<p>In addition, to use resources more effectively, the DP platform distinguishes task types based on CPU-intensive\ndegree/memory-intensive degree and configures different slots for different celery queues to ensure that each machine's\nCPU/memory usage rate is maintained within a reasonable range.</p>\n<h2>Scheduling System Upgrade and Selection</h2>\n<p>Since the official launch of the Youzan Big Data Platform 1.0 in 2017, we have completed 100% of the data warehouse\nmigration plan in 2018. In 2019, the daily scheduling task volume has reached 30,000+ and has grown to 60,000+ by 2021.\nthe platform’s daily scheduling task volume will be reached. With the rapid increase in the number of tasks, DP's\nscheduling system also faces many challenges and problems.</p>\n<h3>1 Pain points of Airflow</h3>\n<ol>\n<li>In-depth re-development is difficult, the commercial version is separated from the community, and costs relatively\nhigh to upgrade ;</li>\n<li>Based on the Python technology stack, the maintenance and iteration cost higher;</li>\n<li>Performance issues:</li>\n</ol>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_U33OWzzfw2Dqn3ryCNbSvw.png\"/>\n</div>\n<p>Airflow's schedule loop, as shown in the figure above, is essentially the loading and analysis of DAG and generates DAG\nround instances to perform task scheduling. Before Airflow 2.0, the DAG was scanned and parsed into the database by a\nsingle point. It leads to a large delay (over the scanning frequency, even to 60s-70s) for the scheduler loop to scan\nthe Dag folder once the number of Dags was largely due to business growth. This seriously reduces the scheduling\nperformance.</p>\n<ol start=\"4\">\n<li>Stability issues:</li>\n</ol>\n<p>The Airflow Scheduler Failover Controller is essentially run by a master-slave mode. The standby node judges whether to\nswitch by monitoring whether the active process is alive or not. If it encounters a deadlock blocking the process\nbefore, it will be ignored, which will lead to scheduling failure. After similar problems occurred in the production\nenvironment, we found the problem after troubleshooting. Although Airflow version 1.10 has fixed this problem, this\nproblem will exist in the master-slave mode, and cannot be ignored in the production environment.</p>\n<p>Taking into account the above pain points, we decided to re-select the scheduling system for the DP platform.</p>\n<p>In the process of research and comparison, Apache DolphinScheduler entered our field of vision. Also to be Apache's top\nopen-source scheduling component project, we have made a comprehensive comparison between the original scheduling system\nand DolphinScheduler from the perspectives of performance, deployment, functionality, stability, and availability, and\ncommunity ecology.</p>\n<p>This is the comparative analysis result below:</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_Rbr05klPmQIc7WPFNeEH-w.png\"/>\n</div>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_Ity1QoRL_Yu5aDVClY9AgA.png\"/>\n</div>\n<p>Airflow VS DolphinScheduler</p>\n<h3>1 DolphinScheduler valuation</h3>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_o8c1Y1TFAOis3KozzJnvfA.png\"/>\n</div>\n<p>As shown in the figure above, after evaluating, we found that the throughput performance of DolphinScheduler is twice\nthat of the original scheduling system under the same conditions. And we have heard that the performance of\nDolphinScheduler will greatly be improved after version 2.0, this news greatly excites us.</p>\n<p>In addition, at the deployment level, the Java technology stack adopted by DolphinScheduler is conducive to the\nstandardized deployment process of ops, simplifies the release process, liberates operation and maintenance manpower,\nand supports Kubernetes and Docker deployment with stronger scalability.</p>\n<p>In terms of new features, DolphinScheduler has a more flexible task-dependent configuration, to which we attach much\nimportance, and the granularity of time configuration is refined to the hour, day, week, and month. In addition,\nDolphinScheduler's scheduling management interface is easier to use and supports worker group isolation. As a\ndistributed scheduling, the overall scheduling capability of DolphinScheduler grows linearly with the scale of the\ncluster, and with the release of new feature task plug-ins, the task-type customization is also going to be attractive\ncharacter.</p>\n<p>From the perspective of stability and availability, DolphinScheduler achieves high reliability and high scalability, the\ndecentralized multi-Master multi-Worker design architecture supports dynamic online and offline services and has\nstronger self-fault tolerance and adjustment capabilities.</p>\n<p>And also importantly, after months of communication, we found that the DolphinScheduler community is highly active, with\nfrequent technical exchanges, detailed technical documents outputs, and fast version iteration.</p>\n<p>In summary, we decided to switch to DolphinScheduler.</p>\n<h2>DolphinScheduler Migration Scheme Design</h2>\n<p>After deciding to migrate to DolphinScheduler, we sorted out the platform's requirements for the transformation of the\nnew scheduling system.</p>\n<p>In conclusion, the key requirements are as below:</p>\n<ol>\n<li>Users are not aware of migration. There are 700-800 users on the platform, we hope that the user switching cost can\nbe reduced;</li>\n<li>The scheduling system can be dynamically switched because the production environment requires stability above all\nelse. The online grayscale test will be performed during the online period, we hope that the scheduling system can be\ndynamically switched based on the granularity of the workflow;</li>\n<li>The workflow configuration for testing and publishing needs to be isolated. Currently, we have two sets of\nconfiguration files for task testing and publishing that are maintained through GitHub. Online scheduling task\nconfiguration needs to ensure the accuracy and stability of the data, so two sets of environments are required for\nisolation.</li>\n</ol>\n<p>In response to the above three points, we have redesigned the architecture.</p>\n<h3>1 Architecture design</h3>\n<ol>\n<li>Keep the existing front-end interface and DP API;</li>\n<li>Refactoring the scheduling management interface, which was originally embedded in the Airflow interface, and will be\nrebuilt based on DolphinScheduler in the future;</li>\n<li>Task lifecycle management/scheduling management and other operations interact through the DolphinScheduler API;</li>\n<li>Use the Project mechanism to redundantly configure the workflow to achieve configuration isolation for testing and\nrelease.</li>\n</ol>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_eusVhW4QAJ2uO-J96bqiFg.png\"/>\n</div>\n<p>Refactoring Design</p>\n<p>We entered the transformation phase after the architecture design is completed. We have transformed DolphinScheduler's\nworkflow definition, task execution process, and workflow release process, and have made some key functions to\ncomplement it.</p>\n<ul>\n<li>Workflow definition status combing</li>\n</ul>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/-1.png\"/>\n</div>\n<p>We first combed the definition status of the DolphinScheduler workflow. The definition and timing management of\nDolphinScheduler work will be divided into online and offline status, while the status of the two on the DP platform is\nunified, so in the task test and workflow release process, the process series from DP to DolphinScheduler needs to be\nmodified accordingly.</p>\n<ul>\n<li>Task execution process transformation</li>\n</ul>\n<p>Firstly, we have changed the task test process. After switching to DolphinScheduler, all interactions are based on the\nDolphinScheduler API. When the task test is started on DP, the corresponding workflow definition configuration will be\ngenerated on the DolphinScheduler. After going online, the task will be run and the DolphinScheduler log will be called\nto view the results and obtain log running information in real-time.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/-1.png\"/>\n</div>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/-3.png\"/>\n</div>\n- Workflow release process transformation\n<p>Secondly, for the workflow online process, after switching to DolphinScheduler, the main change is to synchronize the\nworkflow definition configuration and timing configuration, as well as the online status.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_4-ikFp_jJ44-YWJcGNioOg.png\"/>\n</div>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/-5.png\"/>\n</div>\nThe original data maintenance and configuration synchronization of the workflow is managed based on the DP master, and\nonly when the task is online and running will it interact with the scheduling system. Based on these two core changes,\nthe DP platform can dynamically switch systems under the workflow, and greatly facilitate the subsequent online\ngrayscale test.\n<h3>2 Function completion</h3>\n<p>In addition, the DP platform has also complemented some functions. The first is the adaptation of task types.</p>\n<ul>\n<li>Task type adaptation</li>\n</ul>\n<p>Currently, the task types supported by the DolphinScheduler platform mainly include data synchronization and data\ncalculation tasks, such as Hive SQL tasks, DataX tasks, and Spark tasks. Because the original data information of the\ntask is maintained on the DP, the docking scheme of the DP platform is to build a task configuration mapping module in\nthe DP master, map the task information maintained by the DP to the task on DP, and then use the API call of\nDolphinScheduler to transfer task configuration information.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_A76iOa5LKyPiu-NoopmYrA.png\"/>\n</div>\n<p>Because some of the task types are already supported by DolphinScheduler, it is only necessary to customize the\ncorresponding task modules of DolphinScheduler to meet the actual usage scenario needs of the DP platform. For the task\ntypes not supported by DolphinScheduler, such as Kylin tasks, algorithm training tasks, DataY tasks, etc., the DP\nplatform also plans to complete it with the plug-in capabilities of DolphinScheduler 2.0.</p>\n<h3>3 Transformation schedule</h3>\n<p>Because SQL tasks and synchronization tasks on the DP platform account for about 80% of the total tasks, the\ntransformation focuses on these task types. At present, the adaptation and transformation of Hive SQL tasks, DataX\ntasks, and script tasks adaptation have been completed.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_y7HUfYyLs9NxnTzENKGSCA.png\"/>\n</div>\n### 4 Function complement\n<ul>\n<li>Catchup mechanism realizes automatic replenishment</li>\n</ul>\n<p>DP also needs a core capability in the actual production environment, that is, Catchup-based automatic replenishment and\nglobal replenishment capabilities.</p>\n<p>The catchup mechanism will play a role when the scheduling system is abnormal or resources is insufficient, causing some\ntasks to miss the currently scheduled trigger time. When the scheduling is resumed, Catchup will automatically fill in\nthe untriggered scheduling execution plan.</p>\n<p>The following three pictures show the instance of an hour-level workflow scheduling execution.</p>\n<p>In Figure 1, the workflow is called up on time at 6 o'clock and tuned up once an hour. You can see that the task is\ncalled up on time at 6 o'clock and the task execution is completed. The current state is also normal.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_MvQGZ-FKKLMvKrlWihXHgg.png\"/>\n</div>\n<p>figure 1</p>\n<p>Figure 2 shows that the scheduling system was abnormal at 8 o'clock, causing the workflow not to be activated at 7\no'clock and 8 o'clock.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_1WxLOtd1Oh2YERmtGcRb0Q.png\"/>\n</div>\nfigure 2\n<p>Figure 3 shows that when the scheduling is resumed at 9 o'clock, thanks to the Catchup mechanism, the scheduling system\ncan automatically replenish the previously lost execution plan to realize the automatic replenishment of the scheduling.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/126ec1039f7aa614c.png\"/>\n</div>\n<p>Figure 3</p>\n<p>This mechanism is particularly effective when the amount of tasks is large. When the scheduled node is abnormal or the\ncore task accumulation causes the workflow to miss the scheduled trigger time, due to the system's fault-tolerant\nmechanism can support automatic replenishment of scheduled tasks, there is no need to replenish and re-run manually.</p>\n<p>At the same time, this mechanism is also applied to DP's global complement.</p>\n<ul>\n<li>Global Complement across Dags</li>\n</ul>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_eVyyABTQCLeSGzbbuizfDA.png\"/>\n</div>\n<p>DP platform cross-Dag global complement process</p>\n<p>The main use scenario of global complements in Youzan is when there is an abnormality in the output of the core upstream\ntable, which results in abnormal data display in downstream businesses. In this case, the system generally needs to\nquickly rerun all task instances under the entire data link.</p>\n<p>Based on the function of Clear, the DP platform is currently able to obtain certain nodes and all downstream instances\nunder the current scheduling cycle through analysis of the original data, and then to filter some instances that do not\nneed to be rerun through the rule pruning strategy. After obtaining these lists, start the clear downstream clear task\ninstance function, and then use Catchup to automatically fill up.</p>\n<p>This process realizes the global rerun of the upstream core through Clear, which can liberate manual operations.</p>\n<p>Because the cross-Dag global complement capability is important in a production environment, we plan to complement it in\nDolphinScheduler.</p>\n<h2>Current Status &amp; Planning &amp; Outlook</h2>\n<h3>1 DolphinScheduler migration status</h3>\n<p>The DP platform has deployed part of the DolphinScheduler service in the test environment and migrated part of the\nworkflow.</p>\n<p>After docking with the DolphinScheduler API system, the DP platform uniformly uses the admin user at the user level.\nBecause its user system is directly maintained on the DP master, all workflow information will be divided into the test\nenvironment and the formal environment.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_bXwtKI2HJzQuHCMW5y3hgg.png\"/>\n</div>\n<p>DolphinScheduler 2.0 workflow task node display</p>\n<p>The overall UI interaction of DolphinScheduler 2.0 looks more concise and more visualized and we plan to directly\nupgrade to version 2.0.</p>\n<h3>2 Access planning</h3>\n<p>At present, the DP platform is still in the grayscale test of DolphinScheduler migration., and is planned to perform a\nfull migration of the workflow in December this year. At the same time, a phased full-scale test of performance and\nstress will be carried out in the test environment. If no problems occur, we will conduct a grayscale test of the\nproduction environment in January 2022, and plan to complete the full migration in March.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_jv3ScivmLop7GYjKIECaiw.png\"/>\n</div>\n<h3>3 Expectations for DolphinScheduler</h3>\n<p>In the future, we strongly looking forward to the plug-in tasks feature in DolphinScheduler, and have implemented\nplug-in alarm components based on DolphinScheduler 2.0, by which the Form information can be defined on the backend and\ndisplayed adaptively on the frontend.</p>\n<div align=center>\n<img src=\"https://s1.imgpp.com/2021/12/16/1_3jP2KQDtFy71ciDoUyW3eg.png\"/>\n</div>\n<p>&quot;</p>\n<p>I hope that DolphinScheduler's optimization pace of plug-in feature can be faster, to better quickly adapt to our\ncustomized task types.</p>\n<p>——Zheqi Song, Head of Youzan Big Data Development Platform</p>\n<p>&quot;</p>\n",
  "link": "/dist/en-us/blog/YouZan-case-study.html",
  "meta": {}
}