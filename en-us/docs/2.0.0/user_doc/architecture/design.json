{
  "filename": "design.md",
  "__html": "<h2>System Architecture Design</h2>\n<p>Before explaining the architecture of the scheduling system, let's first understand the commonly used terms of the\nscheduling system</p>\n<h3>1.System Structure</h3>\n<h4>1.1 System architecture diagram</h4>\n<p align=\"center\">\n  <img src=\"/img/architecture-1.3.0.jpg\" alt=\"System architecture diagram\"  width=\"70%\" />\n  <p align=\"center\">\n        <em>System architecture diagram</em>\n  </p>\n</p>\n<h4>1.2 Start process activity diagram</h4>\n<p align=\"center\">\n  <img src=\"/img/master-process-2.0-en.png\" alt=\"Start process activity diagram\"  width=\"70%\" />\n  <p align=\"center\">\n        <em>Start process activity diagram</em>\n  </p>\n</p>\n<h4>1.3 Architecture description</h4>\n<ul>\n<li>\n<p><strong>MasterServer</strong></p>\n<p>MasterServer adopts a distributed and centerless design concept. MasterServer is mainly responsible for DAG task\nsegmentation, task submission monitoring, and monitoring the health status of other MasterServer and WorkerServer at\nthe same time. When the MasterServer service starts, register a temporary node with Zookeeper, and perform fault\ntolerance by monitoring changes in the temporary node of Zookeeper. MasterServer provides monitoring services based on\nnetty.</p>\n<h5>The service mainly includes:</h5>\n<ul>\n<li>\n<p><strong>MasterSchedulerService</strong> is a scanning thread that scans the <strong>command</strong> table in the database regularly,\ngenerates workflow instances, and performs different business operations according to different <strong>command types</strong></p>\n</li>\n<li>\n<p><strong>WorkflowExecuteThread</strong> is mainly responsible for DAG task segmentation, task submission, logical processing of\nvarious command types, processing task status and workflow status events</p>\n</li>\n<li>\n<p><strong>EventExecuteService</strong> handles all state change events of the workflow instance that the master is responsible\nfor, and uses the thread pool to process the state events of the workflow</p>\n</li>\n<li>\n<p><strong>StateWheelExecuteThread</strong> handles timing state updates of dependent tasks and timeout tasks</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>WorkerServer</strong></p>\n<pre><code>WorkerServer also adopts a distributed centerless design concept, supports custom task plug-ins, and is mainly responsible for task execution and log services.\nWhen the WorkerServer service starts, it registers a temporary node with Zookeeper and maintains a heartbeat.\n</code></pre>\n</li>\n</ul>\n<h5>The service mainly includes</h5>\n<pre><code>- **WorkerManagerThread** mainly receives tasks sent by the master through netty, and calls **TaskExecuteThread** corresponding executors according to different task types.\n \n- **RetryReportTaskStatusThread** mainly reports the task status to the master through netty. If the report fails, the report will always be retried.\n\n- **LoggerServer** is a log service that provides log fragment viewing, refreshing and downloading functions\n</code></pre>\n<ul>\n<li>\n<p><strong>Registry</strong></p>\n<p>The registry is implemented as a plug-in, and Zookeeper is supported by default. The MasterServer and WorkerServer\nnodes in the system use the registry for cluster management and fault tolerance. In addition, the system also performs\nevent monitoring and distributed locks based on the registry.</p>\n</li>\n<li>\n<p><strong>Alert</strong></p>\n<p>Provide alarm-related functions and only support stand-alone service. Support custom alarm plug-ins.</p>\n</li>\n<li>\n<p><strong>API</strong></p>\n<p>The API interface layer is mainly responsible for processing requests from the front-end UI layer. The service\nuniformly provides RESTful APIs to provide request services to the outside world. Interfaces include workflow\ncreation, definition, query, modification, release, logoff, manual start, stop, pause, resume, start execution from\nthe node and so on.</p>\n</li>\n<li>\n<p><strong>UI</strong></p>\n<p>The front-end page of the system provides various visual operation interfaces of the system,See more\nat <a href=\"../guide/homepage.md\">Introduction to Functions</a> section。</p>\n</li>\n</ul>\n<h4>1.4 Architecture design ideas</h4>\n<h5>One、Decentralization VS centralization</h5>\n<h6>Centralized thinking</h6>\n<p>The centralized design concept is relatively simple. The nodes in the distributed cluster are divided into roles\naccording to roles, which are roughly divided into two roles:</p>\n<p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/master_slave.png\" alt=\"master-slave character\"  width=\"50%\" />\n </p>\n<ul>\n<li>The role of the master is mainly responsible for task distribution and monitoring the health status of the slave, and\ncan dynamically balance the task to the slave, so that the slave node will not be in a &quot;busy dead&quot; or &quot;idle dead&quot;\nstate.</li>\n<li>The role of Worker is mainly responsible for task execution and maintenance and Master's heartbeat, so that Master can\nassign tasks to Slave.</li>\n</ul>\n<p>Problems in centralized thought design:</p>\n<ul>\n<li>Once there is a problem with the Master, the dragons are headless and the entire cluster will collapse. In order to\nsolve this problem, most of the Master/Slave architecture models adopt the design scheme of active and standby Master,\nwhich can be hot standby or cold standby, or automatic switching or manual switching, and more and more new systems\nare beginning to have The ability to automatically elect and switch Master to improve the availability of the system.</li>\n<li>Another problem is that if the Scheduler is on the Master, although it can support different tasks in a DAG running on\ndifferent machines, it will cause the Master to be overloaded. If the Scheduler is on the slave, all tasks in a DAG\ncan only submit jobs on a certain machine. When there are more parallel tasks, the pressure on the slave may be\ngreater.</li>\n</ul>\n<h6>Decentralized</h6>\n <p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/decentralization.png\" alt=\"Decentralization\"  width=\"50%\" />\n </p>\n<ul>\n<li>\n<p>In the decentralized design, there is usually no concept of Master/Slave, all roles are the same, the status is equal,\nthe global Internet is a typical decentralized distributed system, any node equipment connected to the network is\ndown, All will only affect a small range of functions.</p>\n</li>\n<li>\n<p>The core design of decentralized design is that there is no &quot;manager&quot; different from other nodes in the entire\ndistributed system, so there is no single point of failure. However, because there is no &quot;manager&quot; node, each node\nneeds to communicate with other nodes to obtain the necessary machine information, and the unreliability of\ndistributed system communication greatly increases the difficulty of implementing the above functions.</p>\n</li>\n<li>\n<p>In fact, truly decentralized distributed systems are rare. Instead, dynamic centralized distributed systems are\nconstantly pouring out. Under this architecture, the managers in the cluster are dynamically selected, rather than\npreset, and when the cluster fails, the nodes of the cluster will automatically hold &quot;meetings&quot; to elect new &quot;\nmanagers&quot; To preside over the work. The most typical case is Etcd implemented by ZooKeeper and Go language.</p>\n</li>\n<li>\n<p>The decentralization of DolphinScheduler is that the Master/Worker is registered in Zookeeper to realize the\nnon-centralization of the Master cluster and the Worker cluster. The sharding mechanism is used to fairly distribute\nthe workflow for execution on the master, and tasks are sent to the workers for execution through different sending\nstrategies. Specific task</p>\n</li>\n</ul>\n<h5>Second, the master execution process</h5>\n<ol>\n<li>\n<p>DolphinScheduler uses the sharding algorithm to modulate the command and assigns it according to the sort id of the\nmaster. The master converts the received command into a workflow instance, and uses the thread pool to process the\nworkflow instance</p>\n</li>\n<li>\n<p>DolphinScheduler's process of workflow:</p>\n</li>\n</ol>\n<ul>\n<li>Start the workflow through UI or API calls, and persist a command to the database</li>\n<li>The Master scans the Command table through the sharding algorithm, generates a workflow instance ProcessInstance, and\ndeletes the Command data at the same time</li>\n<li>The Master uses the thread pool to run WorkflowExecuteThread to execute the process of the workflow instance,\nincluding building DAG, creating task instance TaskInstance, and sending TaskInstance to worker through netty</li>\n<li>After the worker receives the task, it modifies the task status and returns the execution information to the Master</li>\n<li>The Master receives the task information, persists it to the database, and stores the state change event in the\nEventExecuteService event queue</li>\n<li>EventExecuteService calls WorkflowExecuteThread according to the event queue to submit subsequent tasks and modify\nworkflow status</li>\n</ul>\n<h5>Three、Insufficient thread loop waiting problem</h5>\n<ul>\n<li>If there is no sub-process in a DAG, if the number of data in the Command is greater than the threshold set by the\nthread pool, the process directly waits or fails.</li>\n<li>If many sub-processes are nested in a large DAG, the following figure will produce a &quot;dead&quot; state:</li>\n</ul>\n <p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/lack_thread.png\" alt=\"Insufficient threads waiting loop problem\"  width=\"50%\" />\n </p>\nIn the above figure, MainFlowThread waits for the end of SubFlowThread1, SubFlowThread1 waits for the end of SubFlowThread2, SubFlowThread2 waits for the end of SubFlowThread3, and SubFlowThread3 waits for a new thread in the thread pool, then the entire DAG process cannot end, so that the threads cannot be released. In this way, the state of the child-parent process loop waiting is formed. At this time, unless a new Master is started to add threads to break such a \"stalemate\", the scheduling cluster will no longer be used.\n<p>It seems a bit unsatisfactory to start a new Master to break the deadlock, so we proposed the following three solutions\nto reduce this risk:</p>\n<ol>\n<li>Calculate the sum of all Master threads, and then calculate the number of threads required for each DAG, that is,\npre-calculate before the DAG process is executed. Because it is a multi-master thread pool, the total number of\nthreads is unlikely to be obtained in real time.</li>\n<li>Judge the single-master thread pool. If the thread pool is full, let the thread fail directly.</li>\n<li>Add a Command type with insufficient resources. If the thread pool is insufficient, suspend the main process. In this\nway, there are new threads in the thread pool, which can make the process suspended by insufficient resources wake up\nto execute again.</li>\n</ol>\n<p>note: The Master Scheduler thread is executed by FIFO when acquiring the Command.</p>\n<p>So we chose the third way to solve the problem of insufficient threads.</p>\n<h5>Four、Fault-tolerant design</h5>\n<p>Fault tolerance is divided into service downtime fault tolerance and task retry, and service downtime fault tolerance is\ndivided into master fault tolerance and worker fault tolerance.</p>\n<h6>1. Downtime fault tolerance</h6>\n<p>The service fault-tolerance design relies on ZooKeeper's Watcher mechanism, and the implementation principle is shown in\nthe figure:</p>\n <p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/fault-tolerant.png\" alt=\"DolphinScheduler fault-tolerant design\"  width=\"40%\" />\n </p>\nAmong them, the Master monitors the directories of other Masters and Workers. If the remove event is heard, fault tolerance of the process instance or task instance will be performed according to the specific business logic.\n<ul>\n<li>Master fault tolerance flowchart：</li>\n</ul>\n <p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/fault-tolerant_master.png\" alt=\"Master fault tolerance flowchart\"  width=\"40%\" />\n </p>\nAfter the fault tolerance of ZooKeeper Master is completed, it is re-scheduled by the Scheduler thread in DolphinScheduler, traverses the DAG to find the \"running\" and \"submit successful\" tasks, monitors the status of its task instances for the \"running\" tasks, and \"commits successful\" tasks It is necessary to determine whether the task queue already exists. If it exists, the status of the task instance is also monitored. If it does not exist, resubmit the task instance.\n<ul>\n<li>Worker fault tolerance flowchart：</li>\n</ul>\n <p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/fault-tolerant_worker.png\" alt=\"Worker fault tolerance flow chart\"  width=\"40%\" />\n </p>\n<p>Once the Master Scheduler thread finds that the task instance is in the &quot;fault-tolerant&quot; state, it takes over the task\nand resubmits it.</p>\n<p>Note: Due to &quot;network jitter&quot;, the node may lose its heartbeat with ZooKeeper in a short period of time, and the node's\nremove event may occur. For this situation, we use the simplest way, that is, once the node and ZooKeeper timeout\nconnection occurs, then directly stop the Master or Worker service.</p>\n<h6>2.Task failed and try again</h6>\n<p>Here we must first distinguish the concepts of task failure retry, process failure recovery, and process failure rerun:</p>\n<ul>\n<li>Task failure retry is at the task level and is automatically performed by the scheduling system. For example, if a\nShell task is set to retry for 3 times, it will try to run it again up to 3 times after the Shell task fails.</li>\n<li>Process failure recovery is at the process level and is performed manually. Recovery can only be performed <strong>from the\nfailed node</strong> or <strong>from the current node</strong></li>\n<li>Process failure rerun is also at the process level and is performed manually, rerun is performed from the start node</li>\n</ul>\n<p>Next to the topic, we divide the task nodes in the workflow into two types.</p>\n<ul>\n<li>\n<p>One is a business node, which corresponds to an actual script or processing statement, such as Shell node, MR node,\nSpark node, and dependent node.</p>\n</li>\n<li>\n<p>There is also a logical node, which does not do actual script or statement processing, but only logical processing of\nthe entire process flow, such as sub-process sections.</p>\n</li>\n</ul>\n<p>Each <strong>business node</strong> can be configured with the number of failed retries. When the task node fails, it will\nautomatically retry until it succeeds or exceeds the configured number of retries. <strong>Logical node</strong> Failure retry is not\nsupported. But the tasks in the logical node support retry.</p>\n<p>If there is a task failure in the workflow that reaches the maximum number of retries, the workflow will fail to stop,\nand the failed workflow can be manually rerun or process recovery operation</p>\n<h5>Five、Task priority design</h5>\n<p>In the early scheduling design, if there is no priority design and the fair scheduling design is used, the task\nsubmitted first may be completed at the same time as the task submitted later, and the process or task priority cannot\nbe set, so We have redesigned this, and our current design is as follows:</p>\n<ul>\n<li>According to <strong>priority of different process instances</strong> priority over <strong>priority of the same process instance</strong>\npriority over <strong>priority of tasks within the same process</strong>priority over <strong>tasks within the same process</strong>submission\norder from high to Low task processing.\n<ul>\n<li>\n<p>The specific implementation is to parse the priority according to the JSON of the task instance, and then save\nthe <strong>process instance priority_process instance id_task priority_task id</strong> information in the ZooKeeper task\nqueue, when obtained from the task queue, pass String comparison can get the tasks that need to be executed first</p>\n<ul>\n<li>\n<p>The priority of the process definition is to consider that some processes need to be processed before other\nprocesses. This can be configured when the process is started or scheduled to start. There are 5 levels in\ntotal, which are HIGHEST, HIGH, MEDIUM, LOW, and LOWEST. As shown below</p>\n  <p align=\"center\">\n     <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/process_priority.png\" alt=\"Process priority configuration\"  width=\"40%\" />\n   </p>\n</li>\n<li>\n<p>The priority of the task is also divided into 5 levels, followed by HIGHEST, HIGH, MEDIUM, LOW, LOWEST. As\nshown below</p>\n  <p align=\"center\">\n     <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/task_priority.png\" alt=\"Task priority configuration\"  width=\"35%\" />\n   </p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5>Six、Logback and netty implement log access</h5>\n<ul>\n<li>\n<p>Since Web (UI) and Worker are not necessarily on the same machine, viewing the log cannot be like querying a local\nfile. There are two options:</p>\n</li>\n<li>\n<p>Put logs on the ES search engine</p>\n</li>\n<li>\n<p>Obtain remote log information through netty communication</p>\n</li>\n<li>\n<p>In consideration of the lightness of DolphinScheduler as much as possible, so I chose gRPC to achieve remote access to\nlog information.</p>\n</li>\n</ul>\n <p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs_cn/images/grpc.png\" alt=\"grpc remote access\"  width=\"50%\" />\n </p>\n<ul>\n<li>We use the FileAppender and Filter functions of the custom Logback to realize that each task instance generates a log\nfile.</li>\n<li>FileAppender is mainly implemented as follows：</li>\n</ul>\n<pre><code class=\"language-java\"><span class=\"hljs-comment\">/**\n * task log appender\n */</span>\n<span class=\"hljs-keyword\">public</span> <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">TaskLogAppender</span> <span class=\"hljs-keyword\">extends</span> <span class=\"hljs-title\">FileAppender</span>&lt;<span class=\"hljs-title\">ILoggingEvent</span>&gt; </span>{\n\n    ...\n\n   <span class=\"hljs-meta\">@Override</span>\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">protected</span> <span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">append</span><span class=\"hljs-params\">(ILoggingEvent event)</span> </span>{\n\n       <span class=\"hljs-keyword\">if</span> (currentlyActiveFile == <span class=\"hljs-keyword\">null</span>){\n           currentlyActiveFile = getFile();\n       }\n       String activeFile = currentlyActiveFile;\n       <span class=\"hljs-comment\">// thread name： taskThreadName-processDefineId_processInstanceId_taskInstanceId</span>\n       String threadName = event.getThreadName();\n       String[] threadNameArr = threadName.split(<span class=\"hljs-string\">&quot;-&quot;</span>);\n       <span class=\"hljs-comment\">// logId = processDefineId_processInstanceId_taskInstanceId</span>\n       String logId = threadNameArr[<span class=\"hljs-number\">1</span>];\n       ...\n       <span class=\"hljs-keyword\">super</span>.subAppend(event);\n   }\n}\n\n\nGenerate logs in the form of /process definition id/process instance id/task instance id.log\n\n- Filter to match the thread name starting with TaskLogInfo:\n\n- TaskLogFilter is implemented as follows：\n\n```java\n<span class=\"hljs-comment\">/**\n*  task log filter\n*/</span>\n<span class=\"hljs-keyword\">public</span> <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">TaskLogFilter</span> <span class=\"hljs-keyword\">extends</span> <span class=\"hljs-title\">Filter</span>&lt;<span class=\"hljs-title\">ILoggingEvent</span>&gt; </span>{\n\n   <span class=\"hljs-meta\">@Override</span>\n   <span class=\"hljs-function\"><span class=\"hljs-keyword\">public</span> FilterReply <span class=\"hljs-title\">decide</span><span class=\"hljs-params\">(ILoggingEvent event)</span> </span>{\n       <span class=\"hljs-keyword\">if</span> (event.getThreadName().startsWith(<span class=\"hljs-string\">&quot;TaskLogInfo-&quot;</span>)){\n           <span class=\"hljs-keyword\">return</span> FilterReply.ACCEPT;\n       }\n       <span class=\"hljs-keyword\">return</span> FilterReply.DENY;\n   }\n}\n\n</code></pre>\n",
  "link": "/dist/en-us/docs/2.0.0/user_doc/architecture/design.html",
  "meta": {}
}