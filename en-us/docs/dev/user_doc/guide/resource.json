{
  "filename": "resource.md",
  "__html": "<h1>Resource Center</h1>\n<p>If you want to use the resource upload function, you can appoint the local file directory as the upload directory for a single machine (this operation does not need to deploy Hadoop). Or you can also upload to a Hadoop or MinIO cluster, at this time, you need to have Hadoop (2.6+) or MinIO or other related environments.</p>\n<blockquote>\n<p><strong><em>Note:</em></strong></p>\n<ul>\n<li>If you want to use the resource upload function, the deployment user in <a href=\"installation/standalone.md\">installation and deployment</a> must have relevant operation authority.</li>\n<li>If you using a Hadoop cluster with HA, you need to enable HDFS resource upload, and you need to copy the <code>core-site.xml</code> and <code>hdfs-site.xml</code> under the Hadoop cluster to <code>/opt/dolphinscheduler/conf</code>, otherwise skip this copy step.</li>\n</ul>\n</blockquote>\n<h2>HDFS Resource Configuration</h2>\n<ul>\n<li>Upload resource files and UDF functions, all uploaded files and resources will be stored on HDFS, so require the following configurations:</li>\n</ul>\n<pre><code>conf/common.properties  \n    # Users who have permission to create directories under the HDFS root path\n    hdfs.root.user=hdfs\n    # data base dir, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have read write permissions。&quot;/dolphinscheduler&quot; is recommended\n    resource.upload.path=/dolphinscheduler\n    # resource storage type : HDFS,S3,NONE\n    resource.storage.type=HDFS\n    # whether kerberos starts\n    hadoop.security.authentication.startup.state=false\n    # java.security.krb5.conf path\n    java.security.krb5.conf.path=/opt/krb5.conf\n    # loginUserFromKeytab user\n    login.user.keytab.username=hdfs-mycluster@ESZ.COM\n    # loginUserFromKeytab path\n    login.user.keytab.path=/opt/hdfs.headless.keytab    \n    # if resource.storage.type is HDFS，and your Hadoop Cluster NameNode has HA enabled, you need to put core-site.xml and hdfs-site.xml in the installPath/conf directory. In this example, it is placed under /opt/soft/dolphinscheduler/conf, and configure the namenode cluster name; if the NameNode is not HA, modify it to a specific IP or host name.\n    # if resource.storage.type is S3，write S3 address，HA，for example ：s3a://dolphinscheduler，\n    # Note，s3 be sure to create the root directory /dolphinscheduler\n    fs.defaultFS=hdfs://mycluster:8020    \n    #resourcemanager ha note this need ips , this empty if single\n    yarn.resourcemanager.ha.rm.ids=192.168.xx.xx,192.168.xx.xx    \n    # If it is a single resourcemanager, you only need to configure one host name. If it is resourcemanager HA, the default configuration is fine\n    yarn.application.status.address=http://xxxx:8088/ws/v1/cluster/apps/%s\n\n</code></pre>\n<h2>File Management</h2>\n<blockquote>\n<p>It is the management of various resource files, including creating basic <code>txt/log/sh/conf/py/java</code> and jar packages and other type files, and can do edit, rename, download, delete and other operations to the files.</p>\n</blockquote>\n<p><img src=\"/img/new_ui/dev/resource/file-manage.png\" alt=\"file-manage\"></p>\n<ul>\n<li>Create a file\n<blockquote>\n<p>The file format supports the following types: txt, log, sh, conf, cfg, py, java, sql, xml, hql, properties.</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"/img/new_ui/dev/resource/create-file.png\" alt=\"create-file\"></p>\n<ul>\n<li>upload files</li>\n</ul>\n<blockquote>\n<p>Upload file: Click the &quot;Upload File&quot; button to upload, drag the file to the upload area, the file name will be automatically completed with the uploaded file name.</p>\n</blockquote>\n<p><img src=\"/img/new_ui/dev/resource/upload-file.png\" alt=\"upload-file\"></p>\n<ul>\n<li>File View</li>\n</ul>\n<blockquote>\n<p>For the files that can be viewed, click the file name to view the file details.</p>\n</blockquote>\n<p align=\"center\">\n   <img src=\"/img/file_detail_en.png\" width=\"80%\" />\n </p>\n<ul>\n<li>Download file</li>\n</ul>\n<blockquote>\n<p>Click the &quot;Download&quot; button in the file list to download the file or click the &quot;Download&quot; button in the upper right corner of the file details to download the file.</p>\n</blockquote>\n<ul>\n<li>File rename</li>\n</ul>\n<p><img src=\"/img/new_ui/dev/resource/rename-file.png\" alt=\"rename-file\"></p>\n<ul>\n<li>\n<p>delete</p>\n<blockquote>\n<p>File list -&gt; Click the &quot;Delete&quot; button to delete the specified file.</p>\n</blockquote>\n</li>\n<li>\n<p>Re-upload file</p>\n<blockquote>\n<p>Re-upload file: Click the &quot;Re-upload File&quot; button to upload a new file to replace the old file, drag the file to the re-upload area, the file name will be automatically completed with the new uploaded file name.</p>\n</blockquote>\n  <p align=\"center\">\n    <img src=\"/img/reupload_file_en.png\" width=\"80%\" />\n  </p>\n</li>\n</ul>\n<h2>UDF Management</h2>\n<h3>Resource Management</h3>\n<blockquote>\n<p>The resource management and file management functions are similar. The difference is that the resource management is the UDF upload function, and the file management uploads the user programs, scripts and configuration files.\nOperation function: rename, download, delete.</p>\n</blockquote>\n<ul>\n<li>Upload UDF resources\n<blockquote>\n<p>Same as uploading files.</p>\n</blockquote>\n</li>\n</ul>\n<h3>Function Management</h3>\n<ul>\n<li>Create UDF function\n<blockquote>\n<p>Click &quot;Create UDF Function&quot;, enter the UDF function parameters, select the UDF resource, and click &quot;Submit&quot; to create the UDF function.</p>\n</blockquote>\n</li>\n</ul>\n<blockquote>\n<p>Currently, only supports temporary UDF functions of Hive.</p>\n</blockquote>\n<ul>\n<li>UDF function name: enter the name of the UDF function.</li>\n<li>Package name Class name: enter the full path of the UDF function.</li>\n<li>UDF resource: set the resource file corresponding to the created UDF function.</li>\n</ul>\n<p><img src=\"/img/new_ui/dev/resource/create-udf.png\" alt=\"create-udf\"></p>\n<h2>Task Group Settings</h2>\n<p>The task group is mainly used to control the concurrency of task instances and is designed to control the pressure of other resources (it can also control the pressure of the Hadoop cluster, the cluster will have queue control it). When creating a new task definition, you can configure the corresponding task group and configure the priority of the task running in the task group.</p>\n<h3>Task Group Configuration</h3>\n<h4>Create Task Group</h4>\n<p><img src=\"/img/new_ui/dev/resource/create-taskGroup.png\" alt=\"create-taskGroup\"></p>\n<p>The user clicks [Resources] - [Task Group Management] - [Task Group option] - [Create Task Group]</p>\n<p><img src=\"/img/new_ui/dev/resource/create-taskGroup.png\" alt=\"create-taskGroup\"></p>\n<p>You need to enter the information inside the picture:</p>\n<ul>\n<li>\n<p>Task group name: the name displayed of the task group</p>\n</li>\n<li>\n<p>Project name: the project range that the task group functions, this item is optional, if not selected, all the projects in the whole system can use this task group.</p>\n</li>\n<li>\n<p>Resource pool size: The maximum number of concurrent task instances allowed.</p>\n</li>\n</ul>\n<h4>View Task Group Queue</h4>\n<p><img src=\"/img/new_ui/dev/resource/view-queue.png\" alt=\"view-queue\"></p>\n<p>Click the button to view task group usage information:</p>\n<p><img src=\"/img/new_ui/dev/resource/view-groupQueue.png\" alt=\"view-queue\"></p>\n<h4>Use of Task Groups</h4>\n<p><strong>Note</strong>: The usage of task groups is applicable to tasks executed by workers, such as [switch] nodes, [condition] nodes, [sub_process] and other node types executed by the master are not controlled by the task group. Let's take the shell node as an example:</p>\n<p><img src=\"/img/new_ui/dev/resource/use-queue.png\" alt=\"use-queue\"></p>\n<p>Regarding the configuration of the task group, all you need to do is to configure these parts in the red box:</p>\n<ul>\n<li>\n<p>Task group name: The task group name is displayed on the task group configuration page. Here you can only see the task group that the project has permission to access (the project is selected when creating a task group) or the task group that scope globally (no project is selected when creating a task group).</p>\n</li>\n<li>\n<p>Priority: When there is a waiting resource, the task with high priority will be distributed to the worker by the master first. The larger the value of this part, the higher the priority.</p>\n</li>\n</ul>\n<h3>Implementation Logic of Task Group</h3>\n<h4>Get Task Group Resources:</h4>\n<p>The master judges whether the task is configured with a task group when distributing the task. If the task is not configured, it is normally thrown to the worker to run; if a task group is configured, it checks whether the remaining size of the task group resource pool meets the current task operation before throwing it to the worker for execution. , if the resource pool -1 is satisfied, continue to run; if not, exit the task distribution and wait for other tasks to wake up.</p>\n<h4>Release and Wake Up:</h4>\n<p>When the task that has occupied the task group resource is finished, the task group resource will be released. After the release, it will check whether there is a task waiting in the current task group. If there is, mark the task with the best priority to run, and create a new executable event. The event stores the task ID that is marked to acquire the resource, and then the task obtains the task group resource and run.</p>\n<h4>Task Group Flowchart</h4>\n<p align=\"center\">\n    <img src=\"/img/task_group_process.png\" width=\"80%\" />\n</p>        \n",
  "link": "/dist/en-us/docs/dev/user_doc/guide/resource.html",
  "meta": {}
}