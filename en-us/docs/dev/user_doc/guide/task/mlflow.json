{
  "filename": "mlflow.md",
  "__html": "<h1>MLflow Node</h1>\n<h2>Overview</h2>\n<p><a href=\"https://mlflow.org\">MLflow</a> is an excellent open source platform to manage the ML lifecycle, including experimentation,\nreproducibility, deployment, and a central model registry.</p>\n<p>MLflow task plugin used to execute MLflow tasks，Currently contains MLflow Projects and MLflow Models. (Model Registry will soon be rewarded for support)</p>\n<ul>\n<li>MLflow Projects: Package data science code in a format to reproduce runs on any platform.</li>\n<li>MLflow Models: Deploy machine learning models in diverse serving environments.</li>\n<li>Model Registry: Store, annotate, discover, and manage models in a central repository.</li>\n</ul>\n<p>The MLflow plugin currently supports and will support the following:</p>\n<ul>\n<li>[x] MLflow Projects\n<ul>\n<li>[x] BasicAlgorithm: contains LogisticRegression, svm, lightgbm, xgboost</li>\n<li>[x] AutoML: AutoML tool，contains autosklean, flaml</li>\n<li>[x] Custom projects: Support for running your own MLflow projects</li>\n</ul>\n</li>\n<li>[ ] MLflow Models\n<ul>\n<li>[x] MLFLOW: Use <code>MLflow models serve</code> to deploy a model service</li>\n<li>[x] Docker: Run the container after packaging the docker image</li>\n<li>[x] Docker Compose: Use docker compose to run the container, it will replace the docker run above</li>\n<li>[ ] Seldon core: Use Selcon core to deploy model to k8s cluster</li>\n<li>[ ] k8s: Deploy containers directly to K8S</li>\n<li>[ ] MLflow deployments: Built-in deployment modules, such as built-in deployment to SageMaker, etc</li>\n</ul>\n</li>\n<li>[ ] Model Registry\n<ul>\n<li>[ ] Register Model: Allows artifacts (Including model and related parameters, indicators) to be registered directly into the model center</li>\n</ul>\n</li>\n</ul>\n<h2>Create Task</h2>\n<ul>\n<li>Click <code>Project -&gt; Management-Project -&gt; Name-Workflow Definition</code>, and click the &quot;Create Workflow&quot; button to enter the\nDAG editing page.</li>\n<li>Drag from the toolbar <img src=\"/img/tasks/icons/mlflow.png\" width=\"15\"/> task node to canvas.</li>\n</ul>\n<h2>Task Example</h2>\n<p>First, introduce some general parameters of DolphinScheduler:</p>\n<ul>\n<li><strong>Node name</strong>: The node name in a workflow definition is unique.</li>\n<li><strong>Run flag</strong>: Identifies whether this node schedules normally, if it does not need to execute, select\nthe <code>prohibition execution</code>.</li>\n<li><strong>Descriptive information</strong>: Describe the function of the node.</li>\n<li><strong>Task priority</strong>: When the number of worker threads is insufficient, execute in the order of priority from high\nto low, and tasks with the same priority will execute in a first-in first-out order.</li>\n<li><strong>Worker grouping</strong>: Assign tasks to the machines of the worker group to execute. If <code>Default</code> is selected,\nrandomly select a worker machine for execution.</li>\n<li><strong>Environment Name</strong>: Configure the environment name in which run the script.</li>\n<li><strong>Times of failed retry attempts</strong>: The number of times the task failed to resubmit.</li>\n<li><strong>Failed retry interval</strong>: The time interval (unit minute) for resubmitting the task after a failed task.</li>\n<li><strong>Delayed execution time</strong>: The time (unit minute) that a task delays in execution.</li>\n<li><strong>Timeout alarm</strong>: Check the timeout alarm and timeout failure. When the task runs exceed the &quot;timeout&quot;, an alarm\nemail will send and the task execution will fail.</li>\n<li><strong>Predecessor task</strong>: Selecting a predecessor task for the current task, will set the selected predecessor task as\nupstream of the current task.</li>\n</ul>\n<p>Here are some specific parameters for the MLFlow component:</p>\n<ul>\n<li><strong>MLflow Tracking Server URI</strong>: MLflow Tracking Server URI, default <a href=\"http://localhost:5000\">http://localhost:5000</a>.</li>\n<li><strong>Experiment Name</strong>: Create the experiment where the task is running, if the experiment does not exist. If the name is empty, it is set to <code>Default</code>, the same as MLflow.</li>\n</ul>\n<h3>MLflow Projects</h3>\n<h4>BasicAlgorithm</h4>\n<p><img src=\"/img/tasks/demo/mlflow-basic-algorithm.png\" alt=\"mlflow-conda-env\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>Register Model</strong>: Register the model or not. If register is selected, the following parameters are expanded.\n<ul>\n<li><strong>Model Name</strong>: The registered model name is added to the original model version and registered as\nProduction.</li>\n</ul>\n</li>\n<li><strong>Data Path</strong>: The absolute path of the file or folder. Ends with .csv for file or contain train.csv and\ntest.csv for folder（In the suggested way, users should build their own test sets for model evaluation）.</li>\n<li><strong>Parameters</strong>: Parameter when initializing the algorithm/AutoML model, which can be empty. For example\nparameters <code>&quot;time_budget=30;estimator_list=['lgbm']&quot;</code> for flaml 。The convention will be passed with '; ' shards\neach parameter, using the name before the equal sign as the parameter name, and using the name after the equal\nsign to get the corresponding parameter value through <code>python eval()</code>.\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\">Logistic Regression</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC\">SVM</a></li>\n<li><a href=\"https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier\">lightgbm</a></li>\n<li><a href=\"https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\">xgboost</a></li>\n</ul>\n</li>\n<li><strong>Algorithm</strong>：The selected algorithm currently supports <code>LR</code>, <code>SVM</code>, <code>LightGBM</code> and <code>XGboost</code> based\non <a href=\"https://scikit-learn.org/\">scikit-learn</a> form.</li>\n<li><strong>Parameter Search Space</strong>: Parameter search space when running the corresponding algorithm, which can be\nempty. For example, the parameter <code>max_depth=[5, 10];n_estimators=[100, 200]</code> for lightgbm 。The convention\nwill be passed with '; 'shards each parameter, using the name before the equal sign as the parameter name,\nand using the name after the equal sign to get the corresponding parameter value through <code>python eval()</code>.</li>\n</ul>\n<h4>AutoML</h4>\n<p><img src=\"/img/tasks/demo/mlflow-automl.png\" alt=\"mlflow-automl\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>Register Model</strong>: Register the model or not. If register is selected, the following parameters are expanded.\n<ul>\n<li><strong>model name</strong>: The registered model name is added to the original model version and registered as\nProduction.</li>\n</ul>\n</li>\n<li><strong>Data Path</strong>: The absolute path of the file or folder. Ends with .csv for file or contain train.csv and\ntest.csv for folder(In the suggested way, users should build their own test sets for model evaluation).</li>\n<li><strong>Parameters</strong>: Parameter when initializing the algorithm/AutoML model, which can be empty. For example\nparameters <code>n_estimators=200;learning_rate=0.2</code> for flaml. The convention will be passed with '; 'shards\neach parameter, using the name before the equal sign as the parameter name, and using the name after the equal\nsign to get the corresponding parameter value through <code>python eval()</code>. The detailed parameter list is as follows:\n<ul>\n<li><a href=\"https://microsoft.github.io/FLAML/docs/reference/automl#automl-objects\">flaml</a></li>\n<li><a href=\"https://automl.github.io/auto-sklearn/master/api.html\">autosklearn</a></li>\n</ul>\n</li>\n<li><strong>AutoML tool</strong>: The AutoML tool used, currently\nsupports <a href=\"https://github.com/automl/auto-sklearn\">autosklearn</a>\nand <a href=\"https://github.com/microsoft/FLAML\">flaml</a>.</li>\n</ul>\n<h4>Custom projects</h4>\n<p><img src=\"/img/tasks/demo/mlflow-custom-project.png\" alt=\"mlflow-custom-project.png\"></p>\n<p><strong>Task Parameter</strong></p>\n<ul>\n<li><strong>parameters</strong>: <code>--param-list</code> in <code>mlflow run</code>. For example <code>-P learning_rate=0.2 -P colsample_bytree=0.8 -P subsample=0.9</code>.</li>\n<li><strong>Repository</strong>: Repository url of MLflow Project，Support git address and directory on worker. If it's in a subdirectory，We add <code>#</code> to support this (same as <code>mlflow run</code>) , for example <code>https://github.com/mlflow/mlflow#examples/xgboost/xgboost_native</code>.</li>\n<li><strong>Project Version</strong>: Version of the project，default master.</li>\n</ul>\n<p>You can now use this feature to run all MLFlow projects on Github (For example <a href=\"https://github.com/mlflow/mlflow/tree/master/examples\">MLflow examples</a> ). You can also create your own machine learning library to reuse your work, and then use DolphinScheduler to use your library with one click.</p>\n<h3>MLflow Models</h3>\n<p>General Parameters:</p>\n<ul>\n<li><strong>Model-URI</strong>: Model-URI of MLflow , support <code>models:/&lt;model_name&gt;/suffix</code> format and <code>runs:/</code> format. See <a href=\"https://mlflow.org/docs/latest/tracking.html#artifact-stores\">https://mlflow.org/docs/latest/tracking.html#artifact-stores</a>.</li>\n<li><strong>Port</strong>: The port to listen on.</li>\n</ul>\n<h4>MLFLOW</h4>\n<p><img src=\"/img/tasks/demo/mlflow-models-mlflow.png\" alt=\"mlflow-models-mlflow\"></p>\n<h4>Docker</h4>\n<p><img src=\"/img/tasks/demo/mlflow-models-docker.png\" alt=\"mlflow-models-docker\"></p>\n<h4>DOCKER COMPOSE</h4>\n<p><img src=\"/img/tasks/demo/mlflow-models-docker-compose.png\" alt=\"mlflow-models-docker-compose\"></p>\n<ul>\n<li><strong>Max Cpu Limit</strong>: For example <code>1.0</code> or <code>0.5</code>, the same as docker compose.</li>\n<li><strong>Max Memory Limit</strong>: For example <code>1G</code> or <code>500M</code>, the same as docker compose.</li>\n</ul>\n<h2>Environment to prepare</h2>\n<h3>Conda env</h3>\n<p>You need to enter the admin account to configure a conda environment variable（Please\ninstall <a href=\"https://docs.continuum.io/anaconda/install/\">anaconda</a>\nor <a href=\"https://docs.conda.io/en/latest/miniconda.html#installing\">miniconda</a> in advance).</p>\n<p><img src=\"/img/tasks/demo/mlflow-conda-env.png\" alt=\"mlflow-conda-env\"></p>\n<p>Note During the configuration task, select the conda environment created above. Otherwise, the program cannot find the\nConda environment.</p>\n<p><img src=\"/img/tasks/demo/mlflow-set-conda-env.png\" alt=\"mlflow-set-conda-env\"></p>\n<h3>Start the mlflow service</h3>\n<p>Make sure you have installed MLflow, using 'pip install mlflow'.</p>\n<p>Create a folder where you want to save your experiments and models and start MLflow service.</p>\n<pre><code class=\"language-sh\">mkdir mlflow\n<span class=\"hljs-built_in\">cd</span> mlflow\nmlflow server -h 0.0.0.0 -p 5000 --serve-artifacts --backend-store-uri sqlite:///mlflow.db\n</code></pre>\n<p>After running, an MLflow service is started.</p>\n<p>After this, you can visit the MLflow service (<code>http://localhost:5000</code>) page to view the experiments and models.</p>\n<p><img src=\"/img/tasks/demo/mlflow-server.png\" alt=\"mlflow-server\"></p>\n",
  "link": "/dist/en-us/docs/dev/user_doc/guide/task/mlflow.html",
  "meta": {}
}