{
  "filename": "faq.md",
  "__html": "<h2>Q: DolphinScheduler service introduction and recommended running memory</h2>\n<p>A: DolphinScheduler consists of 5 services, MasterServer, WorkerServer, ApiServer, AlertServer, LoggerServer and UI.</p>\n<table>\n<thead>\n<tr>\n<th>Service</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MasterServer</td>\n<td>Mainly responsible for DAG segmentation and task status monitoring</td>\n</tr>\n<tr>\n<td>WorkerServer/LoggerServer</td>\n<td>Mainly responsible for the submission, execution and update of task status. LoggerServer is used for Rest Api to view logs through RPC</td>\n</tr>\n<tr>\n<td>ApiServer</td>\n<td>Provides the Rest Api service for the UI to call</td>\n</tr>\n<tr>\n<td>AlertServer</td>\n<td>Provide alarm service</td>\n</tr>\n<tr>\n<td>UI</td>\n<td>Front page display</td>\n</tr>\n</tbody>\n</table>\n<p>Note：<strong>Due to the large number of services, it is recommended that the single-machine deployment is preferably 4 cores and 16G or more.</strong></p>\n<hr>\n<h2>Q: Why can't an administrator create a project?</h2>\n<p>A: The administrator is currently &quot;<strong>pure management</strong>&quot;. There is no tenant, that is, there is no corresponding user on linux, so there is no execution permission, <strong>so there is no project, resource and data source,</strong> so there is no permission to create. <strong>But there are all viewing permissions</strong>. If you need to create a business operation such as a project, <strong>use the administrator to create a tenant and a normal user, and then use the normal user login to operate</strong>. We will release the administrator's creation and execution permissions in version 1.1.0, and the administrator will have all permissions.</p>\n<hr>\n<h2>Q: Which mailboxes does the system support?</h2>\n<p>A: Support most mailboxes, qq, 163, 126, 139, outlook, aliyun, etc. are supported. Support TLS and SSL protocols, optionally configured in alert.properties</p>\n<hr>\n<h2>Q: What are the common system variable time parameters and how do I use them?</h2>\n<p>A: Please refer to 'System parameter' in the system-manual</p>\n<hr>\n<h2>Q: pip install kazoo This installation gives an error. Is it necessary to install?</h2>\n<p>A: This is the python connection zookeeper needs to use, must be installed</p>\n<hr>\n<h2>Q: How to specify the machine running task</h2>\n<p>A: Use <strong>the administrator</strong> to create a Worker group, <strong>specify the Worker group</strong> when the <strong>process definition starts</strong>, or <strong>specify the Worker group on the task node</strong>. If not specified, use Default, <strong>Default is to select one of all the workers in the cluster to use for task submission and execution.</strong></p>\n<hr>\n<h2>Q: Priority of the task</h2>\n<p>A: We also support <strong>the priority of processes and tasks</strong>. Priority We have five levels of <strong>HIGHEST, HIGH, MEDIUM, LOW and LOWEST</strong>. <strong>You can set the priority between different process instances, or you can set the priority of different task instances in the same process instance.</strong> For details, please refer to the task priority design in the architecture-design.</p>\n<hr>\n<h2>Q: dolphinscheduler-grpc gives an error</h2>\n<p>A: Execute in the root directory: mvn -U clean package assembly:assembly -Dmaven.test.skip=true , then refresh the entire project</p>\n<hr>\n<h2>Q: Does DolphinScheduler support running on windows?</h2>\n<p>A: In theory, <strong>only the Worker needs to run on Linux</strong>. Other services can run normally on Windows. But it is still recommended to deploy on Linux.</p>\n<hr>\n<h2>Q: UI compiles node-sass prompt in linux: Error: EACCESS: permission denied, mkdir xxxx</h2>\n<p>A: Install <strong>npm install node-sass --unsafe-perm</strong> separately, then <strong>npm install</strong></p>\n<hr>\n<h2>Q: UI cannot log in normally.</h2>\n<p>A: 1, if it is node startup, check whether the .env API_BASE configuration under dolphinscheduler-ui is the Api Server service address.</p>\n<pre><code>2, If it is nginx booted and installed via **install-dolphinscheduler-ui.sh**, check if the proxy_pass configuration in **/etc/nginx/conf.d/dolphinscheduler.conf** is the Api Server service. address\n\n 3, if the above configuration is correct, then please check if the Api Server service is normal, curl http://192.168.xx.xx:12345/dolphinscheduler/users/get-user-info, check the Api Server log, if Prompt cn.dolphinscheduler.api.interceptor.LoginHandlerInterceptor:[76] - session info is null, which proves that the Api Server service is normal.\n\n4, if there is no problem above, you need to check if **server.context-path and server.port configuration** in **application.properties** is correct\n</code></pre>\n<hr>\n<h2>Q: After the process definition is manually started or scheduled, no process instance is generated.</h2>\n<p>A:   1, first <strong>check whether the MasterServer service exists through jps</strong>, or directly check whether there is a master service in zk from the service monitoring.</p>\n<p>​       2,If there is a master service, check <strong>the command status statistics</strong> or whether new records are added in <strong>t_ds_error_command</strong>. If it is added, <strong>please check the message field.</strong></p>\n<hr>\n<h2>Q : The task status is always in the successful submission status.</h2>\n<p>A:   1, <strong>first check whether the WorkerServer service exists through jps</strong>, or directly check whether there is a worker service in zk from the service monitoring.</p>\n<p>​       2,If the <strong>WorkerServer</strong> service is normal, you need to <strong>check whether the MasterServer puts the task task in the zk queue. You need to check whether the task is blocked in the MasterServer log and the zk queue.</strong></p>\n<p>​       3, if there is no problem above, you need to locate whether the Worker group is specified, but <strong>the machine grouped by the worker is not online</strong>.**</p>\n<hr>\n<h2>Q: Is there a Docker image and a Dockerfile?</h2>\n<p>A: Provide Docker image and Dockerfile.</p>\n<p>Docker image address: <a href=\"https://hub.docker.com/r/escheduler/escheduler_images\">https://hub.docker.com/r/escheduler/escheduler_images</a></p>\n<p>Dockerfile address: <a href=\"https://github.com/qiaozhanwei/escheduler_dockerfile/tree/master/docker_escheduler\">https://github.com/qiaozhanwei/escheduler_dockerfile/tree/master/docker_escheduler</a></p>\n<hr>\n<h2>Q : Need to pay attention to the problem in <a href=\"http://install.sh\">install.sh</a></h2>\n<p>A:   1, if the replacement variable contains special characters, <strong>use the \\ transfer character to transfer</strong></p>\n<p>​       2, installPath=&quot;/data1_1T/dolphinscheduler&quot;, <strong>this directory can not be the same as the <a href=\"http://install.sh\">install.sh</a> directory currently installed with one click.</strong></p>\n<p>​       3, deployUser = &quot;dolphinscheduler&quot;, <strong>the deployment user must have sudo privileges</strong>, because the worker is executed by sudo -u tenant sh xxx.command</p>\n<p>​       4, monitorServerState = &quot;false&quot;, whether the service monitoring script is started, the default is not to start the service monitoring script. <strong>If the service monitoring script is started, the master and worker services are monitored every 5 minutes, and if the machine is down, it will automatically restart.</strong></p>\n<p>​       5, hdfsStartupSate=&quot;false&quot;, whether to enable HDFS resource upload function. The default is not enabled. <strong>If it is not enabled, the resource center cannot be used.</strong> If enabled, you need to configure the configuration of fs.defaultFS and yarn in conf/common/hadoop/hadoop.properties. If you use namenode HA, you need to copy core-site.xml and hdfs-site.xml to the conf root directory.</p>\n<p>​    Note: <strong>The 1.0.x version does not automatically create the hdfs root directory, you need to create it yourself, and you need to deploy the user with hdfs operation permission.</strong></p>\n<hr>\n<h2>Q : Process definition and process instance offline exception</h2>\n<p>A : For <strong>versions prior to 1.0.4</strong>, modify the code under the escheduler-api cn.escheduler.api.quartz package.</p>\n<pre><code>public boolean deleteJob(String jobName, String jobGroupName) {\n    lock.writeLock().lock();\n    try {\n      JobKey jobKey = new JobKey(jobName,jobGroupName);\n      if(scheduler.checkExists(jobKey)){\n        logger.info(&quot;try to delete job, job name: {}, job group name: {},&quot;, jobName, jobGroupName);\n        return scheduler.deleteJob(jobKey);\n      }else {\n        return true;\n      }\n\n    } catch (SchedulerException e) {\n      logger.error(String.format(&quot;delete job : %s failed&quot;,jobName), e);\n    } finally {\n      lock.writeLock().unlock();\n    }\n    return false;\n  }\n</code></pre>\n<hr>\n<h2>Q: Can the tenant created before the HDFS startup use the resource center normally?</h2>\n<p>A: No. Because the tenant created by HDFS is not started, the tenant directory will not be registered in HDFS. So the last resource will report an error.</p>\n<h2>Q: In the multi-master and multi-worker state, the service is lost, how to be fault-tolerant</h2>\n<p>A: <strong>Note:</strong> <strong>Master monitors Master and Worker services.</strong></p>\n<p>​    1，If the Master service is lost, other Masters will take over the process of the hanged Master and continue to monitor the Worker task status.</p>\n<p>​    2，If the Worker service is lost, the Master will monitor that the Worker service is gone. If there is a Yarn task, the Kill Yarn task will be retried.</p>\n<p>Please see the fault-tolerant design in the architecture for details.</p>\n<hr>\n<h2>Q : Fault tolerance for a machine distributed by Master and Worker</h2>\n<p>A: The 1.0.3 version only implements the fault tolerance of the Master startup process, and does not take the Worker Fault Tolerance. That is to say, if the Worker hangs, no Master exists. There will be problems with this process. We will add Master and Worker startup fault tolerance in version <strong>1.1.0</strong> to fix this problem. If you want to manually modify this problem, you need to <strong>modify the running task for the running worker task that is running the process across the restart and has been dropped. The running process is set to the failed state across the restart</strong>. Then resume the process from the failed node.</p>\n<hr>\n<h2>Q : Timing is easy to set to execute every second</h2>\n<p>A : Note when setting the timing. If the first digit (* * * * * ? *) is set to *, it means execution every second. <strong>We will add a list of recently scheduled times in version 1.1.0.</strong> You can see the last 5 running times online at <a href=\"http://cron.qqe2.com/\">http://cron.qqe2.com/</a></p>\n<h2>Q: Is there a valid time range for timing?</h2>\n<p>A: Yes, <strong>if the timing start and end time is the same time, then this timing will be invalid timing. If the end time of the start and end time is smaller than the current time, it is very likely that the timing will be automatically deleted.</strong></p>\n<h2>Q : There are several implementations of task dependencies</h2>\n<p>A：\t1, the task dependency between <strong>DAG</strong>, is <strong>from the zero degree</strong> of the DAG segmentation</p>\n<p>​\t2, there are <strong>task dependent nodes</strong>, you can achieve cross-process tasks or process dependencies, please refer to the (DEPENDENT) node design in the system-manual.</p>\n<p>​\tNote: <strong>Cross-project processes or task dependencies are not supported</strong></p>\n<h2>Q: There are several ways to start the process definition.</h2>\n<p>A:   1, in <strong>the process definition list</strong>, click the <strong>Start</strong> button.</p>\n<p>​       2, <strong>the process definition list adds a timer</strong>, scheduling start process definition.</p>\n<p>​       3, process definition <strong>view or edit</strong> the DAG page, any <strong>task node right click</strong> Start process definition.</p>\n<p>​       4, you can define DAG editing for the process, set the running flag of some tasks to <strong>prohibit running</strong>, when the process definition is started, the connection of the node will be removed from the DAG.</p>\n<h2>Q : Python task setting Python version</h2>\n<p>A：  1，<strong>for the version after 1.0.3</strong> only need to modify PYTHON_HOME in conf/env/.dolphinscheduler_env.sh</p>\n<pre><code>export PYTHON_HOME=/bin/python\n</code></pre>\n<p>Note: This is <strong>PYTHON_HOME</strong> , which is the absolute path of the python command, not the simple PYTHON_HOME. Also note that when exporting the PATH, you need to directly</p>\n<pre><code>export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH\n</code></pre>\n<p>​\t2，For versions prior to 1.0.3, the Python task only supports the Python version of the system. It does not support specifying the Python version.</p>\n<h2>Q：Worker Task will generate a child process through sudo -u tenant sh xxx.command, will kill when kill</h2>\n<p>A：  We will add the kill task in 1.0.4 and kill all the various child processes generated by the task.</p>\n<h2>Q ： How to use the queue in DolphinScheduler, what does the user queue and tenant queue mean?</h2>\n<p>A ： The queue in the DolphinScheduler can be configured on the user or the tenant. <strong>The priority of the queue specified by the user is higher than the priority of the tenant queue.</strong> For example, to specify a queue for an MR task, the queue is specified by mapreduce.job.queuename.</p>\n<p>Note: When using the above method to specify the queue, the MR uses the following methods:</p>\n<pre><code>\tConfiguration conf = new Configuration();\n        GenericOptionsParser optionParser = new GenericOptionsParser(conf, args);\n        String[] remainingArgs = optionParser.getRemainingArgs();\n</code></pre>\n<p>If it is a Spark task --queue mode specifies the queue</p>\n<h2>Q : Master or Worker reports the following alarm</h2>\n<p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs/images/master_worker_lack_res.png\" width=\"60%\" />\n </p>\n<p>A ： Change the value of master.properties <strong>master.reserved.memory</strong> under conf to a smaller value, say 0.1 or the value of worker.properties <strong>worker.reserved.memory</strong> is a smaller value, say 0.1</p>\n<h2>Q: The hive version is 1.1.0+cdh5.15.0, and the SQL hive task connection is reported incorrectly.</h2>\n<p align=\"center\">\n   <img src=\"https://analysys.github.io/easyscheduler_docs/images/cdh_hive_error.png\" width=\"60%\" />\n </p>\n<p>A ： Will hive pom</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;\n    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;\n    &lt;version&gt;2.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>change into</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;\n    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;\n    &lt;version&gt;1.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n",
  "link": "/en-us/docs/faq.html",
  "meta": {}
}